---
title: "Forecasting and Prediction Models"
subtitle: "Further Statistical Methods, Quantitative Methods"
sidebar: side
---

This chapter covers methods for forecasting and prediction. We start with time-series models for forecasting future values. Then, we explore a series of machine learning models that help with prediction/classification tasks.

Use the right sidebar for navigation. R-code provided at the bottom.

------------------------------------------------------------------------

# **Time Series Models**

### Time Series and Stationary Processes

Time series are a set of data collected at successive time points $t \in \mathbb Z$. Each observation $X_t$ is at time point $t$. For example, a time series with $T$ number of time periods would be $(X_1, X_2, \dots, X_T)$. Examples of time series include monthly unemployment data, daily currency exchange rates, and monthly politician approval rates.

The goal of time series modelling is to predict $X_t$ with past data. There are several factors that might affect $X_t$:

1.  Previous values of $X_t$ in the time series.
2.  Long-term **trends**: these are long-term movements in the mean of a section of the time series. This can be an upward trend, a downward trend, or a stationary trend (no changes).
3.  **Seasonality**: cyclical repeating patterns in certain periods, for example, business cycles, or sales being higher during the holiday season.
4.  **Randomness**: other random/systematic fluctuations.

One important idea is a (weak) **stationary process**, where there is zero trend over time. A weak stationary process is when the expected value and variance of our time series is constant over different (but same-sized) groups of time periods: $\E X_t = \mu$, $\V X_t = \sigma^2$.

Most time-series data will not be weak stationary. However, we can transform most time-series data into weak stationary processes through **differencing**. We refer to the new differenced time series as $X'_t$:

-   Zero differencing $d=0$ is no differencing: $X_t' = X_t$. We just use our original time-series.
-   First order differencing $d = 1$ is $X_t' = X_t - X_{t-1}$. Basically we take our $X_t$ at time period $t$, and subtract away the previous time period $X_{t-1}$.
-   Second order differencing $d = 2$ is $X'_t = (X_t - X_{t-1}) - (X_{t-1}-X_{t-2})$.

Any further order of differencing $d$ is increasingly uncommon. The point of differencing is that we keep increasing $d$ until we have a weak stationary process. We denote a time series that has been differenced at $d$-order with $X_i'^d$.

<br />

### MA and AR Models

**MA (Moving Average)** **models** are when $X_t$ at time $t$ depends on past error terms $\epsilon_{t-1}, \dots, \epsilon_{t-q}$. This implies that $X_t$ is dependent on short-term noise/fluctuation. The model is specified with stationarity assumed for $X'_t$:

$$
\begin{align}
MA(q) \ : \ X_t'^d & = \omega +  \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \dots + \theta_q \varepsilon_{t-q} + \varepsilon_t \\
& = \omega + \sum\limits_{i=1}^q \theta_i \epsilon_{t-i} \ + \varepsilon_t
\end{align}
$$

Where $\omega$ is some constant (the mean of the time series), and $\theta_1, \dots, \theta_q$ are the coefficients. This model is estimated with maximum likelihood. To choose $q$, we find the model with the lowest AIC or BIC.

**AR (Auto-regressive) models** are when $X_t$ at time $t$ depends on past values $X_{t-1}, \dots, X_{t-p}$. This implies that $X_t$ is dependent on previous values in the time series, for example, in variables with momentum (ex. stock prices, sales). The model is specified with stationarity assumed for $X'_t$:

$$
\begin{align}
AR(p) \ : \ X_t'^d& = \omega + \phi_1 X_{t-1} + \phi_2 X_{t-2} + \dots + \phi_p X_{t-p} + \varepsilon_t \\
& = \omega + \sum\limits_{i=1}^p \phi_i X_{t-i} \ + \varepsilon_t
\end{align}
$$

Let us demonstrate a unique property of autoregressive models. Consider AR(1):

$$
AR(1) : X_t' = \omega + \phi_1 X_{t-1} + \varepsilon_i
$$

Knowing this, we can also predict $X'_{t-1}$, and then plug this back into our original $X'_t$. Then, we can do the same with $X'_{t-2}$, plug it in, and then $X'_{t-3}$, and so on $k$ number of times, until we can get the following model:

$$
X'_t = \phi^kX_{t-k} + \sum\limits_{j=0}^{k-1} \phi_j \varepsilon_{t-j}
$$

And when $|\phi| < 1$, the model is equivalent to a moving average model where $q = ∞$:

$$
X'_t = \sum\limits_{j=0}^∞ \phi_j \varepsilon_{t-j} = MA (∞)
$$

<br />

### ARIMA Model

The most popular model is the **ARIMA** (Auto-regressive Integrated Moving Average Model), which combines AR and MA models. ARIMA assumes that we have stationary trends:

$$
\text{ARIMA}(p, d, q) \ : \ X_t'^d = \underbrace{\sum\limits_{i=1}^p \phi_iX_{t-i}}_{\text{AR}} + \underbrace{\sum\limits_{i=1}^q \theta_i\varepsilon_{t-i}}_{\text{MA}} + \varepsilon_t
$$

-   $X_t'^d$ is $d$-ordered differenced time series that is stationary.
-   The **AR** (Autoregressive) section is how previous values of $X$ before time period $t$ predict the current $X_t$. $p$ is the number of previous time periods to include (which we have to choose).
-   The **MA** (Moving Average) section is how previous values of error $\epsilon$ before time period $t$ predict the current $\epsilon_t$. $q$ is the number of previous time periods to include (which we have to choose).

The model is estimated with maximum likelihood estimation. We can choose $p$ and $q$ by first choosing a variety of $p$ and $q$, estimating each model, and choosing the model (and $p$ and $q$) that has the least AIC or BIC value.

ARIMA is a non-seasonal models, since seasonality is technically non-stationary by definition. An extension of this model for seasonaility is called SARIMA.

ARIMA is very popular for a multitude of reasons.

1.  First, most situations in the real world have both AR and MA components: generally, $X_t$ is determined by some combination of past $X_t$ and errors.
2.  ARIMA is a general framework. An $\text{ARIMA}(p, 0, 0) = \text{AR}(p)$, and a $\text{ARIM}(0,0,q) = \text{MA}(q)$. This allows us to adapt ARIMA to a wide variety of data without being forced into one specific model selection.
3.  ARIMA has quite good accuracy in time series forecasting, even compared to machine learning and modern techniques.

<br />

### Seasonality Differencing and SARIMA

**SARIMA** (Seasonal Auto-regressive Integrated Moving Average) models are the seasonal extension: we add seasons by doing additional seasonal differencing.

A seasonal differenced is taking $X_t$, and subtracting the $X$ value from the same point in the last season. Let us say each season is $S$ number of time periods long. So, a first-order seasonal difference would be $Z_t = X_t - X_{t-S}$.

We can also combine a seasonal differenced $Z_t$ with a normal difference. For example, if $d= 1$ and $D=1$:

$$
Z^{*d}_t = Z_t - Z_{t-1} = (X_t - X_{t-S}) - (X_{t-1} - X_{t-1-S}), \qquad \text{where } Z_t = X_t - X_{t-S}
$$

A **SARIMA** model will take the following form:

$$
\text{SARIMA}(p, q, d)(P, Q,D)S \ : \ Z_t^{*d} = \underbrace{\overbrace{\sum\limits_{i=1}^p \phi_iX_{t-i}}^{\text{AR}} + \overbrace{\sum\limits_{i=1}^q \theta_i\epsilon_{t-i}}^{\text{MA}}}_{\text{ARIMA}} + \underbrace{\overbrace{\sum\limits_{i=1}^P \Phi_iX_{t-Si}}^{\text{AR}} + \overbrace{\sum\limits_{i=1}^Q \Theta_i\epsilon_{t-Si}}^{\text{MA}}}_{\text{Seasonality}} + \epsilon_t
$$

Where $Z^{*d}$ is a $d$-order difference of $D$-order seasonal differences $Z_t'^D$. Note that the seasonality section's parameters and model components are all specified with capital letters.

Choosing $p, q, P, Q$ is the same as in a ARIMA model - we should estimate models with different combinations, and see which model has the lowest AIC and BIC.

<br />

<br />

------------------------------------------------------------------------

# **Trees and Random Forests**

### Regression Trees

Regression Trees are an alternative way to obtain predictions of an outcome $y$ from explanatory variables $x$. Instead of modelling how $y$ changes with every unit increase in $x$, regression trees instead model stratification.

Tree-based methods will divide the independent variable $x_j$ into 2 regions, splitting $x$ at some value $x = s$. For example, if we had an $x$ variable such that $x \in [0, 100]$, we could split the variable at $x = s$ to create two regions: $x^{(1)} \in [0, s]$ and $x^{(2)} \in [s, 100]$.

Then, tree-based methods will calculate the mean $y$ value in each region created: $\bar y^{(1)}$ and $\bar y^{(2)}$. These means will be the predictions of $\hat y$.

-   If a unit $i$ has an $x$ value that falls into region $x^{(1)} \in [0, s]$, their predicted $\hat y = \bar y^{(1)}$.
-   If a unit $i$ has an $x$ value that falls into region $x^{(2)} \in [s,100]$, their prediction $\hat y = \bar y^{(2)}$.

But how do we decide which threshold $s$? For continuous $y$ variables, to determine the threshold $s$ in which to split $x$, the algorithm of tree regressions will find the optimal threshold $x = s$ that reduces the **residual sum of squares** (RSS) of the predictions the most:

$$
RSS = \sum\limits_{i=1}^n(y_i - \hat y_i)^2
$$

Essentially, the computer tests every possible threshold value of $x_j = s$, and finds the threshold that reduces the sum of squares the most.

For binary $y$ variables, trees will determine the threshold $s$ in which to split $x$, based on the threshold $x=s$ that reduces the **classification error rate**. The classification error is essentially how many predictions $\hat y$ do not match to the actual $y$.

::: {.callout-note collapse="true" appearance="simple"}
## Procedure of Growing a Tree

The process of growing a tree is as follows. Assume now we have more than one explanatory variable.

1.  Test all possible thresholds $x_j = s$ for each $x_j \in \{x_1, \dots, x_k\}$.
2.  Identify the specific variable $x_p \in \{x_2, \dots, x_k\}$ which has a specific threshold $x_p = s^*$ that has the greatest reduction in residual sum of squares (or classification error rate), for all combinations of $x_j$ and threshold $s$.
3.  Now, divide that specific $x_p$ at that specific threshold $s^*$, creating two regions $x_p^{(1)}, x_p^{(2)}$.
4.  Now, we repeat the process of testing all possible thresholds $x_j = s$ for each $x_j \in \{x_1, \dots, x_k \}$ and finding the specific variable-threshold $x_m = s^{**}$ that results the in the greatest reduction in RSS (or classification error rate) within $x_p^{(1)}$. Create 2 more groups from within $x_p^{(1)}$.
5.  Then, do the same for $x_p^{(2)}$. Create 2 more groups from with $x_p^{(2)}$.
6.  Keep on going, finding the variable-thresholds which reduce the RSS the most from each previous subregion we have created.
7.  Continue doing this until some stopping criteria. Find the average $y$ in each of the groups you have, and those are the predicted $\hat y$.

It is possible for a variable to occur multiple times with different thresholds.
:::

::: {.callout-note collapse="true" appearance="simple"}
## Example of Growing a Tree

Below is an example of growing a tree:

![](images/clipboard-3372306608.png){fig-align="center" width="80%"}

We can see that the first (top) split is in the variable *taxpercent*, at threshold 34.2935. That means that specific variable-threshold split reduced the RSS the most of any variable-threshold combination.

Notice how after the first split, we only divide each subregion. We do not go back to the top/whole data set.

Notice how *taxpercent* re-appears again on the right side of the tree. It is possible for a variable to occur multiple times.

At the end, you can see the numbers at the bottom. These are called leaves, and are the final categories/groups of the tree with their mean $y$ labelled. Those mean $y$ will be the predictions for each observation that falls into that group.
:::

The earlier a variable is split, the more *influential* the variable is. We will discuss this idea further later in the chapter.

<br />

### Limitations and Bootstrap Aggregation

One of the best things about decision trees is that they incorporate interactions between variables. Lower-level splits of variables are interacting with higher-level splits of variables. This allows regression trees to be excellent for non-linear predictions. Regression trees are also great for visualisation, and are quite easy to explain visually without invoking complex statistics or mathematics.

The downside of regression trees is that they have extremely high variance. If you just slightly change the data, your predicted results will be completely different, and even the order of variables and thresholds will completely change. This is called **overfitting**. Overfitting causes simple regression trees to be poor predictors of out-of-sample data.

**Bootstrap Aggregation** is a solution to this variance and overfitting problem. This procedure builds on a simple statistical idea: in a set of $n$ samples $Z_1, \dots, Z_n$ each with variance $\sigma^2$, the variance of the means $\bar Z$ of all the samples is $\frac{\sigma^2}{n}$. Since $n$ is in the denominator, that implies increasing the number of samples $n$ will reduce the variance of our predictions.

However, we typically only have one sample of data. How can we increase $n$ if we only have one sample? The answer is **Bootstrap Sampling**. Essentially, we sample with replacement from our original sample.

To create a bootstrap sample, we choose 1 observation at random from our original sample. We add that observation to our bootstrap sample, and replace it back into our original sample. Then, we draw another observation, add it to our bootstrap sample, and replace it back. We do this $n$ times ($n$ being the sample size of our sample).

If we do this procedure multiple times, we will end up with a few similar, but slightly different data sets. We are essentially replicating the process of obtaining new data sets, without gathering more data. Now, with our multiple data sets, we can use bootstrap aggregation to reduce our variance.

<br />

### Bagging and Random Forest

**Bagging** models apply bootstrap sampling to tree regressions.

1.  We first generate $B$ number of bootstrap samples.
2.  Then, we train a tree on each different sample $b$, creating a prediction function $f_b(x) = \hat y_b$ for each sample, which specifies what values of $x$ result in what predicted $\hat y$ according to the tree.
3.  Then we average all of the sample predictions together to obtain our final prediction function $f_{bag}(x)$.

$$
f_{bag}(x) = \frac{1}{B}\sum\limits_{b=1}^Bf_b(x)
$$

Bagging reduces the variance of trees, and is one of the most accurate prediction methods, frequently outperforming both linear and non-linear methods.

However, bagging is still not perfect. This is because bagging trees are heavily correlated: in general, the trees will have the same top-level $x_p$ variable, especially if there is one very strong predictor in our explanatory variables. This is an issue - highly correlated trees, even if averaged, do not reduce the variance as much as we might need.

**Random Forests** solve this issue by not only bootstrapping observations like Bagging does, but also sampling a subset of explanatory variables for each tree. For example, let us say you a set of explanatory variables $\mathcal X = \{x_1, \dots  x_k\}$. For every bootstrap sample of observations $b$ that we did in bagging, Random forest will also sample a subset of explanatory variables $\mathcal X_b$, which contains only $g<k$ number of explanatory variables.

This means that each tree Random Forest produces only has a subset of $g$ explanatory variables, not the total number of $k$ explanatory variables. This means that sometimes, influential variables $x_p$ will not be included in the subset $\mathcal X_b$, which will allow for other predictors to get a chance to shine.

Thus, Random Forest will have less correlated trees, and thus, will typically have more accurate predictions.

Generally, the size of the subset of explanatory variables $\mathcal X_b$ will typically be $g=\sqrt{k}$, the square root of the total number of explanatory variables. However, you can play around with this (discussed in the next section).

<br />

### Model Selection

There are a few choices you must make when specifying the model you are using.

1.  Should you use Random Forest or Bagging? Typically, Random Forest is better, but this is not always the case.
2.  If you do choose Random Forest, what size of the subset of explanatory variables $\mathcal X_b$ should you use? Typically, the standard is to use the square root of the number of explanatory variables, but this is not a fixed rule.

For **prediction** tasks, **Mean of squared residuals** is a general measure of how well the model performed. It is the mean of the squared errors, where the errors are the differences between the actual dataset $y_i$ value and the predicted $\hat y_i$ value. In R, the mean of squared residuals metric is calculated on testing data - testing the model on units that were not included in a specific bootstrap sample. Thus, this is a good measure of how predictive your model is, without worrying about overfitting.

For **classification** tasks, the simplest metric to help you make the decision is the **error rate**. This is simply the percentage of observations the model got wrong (predicted one category when it should have been the other). We can also dig into more detail:

-   The **False Positive Rate** is the observations that are $y=0$, but our model predicted incorrectly as $\hat y = 1$.
-   The **False Negative Rate** is the observations that are $y=1$, but our model predicted incorrectly as $\hat y = 0$.

The inverses of false positives/negatives are specificity and sensitivity:

-   **Specificity** is the percentage of observations that are $y=0$, that our model correctly predicted as $\hat y =0$.
-   **Sensitivity** is the percentage of observations that are $y=1$, that our model correctly predicted as $\hat y = 1$.

The metric on which to focus on depends on our goals of classification. Choosing the right model requires some subjectivity. For example, if we are trying to predict if a patient has a serious disease, we probably want more false positives rather than under-detecting people who are actually sick. But for judicial systems, since we do not want to put an innocent person in jail, we might prefer lower false positives, and more false negatives.

<br >

### Importance Statistics

One of the downsides of Bagging and Random Forest is that they are harder to interpret than traditional statistical models and also regression trees.

Regression Trees produce a nice diagram, with the most important variables near the top. However, Bagging and Random Forest average hundreds or thousands of different trees, so we cannot really draw a diagram out.

This is where importance statistics comes in. To calculate importance, we "remember" the reduction in RSS (or error rate) every time we split a tree, for every bagging sample $b$. Then, we figure out which variables on average reduce the RSS (or error rate) the most.

![](images/clipboard-364907520.png){fig-align="center" width="70%"}

Above is an example of the importance plot. This allows us to determine which explanatory variables are the most influential in determining the predictions, which can be useful in interpretation.

<br />

### Extension: Causal Forests

<br />

<br />

------------------------------------------------------------------------

# **Naive Bayes Classifier**

------------------------------------------------------------------------

# **Implementation in R**

You will need the *randomForest* and *tidyverse* packages.

```{r, eval = FALSE}
library(randomForest)
library(tidyverse)
```

For replication purposes (if you are publishing a paper), you may also want to set a random seed, which will ensure your bootstrap and variable sampling remains consistent in replication.

```{r, eval = FALSE}
set.seed(1234) #any number will work
```

::: {.callout-note collapse="true" appearance="simple"}
## Bagging Model

You can train a bagging model with the *randomForest()* function:

```{r, eval = FALSE}
model <- randomForest(Y ~ x1 + x2 + x3 + x4,
                      data = mydata,
                      na.action = na.omit,
                      mtry = 4, #equal to k
                      importance = TRUE)

#call object to see output
model
```

The output will contain summary statistics.

Note: you can also do $Y \sim .$ in the formula. The $.$ symbolises that you want to include all other variables not $Y$ within your dataframe into your model. This speeds up the process of writing every single explanatory variable out.
:::

::: {.callout-note collapse="true" appearance="simple"}
## Random Forest Model

You can train a random forest model with the *randomForest()* function:

```{r, eval = FALSE}
model <- randomForest(Y ~ x1 + x2 + x3 + x4,
                      data = mydata,
                      na.action = na.omit,
                      mtry = 2, #equal to sqrt of k
                      importance = TRUE)

#call object to see output
model
```

The output will contain summary statistics.

Note: you can also do $Y \sim .$ in the formula. The $.$ symbolises that you want to include all other variables not $Y$ within your dataframe into your model. This speeds up the process of writing every single explanatory variable out.
:::

::: {.callout-note collapse="true" appearance="simple"}
## Predictions

You probably want to make predictions with your model (that is kind of the point of these models).

We can use the *predict()* function to generate predictions:

```{r, eval = FALSE}
mypredictions <- predict(model, newdata = my_new_data)
```

*my_new_data* is a dataframe with a bunch of explanatory variable values (for every explanatory variable) for a collection of observations, that you wish to predict $\hat y$ for.

*model* is the name of the object in which you stored your trained model to.
:::

::: {.callout-note collapse="true" appearance="simple"}
## Importance Plot

To see the importance of each explanatory variable, we can use the *varImpPlot()* function:

```{r, eval = FALSE}
varImpPlot(model,
           main = "Title of the Graph",
           type = 2)
```

Do not change type = 2. That specifies something technical that you do not need to worry about.
:::
