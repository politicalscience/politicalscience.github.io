---
title: "Linear Algebra"
subtitle: "Essential Mathematics"
format:
    html:
        page-layout: article
        grid:
          sidebar-width: 350px
          body-width: 700px
          margin-width: 250px
          gutter-width: 2em
        fontsize: 12pt
        theme: default
        toc: TRUE
        toc-depth: 3
        toc-location: left
        toc-expand: true
        toc-title: "Table of Contents"
        mainfont: "Computer Modern"
        title-block-banner: true
        classoption: fleqn
        html-math-method: katex
editor: visual
---

Use the sidebar for easy navigation.

------------------------------------------------------------------------

# **Vector Spaces**

### Linear Mapping

A mapping is any rule that maps elements from one set to another. A function $f$ is a mapping $f: A \rightarrow B$.

A linear mapping is a mapping that is linear, which must meet the following properties:

-   $f(a+b) = f(a) + f(b)$
-   $f(ca) = c f(a)$

We can represent linear mappings for finite sets by matrices. Let us say $\mathbf X_{n \times m}$ is a matrix, and $\mathbf y_{m}$ is a vector. Let us find their product:

$$
\mathbf {Xy} = \mathbf z_m
$$

What this is saying is take the vector $\mathbf y$, and operate on it using the matrix $\mathbf X$, to produce a new vector $\mathbf z$. Here, the matrix $\mathbf X$ is an operator that maps $\mathbf y \rightarrow \mathbf z$.

<br />

### Vectors and Vector Spaces

Vectors have a norm (which is also called the inner product):

$$
||\mathbf a|| = \sqrt{\mathbf a \cdot \mathbf a} = \sqrt{a_1^2 + a_2^2+\dots+a_n^2}
$$

Vector spaces are any collection of vectors that have a norm, that have some common properties.

Vectors are important because our data is represented in vectors.

<br />

### Linear Combination

A linear combination is a combination of vectors that is linear (i.e. vectors can be added, and scalar multiplied). For example, this is a linear combination:

$$
t \mathbf x + (1-t) \mathbf y
$$

Linear combinations either represents lines (in $\mathbb R^2$), planes (in $\mathbb R^3$), and hyperplanes (a plane of one less dimensions than the space) in higher dimensions.

<br />

### Linear Independence

Start off with a linear combination:

$$
a_1 \mathbf x_1 + a_2 \mathbf x_2+\dots + a_n \mathbf x_n
$$

This set of vectors $(\mathbf x_1, \dots, \mathbf x_n)$ is independent if you cannot get one vector $\mathbf x_j \in \{ \mathbf x_1, \dots, \mathbf x_n \}$ equal to a combination or multiple of the other vectors.

::: {.callout-note collapse="true" appearance="simple"}
## Example of Linear Independence

Let us say we have these two vectors:

$$
\begin{pmatrix} 1 \\ 2 \end{pmatrix}, \begin{pmatrix} 2 \\ 1 \end{pmatrix}
$$

Are these linearly independent? That means I cannot use a linear transformation to go from one to another.

No, there is no constant you can multiply to get from vector 1 to vector 2, and there are no other vectors to add/subtract to to go from one to another.

Now consider these two vectors:

$$
\begin{pmatrix} 1 \\ 2 \end{pmatrix}, \begin{pmatrix} 2 \\ 4 \end{pmatrix}
$$

These are not independent - you can multiply the first vector by a scalar of 2 to get the second vector.
:::

This can be complicated to see in higher dimensions (since it is hard to consider how multiple vectors can be combined to match another). There, we use the matrix rank (see below).

<br />

### Spanning Vectors and Dimension

A collection of spanning vectors spans some space, if you can write any vector in that space, as a linear combination of the spanning vectors.

For example, take vector $\mathbf e_1 = (1 \ 0)$ and $\mathbf e_2 = (0 \ 1)$. These vectors span some space including $\mathbf z$, if vector $\mathbf z$ can be written as:

$$
\mathbf z = a \mathbf e_1 + b \mathbf e_2, \quad \text{e.x.} \quad \mathbf z = (a \ b)
$$

-   In fact, $\mathbf e_1$ and $\mathbf e_2$ span the set of all 2-dimensional vectors.

This allows us to write vectors in terms of the core vectors, and to understand the dimension of the space.

The **dimension** of the space is the smallest number of linearly independent vectors that span the space.

::: {.callout-note collapse="true" appearance="simple"}
## Example of Dimensionality

Take these two vectors:

$$
\begin{pmatrix} 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \end{pmatrix}
$$

These are linearly independent - no factor multipled can get the other vector. Thus, this is 2 dimensional space

Now, let us add a third vector.

$$
\begin{pmatrix} 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \end{pmatrix}, \begin{pmatrix} 3 \\ 2 \end{pmatrix}
$$

Is the third vector linearly independent? No. We can write the third vector with a combination of the other two:

$$
\begin{pmatrix} 3 \\ 2 \end{pmatrix} = 3 \begin{pmatrix} 1 \\ 0 \end{pmatrix} + 2 \begin{pmatrix} 0 \\ 1 \end{pmatrix}
$$

Thus, the third vector is in the space spanned by the first 2 vectors. The first two vectors spans this space, and thus, it is 2 dimensional space.
:::

Generally, the dimensionality of the space matches the number of vectors that span the space (so a 2 dimensional space is often spanned by 2 vectors, 3 spanned by 3, etc.).

Note: Dimensionality of a vector space is not always the same as the dimensionality of the vectors.

<br />

### Matrix Rank

As we discussed before, it can be difficult to find if vectors are linearly independent in higher dimensions.

We can stack the row vectors into a matrix (we can also do them in columns):

$$
\begin{pmatrix} \mathbf x_1 \\ \mathbf x_2 \\ \mathbf x_3 \end{pmatrix} =
\begin{pmatrix} x_{11} & x_{12} & x_{13} \\
x_{21} & x_{22} & x_{23} \\
x_{31} & x_{32} & x_{33} \end{pmatrix}
$$

The **Rank** of a matrix is the number of linearly independent rows/columns in a matrix.

If the Rank is equal to the total number of rows/columns (all rows/columns are linearly independent), the matrix has **full rank**.

-   A matrix with full rank is non-singular, and thus, can be inverted, and has a non-zero determinant.

Thus, if we find the determinant of the matrix, if it is 0, the vectors are [not]{.underline} linearly independent, and if it is not0 , they are linearly independent.

We can also know a matrix is not full rank, if the space of the dimension is less than the number of vectors (as explained above).

<br />

### Quadratic Forms

A quadratic form takes the following form:

$$
\mathbf x^T \mathbf A \mathbf x
$$

-   Where matrix $\mathbf A$ is a square matrix.

If $\mathbf x^T \mathbf A \mathbf x > 0$ for all values, then the matrix is positive definite. If $\mathbf x^T \mathbf A \mathbf x < 0$, then negative definite. If $\mathbf x^T \mathbf A \mathbf x ≤ 0$ it is positive semi-definite.

This will be useful for optimisation.

<br />

<br />

------------------------------------------------------------------------

# **Solving Systems of Equations**

### Matrices and Systems of Equations

You can write any system of linear equations as a matrix times a vector. Let us take this set of equations:

$$
\begin{cases}
a_{11}x_1 + a_{12} x_2 + a_{13} x_3 = y_1 \\
a_{21}x_1 + a_{22}x_2 + a_{23}x_3 = y_2 \\
a_{31}x_1 + a_{32}x_2 + a_{33}x_3 = y_3
\end{cases}
$$

We can write this system of equations as follows:

$$
\begin{pmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix}
 = \begin{pmatrix} y_1 \\ y_2 \\ y_3 \end{pmatrix}
$$

Or even simpler, we can represent it as:

$$
\mathbf A\mathbf x = \mathbf y
$$

A unique solution exists when you have the same amount of equations as unknowns, and the equations are non-contradictory.

-   Overdetermined systems are when there are more equations than unknowns, which might contradict each other.
-   Underdetermined systems are when there are not enough equations compared to unknowns, so we cannot solve it.

<br />

### Matrix Rank and Systems of Equations

Take this system of linear equations:

$$
\begin{cases}
x+y=1 \\
2x + 2y = 2
\end{cases}
$$

We can see that the two equations are a common factor of each other. Or in other words, these two vectors are non-linearly independent.

We know when solving for these equations, we cannot actually solve for a unique solution.

We know if a matrix is full rank, then all rows/columns are linearly independent.

<br />

### Matrix Inversion

Matrix inversion is a way to solve a system of equations. Take this system of equations:

$$
\mathbf{Ax} = \mathbf y
$$

You can solve for $\mathbb x$ by inverting matrix $\mathbf A$ (assuming matrix a is full rank):

$$
\begin{split}
\mathbf{Ax} & = \mathbf y \\
\mathbf{A}^{-1}\mathbf{Ax} & = \mathbf A^{-1}\mathbf y \\
\mathbf x &= \mathbf A^{-1}\mathbf y
\end{split}
$$

::: {.callout-note collapse="true" appearance="simple"}
## Example of Matrix Inversion

Take this system of equations:

$$
\begin{cases}
3x - 7y = -11 \\
5x + 10y = 25
\end{cases}
$$

We can write this in linear algebra:

$$
\begin{pmatrix}
3 & -7 \\
5 & 10
\end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} =
\begin{pmatrix} -11 \\ 25 \end{pmatrix}
$$

Now, let us find the inverse of the first matrix:

$$
\mathbf A^{-1} = \frac{1}{|\mathbf A|}\mathbf C^T
$$

We know the determinant $|\mathbf A| = 3(10) - (-7)(5) = 65$.

Now, let us find the cofactor matrix:

-   $c_{11} = (-1)^{1+1}\times 10 = 10$
-   $c_{12} = (-1)^{1+2} \times 5 = -5$
-   $c_{21} = (-1)^{2+1} \times -7 = 7$
-   $c_{22} = (-1)^{2+2} \times 3 = 3$

Thus, the cofactor matrix transposed should be:

$$
\mathbf C^T = \begin{pmatrix} 10 & -5 \\ 7 & 3 \end{pmatrix}^T = \begin{pmatrix} 10 & 7 \\ -5 & 3 \end{pmatrix}
$$

Thus, the inverse of matrix $\mathbf{A}$ should be:

$$
\mathbf A^{-1} = \frac{1}{65} \begin{pmatrix} 10 & 7 \\ -5 & 3 \end{pmatrix} = \begin{pmatrix} \frac{2}{13} & \frac{7}{65} \\ -\frac{1}{13} & \frac{3}{65} \end{pmatrix}
$$

We know the solution should be:

$$
\begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} \frac{2}{13} & \frac{7}{65} \\ -\frac{1}{13} & \frac{3}{65} \end{pmatrix} \begin{pmatrix} -11 \\ 25 \end{pmatrix}
$$

Thus, doing matrix multiplication to obtain $x$ and $y$:

-   $x =\frac{2}{13}(-11) + \frac{7}{65}(25) = -\frac{22}{13} + \frac{35}{13} = \frac{13}{13}=1$
-   $y=-\frac{1}{13}(-11) + \frac{3}{65}(25) = \frac{11}{13}+ \frac{15}{13} = \frac{26}{13} = 2$

The solution to this system of equations is: $(1, 2)$.
:::

<br />

### Cramer's Rule

Cramer's rule is another rule to solve equations, only for square matrices. It is not super commonly used.

Given the system of equations:

$$
\mathbf{Ax} = \mathbf y
$$

The element $x_i$ is defined as:

$$
x_i = \frac{|\mathbf B_i|}{\mathbf A}
$$

Where $\mathbf B_i$ is a matrix obtained by taking the matrix $\mathbf A$, and replacing the $i$th column with the column vector $\mathbf y$.

<br />

<br />

------------------------------------------------------------------------

# **Eigenvalues and Eigenvectors**

### Definitions

A matrix, as we discussed, can act as an operator that maps some vector to another $\mathbf A : \mathbf x \rightarrow \mathbf y$.

Geometrically, vector $\mathbf x$ points in some dimensional space. Then the matrix $\mathbf A$ comes along, and transforms it into a different vector $\mathbf x$, which might flip the direction, or rotate, or change its norm, etc. Sometimes, $\mathbf y$ will stay in the same direction or opposite direction (along the same line) as the original, even after the operator.

Basically, an eigenvector of matrix $\mathbf A$ is a vector $\mathbf x$, that does not change its direction (stays on the same line) when you apply the operator $\mathbf A$ to get vector $\lambda \mathbf x$. Mathematically:

$$
\mathbf{Ax} = \lambda \mathbf x
$$

The $\lambda$ is an eigenvalue that corresponds to the eigenvector.

<br />

### Computing Eigenvalues

Let us start with the eigenvector formula:

$$
\mathbf{Ax} = \lambda \mathbf x
$$

We can multiply the right side by the identity matrix (which does not change the result):

$$
\mathbf{Ax} = \lambda \mathbf {Ix}
$$

Now, let us move everything to one side, and simplify:

$$
\begin{split}
\mathbf{Ax} - \lambda \mathbf {Ix} & = 0 \\
(\mathbf A - \lambda \mathbf I) \mathbf x & = 0
\end{split}
$$

$(\mathbf A - \lambda \mathbf I)$ is an singular matrix, meaning determinant $|(\mathbf A - \lambda \mathbf I)| = 0$. All values of $\lambda$ that solve this determinant equation will be eigenvalues of the matrix.

::: {.callout-note collapse="true" appearance="simple"}
## Example of Solving for Eigenvalues

We know $|(\mathbf A - \lambda \mathbf I)| = 0$. Let us say:

$$
\mathbf A = \begin{pmatrix} 2 & 1 \\ 3 & 4 \end{pmatrix}, \quad \lambda \mathbf I \begin{pmatrix} \lambda & 0 \\ 0 & \lambda \end{pmatrix}
$$

Thus:

$$
\mathbf A - \lambda \mathbf I = \begin{pmatrix} 2 - \lambda & 1 \\ 3 & 4 - \lambda \end{pmatrix}
$$

We know the determinant should equal 0. We can solve for $\lambda$:

$$
\begin{split}
0 & = |(\mathbf A - \lambda \mathbf I)| \\
0 & = (2-\lambda)(4-\lambda) - 1(3) \\
0 & = 8-2\lambda - 4 \lambda +\lambda^2 - 4 \\
0 & = \lambda^2 - 6\lambda +4 \\
\end{split}
$$

Let us use the quadratic formula:

$$
\begin{split}
\lambda & = \frac{-b ± \sqrt{b^2 - 4ac}}{2a} \\
\lambda & = \frac{6 ± \sqrt{36 - 4(1)(4)}}{2(1)} \\
\lambda & = \frac{6 ± \sqrt{20}}{2} \\
\lambda & = 3 ± \frac{\sqrt{20}}{2} \\
\lambda & = 3 ± \frac{2 \sqrt{5}}{2} \\
\lambda & = 3 ± \sqrt{5}
\end{split}
$$

Thus, we have found our eigenvalues.
:::

<br />

### Calculating Eigenvectors

Let us say we have the matrix $\mathbf{A}$:

$$
\mathbf A = \begin{pmatrix} 2 & 3 \\ 2 & 1 \end{pmatrix}
$$

First, you need to calculate the eigenvalues $\lambda$ (as shown in the previous section).

::: {.callout-note collapse="true" appearance="simple"}
## Example of Calculating Eigenvalues

We know $|(\mathbf A - \lambda \mathbf I)| = 0$.

$$
\mathbf A - \lambda \mathbf I = \begin{pmatrix} 2 - \lambda & 3 \\ 2 & 1 - \lambda \end{pmatrix}
$$

Now solve $|(\mathbf A - \lambda \mathbf I)| = 0$.

$$
\begin{split}
0 & = |(\mathbf A - \lambda \mathbf I)| \\
0 & = (2 - \lambda)(1- \lambda) - 2(3) \\
0 & = 2 -2 \lambda - \lambda + \lambda^2 - 6 \\
0 & = \lambda ^2 - 3\lambda - 4 \\
0 & = (\lambda - 4)(\lambda + 1) \\
\lambda & = 4, -1
\end{split}
$$
:::

For every eigenvalue, you will have an eigenvector that makes the eigenvector equation $\mathbf{Ax} = \lambda \mathbf x$ true. From above:

$$
\begin{split}
& \mathbf{Ax} = 4 \mathbf x \\
& \mathbf{Ax} = -1 \mathbf x
\end{split}
$$

Remember, $\mathbf x$ is a vector here of 2 elements, $x_1, x_2$ - but we only have one equation for each eigenvalue pair. This means there will be one solution without a unique solution.

::: {.callout-note collapse="true" appearance="simple"}
## Eigenvalues and Non-Unique Solutions

It might seem simple to just solve for our $\mathbf x$ with two unknowns $x_1, x_2$.

But the issue is - our equations are not linearly independent, so we do not have enough information to solve for a unique solution for one.

This is actually okay - why? Well, take a look at these potential eigenvectors:

$$
\begin{pmatrix} 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 2 \\ 0 \end{pmatrix}, \begin{pmatrix} 3 \\ 0 \end{pmatrix}
$$

These are different vectors - but they are all eigenvectors. This is because of the equation $\mathbf{Ax} = \lambda \mathbf x$ - if both sides have vector $(1 \ 0)$ multiplied by a constant (like the 2nd and 3rd vectors), the equation is still equal.

Thus, eigenvectors are not defined uniquely - only up to a singular multiplicative constant.
:::

What we typically do is define $x_1 = 1$ (there are some cases where this does not work). Using this, we can solve for the answer.

::: {.callout-note collapse="true" appearance="simple"}
## Example of Calculating Eigenvectors

Let us continue the same example from before.

$$
\mathbf A = \begin{pmatrix} 2 & 3 \\ 2 & 1 \end{pmatrix}, \quad \lambda = 4, -1, \quad \mathbf x = \begin{pmatrix} 1 \\ c \end{pmatrix}
$$

Let us solve for the eigenvalue of $\lambda = 4$.

$$
\begin{split}
\mathbf{Ax} & = 4 \mathbf x \\
\begin{pmatrix} 2 & 3 \\ 2 & 1 \end{pmatrix}\begin{pmatrix} 1 \\ c \end{pmatrix} & = 4\begin{pmatrix} 1 \\ c \end{pmatrix} \\
\begin{pmatrix} 2+3c \\ 2+c \end{pmatrix} & = \begin{pmatrix} 4 \\ 4c \end{pmatrix}
\end{split}
$$

That gives us two equations:

$$
\begin{split}
& 2 + 3c = 4 \\
& 2 + c = 4c
\end{split}
$$

The answer is $c = \frac{2}{3}$ (both answers give us the equation).

Thus, the eigenvector with $\lambda = 4$ is:

$$
\mathbf x = \begin{pmatrix} 1 \\ \frac{2}{3} \end{pmatrix}
$$

We can do the same for $\lambda = -1$, and we will get eigenvector:

$$
\mathbf x = \begin{pmatrix} 1 \\ -1 \end{pmatrix}
$$
:::

<br />

### Eigenvector Decomposition

Matrix decomposition is to take a matrix $\mathbf A$, and decompose it into other matrices, that when multiplied together, get the original matrix.

If matrix $\mathbf A$ has unique eigenvalues, you can write:

$$
\mathbf A = \mathbf {QDQ}^{-1}
$$

Where $\mathbf Q$ is made up of eigenvectors of matrix $\mathbf A$, and $\mathbf D$ is a diagonal matrix with the eigenvalues $\lambda$ on the diagonal:

$$
\mathbf D = \begin{pmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{pmatrix}, \quad
\mathbf Q = \begin{pmatrix} \mathbf x_1 & \mathbf x_2\end{pmatrix}
$$

Eigenvectors decomposition has a few uses:

::: {.callout-note collapse="true" appearance="simple"}
## Taking the Power of Matrices

Let us say you want to find $\mathbf A^z$. We can use matrix decomposition for this:

$$
\mathbf A^z = {QDQ}^{-1}{QDQ}^{-1}{QDQ}^{-1}\dots
$$

Notice how the $\mathbf{Q}^{-1} \mathbf Q$ occurs quite frequently, and we know by properties of inverses, that $\mathbf{Q}^{-1} \mathbf Q = \mathbf I$, and multiplying by $\mathbf I$ does nothing.

Thus, we can rewrite the formula above as:

$$
\mathbf A^z = \mathbf{QD}^z\mathbf Q^{-1}
$$

This is much simpler than calculating $\mathbf A^z$, since taking diagonal matrix to a power is defined as:

$$
\mathbf D^z = \begin{pmatrix} \lambda_1^z & 0 \\ 0 & \lambda_2^z\end{pmatrix}
$$

Which is much easier to do.
:::

::: {.callout-note collapse="true" appearance="simple"}
## Finding Determinants

Let us say you want to find the determinant of $\mathbf A$. The following is true:

$$
\begin{split}
\det (\mathbf A) & = \det(\mathbf{QD}^z\mathbf Q^{-1}) \\
& = \det (\mathbf Q) \det (\mathbf D) \det (\mathbf Q^{-1}) \\
& = \det (\mathbf Q) \det (\mathbf D) \frac{1}{\det (\mathbf Q)} \\
& = \det (\mathbf D)
\end{split}
$$

Since $\mathbf D$ is diagonal, the determinant of a diagonal matrix is just the product of the diagonal elements. Thus:

$$
\det (\mathbf A) = \prod_i \lambda_i
$$
:::

::: {.callout-note collapse="true" appearance="simple"}
## Principle Component Analysis

Principle Component Analysis (PCA) is used when matrix $\mathbf A$ is symmetric, positive semi-definite.

-   This means the eigenvectors are orthogonal (perpendicular) to each other.
-   This means eigenvalues will always be real and non-negative.

PCA is done on the covariance matrix, which is symmetric and positive semi-definite.

It is a data reduction technique commonly used in statistics and data science. We will discuss this in the multivariate statistics section.

-   The eigenvectors of the covariance matrix form the principle components of thee system.
-   The eigenvalues tell us how much of the variance each component explains (more important principle components have higher eigenvalues).
:::
