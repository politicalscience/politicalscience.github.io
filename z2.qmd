---
title: "Multiple Linear Regression"
subtitle: "Chapter 2, Quantitative Methods (Causal Inference)"
sidebar: side
---

In the last chapter, we discussed the basics of statistics, and briefly introduced regression as a way to find correlations. This chapter dives deep into multiple linear regression, the foundational model for all of statistics. We cover the specification of the model, estimation and statistical inference, as well as extensions.

Use the right sidebar for quick navigation. R-code is provided at the bottom.

------------------------------------------------------------------------

# **Basics of the Model**

### Model Specification

There is some random outcome variable $Y_i$, and several random explanatory variables $X_{i1}, X_{i2}, \dots, X_{ip}$. There are $n$ number of observations $i = 1, \dots n$. The population linear model is specified as:

$$
Y_i = \beta_0 + \beta_1X_{i1} + \dots + \beta_pX_{ip} + \eps_i \ = \  \beta_0 + \sum\limits_{j=1}^p \beta_jX_{ij} + \eps_i
$$

Where $\beta_0, \dots, \beta_p$ are population parameters to be estimated, and $\eps_i$ is the error term.

::: {.callout-note collapse="true" appearance="simple"}
## More Info on the Error Term $\varepsilon_i$

The $\varepsilon_i$ is called the error term. This indicates that not every value of $Y_i$ in our data will be exactly on the linear best-fit line. Graphically, it is the highlighted part:

![](images/clipboard-1210742477.png){fig-align="center" width="45%"}

In social science terms, the $\eps_i$ is the effect of any other explanatory variable not included in our model.
:::

::: {.callout-note collapse="true" appearance="simple"}
## Visualising the Geometry of Regression

Our linear model is essentially a hyperplane space $\mathbb R^k$. The figure below shows this:

![](images/clipboard-365376575.png){fig-align="center" width="45%"}
:::

This implies for each observation $i=1,\dots ,n$ with values $(y_1, x_{11} \dots, x_{1p})$ has a regression equation:

$$
\begin{align}
y_1 = & \ \beta_0 + \beta_1x_{11} + \dots + \beta_px_{1p} + \eps_1 \\
y_2 = & \ \beta_0 + \beta_1x_{21} + \dots + \beta_px_{2p} + \eps_2 \\
& \qquad \vdots \qquad \qquad \vdots \\
y_n = & \ \beta_0 + \beta_1x_{n1} + \dots + \beta_px_{np} + \eps_n
\end{align}
$$

We can write this system of regression equations in linear algebra form:

$$
y = X\beta + \eps \quad \iff \quad \begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix}  =
\begin{pmatrix}1 & x_{11} & \dots & x_{1p} \\1 & x_{21} & \dots & x_{2p} \\\vdots & \vdots & \vdots & \vdots \\1 & x_{n1} & \dots & x_{np}\end{pmatrix}
\begin{pmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_p\end{pmatrix}
+ \begin{pmatrix}\eps_1 \\ \eps_2 \\ \vdots \\ \eps_n\end{pmatrix}
$$

Notation note: I generally indicate vectors with a lowercase letter (ex. $y$), and matrices with an uppercase vector (ex. $X$). Random variables will typically have subscript $i$, (ex. $Y_i, X_i$), while realisations of random variables (and also scalars) will be lowercase (ex. $y_1, x_1$).

<br />

### Estimation Process

To [estimate](quant1.qmd#estimators-and-sampling-distributions) the population parameters $\beta_0, \dots, \beta_p$, we use our sample data, and try to find the values $\hat\beta_0, \dots, \hat\beta_p$ that **minimise the square sum of residuals** (SSR). The residuals are the difference from our predicted best-fit line result $\hat Y_i$, and the actual value of $Y_i$ in the data. Below highlighted in red are the residuals.

![](images/clipboard-846785636.png){fig-align="center" width="60%"}

After we have the residual values, we simply square each of them, then sum all of them together. That is the sum of squared residuals (SSR). We will define the SSR as function $S$. In the linear algebra representation (where $\hat\beta$ is the vector of estimated parameters $\hat\beta_0, \dots, \hat\beta_p$):

$$
\begin{align}
S(\hat\beta) & = (y - \hat y)^\top (y - \hat y) \\
& = (y - \color{blue}{X\hat\beta}\color{black})^\top (y - \color{blue}{X\hat\beta}\color{black}) && (\because \color{blue}{\hat y = X\hat\beta} \color{black})
\end{align}
$$ {#eq-ssr}

This estimation is called the **ordinary least squares (OLS) estimator**. The solutions to the OLS estimator can be derived mathematically.

<br />

### Deriving OLS Estimates

OLS wants to minimise the sum of squared residuals $S(\hat{\boldsymbol\beta})$ shown in @eq-ssr.

$$
\begin{align}
S(\hat\beta) &  = (y - X \hat\beta )^\top (y - X \hat \beta) \\
& = y^\top y - \hat\beta^\top X^\top y - y^\top X \hat\beta +  \hat\beta^\top X^\top X \hat\beta && (\text{distribute out}) \\
& = y^\top y \ \color{blue}{-  2 \hat\beta^\top X^\top y} \color{black}  +  \underbrace{\hat\beta^\top X^\top X \hat\beta}_{\text{quadratic}} && (\because \color{blue}{- \hat\beta^\top X^\top y - y^\top X \hat\beta = - 2 \hat\beta^\top X^\top y} \color{black})
\end{align}
$$

Now, let us take the gradient to find the first order condition:

$$
\frac{\partial S(\hat\beta)}{\partial \hat\beta} = -2 X^\top y + 2 X^\top X \hat\beta = 0
$$

::: {.callout-note collapse="true" appearance="simple"}
## First Order Conditions for Simple Linear Regression

Currently, we are deriving the first order conditions for multiple linear regression using linear algebra. For simple linear regression (with one explanatory variable), we can use summation notation. Our sum of squared residuals in summation form is:

$$
SSR = S(\hat\beta_0, \hat\beta_1)= \sum\limits_{i=1}^n(Y_i - \hat\beta_0 - \hat\beta_1X_i)^2
$$

We want to minimise the SSR in respect to both $\hat\beta_0$ and $\hat\beta_1$. We can do this by finding our first order conditions:

$$
\begin{align}
\frac{\partial S(\hat\beta_0, \hat\beta_1)}{\partial \hat\beta_0} & = \sum\limits_{i=1}^n(Y_i - \hat\beta_0 - \hat\beta_1X_i) = 0 \\
\frac{\partial S(\hat\beta_0, \hat\beta_1)}{\partial \hat\beta_1} & = \sum\limits_{i=1}^n X_i(Y_i - \hat\beta_0 - \hat\beta_1X_i) = 0 \\
\end{align}
$$ {#eq-firstordersimple}

These conditions create a system of equations, which you can solve for the OLS solutions of $\hat\beta_0$ and $\hat\beta_1$. I will not do that here, since this method is more tedious than the linear algebra method, and can only apply to simple linear regression.

However, while we are not going to solve for our OLS solutions with summations, it is still useful to know these first order conditions, since we will use them in many proofs that show other estimators are the same as OLS because they have the same first order conditions.
:::

When assuming $X^\top X$ is invertable (which is true if $X$ is full rank), we can isolate $\hat\beta$ to find the solution to OLS:

$$
\begin{align}
-2 X^\top y + 2 X^\top X \hat\beta & = 0 \\
2 X^\top X \hat\beta & = 2 X^\top y && ( + 2X^\top y \text{ to both sides}) \\
\hat\beta & = (2X^\top X)^{-1} -2 X^\top y && (\times (2X^\top X)^{-1} \text{ to both sides}) \\
\hat\beta & = (X^\top X)^{-1} X^\top y && (2^{-1}, 2 \text{ cancel out})
\end{align}
$$

Those are our coefficient solutions to OLS. With the estimated parameters $\hat\beta_0, \dots, \hat\beta_p$, we now have a best-fit line/plane, called the **fitted values** $\hat y$. We can also determine our OLS residuals $\hat\eps$:

$$
\begin{align}
\hat y & = X \hat\beta = X(X^\top X)^{-1}X^\top y \\
y & = X \hat\beta + \hat\eps
\end{align}
$$ {#eq-fit}

We will discuss the OLS estimator in far more detail in the [next chapter](quant3.qmd), discussing its properties, and its strengths/weaknesses.

<br />

### Conditional Expectation Function {#conditional-expectation-function}

Previously, we wrote the linear regression model in respect to $Y_i$. However, we can also write the linear regression model as the best linear approximation of a [conditional expectation](quant1.qmd#conditional-distributions) function (CEF) $\E(Y_i |X_i)$:

$$
\E(Y_i|X_i) = \beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip}
$$

Best-approximation is defined by the lowest mean-squared error (MSE). Let us prove OLS on $Y_i$ gets the same $\beta_0, \dots, \beta_p$ as the best linear approximation of $\E(Y_i|X_i)$. Take this very simple CEF and its MSE:

$$
\begin{align}
\E(Y_i|X_i) & = b_0 + b_1X_i \\
MSE & = \E(Y_i - \E(Y_i|X_i))^2 \\
& =  \E(Y_i - (\color{blue}{b_0 + b_1X_i}\color{black}))^2  && (\because \color{blue}{\E(Y_i|X_i) = b_0 + b_1X_i}\color{black})\\
& = \E(Y_i - b_0 - b_1 X_i) && \text{(distribute negative sign)}
\end{align}
$$

The first order conditions are (using chain rule and partial derivatives):

$$
\begin{split}
& \E(Y_i - b_0 - b_1X_i) = 0 \\
& \E(X_i(Y_i - b_0 - b_1X_i) = 0
\end{split}
$$

Now, recall our OLS minimisation conditions (simple linear regression, presented in the extra info box above). Since by definition, average/expectation is $\E(x) = \frac{1}{n} \sum x_i$, we can rewrite as:

$$
\begin{split}
& \sum\limits_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) = \ n \times \E(Y_i - \hat\beta_0 - \hat\beta_1X_i) = 0 \\
& \sum\limits_{i=1}^n X_i (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) = \  n \times \E(X_i(Y_i - \hat\beta_0 - \hat\beta_1X_i)) = 0
\end{split}
$$

And since anything multiplied to a zero turns into zero, we can ignore the $n$ in the first order condition. Thus, we can see, are the exact same minimisation conditions as the conditional expectation function. [Thus, OLS is the best approximation of the conditional expectation function]{.underline}.

This property is very useful for interpreting our regression results. You will hear "expected changes in $Y_i$ given an increase in $X_i$" very frequently when it comes to regression and its applications.

<br />

<br />

------------------------------------------------------------------------

# **Interpretation**

### Interpretation of Parameters

I define $\hat\beta_j \in \{\hat\beta_1, \dots, \hat\beta_p\}$, multiplied to $X_{ij} \in \{X_{i1}, \dots, X_{ip}\}$. $\hat\beta_0$ is the intercept. We assume a continuous $Y_i$ variable - for a binary $Y_i$, see the below section on the [linear probability model](#linear-probability-model).

|               |                                                                                                                                                     |                                                                                                                                                              |
|---------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------|
|               | **Continuous** $X_{ij}$                                                                                                                             | **Binary** $X_{ij}$                                                                                                                                          |
| $\hat\beta_j$ | For every one unit increase in $X_{ij}$, there is an expected $\hat\beta_j$ unit change in $Y_i$, holding all other explanatory variables constant. | There is a $\hat\beta_j$ unit difference in $Y_i$ between category $X_{ij} = 1$ and category $X_{ij} = 0$, holding all other explanatory variables constant. |
| $\hat\beta_0$ | When all explanatory variables equal 0, the expected value of $Y_i$ is $\hat\beta_0$.                                                               | For category $X_{ij} = 0$, the expected value of $Y_i$ is $\hat\beta_0$ (when all other explanatory variables equal 0).                                      |

Sometimes, these interpretations are not useful. For example, if $Y_i$ is *democracy*, what does a 5 unit increase in democracy actually mean? Instead, w can express the change of $Y_i$ and $X_i$ in terms of standard deviations. Or in other words, we want to find the change in $\frac{\hat Y_i}{\sigma_Y}$ for every one standard deviation $\sigma_X$ increase in $X_i$. For simplicity, let us use a simple linear regression $\E(Y_i|X_i) = \beta_0 + \beta_1 X_i$:

$$
\begin{align}
& \E \left(\frac{Y_i}{\sigma_Y} | X_i = x + \sigma_X \right ) - \E \left(\frac{Y_i}{\sigma_Y} | X_i = x \right ) \\
& = \frac{\E(Y_i|X_i = x+ \sigma_X)}{\sigma_Y} - \frac{\E(Y_i|X_i = x)}{\sigma_Y} &&(\text{property of expectation}) \\
& = \frac{\E(Y_i|X_i = x+ \sigma_X) - \E(Y_i|X_i = x)}{\sigma_Y} && (\text{combine into 1 fraction})\\
& = \frac{\beta_0 + \beta_1(x+\sigma_X) - [\beta_0 + \beta_1(x)]}{\sigma_Y} && (\text{plug in regression models})\\
& = \frac{\beta_1\sigma_X}{\sigma_Y} && (\text{cancel and simplify})
\end{align}
$$

Thus, for a one standard deviation $\sigma_X$ increase in $X_{ij}$, there is an expected $\frac{\beta_j\sigma_X}{\sigma_Y}$-standard deviation change in $Y_i$.

<br />

### R-Squared and Other Fit Metrics

Our fitted values equation, shown in @eq-fit, can be rewritten as:

$$
\begin{align}
\hat y = X(X^\top X)^{-1} X^\top y \ = \ \color{blue}{P}\color{black}y && (\because \color{blue}{P := X(X^\top X)^{-1} X^\top})
\end{align}
$$

We can see that $P$ is a matrix that turns $y \rightarrow \hat y$. Matrix $P$ is our linear model that projects the true values $y$ into a space of our regressors $X$. We will not go too into depth here on this, but see the [next chapter](quant3.qmd#ols-orthogonal-projection) for more details.

One thing we might be interested in is how well our model $Py$ explains the actual $y$. One way we can do this is the scalar product: the scalar product $y^\top Py$ describes the shadow the actual $y$ casts on our projected model. However, this value will change based on the scale of our $y$ variable. Thus, we will divide it by $y^\top y$, which is the "maximum" shadow possible (perfect shadow). This ratio is called $R^2$.

$$
R^2 = \frac{y^\top Py}{y^\top y}
$$

We can also reason about $R^2$ in a another way. The total amount of variation in $y$ is called the total sum of squares (SST). The part of $y$ we cannot explain is the Sum of Squared Residuals (SSR) that we minimised for OLS esimtation. That implies that the remaining part SST-SSR is the variation in $y$ that our model explains, called the sum of explained squares (SSE). $R^2$ can be though of the ratio of explained variation in $y$ by our model to the total variation in $y$:

$$
R^2 = \frac{SSE}{SST} = \frac{SST - SSR}{SST} = 1 - \frac{SSR}{SST} = 1 - \frac{\sum (Y_i - \hat Y_i)^2}{\sum(Y_i - \bar Y)^2}
$$

R-Squared ($R^2$) measures the proportion of variation in $y$ that is explained by our explanatory variables. R-Squared is always between 0 and 1 (0%-100%). Higher values indicate our model better explains the variation in $y$.

There are a few other fit metrics, based around the idea of [maximum likelihood estimation](quant3.qmd#maximum-likelihood-estimator) (explained in the next chapter). These metrics include **Akaike Information Criterion (AIC)** and **Bayesian Information Criterion (BIC)**. Lower values indiciate a better model. However, unlike $R^2$, neither has a nice substantive interpretation.

<br />

<br />

------------------------------------------------------------------------

# **Statistical Inference**

### T-Tests

In the last chapter, we discussed the [basics of hypothesis testing](quant1.qmd#intuition-of-hypothesis-testing). In regression, our typical hypotheses are:

-   $H_0 : \beta_j = 0$ (i.e. there is no relationship between $X_{ij}$ and $Y_i$).
-   $H_1:\beta_j ≠ 0$ (i.e. there is a relationship between $X_{ij}$ and $Y_i$).

Using the robust standard errors, we calculate the $t$-statistic, and using the $t$-statistic and a [t-distribution](quant1.qmd#the-t-distribution), we calculate a p-value. Why a t-distribution and not a standard normal distribution? The reason will be explained in the next chapter, but has to deal with the fact we cannot observe the variance of the error term $\eps_i$, so we have to estimate it (with some uncertainty).

::: {.callout-note collapse="true" appearance="simple"}
## Details of Running a Hypothesis Test

First, we calculate the t-test statistic:

$$
t = \frac{\hat\beta_j - H_0}{\widehat{se}(\hat\beta_j)}
$$

-   Where $H_0$ is typically 0, but if you do decide to alter the null hypothesis, you would plug it in.

Now, we consult a t-distribution of $n-k-1$ degrees of freedom. We use a t-distribution because the standard error calculation used in OLS is slightly imprecise.

-   Note: we can only do this step if we believe the central limit theorem is met (that our errors are asymptotically normal). We need a large enough sample size.

We start from the middle of the t-distribution, and move *t-test-statstic* number of standard deviations from both sides of the middle.

Then, we find the probability of getting a t-test statistic even further from the middle than the one we got. The area highlighted in the figure below showcases this. In the figure, the t-test statistic is 2.228.

![](images/clipboard-1533818238.png){fig-align="center" width="60%"}

The area highlighted, divided by the entire area under the curve, is the p-value.
:::

::: {.callout-note collapse="true" appearance="simple"}
## Robust Standard Errors

In modern econometrics, we tend to use **robust** standard errors, not normal standard errors. To understand why, we need to look at homoscedasticity.

[Homoscedasticity]{.underline} is the idea that no matter the values of any explanatory variable, the error term variance is **constant**. If this is false, then we have [heteroscedasticity]{.underline}. An easy way to identify homoscedasticity is to look at a residual plot (just the plot of all $\widehat{\eps_i}$):

![](images/clipboard-1713529842.png){fig-align="center" width="80%"}

Notice how the homoscedasticity residuals seem to have the same up-down variance, no matter the value of $X_i$. The heteroscedasticity residuals have a clear pattern - the up-down variance is smaller when $X_i$ is smaller, and the up-down variance is larger when $X_i$ is larger.

Essentially, if you see a pattern in the residual plot, it is likely heteroscedasticity.

If you have homoscedasticity, you should use normal OLS standard errors. However, it is often hard to prove your data is homoscedastic. Thus, [we generally default to heteroscedasticity-robust standard errors]{.underline}, unless we can prove we have homoscedasticity.

We will discuss homoscedasticity and heteroscedasticity in more detail in the [next chapter](quant3.qmd), as well as derive the standard errors.
:::

The p-value we get is the probability of getting a test statistic equally or more extreme than the one we got, given the null hypothesis is true.

-   If $p<0.05$, we believe the probability of a null hypothesis is low enough, such that we reject the null hypothesis (that there is no relationship between $X_{ij}$ and $Y_i$), and conclude our alternate hypothesis (that there is a relationship between $X_{ij}$ and $Y_i$).

-   If $p > 0.05$, we cannot reject the null hypothesis, and cannot reject there is no relationship between $X_{ij}$ and $Y_i$.

NOTE: this is not causality - we are only looking at the relationship. Causality needs to be established with an adequate research design, which we will explore in later chapters.

<br />

### Confidence Intervals

In the last chapter, we discussed the idea of [confidence intervals](quant1.qmd#confidence-intervals). The 95% confidence intervals of coefficients have the following bounds:

$$
(\hat\beta_j - 1.96 \widehat{se}(\hat\beta_j), \ \ \hat\beta_j + 1.96 \widehat{se}(\hat\beta_j))
$$

-   The 1.96 is an approximation assuming a normal distribution. The actual confidence intervals (calculated by computers) will use a t-distribution of $n-k-1$, which will result in a slightly different multiplicative factor.

The confidence interval means that under repeated sampling and estimating $\hat\beta_j$, 95% of the confidence intervals that we construct will include the true $\beta_j$ value in the population.

If the confidence interval contains 0, we cannot conclude a relationship between $X_{ij}$ and $Y_i$, as 0 is a plausible value of $\beta_j$. These results will always match those of the t-test.

<br />

### F-Tests {#f-tests}

F-tests are used to test more than one coefficient at a time. For example, you might want to test if two variables or coefficients are jointly significant (this will become more clear in the extensions of regression).

Our hypotheses will be:

-   $M_0 : Y_i = \beta_0 + \sum\limits_{j=1}^g \beta_{j} X_{ij} + \eps_i$ (the smaller null model with $g$ variables).
-   $M_a : Y_i = \beta_0 + \sum\limits_{j=1}^g \beta_{j} X_{ij} + \sum\limits_{j=g+1}^p \beta_{j} X_{ij} + \eps_i$ (the bigger model with the original $g$ variables + additional variables up to $p$).

::: {.callout-note collapse="true" appearance="simple"}
## Details of the F-test

F-tests compare the $R^2$ of the two models through the F-statistic:

$$
F = \frac{(SSR_0 - SSR_a) / (k_a - k_0)}{SSR_a /(n - k_a - 1)}
$$

We then consult a F-distribution with $k_a - k_0$ and $n-k_a - 1$ degrees of freedom, obtaining a p-value (in the same way as the t-test).
:::

The p-value we get is the probability of getting a test statistic equally or more extreme than the one we got, given the null hypothesis is true.

-   If $p<0.05$, the we believe the probability of the null hypothesis is low enough, such that we reject the null hypothesis (that $M_0$ is the better model), and conclude our alternate hypothesis (that $M_a$ is a better model). This also means the extra coefficients in $M_a$ are jointly statistically significant.
-   If $p > 0.05$, we cannot reject the null hypothesis, and cannot reject that $M_0$ is a better model. Thus, the extra coefficients in $M_a$ are jointly not statistically significant.

<br />

<br />

------------------------------------------------------------------------

# **Extension: Different Variables**

### Linear Probability Model {#linear-probability-model}

The standard linear model assumes a continuous $Y_i$ variable. However, we can adapt the linear model to fit binary $Y_i$ variables. When $Y_i$ is binary and only has values $Y_i \in \{0, 1\}$, our linear model is actually no longer a predictor of $Y_i$, since our regression will output values that are not 0 and 1.

Instead, our linear model will now predict the **probability** of unit $i$ having $Y_i = 1$. The is due to the [conditional expectation interpretation of regression](#conditional-expectation-function), and the expectation of the [bernoulli distribution](quant1.qmd#bernoulli-and-binomial-distribution):

$$
\begin{align}
\E(Y_i|X_i) & = \underbrace{0 \times \P(Y_i = 0|X_i) \ + \ \P(Y_i = 1|X_i)}_{\text{a weighted avg. formula}} \\
& = \P(Y_i=1|X_i)
\end{align}
$$

Thus, we can rewrite our linear model with the primary outcome being $\P(Y_i = 1|X_i)$. This model is called the **linear probability model**:

$$
\P(Y_i = 1|X_i) = \beta_0 + \beta_1X_{i1} + \dots + \beta_pX_{ip} + \eps_i
$$

Our interpretations of coefficients also slightly change.

|                     |                                                                                                                                                                                                                          |                                                                                                                                                                                                                                  |
|---------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                     | **Continuous** $X_{ij}$                                                                                                                                                                                                  | **Binary** $X_{ij}$                                                                                                                                                                                                              |
| $\hat\beta_j$       | For every one unit increase in $X_{ij}$, there is an expected $\hat\beta_j \times 100$ percentage point change in the probability of a unit being in category $Y_i=1$, holding all other explanatory variables constant. | There is a $\hat\beta_j\times 100$ percentage point difference in the probability of a unit being in category $Y_i=1$ between category $X_{ij} = 1$ and category $X_{ij} = 0$, holding all other explanatory variables constant. |
| $\widehat{\beta_0}$ | When all explanatory variables equal 0, the expected probability of a unit being in category $Y_i=1$ is $\hat\beta_0 \times 100$                                                                                         | For category $X_{ij} = 0$, the expected probability of a unit being in category $Y_i=1$ is $\hat\beta_j \times 100$ (when all other explanatory variables equal 0).                                                              |

Note: we cannot do linear regression with a categorical (more than 2 categories and undordered) $Y_i$. We will need to use a multinomial logistic regression. We can do a linear regression with an ordinal $Y_i$ (more than 2 categories with an order) by pretending that the ordinal $Y_i$ is continuous. We can also is a ordinal logistic regression for that.

<br />

### Categorical Explanatory Variables

Take an explanatory variable $X_i$, which has $g$ number of categories $1, \dots, g$. To include $X_i$ in our regression, we would create $g-1$ dummy (binary) variables, to create the following regression model:

$$
\E(Y_i|X_i) = \beta_0 + \sum\limits_{j=1}^{g-1} \beta_j X_{ij}
$$

-   Categories $1, \dots, g-1$ get there own binary variable $X_{i1}, \dots, X_{ig-1}$.
-   Category $g$ (the reference category) does not get its own variable. We can change which category we wish to be the reference.

Interpretation is as follows (category $j$ is any one of category $1, \dots, g-1$).

-   $\beta_j$ is the difference in expected $Y_i$ between category $j$ and the reference category $g$.
-   $\beta_0$ is the expected $Y_i$ of the reference category $g$.
-   Thus, category $j$ has an expected $Y_i$ of $\beta_0 + \beta_j$.

::: {.callout-note collapse="true" appearance="simple"}
## Example of a Categorical Explanatory Variable

Let us say that $X_i$ is the variable *development level of a country*, with 3 categories: low (L), medium (M), and high (H). $Y_i$ will be the crime rate of the country.

Let us set *low development (L)* as our reference category. Our regression will be:

$$
E(Y_i|X_i) = \beta_0 + \beta_1X_{iM} + \beta_2 X_{iH}
$$

Now let us interpret the coefficients:

-   $\beta_0$ is the expected crime rate for a country of *low (L)* development.
-   $\beta_1$ is the difference in expected crime rate between a *medium (M)* developed country and a *low (L) developed country* (since low is the reference category).
-   $\beta_2$ is the difference in expected crime rate between a *high (H)* developed country and a *low (L) developed country* (since low is the reference category).

The expected/predicted $Y_i$ (crime rate) for each category is:

-   Low (L): $\beta_0$
-   Medium (M): $\beta_0 + \beta_1$
-   High (H): $\beta_0 + \beta_2$.
:::

Each coefficient $\hat\beta_j$'s statistical significance is a difference-in-means significance test, not the significance of the categorical variable as a whole. To find if the entire categorical variable is significant, you should use a [F-test](#f-tests).

<br />

### Interaction Effects

An interaction between two variables means they are multiplied in the regression equation:

$$
Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \color{red}{\beta_3 X_{i1} X_{i2}}
$$

The red is the interaction term. Interpretation of the relationship between $X_{i1}$ and $Y_i$ is:

+-------------------------+---------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+
|                         | **Binary** $X_{i2}$                                                                                                             | **Continuous** $X_{i2}$                                                                                              |
+-------------------------+---------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+
| **Binary** $X_{i1}$     | When $X_{i2} = 0$, the effect of $X_{i1}$ (going from 0 to 1) on $Y_i$ is $\hat\beta_1$.                                        | The effect of $X_{i1}$ (going from 0 to 1) on $Y_i$ is $\hat\beta_1 + \hat\beta_3 X_{i2}$.                           |
|                         |                                                                                                                                 |                                                                                                                      |
|                         | When $X_{i2} = 1$, the effect of $X_{i1}$ (going from 0 to 1) on $Y_i$ is $\hat\beta_1 + \hat\beta_3$.                          |                                                                                                                      |
+-------------------------+---------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+
| **Continuous** $X_{i1}$ | When $X_{i2} = 0$, for every increase in one unit of $X_{i1}$, there is an expected $\widehat{\beta_1}$ unit change in $Y_i$.   | For every increase of one unit in $X_{i1}$, there is an expected $\hat\beta_1 + \hat\beta_3 X_{i2}$ change in $Y_i$. |
|                         |                                                                                                                                 |                                                                                                                      |
|                         | When $X_{i2} = 1$, for every increase in one unit of $X_{i1}$, there is an expected $\hat\beta_1+ \hat\beta_3$ change in $Y_i$. |                                                                                                                      |
+-------------------------+---------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+

::: {.callout-note collapse="true" appearance="simple"}
## Proof of Interpretations of Interactions

We can solve for the change of $X_{i1}$ on $Y_i$ using a partial derivative of $Y_i$ in respect to $X_{i1}$:

$$
\begin{split}
\frac{\partial \hat Y_i}{\partial X_{i1}} & = \frac{\partial}{\partial X_{i1}} \left[ \widehat{\beta_0} + \widehat{\beta_1}X_{i1} + \widehat{\beta_2}X_{i2} + \widehat{\beta_3}X_{i1}X_{i2}\right] \\
\frac{\partial \hat Y_i}{\partial X_{i1}} & = \hat\beta_1 + \hat\beta_3X_{i2}
\end{split}
$$

This gives us the effect of $X_{i1}$ on $Y_i$.
:::

$\hat\beta_0$ is still the expected $Y_i$ when all explanatory variables equal 0.

The coefficient of the interaction $\hat\beta_3$, when statistically significant, indicates a statistically significant interaction effect. If it is not statistically significant, then the interaction effect is not statistically significant (and can be dropped).

<br />

### Polynomial Transformations

Sometimes the relationship between two variables is not a straight line - we can add more flexibility with polynomials. The most common form of polynomial transformation is the quadratic transformation:

$$
Y_i = \beta_0 + \beta_1X_i + \beta_2 X_i^2 + \eps_i
$$

Our estimated $\hat\beta_0$ remains the expected value of $Y_i$ when all explanatory variables equal 0.

Unfortunately, the $\hat\beta_1$ and $\hat\beta_2$ coefficients are not directly interpretable.

-   $\hat\beta_2$'s sign can tell us if the best-fit parabola opens upward or downward.
-   The significance of $\hat\beta_2$ also indicates if the quadratic term is statistically significant. If it is not, we can remove the transformation.

We can interpret two things about the quadratic transformation:

-   For every one unit increase in $X_i$, there is an expected $\hat\beta_1 + 2 \hat\beta_2X_i$ unit increase in $Y_i$.
-   The minimum/maximum point in the best-fit parabola occurs at $X_i = - \hat\beta_1/2 \hat\beta_2$

::: {.callout-note collapse="true" appearance="simple"}
## Proof of Polynomial Interpretations

We can derive the change in $Y_i$ given a one unit increase in $X_i$ by finding the partial derivative of $Y_i$ in respect to $X_i$:

$$
\begin{split}
\frac{\partial \hat Y_i}{\partial X_i} & = \frac{\partial}{\partial X_i} \left[ \hat\beta_0 + \hat\beta_1X_i + \hat\beta_2X_i^2 \right] \\
\frac{\partial \hat Y_i}{\partial X_i} & = \hat\beta_1 + 2 \hat\beta_2X_i
\end{split}
$$

We can also solve for the $X_i$ that results in the minimum/maximum of the best-fit parabola by setting the partial derivative equal to 0:

$$
\begin{split}
0 & = \hat\beta_1 + 2 \hat\beta_2X_i \\
X_i & = - \hat\beta_1/2 \hat\beta_2
\end{split}
$$
:::

We can go beyond quadratic - as long as we always include lower degree terms in our model.

<br />

### Logarithmic Transformations

Logarithmic transformations are often used to change skewed variables into normally distributed variables. These are not as common in political science as compared to economics, but can be useful in certain situations.

::: {.callout-note collapse="true" appearance="simple"}
## Logging a Skewed Variable

Many monetary variables are heavily skewed. Natural logging these variables can turn them into normal distributions. This is useful, since skewed variables tend to have heteroscedasticity, and by making them normal, we can use the smaller normal standard errors.

For example, take this variable called *expenses* with a significant right skew:

![](images/clipboard-4184932163.png){fig-align="center" width="50%"}

If we take the log of this variable, we get the following distribution that is almost normal:

![](images/clipboard-296612325.png){fig-align="center" width="50%"}
:::

We have 3 types of logarithmic transformations:

+--------------+---------------------------------------------+--------------------------------------------------+
|              | $X_i$                                       | $\log (X_i)$                                     |
+--------------+---------------------------------------------+--------------------------------------------------+
| $Y_i$        | Linear Model:                               | Linear-Log Model:                                |
|              |                                             |                                                  |
|              | $Y_i = \beta_0 + \beta_1 X_i + \eps_i$      | $Y_i = \beta_0 + \beta_1 \log X_i + \eps_i$      |
+--------------+---------------------------------------------+--------------------------------------------------+
| $\log (Y_i)$ | Log-Linear Model:                           | Log-Log Model:                                   |
|              |                                             |                                                  |
|              | $\log Y_i = \beta_0 + \beta_1 X_i + \eps_i$ | $\log Y_i = \beta_0 + \beta_1 \log X_i + \eps_i$ |
+--------------+---------------------------------------------+--------------------------------------------------+

<br />

Interpreting the models:

+--------------+----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+
|              | $X_i$                                                                                        | $\log (X_i)$                                                                              |
+--------------+----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+
| $Y_i$        | Linear Model:                                                                                | Linear-Log Model:                                                                         |
|              |                                                                                              |                                                                                           |
|              | When $X_i$ increases by one unit, there is an expected $\hat\beta_1$ unit change in $Y_i$.   | When $x$ increases by 10%, there is an expected $0.096 \hat\beta_1$ unit change in $Y_i$. |
+--------------+----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+
| $\log (Y_i)$ | Log-Linear Model:                                                                            | Log-Log Model:                                                                            |
|              |                                                                                              |                                                                                           |
|              | For every one unit increase in $X_i$, the expected $Y_i$ is multiplied by $e^{\hat\beta_1}$. | Multiplying $X_i$ by $e$ will multiply the expected value of $Y_i$ by $e^{\hat\beta_1}$.  |
+--------------+----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+

::: {.callout-note collapse="true" appearance="simple"}
## Proof of Interpretations for Log Transformations

Proof of Linear-Log Model:

$$
\begin{split}
& \E(Y_i|X_i = x) = \beta_0 + \beta_1 \log x \\
& \E(Y_i | X_i = e^A x) = \beta_0 + \beta_1 \log(e^A x) \\
& = \beta_0 + \beta_1 (\log(e^A) + \log x) \\
& = \beta_0 + \beta_1 (A + \log x) \\
& = \beta_0 + \beta_1A + \beta_1 \log x
\end{split}
$$

$$
\begin{split}
\E(Y_i|X_i = \alpha x) - \E(Y_i|X_i = x) & = \beta_0 + \beta_1 A + \beta_1 \log (x) - (\beta_0 + \beta_1 \log x) \\
& = \beta_1 A
\end{split}
$$

-   When $A = 0.095$, then $e^A = 1.1$. Thus, a 1.1 times increase of $X_i$ results in a $0.095 \widehat{\beta_1}$ change in $Y_i$.

<br />

Proof of Log-Linear Model:

$$
\begin{split}
\E(\log Y_i | X_i = x) =  \log Y_i & = \beta_0 + \beta_1 x \\
Y_i & = e^{\beta_0 + \beta_1 x} \\
Y_i & = e^{\beta_0}e^{\beta_1 x} \\
\E(\log Y_i|X_i = x+1) = \log Y_i & = \beta_0 + \beta_1(x+1) \\
Y_i & = e^{\beta_0 + \beta_1 + \beta_1 x} \\
Y_i & = e^{\beta_0}e^{\beta_1}e^{\beta_1x}
\end{split}
$$

$$
\begin{split}
\frac{\E(\log Y_i|X_i = x+1)}{\E(\log Y_i | X_i = x)} & = \frac{e^{\beta_0}e^{\beta_1}e^{\beta_1x}}{e^{\beta_0}e^{\beta_1x}} \\
& = e^{\beta_1}
\end{split}
$$

-   Thus, when $X_i$ increases by one, there is a multiplicative increase of $e^{\beta_1}$ for $Y_i$.

<br />

Proof of Log-Log model:

$$
\begin{split}
\E(\log Y_i | X_i = x) =  \log Y_i & = \beta_0 + \beta_1 \log x \\
Y_i & = e^{\beta_0 + \beta_1 \log x} \\
Y_i & = e^{\beta_0}e^{\beta_1 \log x} \\
\E(\log Y_i|X_i = ex) = \log Y_i & = \beta_0 + \beta_1 \log (ex) \\
Y_i & = e^{\beta_0 + \beta_1 \log e + \beta_1 \log x} \\
Y_i & = e^{\beta_0}e^{\beta_1}e^{\beta_1 \log x}
\end{split}
$$

$$
\begin{split}
\frac{\E(\log Y_i|X_i = ex)}{\E(\log Y_i | X_i = x)} & = \frac{e^{\beta_0}e^{\beta_1}e^{\beta_1 \log x}}{e^{\beta_0}e^{\beta_1 \log x}} \\
& = e^{\beta_1}
\end{split}
$$

-   Thus, when $X_i$ is multiplied by $e$, there is a multiplicative increase of $e^{\beta_1}$ in $Y_i$.
:::

<br />

<br />

------------------------------------------------------------------------

# **Extension: Hiearchical Data**

### Clustered and Panel Data

**Hierarchical data** is data where the basic units of analysis $i$ are clustered, grouped, or nested into clusters.

For example, let us say we are measuring how income affects voter turnout in european countries. We have observations from France, Switzerland, Germany, and many other countries. However, these observations can be grouped by the country they came from.

Why is this grouping important? This is because there may be something in common between observations within the same cluster. For example, Switzerland might just have higher voter turnout in general due to something about Swiss institutions or culture.

This means that observations aren't random - i.e. we know that if we select from switzerland, it is likely to have higher turnout - observations from the same country are correlated. Thus, we need some way to account for this clustering of observations. We will explore this below.

**Panel data** is data that can be clustered in two ways - by unit, and by time. For example, let us say we have a dataset on all countries and their GDP between 1960-2020.

We will have clusters based on country: Germany will have an observation in 1960, in 1961, ..., to 2020. Same for every other country. These observations are grouped by the unit (country in this case).

We will also have clusters based on time: We will have all GDP observations for all countries in 1960, in 1961, etc. These observations are grouped by the time (year in this case).

<br />

### Fixed Effects

When we have hierarchical or panel data, we need to control for differences between clusters. We essentially include the cluster variable as a categorical variable in our regression. Let us say we have $m$ number of clusters $i = 1, \dots, m$. Within each cluster, we will have units $t = 1, \dots, n$. Our model takes the form:

$$
\begin{split}
Y_{it} & = \alpha_i + \sum\limits_{j=1}^p\beta_jX_{itj} + \eps_{it} \\
& \text{where } \alpha_i = \beta_{00} + \underbrace{\beta_{02}D_{i2} + \beta_{03}D_{i3} + \dots + \beta_{0m}D_{im}}_{\text{unique intercepts for each cluster}}
\end{split}
$$

-   Where $Y_{it}$ is the value for the $i$th observation within the $t$th cluster.
-   Where $D_{i2}, D_{i3}, \dots, D_{im}$ are dummy variables for clusters $2, \dots, m$. Cluster 1 is the reference category.

We essentially add a unique intercept term for every cluster, accounting for the average differences in $Y_{it}$ between each category. $\beta_{00}$ is the intercept for the reference category 1. $\beta_{00} + \beta_{0i}$ is the intercept for the $i$th category.

For panel data, we use **two-way fixed effects**, which is basically just two fixed effects for different clustering. Let us say we have $i = 1, \dots, m$ units with $t = 1, \dots, n$ different numbers of time periods. Our two way fixed effects model takes the form:

$$
\begin{split}
Y_{it} & = \alpha_i + \gamma_t + \sum\limits_{j=1}^p\beta_jX_{itj} + \eps_{it} \\
& \text{where } \alpha_i =  \alpha_{00} + \underbrace{\alpha_{02}D_{2t} + \alpha_{03}D_{3t} + \dots + \alpha_{0m}D_{mt}}_{\text{unique intercepts for each unit}}, \quad \gamma_t =  \gamma_{00} + \underbrace{\gamma_{02}T_{i2} + \dots + \gamma_{0n}T_{in}}_{\text{unique intercepts for each time}} \\
\end{split}
$$

-   Where $Y_{it}$ is the value of the $i$th unit at time period $t$.
-   Where $D_{i2}, D_{i3}, \dots, D_{im}$ are dummy variables for units $2, \dots, m$., and $T_{i2}, T_{i3}, \dots, T_{in}$ are dummy variables for time periods $2, \dots, n$.

For two-way fixed effects, we add a unique intercept term for every year and country, accounting for the average differences in $Y_{it}$ between each country, and the average differences in $Y_{it}$ between each year.

Note: when conducting statistical inference with fixed-effects models, we must adjust our standard errors to clustered-standard errors. This is because observatios within a cluster are not independent of each other.

<br />

<br />

------------------------------------------------------------------------

# **Extension: Spatial Data**

### Spatial Data and Spatial Weights

Spatial data is data that has some geographic-spatial component. If we have spatial data, a region's neighbours may have some effect on a region's own outcomes:

1.  **Lag y**: The $Y_i$ of a region might be affected by a neighbouring region's $Y_i$ values. For example, one state having a high crime rate may affect another nearby state's crime rate as criminals can move across the border.
2.  **Lag X**: The $Y_i$ of a region might be affected by a neighbour's $X_i$ values. For example, one state's low vaccination rate could affect the prevalence of a disease in a nearby state.
3.  **Spatial Autocorrelation**: The residuals $\eps_i$ of a region might be affected by the residuals in a neigbouring region. For example, perhaps if we are studying how minimum wage affects unemployment rates, there could be some common factor with nearby states that affects unemployment rates (for example, prevalence of a certain industry) which is not included in our model.

To include spatial considerations, we include a spatial weights matrix $\mathbf W$. This defines what regions are "neighbours". For example, imagine 3 regions A, B, C, with A on the left, B in the middle, and C on the right. The spatial weights matrix would be a matrix with rows $i = A, B ,C$ and columns $j = A, B, C$.Element $w_{ij}=1$ if regions $i$ and $j$ border each other, and $w_{ij} = 0$ if regions $i$ and $j$ do not touch each other:

$$
W_{3 \times 3} = \begin{pmatrix}
0 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 0
\end{pmatrix}
$$

We often "normalise/standardise" our weights matrix so that each row adds up to 1. Essentially, we divide an element by the number of 1's in each row:

$$
W_{3 \times 3} = \begin{pmatrix}
0 & 1 & 0 \\
0.5 & 0 & 0,5 \\
0 & 1 & 0
\end{pmatrix}
$$

There are other ways to define neighbours $w_{ij} = 1$. For example, we could consider any region who's centroid is within $x$ miles of another state's centroid as a neighbour. This is up to the modelers discretion.

<br />

### Spatial Regression

We define neighbours with a weights matrix $\mathbf W$. The largest spatial model is the **Manski Model**, which incorporates lag y, lag X, and spatial autocorrelation.

$$
y = \underbrace{\rho Wy}_{\mathrm{lag \ y}} + X\beta + \underbrace{WX\theta}_{\mathrm{lag \ x}} + \underbrace{\lambda Wu}_{\mathrm{autocor.}} + \epsilon
$$

-   $\rho$ (scalar) is the lag y coefficient, and measures how an average of neighbouring region's $Y_i$ values are associated with a region's own $Y_i$ values. For every one unit increase in the average value of $Y_i$ in neighbouring regions, there is an expected $\rho$ unit change in $Y_i$ in the region, holding all else constant.
-   $\beta$ (vector) is our normal OLS coefficients $\beta_0, \dots, \beta_p$, and would be interpreted in the same way.
-   $\theta$ (vector) is the lag X coefficients $\theta_0, \dots, \theta_p$, and measures how an average of neighbouring region's $X_i$ values are associated with a region's own $Y_i$ values. For every one unit increase in the average $X_{ij}$ of neighbouring regions, there is an expected $\theta_j$ unit change in $Y_i$ in the region, holding all else constant.
-   $\lambda$ (scalar) is spatial autocorrelation coefficient, and measures how an average of neighbouring regions' error terms are associated with a region's own error term. Unobserved factors shared by neighbouring regions contribute to a $\lambda$-unit change in $Y_i$, holding all else constant.

However, in reality, we often cannot estimate this entire model due to the large amount of parameters. Thus, multiple variations exist with only parts of the full model:

1.  Keleijian-Prucha Model: $y = \rho W y + X\beta + \lambda W u + \epsilon$.
2.  Spatial Durbin Model: $y = \rho W y + X\beta + WX\theta + \epsilon$.
3.  Spatially Lagged X Model: $y = X\beta + WX\theta + \epsilon$.
4.  Spatial Lag Model (Spatial Autoregressive): $y = \rho W y + X\beta + \epsilon$.
5.  Spatial Error Model: $y = X\beta + \lambda W u + \epsilon$.

The models are estimated with [Maximum Likelihood Estimation](quant3.qmd#maximum-likelihood-estimator) (see next chapter), and model selection can be done with a combination of theory regarding the topic, eliminating insignificant portions, and AIC and BIC metrics.

<br />

<br />

------------------------------------------------------------------------

# **Implementation in R**

You will need package *fixest* and *estimatr*.

```{r, eval = FALSE}
library(fixest)
library(estimatr)
```

Regression with normal standard errors can be done with the *lm()* function:

```{r, eval = FALSE}
model <- lm(y ~ x1 + x2 + x3, data = mydata)
summary(model)
```

Regression with robust standard errors can be done with the *feols()* function or *lm_robust()* function:

```{r, eval = FALSE}
# feols
model <- feols(y ~ x1 + x2 + x3, data = mydata, se = "hetero")
summary(model)

# lm robust
model <- lm_robust(y ~ x1 + x2 + x3, data = mydata)
```

Output will include coefficients, standard errors, p-values, and more.

::: {.callout-note collapse="true" appearance="simple"}
## Binary and Categorical Variables

You can include binary and categorical variables by using the *as.factor()* function:

```{r, eval = FALSE}
feols(y ~ x1 + as.factor(x2) + x3, data = mydata, se = "hetero")
```

You can do the same for $y$ or $x$. Just remember, $y$ cannot be a categorical variable (use multinomial logsitic regression instead).
:::

::: {.callout-note collapse="true" appearance="simple"}
## Fixed Effects

You can include one-way fixed effects by adding a \| after your regression formula in *feols()*:

```{r, eval = FALSE}
model <- feols(y ~ x1 + x2 + x3 | cluster,
               data = mydata, se = "hetero")
summary(model)
```

You can add two-way fixed effects as follows:

```{r, eval = FALSE}
model <- feols(y ~ x1 + x2 + x3 | unit + year,
               data = mydata, se = "hetero")
summary(model)
```
:::

::: {.callout-note collapse="true" appearance="simple"}
## Interaction Effects

Two interact two variables, use \* between them. This will automatically include both the interaction term, and the two variables by themselves.

```{r, eval = FALSE}
feols(y ~ x1 + x2*x3, data = mydata, se = "hetero")
```

If for some reason, you only want the interaction term, but not the variables by themselves, you can use a colon : between the two variables:

```{r, eval = FALSE}
feols(y ~ x1 + x2:x3, data = mydata, se = "hetero")
```
:::

::: {.callout-note collapse="true" appearance="simple"}
## Polynomial Transformations

To conduct a polynomial transformation, you can use the *I()* function. The second argument is the degree of the polynomial:

```{r, eval = FALSE}
feols(y ~ x1 + I(x2, 3), data = mydata, se = "hetero") #cubic for x2
```
:::

::: {.callout-note collapse="true" appearance="simple"}
## Logarithmic Transformations

The best way to do a logarithmic transformation is to create a new variable that is the log of the variable you want to transform using the *log()* function, before you even start the regression:

```{r, eval = FALSE}
mydata$x1_log <- log(mydata$x1)
```
:::

::: {.callout-note collapse="true" appearance="simple"}
## Confidence Intervals

To find the confidence intervals for coefficients, first estimate the model with *lm()* or *feols()* as shown previously, then use the *confint()* command:

```{r, eval = FALSE}
confint(model)
```
:::

::: {.callout-note collapse="true" appearance="simple"}
## F-Tests

To run a f-test, use the *anova()* command, and input your two different models, with the null model going first.

```{r, eval = FALSE}
anova(model1, model2)
```

Note: F-tests only work with models that are run with homoscedastic standard errors. Robust standard errors will not work.
:::

::: {.callout-note collapse="true" appearance="simple"}
## LaTeX Regression Tables

You can use the *texreg* package to make nice regression tables automatically.

```{r, eval = FALSE}
library(texreg)
```

The syntax for *texreg()* is as follows:

```{r, eval = FALSE}
texreg(l = list(model1, model2, model3),
       custom.model.names = c("model 1", "model 2", "model 3"),
       custom.coef.names = c("intercept", "x1", "x2"),
       digits = 3)
```

You can replace *texreg()* with *screenreg()* if you want a nicer regression table in the R-console.

Note: you must have the same amount of model names as total models in your texreg, and you must have the same amount of coeficient names as the total amount of coefficients in all of your models.
:::

::: {.callout-note collapse="true" appearance="simple"}
## Prediction

We can use the *predict()* function to generate fitted value predictions in R:

```{r, eval = FALSE}
my_predictions <- predict(model, newdata = my_new_data)
```

*my_new_data* is a dataframe with a bunch of explanatory variable values (for every explanatory variable) for a collection of observations, that you wish to predict $\hat y$ for.
:::
