---
title: "Unconstrained Optimisation"
format:
    html:
        theme: darkly
        max-width: 800px
        fontsize: 12pt
subtitle: "Lesson 3.2, Maths for Political Science"
---

[Course Homepage](https://politicalscience.github.io/math)

## Table of Contents {#contents}

1.  [Optimisation in Decision Theory](#decision)
2.  [Optimisation in Linear Models](#stats)

------------------------------------------------------------------------

[Table of Contents](#contents) \| [Course Homepage](https://politicalscience.github.io/math)

# Optimisation in Decision Theory {#decision}

#### Unconstrained Optimisation

Unconstrained optimisation is to find the minima/maxima of a function in order to optimise it. This can take the form of minimising errors, or maxmimising utility, and many other forms.

An optimisation problem is unconstrained if there are no constraints on the inputs. We will discusss constrained optimisation in the following lectures.

<br />

#### Decision Theory

The point of decision theory is to maximise your payoff/utility - essentially, make the decision that gets you the most benefit.

To do this, we need to maximise utility, or in other words, find the maximum of the $U(x)$ utility function.

<br />

Let us imagine this hypothetical situation:

-   You want to contribute to some political campaign, because you might get some benefit (whether it be feeling good because you expressed yourself, or you expect the campaign will give you something in return).

-   Let us assume you get twice the utility/payoff $U$ of the contribution you put int $x$: $U(x)=2x$

-   But, you also have other uses for your money. You could instead spend you money on some other stuff.

-   Let us say the cost of using the money on the campaign is $-\frac{1}{2}x^2$ , so we have to subtract this from our utility.

Thus, our utility function for the scenario is:

$$
U(x) = 2x - \frac{1}{2}x^2
$$

<br />

We have already discussed finding [maxima and minima](https://politicalscience.github.io/math/3-1.html) in the previous lecture. Let us apply what we previously learned here and find the maximum.

<br />

First, let us find the 1st derivative of $U(x)$:

$$
U(x) = 2x - \frac{1}{2}x^2
$$

$$
U'(x) = \frac{d}{dx} [2x] + \frac{d}{dx} \left[ - \frac{1}{2} x^2 \right]
$$

$$
U'(x) = 2 - x
$$

<br />

Now, let us find the critical points of $U(x)$ by setting $U'(x) = 0$:

$$
0 = 2-x
$$

$$
x=2
$$

We have a critical point $x^*$ at $x=2$

<br />

Now, let us find the second derivative of $U(x)$

$$
U''(x) = \frac{d}{dx} [2] + \frac{d}{dx} [-x]
$$

$$
U''(x) = -1
$$

<br />

Remember, that a Local Maximum exists at $x_0$ if $f'(x_0) = 0$ and $f''(x_0) < 0$

-   Both these conditions are met.

So we know we have a local maximum at $x=2$

<br />

Because $U''(x)$ is a negative constant, we know it is globally concave. Thus, our only critical point at $x=2$, is also a global maximum.

<br />

------------------------------------------------------------------------

[Table of Contents](#contents) \| [Course Homepage](https://politicalscience.github.io/math)

# Optimisation in Linear Models {#stats}

Optimisation is also frequently used in statistics.

Let us imagine a linear relationship between education ( $x$ ) and voter turnout ( $y$ ), where a one unit increase of education $x$ led to a $\beta$ increase of voter turnout $y$:

$$
y = \beta x
$$

<br />

However, this isn't a deterministic model - it is a prediction model. No prediction model is perfectly accurate (especially not a linear model, as almost nothing is perfectly linear). Thus, we need to add an error term $\epsilon$ to our model:

$$
y = \beta x + \epsilon
$$

The error $\epsilon$ is basically everything else that explains turnout $y$, that is not accounted for by a change in education $x$.

-   This could be another variable that is missing from the model

-   Or, this could simply just be randomness - there is always an element of randomness, even if we include every single possible predictor variable.

<br />

What do we want out of this model? Well, we want to find the $\beta$ that best predicts $y$ from an input $x$.

How do we do this? We find the $\beta$ that minimizes the squared error $\epsilon^2$. Minimisation is a form of optimisation, turning this into an optimisation problem.

-   Why squared error $\epsilon^2$? We will cover this more in depth in the Regression Analysis course, but essentially, squaring the error removes the different between negative and positive errors, as we are concerned with the magnitude of error, not direction.

<br />

How do we minimise squared error $\epsilon^2$?

We could first start by solving our linear model for $\epsilon^2$

$$
y = \beta x + \epsilon
$$

$$
\epsilon = y - \beta x
$$

$$
\epsilon^2 = (y - \beta x)^2
$$

<br />

Let us call the above, a function of $\beta$:

$$
f(\beta) = (y-\beta x)^2
$$

<br />

How do we minimise this function? We do the same as we did in the Decision Theory section, and the previous lecture on Minima and Maxima.

Let us find the first derivative $f'(\beta)$. We will need the chain rule: $[(f \circ g)(x)]'=f'[g(x)]*g'(x)$.

$$
f(\beta) = (y - \beta x)^2
$$

<br />

Let us make $f(\beta) = \beta^2$, and $g(\beta) = y - \beta x$. Then we can find $f'(x)$ and $g'(x)$ in respect to $\beta$:

$$
f'(\beta) = 2\beta, \space g'(\beta) = -x 
$$

<br />

Now, plug back into chain rule $[(f \circ g)(x)]'=f'[g(x)]*g'(x)$

$$
f'(\beta) = 2(y-\beta x)*-x
$$

$$
f'(\beta) = -2x(y-\beta x)
$$

<br />

Now, let us set $f'(\beta) = 0$ to find at what value of $\beta$ the critical points are:

$$
0 = -2x(y-\beta x)
$$

$$
0 = -2xy + 2x^2 \beta
$$

$$
2x^2 \beta = 2xy
$$

$$
\beta = \frac{2xy}{2x^2}
$$

$$
\beta = \frac{xy}{x^2}
$$

So, we have a critical point at $\beta = \frac{xy}{x^2}$

<br />

Now, let us do the second derivative test:

$$
f''(\beta) = \frac{d}{d \beta} [-2x(y-\beta x)
$$

$$
f''(\beta) = \frac{d}{d \beta}[-2xy + 2x^2 \beta]
$$

$$
f''(\beta) = 0 + 2x^2
$$

$$
f''(\beta) = 2x^2
$$

<br />

$f''(\beta) = 2x^2$ is always positive, so we know the function is convex. Thus, we know we have a local minimum at $\beta = \frac{xy}{x^2}$, and that is also a global minimum.

<br />

Note: In a real linear regression, you will be working with a set of different $X$ inputs, and a set of different $Y$ outputs.

-   $X$ and $Y$ take the form of vectors, which we will cover later in the course.

However, this minimisation actually is quite close to the actual minimised value of $\beta$ as the one you would get with vectors.

<br />

------------------------------------------------------------------------

[Table of Contents](#contents) \| [Course Homepage](https://politicalscience.github.io/math)