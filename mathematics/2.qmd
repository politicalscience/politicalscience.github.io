---
title: "Single Variable Calculus"
subtitle: "Module 2, Mathematics for Political Science"
format:
    html:
        theme: sketchy
        max-width: 800px
        toc: TRUE
        toc-depth: 3
        toc-location: left
        toc-title: Chapters
        toc-expand: TRUE
---

This module will discuss key topics in Calculus, which many methods, including statistics and game theory, build on. We start off derivatives, what they are, and how to find them. Then, we discuss how to use derivatives for optimisation - an important concept in most methods. Finally, we introduce integration - a key part of statistics and probability theory

[Prerequisites]{.underline}: Module 1: Topics in Algebra

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/#mathematics-for-political-science)

# Chapter 1: Derivatives

### 1.1: Definition of Derivatives

A derivative function $f'(x)$, has the value at $x$, which is equal to the rate of change of $f(x)$ at $x$

-   Basically, [derivatives measure the "slope"]{.underline} of any point of $x$ on $f(x)$

-   For linear functions, $f'(x)$ is a constant value - since linear functions have a constant slope

-   For curved functions, $f'(x)$ is not constant - since the slope is not constant. $f'(x)$ is thus equal to the slope of the line [tangent]{.underline} to the curve at point $x$

<br />

How do we normally measure slope?

-   Well, we do change in $y$ over change in $x$, which more mathematically, is $(y_1 - y) / (x_1 - x)$

-   We can rewrite this in function form by replacing $y$ with $f(x)$ as follows: $[f(x_1) - f(x)] / [x_1 - x]$

-   We can further rewrite $x_1$: what is $x_1$ conceptually? It is a point that is some distance from $x$.

    -   We can represent that distance between $x_1$ and $x$ as $h$

    -   Thus, $x_1 = x + h$

-   Thus, finally rewriting, we get the slope is $[f(x+h)-f(x)] / [(x+h) - x ]$

-   We can simplify the denominator so we get: $[f(x+h)-f(x)] / [h ]$

<br />

However, the normal way of finding slope involves finding two different points, and the slope between them

-   However, a derivative is the slope at one specific point, not between 2

So how do we do this?

-   To find the slope at one specific point, we can slowly reduce the size of $h$ (the $x$ distance between the two points), until it reaches $0$

-   Remember limits from *Module 1: Topics in Algebra*?

-   Thus, we can make $h$ approach $0$ by doing the following limit: $\lim\limits_{h \rightarrow 0}$.

-   This will tell us the value of the slope between two points, where the distance between two points gets increasingly small, all the way until the two points merge into one, which is the derivative!

<br />

So, let us apply this limit of $h$ as it approaches $0$ to our slope formula we found above:

$$
f'(x) = \lim\limits_{h \rightarrow 0 } \frac{f(x+h)-f(x)}{h}
$$

[That is the formal definition of a derivative]{.underline}

-   If we want to find any possible derivative, we can use that function (although, sometimes, it will be very difficult algebraically)

<br />

So far, I have used apostrophes (ex. $f'(x)$ ) to represent derivatives of functions.

-   We can also use the form $\frac{dy}{dx}$ to express derivatives

    -   This literally means - the change in $y$ over the change in $x$ - we can substitute $y$ and $x$ if we are using different variables

-   Sometimes, we want to take the derivative of a derivative (this will become clear later)

    -   We can notate this with an additional apostrophe: $f''(x)$

<br />

### 1.2: Calculating Derivatives

Like I said above, you can calculate the derivative of any function by plugging it in to the definition of a derivative

-   However, this can be very difficult algebraically due to the complexity of some functions

<br />

Luckily for us, there are a few generalised forms of functions, whose derivatives are consistent when applied to the definition of a derivative.

These rules allow us to quickly calculate derivatives. The most important are underlined:

-   [Power Rule]{.underline}: $[x^k]' = kx^{k-1}$

-   [Sum Rule]{.underline}: $[f(x) ± g(x)]' = f'(x) ± g'(x)$

-   [Constant Rule]{.underline}: $[\alpha f(x)]' = \alpha f'(x)$

-   Product Rule: $[f(x)g(x)]' = f'(x)g(x) + f(x)g'(x)$

-   Quotient rule: $[f(x)/g(x)] = [f'(x)g(x) + f(x)g'(x)]/[g(x)]^2$

-   [Linear Rule]{.underline}: $[mx]' = m$, where $m$ is a constant

-   [Derivative of a constant]{.underline}: $[c]' = 0$, where $c$ is a constant

-   Chain Rule: $[f(g(x))]' = f'[g(x)] \times g'(x)$

-   Exponential rule: $[e^x]' = e^x$

-   Exponential Nested Rule: $[e^{u(x)}]' = e^{u(x)} \times u'(x)$

-   Non-base $e$ Exponential: $[b^x]' = \ln (b) \times b^x$

-   Natural Log Derivative: $[ \ln (x)]' = 1/x$

-   Log power derivative: $[ \ln(x^k)]' = k/x$

-   Nested Log Derivative: $[ \ln (u(x)) ]' = u'(x) / u(x)$

<br />

### 1.3: Examples of Derivatives

**Example 1 (Sum, Product, Constant Rules):**

Find $f'(x)$ if $f(x) = 5x^4 - 6x^3 + x^2 - 5x + 6$

-   1st, split up the function using sum rule: $f'(x) = [5x^4]' - [6x^3]' + [x^2]' - [5x]' + [6]'$

-   2nd, use linear rule and derivative of a constant on the final 2 sections: $[5x]' = 5, [6]' = 0$

-   After that, we have: $f'(x) = [5x^4]' - [6x^3]' + [x^2]' - 5 + 0$

-   3rd, use constant rule to simplify: $f'(x) = 5[x^4]' - 6[x^3]' + [x^2]' - 5$

-   4th, use chain rule: $f'(x) = 5(4x^3) - 6(3x^2) + (2x) - 5$

-   5th, multiply out: $f'(x) = 20x^3 - 18x^2 + 2x - 5$

<br />

**Example 2 (Product Rule):**

Find $f'(x)$ if $f(x) = (x^3)(2x^4)$

-   We know the product rule says: $[f(x)g(x)]' = f'(x)g(x) + f(x)g'(x)$\$

    -   Thus, in our equation, $f(x) = x^3, g(x) = 2x^4$

-   Then, find the derivative of both parts

    -   $f'(x) = 3x^2$ (Power rule)

    -   $g'(x) = 2[x^4]' = 2(4x^3) = 8x^3$ (constant rule, power rule)

-   Now, put it into the product rule: $f'(x) = (3x^2)(2x^4)+(x^3)(8x^3)$

-   Simplify by multiplying and exponent rules: $f'(x) = 6x^6 + 8x^6 = 14x^6$

<br />

**Example 3 (Chain Rule, Power Rule, Sum Rule):**

Find $f'(x)$ if $f(x) = (3x^2 + 5x - 7)^6$

-   This is a nested composite function, so we use chain rule: $[f(g(x))]' = f'[g(x)] \times g'(x)$

-   Identify the parts that of our initial function that fit the chain rule parts:

    -   So in the chain rule format, $f(x) = x^6, g(x) = 3x^2 + 5x - 7$

-   Now, find the derivatives of both parts:

    -   $f'(x) = 6x^5$ (power rule)

    -   $g'(x) = 3[x^2]' + [5x]' - [7]' = 6x+5$

-   Now, let us fit it into the chain rule format: $f'(x) = 6(3x^2 + 5x - 7)^5 \times (6x+5)$

-   Distribute out $6x+5$: $f'(x) = 36x(3x^2+5x-7)^5+30(3x^2+5x-7)^4$

<br />

------------------------------------------------------------------------

# Chapter 2: Partial Derivatives

### 2.1: Partial Derivatives

<br />

### 2.2: Application of Partial Derivatives

<br />

------------------------------------------------------------------------

# Chapter 3: Optimisation

### 3.1: Slope and Concavity

In the previous chapter, we discussed how derivatives are the rate of change of a function.

From this we know the following:

-   If $f'(x) < 0$, then $f(x)$ is decreasing

-   If $f'(x) > 0$, then $f(x)$ is increasing

-   If $f'(x) = 0$, then $f(x)$ is neither increasing or decreasing

<br />

[Concavity is whether a curve opens upward or downward]{.underline}

-   A function is concave up when it opens upward (like $y=x^2$ )

-   A function is concave down when it opens downward (like $y = -x^2$)

But, what makes a function open upward or downward?

-   It is actually the rate of change, of the rate of change, of the function

-   Or in other words, the derivative of the derivative, or also called the second order derivative

<br />

Thus, a [based on the second order derivative of a function, we know if it is concave up or down]{.underline}:

-   If $f''(x) < 0$, then $f(x)$ is concave down

-   If $f''(x) > 0$, then $f(x)$ is concave up

-   If $f''(x) = 0$, then $f(x)$ is neither concave up or down

<br />

Thus, given this, we can find the concavity of a function based on its second derivative

-   For example, take $f(x) = x^2$

-   The first derivative is $f'(x) = 2x$ (power rule).

-   The second order derivative is $f''(x) = 2$

-   $2>0$, so we know the function is concave up

-   This makes sense - we know the graph of $f(x) = x^2$ opens upward

<br />

### 3.2: Finding Mininums and Maximums

A minimum and maximum value of a function is just what it sounds like - at what point does the function output the highest or lowest value?

-   There can be two types of minimums/maximums: global and local

-   Global minimums/maximums are the minimum/maximum point of the entire function's possible inputs

-   Local minimums/maximums are like the local peaks/valleys of a function - they might not be the highest/lowest point in the entire function, but they are within their neighbourhood

Why is this important? Well often in political science, we are either trying to maximise utility functions, or minimise error in our models

<br />

How do we find minimums and maximums?

1.  We have to find $f'(x)$, and what value of $x$ makes $f'(x) = 0$
    -   So basically, we find the derivative, and solve for $x$ when we set the derivative equal to $0$
    -   Why? Well, think about maximums and minimums - they are when the slope of a function reverses - either positive $\rightarrow$ negative, or negative $\rightarrow$ positive
    -   Thus, $f'(x) = 0$ is the point when they reach their minimum/maximum
2.  Then, we have to find the value of $f''(x)$ at the $x$ value we found in step one
    -   If $f''(x) > 0$ (or concave up), we have a minimum
    -   If $f''(x) < 0$ (or concave down), we have a maximum
    -   Why? well if a function is concave down (opening downwards), then clearly, there is a maximum point. Vice versa

<br />

Often times, we will have to find a minimum and maximum over an [interval]{.underline}

-   We do the same process as above

-   HOWEVER, [we also have to check the $f(x)$ value outputted at our interval boundaries]{.underline}

    -   Why? let us take the function $y=x$, or any other function that goes towards infinity or negative infinity, as $x$ goes towards infinity or negative infinity

    -   These cases, we are often only finding the local minimum/maximums, as the global ones are infinity

    -   Often times, depending on the interval, the edge (where the function is headed towards infinity/negative infinity) is the actual maximum/minimum in our interval

<br />

**Example:**

Find the maximum value of the function $f(x) = 2x^3 - 9x^2 + 12x$ on the interval $[0,3]$

-   1st, find the first derivative: $f'(x) = 6x^2 - 18x + 12$ (power, sum, constant rule)

-   2nd, find when $f'(x) = 0$

    -   So we have to find $0 = 6x^2 - 18x + 12$

    -   We can first take out common factor 6: $0 = 6(x^2 - 3 + 2)$

    -   Now factor: $0 = 6(x-2)(x-1)$

    -   So the two $x$ values which make $f(x) = 0$ are $x=2, x=1$

-   3rd, find the second derivative: $f''(x) = 12x-18$ (power, sum, constant rule)

-   4th, plug in our $x$ values we found that make $f'(x) = x$

    -   When $x=1$, $f''(1) = 12(1)-18 = -6$

    -   When $x=2$, $f''(2) = 12(2) - 18 = 6$

-   A maximum is when $f''(x) < 0$, so only $x=1$ is a maximum

-   Now, plug in $x=1$, as well as the edge cases $x=0, x=3$

    -   $f(0) = 0, f(1) = 5, f(3) = 9$

-   Thus, the edge case $x=3$ where $f(x) = 9$ is our maximum

<br />

### 3.3: Optimisation in Linear Models

Finding minimum and maximum values is often called [optimisation]{.underline}. This is frequently used in Game Theory and Statistics.

For example, in statistics, we often want to create a linear model that best represents our data

-   The linear model takes the form $y = \beta x$

-   However, in social science, no relationship is a perfect, one to one correlation. There is always some variation/randomness

-   Thus, we need to add an error term: $y = \beta x + \epsilon$

<br />

To make the best linear prediction, we need to find a line with some value $\beta$ based on our data, that minimises the squared error $\epsilon^2$

-   Why squared error? Well, we will cover this more in depth in the module on Regression Analysis, but essentially, squaring the error removes the difference between negative and positive errors - after all, we are considered with the magnitude of error, not direction

<br />

How do we minimise squared error $\epsilon^2$?

-   1st, let us solve for $\epsilon^2$ in our linear model

    -   Our linear model is $y = \beta x + \epsilon$

    -   Now, isolate $\epsilon$: $\epsilon = y - \beta x$

    -   Now, square both sides: $\epsilon ^2 = (y- \beta x)^2$

-   Now, let us call the above function $\epsilon^2 = f(\beta) = (y-\beta x)^2$. We want to minimise this function (and thus, the squared error)

    -   How do we minimise? follow the same steps as previously discussed!

-   Let us find the first derivative of $f'(\beta)$. We will need the chain rule:

    -   Remember, chain rule says $[f(g(x))]' = f'[g(x)] \times g'(x)$

    -   Let us identify the parts of $f'(\beta)$ that fit the chain rule: Let us make $f(\beta) = \beta^2$, and $g(\beta) = y - \beta x$

    -   Now, find $f'(x), g'(x)$: $f'(\beta) = 2 \beta, g'(\beta) = -x$

        -   Remember, we are finding the derivative in respect to $\beta$, so $-x$ is just a constant for our purposes

    -   Now, plug into chain rule: $f'(\beta) = 2(y-\beta x) \times -x$

    -   Simplify: $f'(\beta) = -2x(y - \beta x)$

-   Now, set $f'(\beta) = 0$ to find $\beta$ values that might be minimums

    -   So we solve for $\beta$ given: $0 = -2x(y-\beta x)$

    -   First, multiply out: $0 = -2xy + 2x^2 \beta)$

    -   Isolate $\beta$ on one side: $2x^2 \beta = 2xy$

    -   Divide both sides by $2x^2$: $\beta = 2xy / 2x^2$

    -   Simplify: $\beta = xy/x^2$

-   So, we know $f''(\beta)=0$ at $\beta = xy/x^2$. Now, let us take the second derivative of $f(\beta)$ to check if this is a minimum:

    -   Previously, we found $f'(\beta) = -2x(y-\beta x)$

    -   Multiply out: $f'(\beta) = -2xy - 2x^2 \beta$

    -   Find the derivative of $f'(\beta)$ in respect to $\beta$: $f''(\beta) = 0 + 2x^2$

        -   Remember, we are finding the derivative in respect to $\beta$, so $x$ and $y$ are just constants for our purposes

    -   Now we know $f''(\beta) = 2x^2$. Since $x^2$ will always be positive, we know that $f''(\beta) > 0$ at all points, and thus, it is concave up, and has a minimum

-   Thus, $\beta = xy/x^2$ is the value of $\beta$ which minimises our simple linear model

<br />

It is important to note that this isn't the actual formula for linear regression minimisation

-   Why? we only used one $x$ and $y$ value in this example

-   But in reality, we will have vectors $X$ and $Y$ - so many more points with different $x$ and $y$ variables

-   We will discuss this later

### 3.4: Constrained Optimisation