---
title: "Essential Probability Theory"
subtitle: "Module 3 (Section 0: Essentials)"
format:
    html:
        theme: sketchy
        max-width: 800px
        toc: TRUE
        toc-depth: 3
        toc-location: left
        toc-title: Chapters
        toc-expand: TRUE
---

This module will provide the foundations of probability, that are essential for the remainder of the modules. We start with set theory and notation, which will be very important for statistics and probability. Then, we introduce key probability principles and rules. Finally, we discuss random variables.

[Prerequisites]{.underline}: Modules 1 and 2 (Or university level Single Variable Calculus and basic linear algebra)

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/#section-5-mathematics-for-political-science)

# Chapter 1: Set Theory

### 1.1: Sets

A set, is any collection of objects

-   It could be a collection of all oranges in a basket

-   It could be a set of all possible fruits in the store

-   It could be the set of all things in the universe

[The **set** is the collection of objects, while the **elements** of the set are the specific objects within a set]{.underline}

<br />

Sets have a formal way of being notated:

-   A capital letter represents a set. For example, the set $A$

-   A lowercase letter represents an element in that set. For example, element $a$ within set $A$

-   We can use any letters - just be consistent - the elements of a set should be the same as the letter of the set (ex. $b$ and $B$)

<br />

Common types of sets we use include:

-   $N$: Natural numbers - numbers we use to count 1 onwards $\{1, 2, 3, ... \}$

-   $W$: Whole numbers - counting numbers from 0 onwards $\{ 0, 1, 2, ... \}$

-   $Z$: Integers - non-decimal numbers both negative and positive $\{ ... ,-2, -1, 0, 1, 2, ... \}$

-   $Q$: Rational numbers - numbers that can be written as fractions

-   $R$: [Real numbers]{.underline}: any numbers on the real number line, including irrational ones like $\pi$

-   $C$: Complex numbers - values with an imaginary pane (ex. $3 + 4i$)

<br />

### 1.2: Defining Sets

To define a set, we need some additional notation the following notation:

-   The symbol $\in$ means "in" or "belongs to"

    -   For example $a \in A$ means element $a$ belongs to set $A$

-   The symbol $:$ means "such that" - often used to specify a criteria

-   The symbol $\equiv$ means "is equivalent to" or "represents"

We also need to introduce interval notation:

-   $()$ parentheses on an interval imply not including the boundaries

    -   For example, interval $(a,b)$ implies $a < x < b$

-   $[]$ brackets on an interval imply including boundaries

    -   For example, interval $[a,b]$ implies $a ≤ x ≤ b$

-   We can also mix and match them, for example, $[a,b)$

<br />

We can define sets in many different ways.

-   We can list out all the elements of a set

    -   For example, $A = \{ 1, 2, 3, 4, 6, 9 \}$

-   We can define them with an interval

    -   For example, $A = [0,1]$

-   We can also define them in a very formal way as follows:

    -   For example, $A = \{x : 0 ≤ x ≤ 1, x \in R \}$

    -   That basically means, set $A$ is defined as the values of $x$ such that $x$ is between or equals $0$ and $1$, and $x$ belongs to the set of all real numbers $R$

<br />

### 1.3: Set Operators

There are a few different set operators that are important to understand.

[Intersection]{.underline}: The overlap between two sets

-   The intersection of set $A$ and $B$ is notated as $A \cap B$

-   Basically, this means the set of elements that are contained in both set $A$ and $B$

-   For example, if $A = \{1,2,3 \}$, and $B = \{ 2, 3, 4 \}$, then $A \cap B = \{ 2, 3 \}$

-   If there is no overlap between $A$ and $B$, then we notate it as $A \cap B = \varnothing$

<br />

[Union]{.underline}: The combination of two sets

-   The union of set $A$ and $B$ is notated as $A \cup B$

-   Basically, this means the combination of all elements in $A$ and $B$

-   For example, if $A = \{1,2,3 \}$, and $B = \{ 2, 3, 4 \}$, then $A \cup B = \{ 1,2,3,4 \}$

<br />

[Complement]{.underline}: Everything not in a set

-   The complement of set $A$ is notated as $A'$ or $A^c$

-   Basically, this means all elements that are not in set $A$, but are part of the universal set

    -   Universal set basically means all the elements that are plausible in our current situation

-   So, if the universal set contains $\{ 1,2,3,4,5 \}$, and $A = \{ 1, 2 \}$, then $A^c = \{ 3,4,5 \}$

<br />

[Subset]{.underline}: Meaning a set's elements, belongs entirely within another set

-   If $A$ is a subset of $B$, that means all elements of $A$ belong to $B$

-   There are two types of subsets - proper subsets, and improper subsets

-   Proper subsets are a subset, when the number of elements in $A$ is less than the number of elements in $B$

    -   In mathematically terms, $|A| < |B|$

    -   Proper subsets are notated as $A \subset B$

-   Improper subsets are a subset, when the number of elements in $A$ is less than OR EQUAL to the number of elements in $B$

    -   In mathematical terms, $|A| ≤ |B|$

    -   Improper subsets are notated as $A \subseteq B$

<br />

Set operations have a few key properties, that are important to understand. They are commonly used in probability and statistics.

[Commutative Property]{.underline}: For unions and intersections, the order of the sets in which we do the operation does not matter

-   Essentially, $A \cup B = B \cup A; A \cap B = B \cap A$

<br />

[Associative Property]{.underline}: For unions and intersections, the way sets are grouped does not change the result of the operation

-   Essentially, $A \cup (B \cup C) = (A \cup B) \cup C$

-   Also, $A \cap (B \cap C) = (A \cap B) \cap C$

<br />

[Distributive Property]{.underline}: For unions and intersections, the following distributions apply:

-   $A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$

-   $A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$

<br />

[De Morgan's Law]{.underline}: this critical law allows us to work with complements. It says the following:

-   $(A \cup B)^c = A^c \cap B^c$

-   $(A \cap B)^c = A^c \cup B^c$

<br />

------------------------------------------------------------------------

[Section Homepage](https://politicalscience.github.io/#section-1-statistical-methods)

# Chapter 2: Probability

### 2.1: Basics of Probability

[Kolmogrov's Axioms]{.underline}, also known as the Axioms of Probability, form the backbones of probability theory. The following are true about probability:

1.  For any event $A$, $Pr(A) ≥ 0$
    -   In more intuitive terms - the probability of an event, is a non-negative real number
2.  $Pr(S) = 1$, where $S$ is the sample space of all possible events
    -   In more intuitive terms - $S$ is the set of all possible events (any possible thing that could happen), and the probability of all possible events is 1.
3.  For any sequence of mutually exclusive events $A_1, A_2,..., A_k$, $Pr \left( \bigcup\limits_{i=1}^k A_i \right) = \sum\limits_{i=1}^k Pr(A_i)$
    -   More intuitively, if we have a group of mutually exclusive events (events that cannot occur together at the same time), then the probability of all the events occurring, is equal to the sum of the probabilities of the individual events

<br />

There are other fundamental rules of probability:

-   $Pr(\varnothing) = 0$ - the probability of nothing is 0

-   $0 ≤ Pr(A) ≤ 1)$ - the [probability of any event is between 0 and 1]{.underline}

-   $Pr(A^c) = 1 - Pr(A)$ - the probability of not-A occuring, is the same as 1 minus the probability of A

-   If $A \subset B$, then $Pr(A) ≤ Pr(B)$ - if $A$ is a subset of $B$, then the probability of $A$ must be less than or equal to $B$

-   $Pr(A \cup B) = Pr(A) + Pr(B) - Pr (A \cap B)$

    -   This basically says, the combined probability of $A$ and $B$, is equal to the probability of $A$ + $B$, then subtract the probability of $A$ and $B$ occurring together

    -   Why? Well $A$ and $B$ already contain the possibility of $A$ and $B$ occurring together twice (in both $A$ and $B$ ), so we have to subtract the double counting

-   For any set of events, $Pr \left( \bigcup\limits_{i=1}^k A_i \right) ≤ \sum\limits_{i=1}^k Pr(A_i)$

    -   Why? We established the 2 are equal when events are mutually exclusive

    -   The other possibility is that they are not mutually exclusive - which means they will have overlap, thus, the probability will be less than the sum

<br />

### 2.2: Independence

Events $A$ and $B$ are independent, when the occurrence of $A$ does not affect the probability of $B$

For example, let us say event $A$ is flipping a coin and getting heads, and event $B$ is rolling a 6-sided die and getting #5. Initially, $Pr(B) = 1/6$ before we flip the coin. After we flip the coin, and get heads, $Pr(B)=1/6$ - the die is still the same. Thus, these events are independent

On the opposite hand, say we have a box with 5 apples and 4 oranges. Event $A$ is selecting an apple and eating it, and $B$ is selecting an orange and eating it. Initially $Pr(B) = 4/9$, because 4 oranges out of 9. However, if $A$ occurs, that means one apple is gone and eaten, so we only have 4 apples and 4 oranges left. Now, $Pr(B) = 4/8$, because 4 oranges out of only 8 remaining. Since $Pr(B)$ changes with the occurrence of $A$, these events are not independent.

<br />

Independence is important to understand, since many of our models require independence as an assumption. If we fail to meet this assumption, we have to do additional modifications to make our models work.

<br />

Independent events have a few properties:

-   $Pr(A|B) = Pr(A)$ and $Pr(B|A) = Pr(B)$

    -   $Pr(A|B)$ means the probability of $A$, given $B$ has occurred. Vice versa for the second. We will expand on this in a later lesson.

    -   The expressions state that since $A$ and $B$ are independent, the occurrence of the other does not change the probability

-   $Pr (A \cap B) = Pr(A) \times Pr(B)$

    -   [The probability of two independent events occurring, is the product of the individual probabilities]{.underline}

-   $Pr \left( \bigcap\limits_{i=1}^k A_i \right) = \prod\limits_{i=1}^k Pr(A_i)$

    -   This is just the previous rule, but expanded to more than 2 events

    -   Essentially, the probability of $k$ number of independent events occurring, is the product of all their probabilities

-   [If $A$ and $B$ are mutually exclusive, they are not independent]{.underline}

    -   $A$ and $B$ being mutually exclusive means if one occurs, the other cannot

    -   This rule is true because, that means, if $A$ occurs, the probability of $B$ changes to 0, since it can no longer occur.

    -   Thus, $A$ affects the $Pr(B)$, thus meaning they are not independent

<br />

### 2.3: Counting, Permutations, Combinations

The Fundamental Counting Principle, also called the multiplication rule, says that if there are $a$ number ways of doing something, and $b$ number of ways to do another thing, there are $a \times b$ ways of doing both actions.

-   This can be extended to however many things - they are all multiplied

For example, let us say you are planning a vacation. You have 15 hotels to choose from, 6 different rental cars to choose from, and 8 different flights to choose from. How many different ways could you take your vacation?

-   The fundamental counting principle says that the number of ways is simply the possibilities of each event multiplied

-   Thus, the total possible amounts of different vacations is $15 \times 6 \times 8 = 720$

Counting is very important, since to find the probability of something, we need to know how many possibilities to put in the denominator

<br />

[Permutations]{.underline} are a way to count the number of arrangements [when order matters]{.underline}, when you select $r$ number of objects from $n$ total objects.

What does "order matters" mean? Simply, it means $A,B,C$ is considered a different arrangement than $C,B,A$ or $B,C,A$.

Permutations are given by the following formula:

$$
P(n,r) = \frac{n!}{(n-r)!}
$$

<br />

For example, what is the number of ways to arrange 3 books out of 5, taken from a shelf. We are selecting 3 books, so $r=3$, and we have a total of 5 books, so $n=5$. So, let us plug it into the permutation formula:

$$
P(5,3) = \frac{5!}{(5-3)!} = \frac{5 \times 4 \times 3 \times 2 \times 1}{2 \times 1} = 5 \times 4 \times 3 = 60
$$

We can actually prove this quite easily using the fundamental counting theorem. We are selecting 3 books, where order matters. For the first book, we can choose from any of the 5 books on the shelf. For the second book, we can choose from any of the 4 on the shelf remaining. For the third book, we can choose from any of the 3 on the shelf remaining. Using fundamental counting theorem, we know that the possibilities are:

$$
5 \times 4 \times 3 = 60
$$

Which is the same answer as our permutation got

<br />

[Combinations]{.underline} are a way to count the number of arrangements [when order does not matter]{.underline}, when you select $r$ number of objects from $n$ total objects.

What does "order does not matter" mean? It means that the arrangements $A,B,C$, and $C,B,A$, and $B,C,A$ are all considered the same arrangement. Basically, we only care about what elements are in the arrangement, not the order they are in.

Combinations are given by the following formula:

$$
C(n,r) = \frac{n!}{r!(n-r)!}
$$

<br />

For example, let us just select 3 books from the shelf of 5 books to give to a friend - we don't care about the order at all, just throw the books in a bag. So, $r=3$ as we are selecting 3 books, from a total of $n=5$ books. Let us plug it into the formula:

$$
C(5,3) = \frac{5!}{3!(5-3)!} = 10
$$

Why do we divide by an extra $r!$ in the combination? Well, it is quite inuitive. We first do the same thing as a permutation, selecting books with an order, so that gets us $5 \times 4 \times 3$ (see above). However, we need to now, get rid of order. So, 3 books have $3 \times 2 \times 1 = 3!$ ways of being ordered internally, so, we divide the permutation by $3!$ to get rid of order

<br />

### 2.4: Conditional and Joint Probability

[Joint probability is the probability of two or more events occurring simultaneously]{.underline} (at the same time)

-   The probability of $A$ and $B$ occurring together is $P(A \cap B)$

For example, in a deck of cards, $A$ could be the event of drawing a ace card, and $B$ could be the event of drawing a spade. $P(A \cap B)$ would then be the probability of drawing a ace card is a spade.

<br />

[Conditional probability is the probability of one event occurring, given another event has already occurred]{.underline}

-   Probability of event $A$, given $B$ has occurred is notated as $P(A|B)$

Conditional probability $P(A|B)$ is calculated by taking the joint probability of $A$ and $B$ divided by the probability of $B$. More formally:

$$
P(A|B) = \frac{P(A \cap B)}{P(B)}
$$

<br />

For example, what is the probability of drawing an ace, given you already drew a spade?

-   The probability of drawing both an ace and spade is $P(A \cap B) = 1/52$ (only one card in the deck of 52 meets this requirement)

-   The probability of drawing a spade is $P(B) = 13/52$ (there are 13 spades in a deck of cards)

-   Thus, $P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{1/52}{13/52} = \frac{1}{13}$

<br />

### 2.5: Bayes' Theorem

[Bayes' Theorem provides a way to "update" the probability of an event $A$, based on new evidence]{.underline}

Bayes' theorem is arguably the most important thing in probability and statistics - its applications are far and wide throughout Political Science. Thus, we will actually derive Bayes' Theorem.

We start with the definition of conditional probability, as defined in the last lesson:

$$
P(A|B) = \frac{P(A \cap B)}{P(B)}
$$

Then, we can rearrange that equation by isolating $P(A \cap B)$ to get:

$$
P(A \cap B) = P(A|B) \times P(B)
$$

Now, let us consider the conditional probability of $B|A$ (the opposite way around). Using the same conditional probability formula, but switching $B$ and $A$:

$$
P(B|A) = \frac{P(B \cap A)}{P(A)}
$$

Let us rearrange the equation, and isolate for $P(B \cap B)$ to get:

$$
P(B \cap A) = P(B|A) \times P(A)
$$

<br />

So now, we have the following two equations:

$$
P(A \cap B) = P(A|B) \times P(B), \text{ and } P(B \cap A) = P(B|A) \times P(A)
$$

From the commutative property of sets (see chapter 1), we know the following is true:

$$
P(A \cap B) = P(B \cap A)
$$

Thus, the other parts of the equation must also be equal:

$$
P(A|B) \times P(B) = P(B|A) \times P(A)
$$

Let us isolate $P(A|B)$. This gets us the [final form of Bayes' theorem]{.underline}

$$
P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}
$$

<br />

Each part of Bayes' theorem has its own name. These are important to remember:

-   $Pr(A|B)$ is the [conditional]{.underline} probability

-   $P(B|A)$ is called the [posterior]{.underline} probability

-   $Pr(A)$ is called the [prior]{.underline} probability

-   $Pr(B)$ is called the [marginal]{.underline} probability

<br />

### 2.6 Law of Total Probability

The law of total probability helps calculate the probability of an event, by considering mutually exclusive events that completely partition the same sample space.

For example, if we know the probability of a male being a smoker, and a female being a smoker, we can calculate the probability of all humans for smoking

-   Male and female are mutually exclusive events - they also cover the entire sample space of humans (I know, new social norms are challenging this, but let us assume this is true)

The law of total probability states the probability of $A$, given conditional probabilities $P(A|B_i)$:

$$
P(A) = \sum P(A|B_i) \times P(B_i)
$$

Or more specifically, in our example of smoking for males and females, we would have the following equation, where $P(A)$ is the probability of smoking, $B_M$ means male, and $B_F$ means female:

$$
P(A) = P(A|B_M) \times P(B_M) + P(A|B_F) \times P(B_F)
$$

<br />

For example, imagine you have 3 ways (and only 3 ways) you can get to work:

-   $P(A)$ is the probability you arrive on time

-   $P(B_1), P(B_2), P(B_3)$ are the probabilities you take route 1, route 2, or route 3

-   $P(A|B_1)$ is the probability you are on time, given you take route 1. Same for $P(A|B_2), P(A|B_3)$

Thus, the total probability of arriving on time $P(A)$, given we know the probability of arriving on time for route 1, route 2, and route 3, is:

$$
P(A) = P(A|B_1)P(B_1) + P(A|B_2)P(B_2) + P(A|B_3)P(B_3)
$$

<br />

------------------------------------------------------------------------

[Section Homepage](https://politicalscience.github.io/#section-1-statistical-methods)

# Chapter 3: Random Variables

### 3.1: Discrete Distributions

### 3.2: Continuous Distributions

### 3.3 Expectations and Variance

### 3.4: Normal Distribution

### 3.5: Other Distributions

<br />