<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Essential Probability Theory</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="3_files/libs/clipboard/clipboard.min.js"></script>
<script src="3_files/libs/quarto-html/quarto.js"></script>
<script src="3_files/libs/quarto-html/popper.min.js"></script>
<script src="3_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="3_files/libs/quarto-html/anchor.min.js"></script>
<link href="3_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="3_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="3_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="3_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="3_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Chapters</h2>
   
  <ul>
  <li><a href="#preface" id="toc-preface" class="nav-link active" data-scroll-target="#preface">Preface</a></li>
  <li><a href="#chapter-1-set-theory" id="toc-chapter-1-set-theory" class="nav-link" data-scroll-target="#chapter-1-set-theory">Chapter 1: Set Theory</a>
  <ul class="collapse">
  <li><a href="#sets" id="toc-sets" class="nav-link" data-scroll-target="#sets">1.1: Sets</a></li>
  <li><a href="#defining-sets" id="toc-defining-sets" class="nav-link" data-scroll-target="#defining-sets">1.2: Defining Sets</a></li>
  <li><a href="#set-operators" id="toc-set-operators" class="nav-link" data-scroll-target="#set-operators">1.3: Set Operators</a></li>
  </ul></li>
  <li><a href="#chapter-2-probability" id="toc-chapter-2-probability" class="nav-link" data-scroll-target="#chapter-2-probability">Chapter 2: Probability</a>
  <ul class="collapse">
  <li><a href="#basics-of-probability" id="toc-basics-of-probability" class="nav-link" data-scroll-target="#basics-of-probability">2.1: Basics of Probability</a></li>
  <li><a href="#independence" id="toc-independence" class="nav-link" data-scroll-target="#independence">2.2: Independence</a></li>
  <li><a href="#counting-permutations-combinations" id="toc-counting-permutations-combinations" class="nav-link" data-scroll-target="#counting-permutations-combinations">2.3: Counting, Permutations, Combinations</a></li>
  <li><a href="#conditional-and-joint-probability" id="toc-conditional-and-joint-probability" class="nav-link" data-scroll-target="#conditional-and-joint-probability">2.4: Conditional and Joint Probability</a></li>
  <li><a href="#bayes-theorem" id="toc-bayes-theorem" class="nav-link" data-scroll-target="#bayes-theorem">2.5: Bayes’ Theorem</a></li>
  <li><a href="#law-of-total-probability" id="toc-law-of-total-probability" class="nav-link" data-scroll-target="#law-of-total-probability">2.6 Law of Total Probability</a></li>
  </ul></li>
  <li><a href="#chapter-3-random-variables" id="toc-chapter-3-random-variables" class="nav-link" data-scroll-target="#chapter-3-random-variables">Chapter 3: Random Variables</a>
  <ul class="collapse">
  <li><a href="#discrete-random-variables" id="toc-discrete-random-variables" class="nav-link" data-scroll-target="#discrete-random-variables">3.1: Discrete Random Variables</a></li>
  <li><a href="#continuous-random-variables" id="toc-continuous-random-variables" class="nav-link" data-scroll-target="#continuous-random-variables">3.2: Continuous Random Variables</a></li>
  <li><a href="#expectation-of-a-random-variable" id="toc-expectation-of-a-random-variable" class="nav-link" data-scroll-target="#expectation-of-a-random-variable">3.3 Expectation of a Random Variable</a></li>
  <li><a href="#variance-and-standard-deviation" id="toc-variance-and-standard-deviation" class="nav-link" data-scroll-target="#variance-and-standard-deviation">3.4: Variance and Standard Deviation</a></li>
  <li><a href="#normal-distribution" id="toc-normal-distribution" class="nav-link" data-scroll-target="#normal-distribution">3.5: Normal Distribution</a></li>
  <li><a href="#other-distributions" id="toc-other-distributions" class="nav-link" data-scroll-target="#other-distributions">3.6: Other Distributions</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Essential Probability Theory</h1>
<p class="subtitle lead">Module 3 (Section 0: Essentials)</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="preface" class="level1">
<h1>Preface</h1>
<p>Probability forms the basis of much of modern statistical methods. Probability is also essential for many formal mathematical models of political situations. Thus, it is essential for Political Scientists to understand the basics of probability.</p>
<p>This module introduces the core concepts of probability needed to conduct quantitative analysis. Chapter 1 focuses on Set Theory, which forms the basis of many probability rules, as well as provides much of the notation we will use throughout the book. Chapter 2 focuses on the fundamentals of probability, including the axioms of probability, counting, conditional probabilities, and Bayes’ Theorem. Chapter 3 is an introduction to random variables, their respective probability mass/density functions, expectations and variance, as well as a few common distributions.</p>
<p><u>Prerequisites</u>: Modules 1 and 2 (Or university level Single Variable Calculus and basic linear algebra)</p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io">Handbook Homepage</a></p>
</section>
<section id="chapter-1-set-theory" class="level1">
<h1>Chapter 1: Set Theory</h1>
<section id="sets" class="level3">
<h3 class="anchored" data-anchor-id="sets">1.1: Sets</h3>
<p>A set, is any collection of objects</p>
<ul>
<li><p>It could be a collection of all oranges in a basket</p></li>
<li><p>It could be a set of all possible fruits in the store</p></li>
<li><p>It could be the set of all things in the universe</p></li>
</ul>
<p><u>The <strong>set</strong> is the collection of objects, while the <strong>elements</strong> of the set are the specific objects within a set</u></p>
<p><br></p>
<p>Sets have a formal way of being notated:</p>
<ul>
<li><p>A capital letter represents a set. For example, the set <span class="math inline">\(A\)</span></p></li>
<li><p>A lowercase letter represents an element in that set. For example, element <span class="math inline">\(a\)</span> within set <span class="math inline">\(A\)</span></p></li>
<li><p>We can use any letters - just be consistent - the elements of a set should be the same as the letter of the set (ex. <span class="math inline">\(b\)</span> and <span class="math inline">\(B\)</span>)</p></li>
</ul>
<p><br></p>
<p>Common types of sets we use include:</p>
<ul>
<li><p><span class="math inline">\(N\)</span>: Natural numbers - numbers we use to count 1 onwards <span class="math inline">\(\{1, 2, 3, ... \}\)</span></p></li>
<li><p><span class="math inline">\(W\)</span>: Whole numbers - counting numbers from 0 onwards <span class="math inline">\(\{ 0, 1, 2, ... \}\)</span></p></li>
<li><p><span class="math inline">\(Z\)</span>: Integers - non-decimal numbers both negative and positive <span class="math inline">\(\{ ... ,-2, -1, 0, 1, 2, ... \}\)</span></p></li>
<li><p><span class="math inline">\(Q\)</span>: Rational numbers - numbers that can be written as fractions</p></li>
<li><p><span class="math inline">\(R\)</span>: <u>Real numbers</u>: any numbers on the real number line, including irrational ones like <span class="math inline">\(\pi\)</span></p></li>
<li><p><span class="math inline">\(C\)</span>: Complex numbers - values with an imaginary pane (ex. <span class="math inline">\(3 + 4i\)</span>)</p></li>
</ul>
<p><br></p>
</section>
<section id="defining-sets" class="level3">
<h3 class="anchored" data-anchor-id="defining-sets">1.2: Defining Sets</h3>
<p>To define a set, we need some additional notation the following notation:</p>
<ul>
<li><p>The symbol <span class="math inline">\(\in\)</span> means “in” or “belongs to”</p>
<ul>
<li>For example <span class="math inline">\(a \in A\)</span> means element <span class="math inline">\(a\)</span> belongs to set <span class="math inline">\(A\)</span></li>
</ul></li>
<li><p>The symbol <span class="math inline">\(:\)</span> means “such that” - often used to specify a criteria</p></li>
<li><p>The symbol <span class="math inline">\(\equiv\)</span> means “is equivalent to” or “represents”</p></li>
</ul>
<p>We also need to introduce interval notation:</p>
<ul>
<li><p><span class="math inline">\(()\)</span> parentheses on an interval imply not including the boundaries</p>
<ul>
<li>For example, interval <span class="math inline">\((a,b)\)</span> implies <span class="math inline">\(a &lt; x &lt; b\)</span></li>
</ul></li>
<li><p><span class="math inline">\([]\)</span> brackets on an interval imply including boundaries</p>
<ul>
<li>For example, interval <span class="math inline">\([a,b]\)</span> implies <span class="math inline">\(a ≤ x ≤ b\)</span></li>
</ul></li>
<li><p>We can also mix and match them, for example, <span class="math inline">\([a,b)\)</span></p></li>
</ul>
<p><br></p>
<p>We can define sets in many different ways.</p>
<ul>
<li><p>We can list out all the elements of a set</p>
<ul>
<li>For example, <span class="math inline">\(A = \{ 1, 2, 3, 4, 6, 9 \}\)</span></li>
</ul></li>
<li><p>We can define them with an interval</p>
<ul>
<li>For example, <span class="math inline">\(A = [0,1]\)</span></li>
</ul></li>
<li><p>We can also define them in a very formal way as follows:</p>
<ul>
<li><p>For example, <span class="math inline">\(A = \{x : 0 ≤ x ≤ 1, x \in R \}\)</span></p></li>
<li><p>That basically means, set <span class="math inline">\(A\)</span> is defined as the values of <span class="math inline">\(x\)</span> such that <span class="math inline">\(x\)</span> is between or equals <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, and <span class="math inline">\(x\)</span> belongs to the set of all real numbers <span class="math inline">\(R\)</span></p></li>
</ul></li>
</ul>
<p><br></p>
</section>
<section id="set-operators" class="level3">
<h3 class="anchored" data-anchor-id="set-operators">1.3: Set Operators</h3>
<p>There are a few different set operators that are important to understand.</p>
<p><u>Intersection</u>: The overlap between two sets</p>
<ul>
<li><p>The intersection of set <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is notated as <span class="math inline">\(A \cap B\)</span></p></li>
<li><p>Basically, this means the set of elements that are contained in both set <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span></p></li>
<li><p>For example, if <span class="math inline">\(A = \{1,2,3 \}\)</span>, and <span class="math inline">\(B = \{ 2, 3, 4 \}\)</span>, then <span class="math inline">\(A \cap B = \{ 2, 3 \}\)</span></p></li>
<li><p>If there is no overlap between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, then we notate it as <span class="math inline">\(A \cap B = \varnothing\)</span></p></li>
</ul>
<p>The figure below shows an intersection with a venn diagram: only elements that are contained in both set <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are included:</p>
<p><img src="figures/figure3-intersection.webp" class="img-fluid" style="width:50.0%"></p>
<p><br></p>
<p><u>Union</u>: The combination of two sets</p>
<ul>
<li><p>The union of set <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is notated as <span class="math inline">\(A \cup B\)</span></p></li>
<li><p>Basically, this means the combination of all elements in <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span></p></li>
<li><p>For example, if <span class="math inline">\(A = \{1,2,3 \}\)</span>, and <span class="math inline">\(B = \{ 2, 3, 4 \}\)</span>, then <span class="math inline">\(A \cup B = \{ 1,2,3,4 \}\)</span></p></li>
</ul>
<p>The venn diagram shows a union: everything within <span class="math inline">\(A\)</span> + within <span class="math inline">\(B\)</span> + the intersection <span class="math inline">\(A \cap B\)</span></p>
<p><img src="figures/figure3-union.png" class="img-fluid" style="width:60.0%"></p>
<p><br></p>
<p><u>Complement</u>: Everything not in a set</p>
<ul>
<li><p>The complement of set <span class="math inline">\(A\)</span> is notated as <span class="math inline">\(A'\)</span> or <span class="math inline">\(A^c\)</span></p></li>
<li><p>Basically, this means all elements that are not in set <span class="math inline">\(A\)</span>, but are part of the universal set</p>
<ul>
<li>Universal set basically means all the elements that are plausible in our current situation</li>
</ul></li>
<li><p>So, if the universal set contains <span class="math inline">\(\{ 1,2,3,4,5 \}\)</span>, and <span class="math inline">\(A = \{ 1, 2 \}\)</span>, then <span class="math inline">\(A^c = \{ 3,4,5 \}\)</span></p></li>
</ul>
<p>In the diagram below, everything within circle <span class="math inline">\(A\)</span> is within set <span class="math inline">\(A\)</span>, while everything outside is the complement of <span class="math inline">\(A\)</span>, notated as <span class="math inline">\(A'\)</span> or <span class="math inline">\(A^c\)</span></p>
<p><img src="figures/figure3-complement.png" class="img-fluid" style="width:80.0%"></p>
<p><br></p>
<p><u>Subset</u>: Meaning a set’s elements, belongs entirely within another set</p>
<ul>
<li><p>If <span class="math inline">\(A\)</span> is a subset of <span class="math inline">\(B\)</span>, that means all elements of <span class="math inline">\(A\)</span> belong to <span class="math inline">\(B\)</span></p></li>
<li><p>There are two types of subsets - proper subsets, and improper subsets</p></li>
<li><p>Proper subsets are a subset, when the number of elements in <span class="math inline">\(A\)</span> is less than the number of elements in <span class="math inline">\(B\)</span></p>
<ul>
<li><p>In mathematically terms, <span class="math inline">\(|A| &lt; |B|\)</span></p></li>
<li><p>Proper subsets are notated as <span class="math inline">\(A \subset B\)</span></p></li>
</ul></li>
<li><p>Improper subsets are a subset, when the number of elements in <span class="math inline">\(A\)</span> is less than OR EQUAL to the number of elements in <span class="math inline">\(B\)</span></p>
<ul>
<li><p>In mathematical terms, <span class="math inline">\(|A| ≤ |B|\)</span></p></li>
<li><p>Improper subsets are notated as <span class="math inline">\(A \subseteq B\)</span></p></li>
</ul></li>
</ul>
<p>The figure below shows both a improper and proper subset.</p>
<p><img src="figures/figure3-subset.png" class="img-fluid" style="width:100.0%"></p>
<p><br></p>
<p>Set operations have a few key properties, that are important to understand. They are commonly used in probability and statistics.</p>
<p><u>Commutative Property</u>: For unions and intersections, the order of the sets in which we do the operation does not matter</p>
<ul>
<li>Essentially, <span class="math inline">\(A \cup B = B \cup A; A \cap B = B \cap A\)</span></li>
</ul>
<p><br></p>
<p><u>Associative Property</u>: For unions and intersections, the way sets are grouped does not change the result of the operation</p>
<ul>
<li><p>Essentially, <span class="math inline">\(A \cup (B \cup C) = (A \cup B) \cup C\)</span></p></li>
<li><p>Also, <span class="math inline">\(A \cap (B \cap C) = (A \cap B) \cap C\)</span></p></li>
</ul>
<p><br></p>
<p><u>Distributive Property</u>: For unions and intersections, the following distributions apply:</p>
<ul>
<li><p><span class="math inline">\(A \cap (B \cup C) = (A \cap B) \cup (A \cap C)\)</span></p></li>
<li><p><span class="math inline">\(A \cup (B \cap C) = (A \cup B) \cap (A \cup C)\)</span></p></li>
</ul>
<p><br></p>
<p><u>De Morgan’s Law</u>: this critical law allows us to work with complements. It says the following:</p>
<ul>
<li><p><span class="math inline">\((A \cup B)^c = A^c \cap B^c\)</span></p></li>
<li><p><span class="math inline">\((A \cap B)^c = A^c \cup B^c\)</span></p></li>
</ul>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io">Handbook Homepage</a></p>
</section>
</section>
<section id="chapter-2-probability" class="level1">
<h1>Chapter 2: Probability</h1>
<section id="basics-of-probability" class="level3">
<h3 class="anchored" data-anchor-id="basics-of-probability">2.1: Basics of Probability</h3>
<p><u>Kolmogrov’s Axioms</u>, also known as the Axioms of Probability, form the backbones of probability theory. The following are true about probability:</p>
<ol type="1">
<li>For any event <span class="math inline">\(A\)</span>, <span class="math inline">\(Pr(A) ≥ 0\)</span>
<ul>
<li>In more intuitive terms - the probability of an event, is a non-negative real number</li>
</ul></li>
<li><span class="math inline">\(Pr(S) = 1\)</span>, where <span class="math inline">\(S\)</span> is the sample space of all possible events
<ul>
<li>In more intuitive terms - <span class="math inline">\(S\)</span> is the set of all possible events (any possible thing that could happen), and the probability of all possible events is 1.</li>
</ul></li>
<li>For any sequence of mutually exclusive events <span class="math inline">\(A_1, A_2,..., A_k\)</span>, <span class="math inline">\(Pr \left( \bigcup\limits_{i=1}^k A_i \right) = \sum\limits_{i=1}^k Pr(A_i)\)</span>
<ul>
<li>More intuitively, if we have a group of mutually exclusive events (events that cannot occur together at the same time), then the probability of all the events occurring, is equal to the sum of the probabilities of the individual events</li>
<li>If you are struggling with summation notation, see <a href="https://politicalscience.github.io/section0/1.html#chapter-2-sum-product-notation">Module 1, Chapter 2</a></li>
</ul></li>
</ol>
<p><br></p>
<p>There are other fundamental rules of probability:</p>
<ul>
<li><p><span class="math inline">\(Pr(\varnothing) = 0\)</span> - the probability of nothing is 0</p></li>
<li><p><span class="math inline">\(0 ≤ Pr(A) ≤ 1)\)</span> - the <u>probability of any event is between 0 and 1</u></p></li>
<li><p><span class="math inline">\(Pr(A^c) = 1 - Pr(A)\)</span> - the probability of not-A occuring, is the same as 1 minus the probability of A</p></li>
<li><p>If <span class="math inline">\(A \subset B\)</span>, then <span class="math inline">\(Pr(A) ≤ Pr(B)\)</span> - if <span class="math inline">\(A\)</span> is a subset of <span class="math inline">\(B\)</span>, then the probability of <span class="math inline">\(A\)</span> must be less than or equal to <span class="math inline">\(B\)</span></p></li>
<li><p><span class="math inline">\(Pr(A \cup B) = Pr(A) + Pr(B) - Pr (A \cap B)\)</span></p>
<ul>
<li><p>This basically says, the combined probability of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, is equal to the probability of <span class="math inline">\(A\)</span> + <span class="math inline">\(B\)</span>, then subtract the probability of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occurring together</p></li>
<li><p>Why? Well <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> already contain the possibility of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occurring together twice (in both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> ), so we have to subtract the double counting</p></li>
</ul></li>
<li><p>For any set of events, <span class="math inline">\(Pr \left( \bigcup\limits_{i=1}^k A_i \right) ≤ \sum\limits_{i=1}^k Pr(A_i)\)</span></p>
<ul>
<li><p>Why? We established the 2 are equal when events are mutually exclusive</p></li>
<li><p>The other possibility is that they are not mutually exclusive - which means they will have overlap, thus, the probability will be less than the sum</p></li>
</ul></li>
</ul>
<p><br></p>
</section>
<section id="independence" class="level3">
<h3 class="anchored" data-anchor-id="independence">2.2: Independence</h3>
<p>Events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent, when the occurrence of <span class="math inline">\(A\)</span> does not affect the probability of <span class="math inline">\(B\)</span></p>
<p>For example, let us say event <span class="math inline">\(A\)</span> is flipping a coin and getting heads, and event <span class="math inline">\(B\)</span> is rolling a 6-sided die and getting #5. Initially, <span class="math inline">\(Pr(B) = 1/6\)</span> before we flip the coin. After we flip the coin, and get heads, <span class="math inline">\(Pr(B)=1/6\)</span> - the die is still the same. Thus, these events are independent</p>
<p>On the opposite hand, say we have a box with 5 apples and 4 oranges. Event <span class="math inline">\(A\)</span> is selecting an apple and eating it, and <span class="math inline">\(B\)</span> is selecting an orange and eating it. Initially <span class="math inline">\(Pr(B) = 4/9\)</span>, because 4 oranges out of 9. However, if <span class="math inline">\(A\)</span> occurs, that means one apple is gone and eaten, so we only have 4 apples and 4 oranges left. Now, <span class="math inline">\(Pr(B) = 4/8\)</span>, because 4 oranges out of only 8 remaining. Since <span class="math inline">\(Pr(B)\)</span> changes with the occurrence of <span class="math inline">\(A\)</span>, these events are not independent.</p>
<p><br></p>
<p>Independence is important to understand, since many of our models require independence as an assumption. If we fail to meet this assumption, we have to do additional modifications to make our models work.</p>
<p><br></p>
<p>Independent events have a few properties:</p>
<ul>
<li><p><span class="math inline">\(Pr(A|B) = Pr(A)\)</span> and <span class="math inline">\(Pr(B|A) = Pr(B)\)</span></p>
<ul>
<li><p><span class="math inline">\(Pr(A|B)\)</span> means the probability of <span class="math inline">\(A\)</span>, given <span class="math inline">\(B\)</span> has occurred. Vice versa for the second. We will expand on this in a later lesson.</p></li>
<li><p>The expressions state that since <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent, the occurrence of the other does not change the probability</p></li>
</ul></li>
<li><p><span class="math inline">\(Pr (A \cap B) = Pr(A) \times Pr(B)\)</span></p>
<ul>
<li><u>The probability of two independent events occurring, is the product of the individual probabilities</u></li>
</ul></li>
<li><p><span class="math inline">\(Pr \left( \bigcap\limits_{i=1}^k A_i \right) = \prod\limits_{i=1}^k Pr(A_i)\)</span></p>
<ul>
<li><p>This is just the previous rule, but expanded to more than 2 events</p></li>
<li><p>Essentially, the probability of <span class="math inline">\(k\)</span> number of independent events occurring, is the product of all their probabilities</p></li>
</ul></li>
<li><p><u>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are mutually exclusive, they are not independent</u></p>
<ul>
<li><p><span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> being mutually exclusive means if one occurs, the other cannot</p></li>
<li><p>This rule is true because, that means, if <span class="math inline">\(A\)</span> occurs, the probability of <span class="math inline">\(B\)</span> changes to 0, since it can no longer occur.</p></li>
<li><p>Thus, <span class="math inline">\(A\)</span> affects the <span class="math inline">\(Pr(B)\)</span>, thus meaning they are not independent</p></li>
</ul></li>
</ul>
<p><br></p>
</section>
<section id="counting-permutations-combinations" class="level3">
<h3 class="anchored" data-anchor-id="counting-permutations-combinations">2.3: Counting, Permutations, Combinations</h3>
<p>The Fundamental Counting Principle, also called the multiplication rule, says that if there are <span class="math inline">\(a\)</span> number ways of doing something, and <span class="math inline">\(b\)</span> number of ways to do another thing, there are <span class="math inline">\(a \times b\)</span> ways of doing both actions.</p>
<ul>
<li>This can be extended to however many things - they are all multiplied</li>
</ul>
<p>For example, let us say you are planning a vacation. You have 15 hotels to choose from, 6 different rental cars to choose from, and 8 different flights to choose from. How many different ways could you take your vacation?</p>
<ul>
<li><p>The fundamental counting principle says that the number of ways is simply the possibilities of each event multiplied</p></li>
<li><p>Thus, the total possible amounts of different vacations is <span class="math inline">\(15 \times 6 \times 8 = 720\)</span></p></li>
</ul>
<p>Counting is very important, since to find the probability of something, we need to know how many possibilities to put in the denominator</p>
<p><br></p>
<p><u>Permutations</u> are a way to count the number of arrangements <u>when order matters</u>, when you select <span class="math inline">\(r\)</span> number of objects from <span class="math inline">\(n\)</span> total objects.</p>
<p>What does “order matters” mean? Simply, it means <span class="math inline">\(A,B,C\)</span> is considered a different arrangement than <span class="math inline">\(C,B,A\)</span> or <span class="math inline">\(B,C,A\)</span>.</p>
<p>Permutations are given by the following formula:</p>
<p><span class="math display">\[
P(n,r) = \frac{n!}{(n-r)!}
\]</span></p>
<p><br></p>
<p>For example, what is the number of ways to arrange 3 books out of 5, taken from a shelf. We are selecting 3 books, so <span class="math inline">\(r=3\)</span>, and we have a total of 5 books, so <span class="math inline">\(n=5\)</span>. So, let us plug it into the permutation formula:</p>
<p><span class="math display">\[
P(5,3) = \frac{5!}{(5-3)!} = \frac{5 \times 4 \times 3 \times 2 \times 1}{2 \times 1} = 5 \times 4 \times 3 = 60
\]</span></p>
<p>We can actually prove this quite easily using the fundamental counting theorem. We are selecting 3 books, where order matters. For the first book, we can choose from any of the 5 books on the shelf. For the second book, we can choose from any of the 4 on the shelf remaining. For the third book, we can choose from any of the 3 on the shelf remaining. Using fundamental counting theorem, we know that the possibilities are:</p>
<p><span class="math display">\[
5 \times 4 \times 3 = 60
\]</span></p>
<p>Which is the same answer as our permutation got</p>
<p><br></p>
<p><u>Combinations</u> are a way to count the number of arrangements <u>when order does not matter</u>, when you select <span class="math inline">\(r\)</span> number of objects from <span class="math inline">\(n\)</span> total objects.</p>
<p>What does “order does not matter” mean? It means that the arrangements <span class="math inline">\(A,B,C\)</span>, and <span class="math inline">\(C,B,A\)</span>, and <span class="math inline">\(B,C,A\)</span> are all considered the same arrangement. Basically, we only care about what elements are in the arrangement, not the order they are in.</p>
<p>Combinations are given by the following formula:</p>
<p><span class="math display">\[
C(n,r) = \frac{n!}{r!(n-r)!}
\]</span></p>
<p><br></p>
<p>For example, let us just select 3 books from the shelf of 5 books to give to a friend - we don’t care about the order at all, just throw the books in a bag. So, <span class="math inline">\(r=3\)</span> as we are selecting 3 books, from a total of <span class="math inline">\(n=5\)</span> books. Let us plug it into the formula:</p>
<p><span class="math display">\[
C(5,3) = \frac{5!}{3!(5-3)!} = 10
\]</span></p>
<p>Why do we divide by an extra <span class="math inline">\(r!\)</span> in the combination? Well, it is quite inuitive. We first do the same thing as a permutation, selecting books with an order, so that gets us <span class="math inline">\(5 \times 4 \times 3\)</span> (see above). However, we need to now, get rid of order. So, 3 books have <span class="math inline">\(3 \times 2 \times 1 = 3!\)</span> ways of being ordered internally, so, we divide the permutation by <span class="math inline">\(3!\)</span> to get rid of order</p>
<p><br></p>
</section>
<section id="conditional-and-joint-probability" class="level3">
<h3 class="anchored" data-anchor-id="conditional-and-joint-probability">2.4: Conditional and Joint Probability</h3>
<p><u>Joint probability is the probability of two or more events occurring simultaneously</u> (at the same time)</p>
<ul>
<li>The probability of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occurring together is <span class="math inline">\(P(A \cap B)\)</span></li>
</ul>
<p>For example, in a deck of cards, <span class="math inline">\(A\)</span> could be the event of drawing a ace card, and <span class="math inline">\(B\)</span> could be the event of drawing a spade. <span class="math inline">\(P(A \cap B)\)</span> would then be the probability of drawing a ace card is a spade.</p>
<p><br></p>
<p><u>Conditional probability is the probability of one event occurring, given another event has already occurred</u></p>
<ul>
<li>Probability of event <span class="math inline">\(A\)</span>, given <span class="math inline">\(B\)</span> has occurred is notated as <span class="math inline">\(P(A|B)\)</span></li>
</ul>
<p>Conditional probability <span class="math inline">\(P(A|B)\)</span> is calculated by taking the joint probability of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> divided by the probability of <span class="math inline">\(B\)</span>. More formally:</p>
<p><span class="math display">\[
P(A|B) = \frac{P(A \cap B)}{P(B)}
\]</span></p>
<p><br></p>
<p>For example, what is the probability of drawing an ace, given you already drew a spade?</p>
<ul>
<li><p>The probability of drawing both an ace and spade is <span class="math inline">\(P(A \cap B) = 1/52\)</span> (only one card in the deck of 52 meets this requirement)</p></li>
<li><p>The probability of drawing a spade is <span class="math inline">\(P(B) = 13/52\)</span> (there are 13 spades in a deck of cards)</p></li>
<li><p>Thus, <span class="math inline">\(P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{1/52}{13/52} = \frac{1}{13}\)</span></p></li>
</ul>
<p><br></p>
</section>
<section id="bayes-theorem" class="level3">
<h3 class="anchored" data-anchor-id="bayes-theorem">2.5: Bayes’ Theorem</h3>
<p><u>Bayes’ Theorem provides a way to “update” the probability of an event <span class="math inline">\(A\)</span>, based on new evidence</u></p>
<p>Bayes’ theorem is arguably the most important thing in probability and statistics - its applications are far and wide throughout Political Science. Thus, we will actually derive Bayes’ Theorem.</p>
<p>We start with the definition of conditional probability, as defined in the last lesson:</p>
<p><span class="math display">\[
P(A|B) = \frac{P(A \cap B)}{P(B)}
\]</span></p>
<p>Then, we can rearrange that equation by isolating <span class="math inline">\(P(A \cap B)\)</span> to get:</p>
<p><span class="math display">\[
P(A \cap B) = P(A|B) \times P(B)
\]</span></p>
<p>Now, let us consider the conditional probability of <span class="math inline">\(B|A\)</span> (the opposite way around). Using the same conditional probability formula, but switching <span class="math inline">\(B\)</span> and <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[
P(B|A) = \frac{P(B \cap A)}{P(A)}
\]</span></p>
<p>Let us rearrange the equation, and isolate for <span class="math inline">\(P(B \cap B)\)</span> to get:</p>
<p><span class="math display">\[
P(B \cap A) = P(B|A) \times P(A)
\]</span></p>
<p><br></p>
<p>So now, we have the following two equations:</p>
<p><span class="math display">\[
P(A \cap B) = P(A|B) \times P(B), \text{ and } P(B \cap A) = P(B|A) \times P(A)
\]</span></p>
<p>From the commutative property of sets (see chapter 1), we know the following is true:</p>
<p><span class="math display">\[
P(A \cap B) = P(B \cap A)
\]</span></p>
<p>Thus, the other parts of the equation must also be equal:</p>
<p><span class="math display">\[
P(A|B) \times P(B) = P(B|A) \times P(A)
\]</span></p>
<p>Let us isolate <span class="math inline">\(P(A|B)\)</span>. This gets us the <u>final form of Bayes’ theorem</u></p>
<p><span class="math display">\[
P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}
\]</span></p>
<p><br></p>
<p>Each part of Bayes’ theorem has its own name. These are important to remember:</p>
<ul>
<li><p><span class="math inline">\(Pr(A|B)\)</span> is the <u>conditional</u> probability</p></li>
<li><p><span class="math inline">\(P(B|A)\)</span> is called the <u>posterior</u> probability</p></li>
<li><p><span class="math inline">\(Pr(A)\)</span> is called the <u>prior</u> probability</p></li>
<li><p><span class="math inline">\(Pr(B)\)</span> is called the <u>marginal</u> probability</p></li>
</ul>
<p><br></p>
</section>
<section id="law-of-total-probability" class="level3">
<h3 class="anchored" data-anchor-id="law-of-total-probability">2.6 Law of Total Probability</h3>
<p>The law of total probability helps calculate the probability of an event, by considering mutually exclusive events that completely partition the same sample space.</p>
<p>For example, if we know the probability of a male being a smoker, and a female being a smoker, we can calculate the probability of all humans for smoking</p>
<ul>
<li>Male and female are mutually exclusive events - they also cover the entire sample space of humans (I know, new social norms are challenging this, but let us assume this is true)</li>
</ul>
<p>The law of total probability states the probability of <span class="math inline">\(A\)</span>, given conditional probabilities <span class="math inline">\(P(A|B_i)\)</span>:</p>
<p><span class="math display">\[
P(A) = \sum P(A|B_i) \times P(B_i)
\]</span></p>
<p>Or more specifically, in our example of smoking for males and females, we would have the following equation, where <span class="math inline">\(P(A)\)</span> is the probability of smoking, <span class="math inline">\(B_M\)</span> means male, and <span class="math inline">\(B_F\)</span> means female:</p>
<p><span class="math display">\[
P(A) = P(A|B_M) \times P(B_M) + P(A|B_F) \times P(B_F)
\]</span></p>
<p><br></p>
<p>For example, imagine you have 3 ways (and only 3 ways) you can get to work:</p>
<ul>
<li><p><span class="math inline">\(P(A)\)</span> is the probability you arrive on time</p></li>
<li><p><span class="math inline">\(P(B_1), P(B_2), P(B_3)\)</span> are the probabilities you take route 1, route 2, or route 3</p></li>
<li><p><span class="math inline">\(P(A|B_1)\)</span> is the probability you are on time, given you take route 1. Same for <span class="math inline">\(P(A|B_2), P(A|B_3)\)</span></p></li>
</ul>
<p>Thus, the total probability of arriving on time <span class="math inline">\(P(A)\)</span>, given we know the probability of arriving on time for route 1, route 2, and route 3, is:</p>
<p><span class="math display">\[
P(A) = P(A|B_1)P(B_1) + P(A|B_2)P(B_2) + P(A|B_3)P(B_3)
\]</span></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io">Handbook Homepage</a></p>
</section>
</section>
<section id="chapter-3-random-variables" class="level1">
<h1>Chapter 3: Random Variables</h1>
<section id="discrete-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="discrete-random-variables">3.1: Discrete Random Variables</h3>
<p><u>Randomness</u> indicates that there is some probability an event will occur - it is not-for-sure going to happen (deterministic).</p>
<p>A <u>Random Variable</u> is a variable that describes unobserved events. These unobserved events have some randomness - a probability to occur, but they have not yet happened, so it is not deterministic</p>
<p>For example, if you flip a coin 10 times, and you count the number of heads. It is possible to get 5 heads, or 6 heads, or 4 heads, or any amount between 0 and 10. We are not sure what will happen - but we do have probabilities associate with each outcome</p>
<p><br></p>
<p><u>Discrete Random Variables</u> are random variables, whose outcomes can only assume a finite, or countable but infinite number of distinct values.</p>
<ul>
<li><p>So, there are distinct outcomes, like categories, that can occur</p></li>
<li><p>Example: the number of wars per year (whole numbers, no decimals, so a countable number of distinct values)</p></li>
<li><p>Example: Heads or tails (only 2 distinct categories)</p></li>
</ul>
<p><br></p>
<p>A <u>Probability Mass Function</u> <span class="math inline">\(p(y)\)</span> is a function, where input <span class="math inline">\(y\)</span> is a potential outcome, while the output <span class="math inline">\(p(y)\)</span> is the probability of the input outcome occurring.</p>
<ul>
<li><p>For example, if <span class="math inline">\(y=1\)</span> is a potential outcome, <span class="math inline">\(p(1)\)</span> would tell us the probability of the outcome <span class="math inline">\(y=1\)</span> occurring</p></li>
<li><p>Probability mass function <span class="math inline">\(p(y)\)</span> can also be notated as <span class="math inline">\(Pr(Y=y)\)</span></p></li>
</ul>
<p>The probability mass function creates a <u>probability distribution</u>: basically - a graph of potential outcomes and their respective probabilities. This helps us explain the potential outcomes of random variables.</p>
<p><br></p>
<p>For example, take a fair 6-sided die. The potential outcomes are <span class="math inline">\(Y = \{ 1, 2, 3, 4, 5,6\}\)</span>, as there are 6 sides the die could land on. Each outcome has a <span class="math inline">\(1/6\)</span> probability of occurring. Thus, the probability mass function is <span class="math inline">\(p(y) = 1/6\)</span></p>
<ul>
<li><p>This tells us the probability of any outcome occurring</p></li>
<li><p>For example, if we want to know the probability we land on a <span class="math inline">\(1\)</span>, then <span class="math inline">\(p(1) = 1/6\)</span></p></li>
</ul>
<p>Graphically, if we put the potential outcomes on the <span class="math inline">\(x\)</span>-axis, and probabilities on the <span class="math inline">\(y\)</span> axis, we get a probability mass function that takes the following form:</p>
<p><img src="figures/figure3-discrete.webp" class="img-fluid" style="width:70.0%"></p>
<p><br></p>
<p>Probability Mass Functions have a few properties:</p>
<ol type="1">
<li>The function’s output must be between 0 and 1, since the probability of any event occurring must be between 0 and 1 (see probability properties). Mathematically, this is notated as <span class="math inline">\(0 ≤ p(y) ≤ 1\)</span></li>
<li>The sum of all potential <span class="math inline">\(p(y)\)</span> with all outcomes <span class="math inline">\(y\)</span> should be equal to 1. This is because set <span class="math inline">\(Y\)</span> contains all potential <span class="math inline">\(y\)</span>, and the probability of the sample space is 1 (see probability properties). Mathematically, it is <span class="math inline">\(\sum\limits_y p(y) = 1\)</span></li>
</ol>
<p><br></p>
<p>A <u>cumulative density function</u> measures the cumulative property of an event. Unlike a probability mass function, which measures the probability of one outcome <span class="math inline">\(y\)</span>, the cumulative density function measures a group of probabilities less than and up to <span class="math inline">\(y\)</span></p>
<ul>
<li>For example, the cumulative density function of a 6 sided die, when the input is 3, means the probability of getting a 1, 2, and 3 combined</li>
</ul>
<p>Cumulative density functions take the following form:</p>
<p><span class="math display">\[
P(Y≤y) = \sum_{i≤y}p(i)
\]</span></p>
<ul>
<li>Where <span class="math inline">\(Y\)</span> is the set of all outcomes, <span class="math inline">\(y\)</span> is the inputted value, so <span class="math inline">\(Y ≤ y\)</span> means all values in set <span class="math inline">\(Y\)</span> less than input <span class="math inline">\(y\)</span></li>
</ul>
<p><span class="math inline">\(\lim\limits_{y \rightarrow -∞}CDF = 0\)</span> and the <span class="math inline">\(\lim\limits_{y \rightarrow ∞}CDF = 1\)</span>, which basically means that the cumulative density function of nothing is 0, and the cumulative density function of everything is 1.</p>
<p><br></p>
</section>
<section id="continuous-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="continuous-random-variables">3.2: Continuous Random Variables</h3>
<p><u>Continuous Random Variables</u> are random variables, whose outcomes can assume any real number <span class="math inline">\(y \in (-∞,∞)\)</span>. Thus, there are infinite number of possible outcomes, including any subdivision.</p>
<ul>
<li>Example: the time it takes to drive to school tomorrow is a continuous random variable, since it can take any real numbers above 0, such as 2 minutes, 2.643 minutes, 2.432478 minutes, etc.</li>
</ul>
<p>The important distinction between continuous and discrete random variables is how we measure the probability. Since continuous random variables have an infinite number of outcomes, it doesn’t make sense to measure the probability of one specific outcome.</p>
<ul>
<li>For example, it makes no sense to measure the probability of driving to school being exactly and precisely 4.3432478323 minutes.</li>
</ul>
<p>Thus, for continuous random variables, we measure the <u>probability of a range of outcomes</u> occurring.</p>
<p><br></p>
<p>The <u>Probability Density Function</u> <span class="math inline">\(f(y)\)</span> outputs the probability of a range of outcomes occurring, with the outcomes in question defined in set <span class="math inline">\(A\)</span> consisting of all outcomes between <span class="math inline">\(y = [a,b]\)</span>. The probability density function takes the following form:</p>
<p><span class="math display">\[
Pr(Y \in A) = \int\limits_a^b f(y)dy
\]</span></p>
<p>If you need a refresher on integrals, see <a href="https://politicalscience.github.io/section0/2.html#section-4-integrals">Module 2 Chapter 4</a></p>
<p>Graphically, if we put potential outcomes on the <span class="math inline">\(x\)</span> axis, and probabilities on the <span class="math inline">\(y\)</span> axis, the shaded area is the probability of an outcome between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> occurring.</p>
<p><img src="figures/figure3-continuous.jpeg" class="img-fluid" style="width:80.0%"></p>
<p><br></p>
<p>For example, if we are interested in the probability that the drive to school tomorrow will take between 2 to 3 minutes, the probability density function would be as follows:</p>
<p><span class="math display">\[
Pr(Y \in A) = \int\limits_2^3 f(y)dy
\]</span></p>
<p>The probability density function has one main property: the total probability of all outcomes should be 1. Mathematically noted: <span class="math inline">\(\int\limits_{-∞}^{∞}f(y) = 1\)</span></p>
<p><br></p>
<p>A <u>cumulative density function</u> measures the cumulative property of an event, just like for discrete random variables. Unlike a probability mass function, which measures the probability of one range of outcome, the cumulative density function measures a group of probabilities less than and up to <span class="math inline">\(y\)</span></p>
<ul>
<li>For example, the cumulative density function of the drive to school tomorrow, given <span class="math inline">\(y=6\)</span>, means the probability that the drive to school tomorrow will be less than 6 minutes</li>
</ul>
<p>Cumulative density functions take the following form:</p>
<p><span class="math display">\[
Pr(Y ≤ y) = \int\limits_{-∞}^y f(y)dy
\]</span></p>
<p><br></p>
<p>Finally, a useful property is that if we are interested in the probability of an outcome or greater occurs, we should note <span class="math inline">\(Pr(Y &gt; y) = 1 - Pr(Y ≤ y)\)</span>. This is because <span class="math inline">\(Y\)</span> contains all possible set of outcomes, <span class="math inline">\(Pr(Y) = 1\)</span></p>
<p><br></p>
</section>
<section id="expectation-of-a-random-variable" class="level3">
<h3 class="anchored" data-anchor-id="expectation-of-a-random-variable">3.3 Expectation of a Random Variable</h3>
<p>The <u>expectation</u> of a random variable, also known as the <u>expected value</u> or <u>mean</u>, is one of the most important summary measures of a random variable. This is because, the mean is the statistically best guess of the outcome of a random variable, given a random guess with no information outside of the distribution.</p>
<p><br></p>
<p>For Discrete Random Variables, the expected value is the weighted average of all possible outcome values in set <span class="math inline">\(Y\)</span>, weighted by their probabilities</p>
<ul>
<li><p>Mathematically: <span class="math inline">\(E[Y] = \sum\limits_y y \times P(Y=y)\)</span></p></li>
<li><p>Or more intuitively, take every possible outcome, multiply it by its respective probability, then sum all the products</p></li>
</ul>
<p>For example, what is the expected value of a fair 6-sided die? Well, all the possible outcomes are <span class="math inline">\(Y = \{ 1, 2, 3, 4,5 ,6\}\)</span>. Each outcome has a <span class="math inline">\(1/6\)</span> probability of occurring. Thus, the expected value is:</p>
<p><span class="math display">\[
1(1/6) + 2(1/6) + 3(1/6) + 4(1/6)+ 5(1/6) + 6(1/6) = 3.5
\]</span></p>
<p><br></p>
<p>For Continuous Random Variables, the mean is very similar to the expected value for discrete random variables - we also want to take the weighted average of the probabilities.</p>
<p>However, there is one issue - there are an infinite amount of possible outcomes in a continuous random variable. To address this issue, we will use integration</p>
<ul>
<li>Mathematically: <span class="math inline">\(E[Y] = \int\limits_{a}^b y \times f(y) dy\)</span>, where the potential outcomes lay between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span></li>
</ul>
<p>For example, what is the expected value, given the probability density function <span class="math inline">\(f(y) = 1/1.5\)</span>, between <span class="math inline">\(0 &lt; y &lt; 1.5\)</span>?</p>
<ul>
<li><p>So, we have the function: <span class="math inline">\(E[Y] = \int\limits_{0}^{1.5} y \times \frac{1}{1.5} dy\)</span></p></li>
<li><p>To solve this definite integral, we need to use the fundamental theorem of calculus: <span class="math inline">\(\int\limits_a^b f(x)dx = F(b) - F(a)\)</span></p></li>
<li><p>So, let us find the indefinite integral</p></li>
<li><p>1st, let us simplify the function to: <span class="math inline">\(\int\ \left[ \frac{2}{3}y \right] dy\)</span></p></li>
<li><p>Use the constant and reverse power rule: <span class="math inline">\(=\frac{2}{3} \times \frac{1}{2}y^2+c\)</span></p></li>
<li><p>Simplify: <span class="math inline">\(= \frac{1}{3}y^2\)</span></p></li>
<li><p>Now, fundamental theorem of calculus: <span class="math inline">\(F(b) - F(a) = \frac{1}{3}(1.5)^2 - \frac{1}{3} (0) = 0.75\)</span></p></li>
</ul>
<p>Thus, <span class="math inline">\(E[Y] = 0.75\)</span></p>
<p><br></p>
<p>Expected values have a few properties:</p>
<ol type="1">
<li>Expected value of a constant is the constant: <span class="math inline">\(E[c] = c\)</span></li>
<li>Expected value of the expected value, is the expected value: <span class="math inline">\(E[E[Y]] = E[Y]\)</span></li>
<li>Expected value with a common factor: <span class="math inline">\(E[c \times g(Y)] = c \times E[g(Y)]\)</span></li>
<li>Expected value of independent events <span class="math inline">\(X, Y\)</span>: <span class="math inline">\(E[X,Y] = E[X] \times E[Y]\)</span></li>
</ol>
<p><br></p>
</section>
<section id="variance-and-standard-deviation" class="level3">
<h3 class="anchored" data-anchor-id="variance-and-standard-deviation">3.4: Variance and Standard Deviation</h3>
<p>Expected Value is a measure of the “centre” of the data. However, that isn’t the only thing interested about a random variable - another is the spread of the potential outcomes.</p>
<p><u>Variance and Standard Deviation is a measure of spread</u> - how much does the distribution spread around the mean. A distribution with a small variance will have a short distance between the mean and either end of the distribution. A distribution with a large variance will have a large distance between the mean and either end of the distribution</p>
<ul>
<li>Note: only continuous random variables can have variance and standard deviation</li>
</ul>
<p>Variance and Standard Deviation are important concepts as we go into statistical inference, hypothesis testing, and other more advanced methods.</p>
<p><br></p>
<p>Variance <span class="math inline">\(\sigma^2\)</span> is defined by the following formula, where <span class="math inline">\(n\)</span> is the number of observations:</p>
<p><span class="math display">\[
\sigma^2 = \frac{ \sum(y_i - E[Y])^2}{n} = E[ \space (Y - E[Y] \space)^2 \space ]
\]</span></p>
<p>Or in more intuitive terms:</p>
<ol type="1">
<li>Take any point <span class="math inline">\(y_i\)</span> in the data, and subtract the mean <span class="math inline">\(E[Y]\)</span> from it.</li>
<li>Square that difference</li>
<li>Now calculate that for every point <span class="math inline">\(y_i\)</span> in the data</li>
<li>Now sum up all those squares</li>
<li>Finally, divide by the number of points in the data <span class="math inline">\(n\)</span></li>
</ol>
<p><br></p>
<p>Standard Deviation is simply the square root of variance: <span class="math inline">\(\sigma = \sqrt{\sigma^2}\)</span></p>
<p><br></p>
</section>
<section id="normal-distribution" class="level3">
<h3 class="anchored" data-anchor-id="normal-distribution">3.5: Normal Distribution</h3>
<p>Normal Distributions are a form that many continuous random variables take. Normal Distributions are essential to much of statistical inference.</p>
<p>Normal Distributions take the shape of a “<u><strong>bell curve</strong></u>”, as seen in the figure below. All 3 measures of central tendency - mean, mode, and median, are equivalent at the centre of the bell curve. The Normal Distribution is also symmetrical.</p>
<p><img src="figures/figure3-normal.png" class="img-fluid" style="width:80.0%"></p>
<p>Normal Distributions have to key features: mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span></p>
<ul>
<li>We discussed how to calculate both in the previous 2 lessons</li>
</ul>
<p><u>Any normal distribution can be described with the 2 features of mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span></u> : <span class="math inline">\(X \sim \mathcal{N}(\mu, \sigma^2)\)</span>. For example, distribution <span class="math inline">\(X \sim \mathcal{N}(10, 3)\)</span> means random variable <span class="math inline">\(X\)</span> is randomly distributed in a normal distribution with mean 10 and variance 3.</p>
<p><br></p>
<p>Another feature of Normal Distributions, as shown in the figure above, is the <u>68-95-99.7 rule</u>. This states that the percentage of area under the probability density function of a Normal Distribution function is as follows:</p>
<ul>
<li><p>Between <span class="math inline">\(\mu - \sigma\)</span> and <span class="math inline">\(\mu + \sigma\)</span> (one standard deviation on both sides of the mean), there contains 68.26% of the total area under the curve</p></li>
<li><p>Between <span class="math inline">\(\mu - 2 \sigma\)</span> and <span class="math inline">\(\mu + 2 \sigma\)</span> (two standard deviations on both sides of the mean), there contains 95.44% of the total area under the curve</p></li>
<li><p>Between <span class="math inline">\(\mu - 3 \sigma\)</span> and <span class="math inline">\(\mu + 3 \sigma\)</span> (three standard deviations on both sides of the mean), there contains 99.72% of the total area under the curve</p></li>
</ul>
<p>These rules are consistent across any data that is normally distributed.</p>
<p>The importance of this is that, from previous discussion on continuous random variables, we know that the area under the probability density function is the probability. Thus, we can determine what probability certain outcomes have of occurring based on the standard deviations.</p>
<p><br></p>
<p>The <u>Standard Normal Distribution</u> is a specific normal distribution <span class="math inline">\(Z \sim \mathcal{N} (0,1)\)</span>. Or in other words, a normal distribution with a mean of 0 and a standard deviation of 1. The standard normal distribution is useful for the calculation of z-scores and probability.</p>
<p>We can actually transform any normal distribution into a standard normal distribution:</p>
<ul>
<li><p>First, we take any normal distribution <span class="math inline">\(X\)</span>, and subtract the mean of <span class="math inline">\(X\)</span>, <span class="math inline">\(\mu\)</span>, from all values: <span class="math inline">\(X - \mu\)</span></p></li>
<li><p>Then, we multiply the resulting distribution by <span class="math inline">\(1/\sigma\)</span>, with <span class="math inline">\(\sigma\)</span> being the standard deviation of <span class="math inline">\(X\)</span>. Thus, we get: <span class="math inline">\(\frac{X - \mu}{\sigma}\)</span></p></li>
</ul>
<p>The result of these transformations, on any normal distribution, results in the standard normal distribution: <span class="math inline">\(Z \sim \mathcal{N} (0,1)\)</span></p>
<p><br></p>
</section>
<section id="other-distributions" class="level3">
<h3 class="anchored" data-anchor-id="other-distributions">3.6: Other Distributions</h3>
<p>Aside from normal distributions, there are several other common types of distributions seen in political science.</p>
<p><br></p>
<p><strong>Uniform Distributions</strong></p>
<p>Uniform Distributions are the simplest type of distribution - where <u>every possible outcome has the same probability of occurring</u>.</p>
<p>Thus, the probability mass function/density function will be a straight line - since every outcome has the same probability.</p>
<p>Uniform distributions have a few properties:</p>
<ul>
<li><p>If all outcomes are between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, then <span class="math inline">\(f(y) = 1/(b-a)\)</span></p></li>
<li><p>The mean/expectation of a uniform distribution is <span class="math inline">\(E[Y] = (a+b)/2\)</span></p></li>
<li><p>The variance of a uniform distribution is <span class="math inline">\(\sigma^2 = (b-a)^2/12\)</span></p></li>
</ul>
<p>For example, rolling a dice creates a uniform distribution, since each side has an equal probability of occurring:</p>
<p><img src="figures/figure3-discrete.webp" class="img-fluid" style="width:70.0%"></p>
<p><br></p>
<p><strong>Binomial Distributions</strong></p>
<p>Binomial Distributions are a discrete distribution for counts. For example suppose you have a binary event with two outcomes (yes/no, success/fail). <u>Binomial distributions record probability of the number of “successes” observed after a certain number of trials</u>.</p>
<p>For example, if we flip a die 10 times, assuming that we want heads (heads = success), what is the probability of getting 0 heads? 1 heads? … 10 heads? A binomial distribution would plot all possible outcomes, and the probability associated with each.</p>
<p>Binomial Distributions must meet a few criteria:</p>
<ol type="1">
<li>Number of observations/trials is fixed</li>
<li>Each observation/trial is independent - i.e.&nbsp;flipping the first coin doesn’t affect the odds of the second coin</li>
<li>The probability of success for each observation/trial must be the same - i.e.&nbsp;for all 10 trials, the chance of getting heads must be consistent at 0.5</li>
</ol>
<p>Binomial distributions are commonly used in many regression techniques</p>
<p><br></p>
<p>The probability mass function of binomial distributions take the following form:</p>
<p><span class="math display">\[
p(y)= C(n,y)\times p^y q^{n-y}
\]</span></p>
<p>Where <span class="math inline">\(y\)</span> is the number of success we want to calculate the probability for, <span class="math inline">\(n\)</span> is the number of trials, <span class="math inline">\(p\)</span> is the probability of success in any trial, <span class="math inline">\(q\)</span> is the probability of failure so <span class="math inline">\(q=1-p\)</span> and <span class="math inline">\(p(y)\)</span> is the probability of a certain number of success <span class="math inline">\(y\)</span> occurring</p>
<p>The mean <span class="math inline">\(\mu, E[Y]\)</span> of a binomial distribution is <span class="math inline">\(n \times p\)</span>, and the variance <span class="math inline">\(\sigma^2\)</span> is equal to <span class="math inline">\(n \times p \times q\)</span></p>
<p><br></p>
<p><strong>Poisson Distribution</strong></p>
<p>A Poisson distribution is a discrete distribution, which measures the <u>probability of a given number of events occurring in an interval of time</u>.</p>
<p>For example, consider a tech support call centre. It, on average, receives 3 calls per minute during the entire day. Assuming that the random event of receiving a call is independent, we know the timing of the first call does not change the probability of when the next will occur. Poisson distribution can help us know the chances of receiving less than 3 calls per minute, or at least 5 calls per minute.</p>
<p><br></p>
<p>The probability mass function of a Poisson distribution is:</p>
<p><span class="math display">\[
p(y) = \frac{\lambda^y}{y!} e^{- \lambda}
\]</span></p>
<p>Where <span class="math inline">\(\lambda\)</span> is the average number of events that generally occur within the time interval, <span class="math inline">\(y\)</span> is the number of occurrences we are interested in finding the probability for, and <span class="math inline">\(p(y)\)</span> tells us the probability of <span class="math inline">\(y\)</span> occurrences occurring</p>
<p>Interestingly, in a Poisson distribution, the expectation equals the variance: <span class="math inline">\(E[Y] = \sigma^2 = \lambda\)</span></p>
<p>The Poisson distribution is often used to model rare events, and is used in the Poisson regression for counts. For example, it is used to predict the number of civil wars that occur in a specified time period.</p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io">Handbook Homepage</a></p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>