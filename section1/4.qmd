---
title: "Introductory Statistical Inference"
subtitle: "Module 4 (Section 1: Core Statistical Methods)"
format:
    html:
        theme: sketchy
        max-width: 800px
        toc: TRUE
        toc-depth: 3
        toc-location: left
        toc-title: Chapters
        toc-expand: TRUE
---

# Preface

Political Science research is heavily dominated by statistics. Statistics allows us to describe relationships between variables, infer beyond samples, and provides the basis for causal inference. However, to utilise common statistical techniques such as regression or quasi-experimental techniques, we first need to understand statistical inference.

This module is an introduction statistical inference. Chapter 1 focuses on how we can use samples and extrapolate to the population. Chapter 2 discusses hypothesis testing, and applies that to a difference of means test. Chapter 3 introduces relationships between variables, and provides a solid foundation for the upcoming linear regression module.

[Prerequisites]{.underline}: Section 0 (Or Equivalent)

------------------------------------------------------------------------

[Section Homepage](https://politicalscience.github.io/#section-1-statistical-methods)

# Chapter 1: Statistical Inference

### 1.1: Samples and Population

In political science and the social sciences, we are often interested in studying large groups of people and entities. For example, we might be interested some feature regarding all people in a country, such as the average income, or average working hours, or average education level.

However, if we are dealing with large population sizes, it is often impossible to ask every single individual in the population. For example, if we wanted to study the average educational level of the UK, we would need to ask nearly 70 million people. This is completely impractical.

[A sample is a subset of a population]{.underline}, where ideally, the sample can tell us something about the population. If our sample can reflect the greater population, then we can use the sample in our study, instead of the large population.

<br />

[Sampling]{.underline} is the process by which we select a sample from a larger population. Like I just mentioned, we want the sample to be representative of the larger population, so that we can use the sample to make claims about the population.

To make sure the sample is representative of the population, we need to determine is a given sample is a good sample or a bad sample. The quality of sample depends on two major factors:

1.  The sampling procedure which we decide to implement
2.  Luck

We will go back to luck in the future sections (hypothesis testing allows us to deal with that issue). Now, let us focus on sampling procedure.

The gold standard of sampling procedure is a [random sample]{.underline} - where individuals in the sample are selected at random from the population. This is because in a random sample, every possible individual has an equal chance of being selected, and thus, the resulting sample is likely to be reflective of the common traits of the population.

<br />

However, in real research, random samples are often very difficult to obtain. If a sample isn't randomly selected, then it may contain [sample bias]{.underline} - i.e., the sample does not accurately reflect the population. Common examples of sample bias include:

1.  [Self-Selection Bias]{.underline}: if you are sampling people because they agree to participate in your survey, your results may be biased. This is because people who do agree to participate in your sample may have a common trait together, which means one group of people may be over-represented in the sample compared to the population.
2.  [Non-Response Bias]{.underline}: This is the opposite of self-selection. Perhaps, you have randomly selected individuals, but some refuse to respond. It is very possible that people who don't want to respond may have some common trait together, which means that trait will be under-represented in your sample compared to the population
3.  [Survivorship Bias]{.underline}: This is when our sample only includes people who have successfully passed some selection process. A classic example is shark attacks - if you only ask people who have experienced shark attacks to estimate the percentage of people who survive shark attacks, you will get the result that 100% of people survive shark attacks. Why? Well, the people you ask have to have survived, or else, they would be dead and wouldn't respond to your sample!
4.  [Undercoverage Bias]{.underline}: This is when our sample under-represents under-represented groups. This is because under-represented groups are naturally, less common, so it is plausible that our sampling procedure may fail to contain them.
5.  [Proximity Sampling Bias]{.underline}: This is when we only sample people that are easy to sample. However, this can lead to bias. For example, if I am sampling students for how long they study every day, but I am lazy so I stand at the entrance of the library (since a lot of people enter/exit it), I might get a biased sample, since people entering the library are likely people who study more than the average student.

<br />

### 1.2: Central Limit Theorem

Before we introduce the Central Limit Theorem, we need to explain a [distribution of sample means]{.underline}.

Imagine that we take a random sample from a population. Then, we find the mean of the variable we are interested in the sample. That is a sample mean.

Then, let us take another sample from the same population, and find the mean. This will be slightly different than the first sample, since we are randomly sampling. That is another sample mean.

We keep taking samples from the same population, and getting more and more sample means.

Now, let us plot all our sample means into a "histogram" or density plot. The $x$ axis labels the possible sample means values, and the $y$ axis is how frequently a specific sample mean occurs. We will get a distribution, just like a random variable distribution.

That distribution is the distribution of sample means - it basically measures the frequency of different sample means that we get, given we keep drawing samples from the same population and calculating their means.

<br />

[The Central Limit Theorem states that the distribution of sample means of a variable, will be distributed in a approximately normal distribution]{.underline}. This is regardless of the variable's population distribution shape.

-   Essentially, no matter what distribution the population takes, if we take enough samples, the sample means plotted in a distribution will resemble a normal distribution.

There are a few criteria for the Central Limit Theorem to be true:

1.  The sample size (of the individual samples) should be at least 30
2.  Sample independence - one sample's results should not affect other sample's results
3.  Samples must be randomly sampled

There are a few additional points:

-   If the sample size condition is not met, but the population is normally distributed, we can still assume that central limit theorem occurs

-   If the sample size condition is not met, and the population is not normally distributed, we cannot use central limit theorem. The t-distribution may be of more use here.

<br />

[The Central Limit Theorem is arguably, the most important part of statistical inference]{.underline}. Why? If we recall from module 3, the normal distribution has some key properties, which are very useful for statistical inference purposes.

If we recall, the normal distribution has the 68-95-99.7 rule:

-   Between $\mu - \sigma$ and $\mu + \sigma$ (one standard deviation on both sides of the mean), there contains 68.26% of the total area under the curve

-   Between $\mu - 2 \sigma$ and $\mu + 2 \sigma$ (two standard deviations on both sides of the mean), there contains 95.44% of the total area under the curve

-   Between $\mu - 3 \sigma$ and $\mu + 3 \sigma$ (three standard deviations on both sides of the mean), there contains 99.72% of the total area under the curve

![](images/figure3-normal.png){width="90%"}

The importance of this is that, from previous discussion on continuous random variables in module 3, we know that the area under the probability density function is the probability. Thus, we can determine what probability certain outcomes have of occurring based on the standard deviations.

<br />

Furthermore, since the distribution of sample means takes the form of a normal distribution, and the distribution of sample means tells us the probability of getting some sample mean, [we can now tell how likely a sample mean is to occur if a sample was drawn from the population]{.underline}.

This goes back to the "luck" aspect of sampling. What if we are unlucky in sampling, and end up randomly drawing all the tall people? All the smartest people?

-   Well, we can actually know how likely that we would pick such a sample!

-   Just use the distribution of the sample means, and since it is normally distributed, we can find how many standard deviations it is from the mean, and thus, calculate how likely that sample is to occur!

-   For example, if a certain sample mean is located 2 standard deviations above the mean, there is only a 2.14% chance that that sample mean would be that value or higher (see figure above)

[Thus, Central Limit Theorem allows us to account for the luck aspect of sampling]{.underline}.

<br />

### 1.3: Z-Scores and Probability

A Z-score, also called the standard score, is [how many standard deviations a point is away from the mean]{.underline}. For example, a point 2 standard deviations away from the mean $\mu + 2\sigma$ has a z score of 2 - since it is 2 standard deviations away.

Z-score is basically a generalisation of the 68-95-99 rule. In that rule, we can only know the probabilities under the normal distribution for neat values of 1 standard deviation, 2 standard deviations, and 3 standard deviations. Z-scores can take any value - such as 1.96 standard deviations.

Calculating the z-score of a point is very simple. Let us say you have a variable $X$, and you get some point $x$. You want to find the z score of $x$ as follows:

$$
z = (x - E[X]) / \sigma
$$

Intuitively, you just subtract the mean from your value, then divide by the standard deviation to find the number of standard deviations away from the mean your value is.

<br />

Now you have the z-score, how do you find the probability of such a value occurring? What we do is we consult a z-table - which matches z-scores to probabilities.

-   The reason we do this is because while there is a rule for each z-score and its associated probabilities, there is no easy function to change a z-score into a probability. Thus, we use a table.

[Here](https://math.arizona.edu/~rsims/ma464/standardnormaltable.pdf) is a link to a z-score table. Note that the z-score tables only show the area to the left of the z-score point, or in other words, the chance of that value or something lower occuring.

-   If you want to find the probability of that value or something higher occurring, then do $1 -$ the probability.

-   Why does this work? Well, the total area under a normal distribution should equal 1 (since total probability is 1). Thus, if we are given the left side, the right side must be the remaining part.

<br />

### 1.4: T-Distributions

The t-distribution is a distribution very similar to that of a normal curve, with the bell-shape, however, it has a shorter peak, and thicker tails.

-   It is basically a normal distribution, but designed to account for smaller sample sizes

T-distributions are used when our sample size is too small to meet the Central Limit Theorem, and our population underneath is not normally distributed. T-distributions are also used in the t-Difference of Means test, which we will explore later.

<br />

Unlike the normal distribution, which has the parameters of mean $\mu$ and standard deviation $\sigma$, the t-Distribution only has one parameter - degrees of freedom $DF$

-   We will talk about how to calculate $DF$ when we use the t-distribution in statistical tests. It is usually the number of observations in a sample $n$ minus the number of variables involved (typically 1 or 2).

Just know in general, when $DF$ becomes higher, the tails become thinner and the peak becomes higher. When $DF$ becomes lower, the tails become thicker and the peak becomes lower.

-   Around 30 $DF$, the t-test approximates a standard normal distribution. This is why we generally switch the Central Limit Theorem to the t-Distribution under 30 sample size.

<br />

Just like how normal distributions have a z-score which is associated with some probability, t-Distributions have a t-value which is also associated with some probability

-   This allows us to do some statistical tests, as we will see later.

<br />

------------------------------------------------------------------------

[Section Homepage](https://politicalscience.github.io/#section-1-statistical-methods)

# Chapter 2: Hypothesis Testing

### 2.1: Hypothesis Testing

Hypothesis testing allows us to test, well, hypothesis. In statistics, we adopt a principle similar to "innocent until proven guilty" in English Law. In statistics, this basically means that we do not assume any relationship between variables or any significant difference, until we can comfortable reject the assumption that the "status-quo" is incorrect.

-   For example, if you are arguing that two things are different, in statistics, we cannot conclude that they are different, until we have [rejected]{.underline} "beyond a reasonable doubt" (usually 95% confidence) that the two are the same

-   For example, if you argue that two variables are related in some way, we cannot conclude they are related until we have [rejected]{.underline}, with 95% confidence, that the two do not have a relationship

<br />

Let us put all that into statistical terms:

The [null hypothesis]{.underline} is what we call the assumption that there is no difference/relationship - it is basically the "status-quo". It is often labeled as $H_0$

The [alternate hypothesis]{.underline} is the hypothesis we are trying to prove. It is often labeled as $H_1$

So, in statistical terms, [we assume the null hypothesis (status-quo) is true, unless we are 95% confident we can reject the null hypothesis, and only then, can we conclude that the alternate hypothesis is true]{.underline}.

We will discuss what 95% confidence means when we do a difference-of-means test later.

<br />

Of course, with 95% confidence, we will have errors. There are two types of errors:

1.  [Type-I error]{.underline}: this is when the null hypothesis is actually correct in the real world, but we accidentally reject it.
2.  [Type-II error]{.underline}: This is when the null hypothesis is actually wrong in the real world, but we accidentally do not reject it

You might ask, why 95%? Well, it is just convention in the social sciences. Other fields may have different confidence levels - for example, in drug trials, it is much much higher - since it would be a disaster to approve a faulty drug.

-   There is nothing special about 95%, in fact many argue it is a bad measure. After all, what makes something magically better being 95% confident than 94% confident?

<br />

### 2.2: Difference of Means Test

There are multiple types of difference of means tests, including the common z-test and t-test. We will focus on t-tests, since the z-test requires us to know the population's standard deviation, which we rarely do know in political science.

A difference of means test is used to show if there is a statistically significant difference between two population means, based on the 2 sample means we have. For example, perhaps you have 2 samples on the average weekly working hours of individuals, one from the UK, and one from France. You might be interested in knowing if there is a significant difference in working hours between the two countries.

-   What does [significant difference]{.underline} mean? Well remember, when we sample, there is a luck element to sampling. For two means to be different, we want to be 95% confident that the difference between the samples is not due to luck, but because the two population means are actually different.

More mathematically, for the mean of population 1 $\mu_1$ and the mean of population 2 $\mu_2$, we are seeing if the difference between them $\Delta = \mu_2 - \mu_1$ is not 0, since 0 would imply they have the same mean.

<br />

### 2.3: Programming: Difference of Means

We will execute this difference of means test in 3 different applications: R, Python, and Stata

**In R:**

**In Python:**

**In Stata:**

<br />

------------------------------------------------------------------------

[Section Homepage](https://politicalscience.github.io/#section-1-statistical-methods)

# Chapter 3: Correlation

### 3.1: Covariance

In political science, we are often interested in the relationship between two variables. For example, are oil producers more likely to be democratic? Are more educated voters more likely to turn out and vote?

The relationship between two features, also called correlation, is the extent to which they tend to occur together.

-   A positive correlation/relationship is when we are more likely to observe feature $Y$, if feature $X$ is present

-   A negative correlation/relationship is when we are less likely to observe feature $Y$, if feature $X$ is present

-   No correlation/relationship is when we see feature $X$, it doesn't tell us anything about the likelihood of observing $Y$

Graphically, a positive and negative correlation are as follows:

![](figures/4-correlation.png){width="100%"}

<br />

Covariance is a way to measure to relationship between two variables. Essentially, [covariance is the extent that $X$ and $Y$ vary together]{.underline}. Covariance is calculated as follows:

$$
Cov(X,Y) = \sigma_{XY} = \frac{\sum (X_i - E[X])(Y_i - E[Y])}{n}
$$

These are the parts of the equation:

-   In our data, we have many different pairs of data points $(X_i, Y_i)$

<!-- -->

-   $X_i$ is some point of $X$, while $E[X]$ is the mean of $X$. Same goes for $Y_i$ and $E[Y]$

-   Thus, $X_i - E[X]$ is the distance between any point $X_i$ and its mean $E[X]$. Same goes for $Y_i - E[Y]$

-   $n$ is the number of observations in our data.

<br />

[We can interpret the sign of the covariance]{.underline}

-   When the covariance is positive, then their is a positive correlation/relationship between $X$ and $Y$.

-   When the covariance is negative, then their is a negative correlation/relationship between $X$ and $Y$

-   When the covariance is 0, then there is no relationship between $X$ and $Y$

However, [we cannot interpret the size of the covariance]{.underline}

-   This is because covariance is sensitive to the way we measure $X$ and $Y$

-   For example, let us consider that $X$ is a measurement of age. We could measure age in years, or months. But by measuring in months, we are increasing all values of $X$ by 12 times. Yet, the data, and its variation are still exactly the same - we just changed the scale. However, covariance will also increase.

-   Thus, covariance is sensitive to scale, so we cannot interpret the numerical value of covariance.

<br />

### 3.2: Correlation Coefficient

Like we just mentioned, the magnitude of covariance cannot be measured.

However, we can "normalise" covariance, so that measurement scale does not impact the value. We do this by dividing the covariance by the standard deviation of $X$ multiplied by the standard deviation of $Y$

This standardised version of covariance is called the [correlation coefficient, which is always between -1 and 1]{.underline}. Thus, this allows us to [measure the strength of a correlation]{.underline}. The formula is as follows:

$$
Corr(X,Y) = r = \rho= \frac{Cov(X,Y)}{\sigma_X \sigma_Y}
$$

Like mentioned, this correlation coefficient, no matter the data, is always between -1 and 1.

-   A negative correlation coefficient means a negative correlation

-   A positive correlation coefficient means a positive correlation

-   A correlation coefficient of 0 means no correlation

However, we can also interpret the [strength of a correlation]{.underline} through the correlation coefficient

-   Closer to -1 or 1 means a strong negative/positive correlation

-   Closer to 0 means a weaker negative/positive correlation

Generally, the rule of thumb is that around ±0.8 or higher is a strong correlation. However, this depends on the field/subfield, and the specific topic we are studying.

Below is a figure of different plots. Plots $a,b,c$ have positive correlation coefficients, and plots $d,e,f$ have negative correlation coefficients. Plots $a$ and $d$ have strong correlations (close to ±1), plots $b$ and $e$ have moderately strong correlations (close to ±0.5), and plots $c$ and $f$ have weak correlations (close to 0)

![](figures/4-corrStrength.png){width="100%"}

<br />

There is one extension of the correlation coefficient - called the [r-squared value]{.underline}. R-squared is exactly what it sounds like - the correlation coefficient squared. Since it is squared, the r-squared value is always between 0 and 1.

The r-squared is useful because it gets rid of the sign on the correlation coefficient, and thus, only shows the strength of the relationship between two variables. More accurately, the r-squared value shows the percentage of variation of $Y$ explained by $X$

The r-squared value will become especially important as we dive into regression - especially multivariate models, since the r-squared value can tell us generally how "good" our models are at predicting $Y$ from $X$

<br />

### 3.3: Best Linear Estimator