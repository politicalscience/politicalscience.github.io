<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Classical Least Squares Theory – Political Science &amp; Political Economy</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="mathjax-config.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./quant3.html">3 Classical Least Squares Theory</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Kevin’s PSPE Resources</a> 
    </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./games.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guide to Game Theory</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
 <span class="menu-text">Quantitative Methods (Causal Inference)</span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 Introductory Statistics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 Multiple Linear Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant3.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">3 Classical Least Squares Theory</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 Causal Frameworks</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 Randomised Controlled Trials</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 Selection on Observables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant7.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7 Instrumental Variables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant8.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8 Regression Discontinuity</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant9.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9 Differences-in-Differences</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant10.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10 Synthetic Controls Method</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
 <span class="menu-text">Further Statistical Models</span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model1.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Logistic Regression Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model2.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Regression for Counts</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Random Forests and Bagging</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principle Components Analysis</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Factor Analysis Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model6.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Latent Trait Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model7.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Latent Class Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model8.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Cluster Analysis</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model9.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Structural Equation Models</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
 <span class="menu-text">Additional Materials</span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear Algebra Reference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./calc.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Calculus Reference</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On This Page</h2>
   
  <ul>
  <li><a href="#ordinary-least-squares-estimator" id="toc-ordinary-least-squares-estimator" class="nav-link active" data-scroll-target="#ordinary-least-squares-estimator"><strong>Ordinary Least Squares Estimator</strong></a>
  <ul class="collapse">
  <li><a href="#deriving-the-estimator" id="toc-deriving-the-estimator" class="nav-link" data-scroll-target="#deriving-the-estimator">Deriving the Estimator</a></li>
  <li><a href="#ols-orthogonal-projection" id="toc-ols-orthogonal-projection" class="nav-link" data-scroll-target="#ols-orthogonal-projection">OLS Orthogonal Projection</a></li>
  </ul></li>
  <li><a href="#sample-properties-of-ols" id="toc-sample-properties-of-ols" class="nav-link" data-scroll-target="#sample-properties-of-ols"><strong>Sample Properties of OLS</strong></a>
  <ul class="collapse">
  <li><a href="#ols-as-an-unbiased-estimator" id="toc-ols-as-an-unbiased-estimator" class="nav-link" data-scroll-target="#ols-as-an-unbiased-estimator">OLS as an Unbiased Estimator</a></li>
  <li><a href="#deriving-ols-variance" id="toc-deriving-ols-variance" class="nav-link" data-scroll-target="#deriving-ols-variance">Deriving OLS Variance</a></li>
  <li><a href="#gauss-markov-theorem" id="toc-gauss-markov-theorem" class="nav-link" data-scroll-target="#gauss-markov-theorem">Gauss-Markov Theorem</a></li>
  <li><a href="#heteroscedasticity-robust-standard-errors" id="toc-heteroscedasticity-robust-standard-errors" class="nav-link" data-scroll-target="#heteroscedasticity-robust-standard-errors">Heteroscedasticity-Robust Standard Errors</a></li>
  <li><a href="#asymptotic-consistency-of-ols" id="toc-asymptotic-consistency-of-ols" class="nav-link" data-scroll-target="#asymptotic-consistency-of-ols">Asymptotic Consistency of OLS</a></li>
  <li><a href="#ols-as-a-conditional-expectation-function" id="toc-ols-as-a-conditional-expectation-function" class="nav-link" data-scroll-target="#ols-as-a-conditional-expectation-function">OLS as a Conditional Expectation Function</a></li>
  </ul></li>
  <li><a href="#regression-anatomy-and-controls" id="toc-regression-anatomy-and-controls" class="nav-link" data-scroll-target="#regression-anatomy-and-controls"><strong>Regression Anatomy and Controls</strong></a>
  <ul class="collapse">
  <li><a href="#controlling-for-regressors" id="toc-controlling-for-regressors" class="nav-link" data-scroll-target="#controlling-for-regressors">Controlling for Regressors</a></li>
  <li><a href="#partitioned-regression-model" id="toc-partitioned-regression-model" class="nav-link" data-scroll-target="#partitioned-regression-model">Partitioned Regression Model</a></li>
  <li><a href="#unbiasedness-of-regression-anatomy" id="toc-unbiasedness-of-regression-anatomy" class="nav-link" data-scroll-target="#unbiasedness-of-regression-anatomy">Unbiasedness of Regression Anatomy</a></li>
  <li><a href="#variance-of-regression-anatomy" id="toc-variance-of-regression-anatomy" class="nav-link" data-scroll-target="#variance-of-regression-anatomy">Variance of Regression Anatomy</a></li>
  <li><a href="#including-irrelevant-variables" id="toc-including-irrelevant-variables" class="nav-link" data-scroll-target="#including-irrelevant-variables">Including Irrelevant Variables</a></li>
  <li><a href="#omitted-variable-bias" id="toc-omitted-variable-bias" class="nav-link" data-scroll-target="#omitted-variable-bias">Omitted Variable Bias</a></li>
  </ul></li>
  <li><a href="#method-of-moments-estimator" id="toc-method-of-moments-estimator" class="nav-link" data-scroll-target="#method-of-moments-estimator"><strong>Method of Moments Estimator</strong></a>
  <ul class="collapse">
  <li><a href="#method-of-moments" id="toc-method-of-moments" class="nav-link" data-scroll-target="#method-of-moments">Method of Moments</a></li>
  <li><a href="#population-mean-estimator" id="toc-population-mean-estimator" class="nav-link" data-scroll-target="#population-mean-estimator">Population Mean Estimator</a></li>
  <li><a href="#ols-as-a-method-of-moments-estimator" id="toc-ols-as-a-method-of-moments-estimator" class="nav-link" data-scroll-target="#ols-as-a-method-of-moments-estimator">OLS as a Method of Moments Estimator</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Classical Least Squares Theory</h1>
<p class="subtitle lead">Chapter 3, Quantitative Methods (Causal Inference)</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Last chapter, we discussed the multiple linear regression model, and how it can help us measure relationships between explanatory and outcome variables.</p>
<p>This chapter introduces some key theory regarding the ordinary least squares estimator behind linear regression. Topics covered includes properties of estimators, the OLS estimator, and the Method of Moments estimator.</p>
<p>Use the right sidebar for quick navigation.</p>
<hr>
<section id="ordinary-least-squares-estimator" class="level1">
<h1><strong>Ordinary Least Squares Estimator</strong></h1>
<section id="deriving-the-estimator" class="level3">
<h3 class="anchored" data-anchor-id="deriving-the-estimator">Deriving the Estimator</h3>
<p>Our <a href="./quant2.html">linear regression</a> model, and the fitted values <span class="math inline">\(\hat{\mathbf{y}}\)</span>, take the following form:</p>
<p><span class="math display">\[
\mathbf y = \mathbf X \boldsymbol\beta + \mathbf u, \qquad \hat{\mathbf y} = \mathbf X \hat{\boldsymbol\beta}
\]</span></p>
<p>OLS minimises the sum of squared residuals <span class="math inline">\(S(\hat{\boldsymbol\beta})\)</span> - the differences between the actual <span class="math inline">\(\mathbf y\)</span> and our predicted <span class="math inline">\(\hat{\mathbf y}\)</span>:</p>
<p><span class="math display">\[
\begin{align}
S(\hat{\boldsymbol\beta}) &amp; = (\mathbf y - \hat{\mathbf y})^\mathsf{T} (\mathbf y - \hat{\mathbf y})\\
&amp; = (\mathbf y - \color{blue}{\mathbf X \hat{\boldsymbol\beta}}\color{black})^\mathsf{T} (\mathbf y - \color{blue}{\mathbf{X} \hat{\boldsymbol\beta}}\color{black}) &amp;&amp; (\text{plug in } \color{blue}{\hat{\mathbf y}  = \mathbf X \hat{\boldsymbol\beta}}\color{black}) \\
&amp; = \mathbf y^\mathsf{T} \mathbf y - \hat{\boldsymbol\beta}^\mathsf{T} \mathbf X^\mathsf{T} \mathbf y - \mathbf y^\mathsf{T} \mathbf{X}\hat{\boldsymbol\beta} + \hat{\boldsymbol\beta}^\mathsf{T} \mathbf X^\mathsf{T} \mathbf{Xb} &amp;&amp; (\text{distribute out)} \\
&amp; = \mathbf y^\mathsf{T} \mathbf y - \color{blue}{2\hat{\boldsymbol\beta}^\mathsf{T} \mathbf X^\mathsf{T} \mathbf y}\color{black} + \hat{\boldsymbol\beta}^\mathsf{T} \mathbf X^\mathsf{T} \mathbf{X} \hat{\boldsymbol\beta} &amp;&amp;(\text{combine } \color{blue}{- \hat{\boldsymbol\beta}^\mathsf{T} \mathbf X^\mathsf{T} \mathbf y - \mathbf y^\mathsf{T} \mathbf{X}\hat{\boldsymbol\beta}}\color{black})
\end{align}
\]</span></p>
<p>Now, let us find the first order condition:</p>
<p><span class="math display">\[
\frac{\partial S(\hat{\boldsymbol\beta})}{\partial \hat{\boldsymbol\beta}} = -2\mathbf X^\mathsf{T} \mathbf y + 2 \mathbf X^\mathsf{T} \mathbf{X} \hat{\boldsymbol\beta} = 0
\]</span></p>
<p>When assuming <span class="math inline">\(\mathbf X^\mathsf{T} \mathbf X\)</span> is invertable (which is true if <span class="math inline">\(\mathbf X\)</span> is full rank), we can isolate <span class="math inline">\(\hat{\beta}\)</span> to find the solution to OLS:</p>
<p><span id="eq-ols"><span class="math display">\[
\begin{align}
-2\mathbf X^T\mathbf y + 2 \mathbf X^\mathsf{T} \mathbf X \boldsymbol{\hat{\beta}} &amp; = 0 \\
2 \mathbf X^\mathsf{T} \mathbf X \boldsymbol{\hat\beta} &amp; = 2\mathbf X^\mathsf{T} \mathbf y &amp;&amp; (+ 2\mathbf X^\mathsf{T} \mathbf y \text{ to both sides}) \\
\boldsymbol{\hat\beta} &amp; = (2\mathbf X^\mathsf{T} \mathbf X)^{-1} 2 \mathbf X^\mathsf{T} \mathbf y &amp;&amp; (\times (2\mathbf X^\mathsf{T} \mathbf X)^{-1} \text{ to both sides})\\
\boldsymbol{\hat\beta} &amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf y &amp;&amp;(\text{cancel out } 2^{-1}\times 2)
\end{align}
\tag{1}\]</span></span></p>
<p>Those are our coefficient solutions to OLS.</p>
<p><br></p>
</section>
<section id="ols-orthogonal-projection" class="level3">
<h3 class="anchored" data-anchor-id="ols-orthogonal-projection">OLS Orthogonal Projection</h3>
<p>We can use the OLS solution from <a href="#eq-ols" class="quarto-xref">Equation&nbsp;1</a> to get our fitted values <span class="math inline">\(\hat{\mathbf y}\)</span> and residuals <span class="math inline">\(\hat{\mathbf u}\)</span>:</p>
<p><span id="eq-projection"><span class="math display">\[
\begin{align}
\hat{\mathbf y} &amp; = \mathbf X \hat{\boldsymbol\beta} \\
&amp; = \mathbf X \color{blue}{(\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf y} &amp;&amp; \color{black}(\text{plug in OLS solution } \color{blue}{\hat{\boldsymbol\beta} = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf y}) \\
&amp; = \color{red}{\mathbf P}\color{black}{\mathbf y} &amp;&amp; \text{(where } \color{red}{\mathbf P :=\mathbf X (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T}})
\end{align}
\tag{2}\]</span></span></p>
<p><span class="math display">\[
\begin{align}
\hat{\mathbf u} &amp; = \mathbf y - \hat{\mathbf y} \\
&amp; = \mathbf y - \color{blue}{\mathbf P\mathbf y} &amp;&amp; \color{black}(\text{from equation (2) } \color{blue}{\hat{\mathbf y} = \mathbf P \mathbf y} \color{black}) \\
&amp; = (\mathbf I - \mathbf P)\mathbf y &amp;&amp; (\text{factor out } \mathbf y) \\
&amp; = \color{purple}{\mathbf M} \color{black}{\mathbf y} &amp;&amp; (\text{where } \color{purple}{\mathbf M = \mathbf I - \mathbf P} \color{black})
\end{align}
\]</span></p>
<p>Matrix <span class="math inline">\(\color{red}{\mathbf P}\)</span>, called the <strong>projection matrix</strong>, is a matrix operator that performs the operation <span class="math inline">\(\mathbf y \rightarrow \hat{\mathbf y}\)</span>. Matrix <span class="math inline">\(\color{purple}{\mathbf M}\)</span>, called the <strong>residual maker</strong>, is a matrix operator that performs the operation <span class="math inline">\(\mathbf y \rightarrow \hat{\mathbf u}\)</span>.</p>
<p>We know that our fitted values <span class="math inline">\(\hat{\mathbf y}\)</span> are created as a linear combination of our explanatory variables <span class="math inline">\(\mathbf X\)</span> (hence linear regression). That means, by the definition of <a href="linear.qmdl#spanning-vectors-and-dimension">vector spaces</a>, that our explanatory variable vectors <span class="math inline">\(\mathbf x_j\)</span> span a space that includes our fitted values <span class="math inline">\(\hat{\mathbf y}\)</span>.</p>
<p>So, what the matrix operator <span class="math inline">\(\color{red}{\mathbf P}\)</span> is doing is essentially taking our original data <span class="math inline">\(\mathbf y\)</span> values, and projecting it into the space spanned by our explanatory variables <span class="math inline">\(\mathbf X\)</span> (called the <strong>column space</strong>). We can see in the figure below, our observed <span class="math inline">\(\mathbf y\)</span> vector is being projected onto the blue plane spanned by <span class="math inline">\(\mathbf X\)</span> to create our fitted values vector <span class="math inline">\(\hat{\mathbf y}\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-880030792.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>Residual maker matrix <span class="math inline">\(\color{purple}{\mathbf M}\)</span> projects <span class="math inline">\(\mathbf y\)</span> onto the space orthogonal to the column space of <span class="math inline">\(\mathbf X\)</span> to get our residuals <span class="math inline">\(\hat{\mathbf u}\)</span>. We can see this in the figure above, where the residuals vector (notated <span class="math inline">\(\mathbf e\)</span> in the figure) is orthogonal/perpendicular to the space of <span class="math inline">\(\mathbf X\)</span>.</p>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="sample-properties-of-ols" class="level1">
<h1><strong>Sample Properties of OLS</strong></h1>
<section id="ols-as-an-unbiased-estimator" class="level3">
<h3 class="anchored" data-anchor-id="ols-as-an-unbiased-estimator">OLS as an Unbiased Estimator</h3>
<p>OLS is an <a href="#unbiased-estimators">unbiased estimator</a> of the relationship between any <span class="math inline">\(x_j\)</span> and <span class="math inline">\(y\)</span> under 4 conditions:</p>
<ol type="1">
<li><strong>Linearity</strong> in parameters: the model of the population (data generating process) can be modelled as <span class="math inline">\(\mathbf y = \mathbf X \boldsymbol\beta + \mathbf u\)</span>.</li>
<li><strong>Random Sampling</strong>: the observations in our sample are randomly sampled.</li>
<li><strong>No Perfect Multicolinearity</strong>: There is no exact linear relationships between the regressors. This ensures that <span class="math inline">\(\mathbf X^\mathsf{T} \mathbf X\)</span> is invertible, which is required for the derivation of OLS.</li>
<li><strong>Zero Conditional Mean</strong>: <span class="math inline">\(E(\mathbf u|\mathbf X) = 0\)</span>. This implies that no <span class="math inline">\(x_j\)</span> is correlated with <span class="math inline">\(\mathbf u\)</span> (exogeneity), and no function of multiple regressors is correlated with <span class="math inline">\(\mathbf u\)</span>.</li>
</ol>
<p>Let us prove OLS is unbiased - i.e.&nbsp;<span class="math inline">\(E(\hat{\boldsymbol\beta}) = \boldsymbol\beta\)</span>. Let us manipulate our OLS solution:</p>
<p><span class="math display">\[
\begin{align}
\boldsymbol{\hat\beta} &amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf y \\
&amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T}\color{blue}{(\mathbf X \boldsymbol\beta + \mathbf u)} &amp;&amp; \color{black}(\text{plug in } \color{blue}{\mathbf y = \mathbf X \boldsymbol\beta + \mathbf u}\color{black}) \\
&amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf X \boldsymbol\beta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u &amp;&amp;(\text{multiply out})\\
&amp; = \color{blue}{\mathbf I}\color{black}{\boldsymbol\beta} + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u &amp;&amp;( \ \color{blue}{(\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf X = \mathbf I}\color{black})\\
&amp; = \boldsymbol\beta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u &amp;&amp; (\text{identity property of } \mathbf I)
\end{align}
\]</span></p>
<p>Now, let us take the expectation of <span class="math inline">\(\boldsymbol{\hat\beta}\)</span> conditional on <span class="math inline">\(\mathbf X\)</span>. Remember condition 4, <span class="math inline">\(E(\mathbf u | \mathbf X) = 0\)</span>:</p>
<p><span class="math display">\[
\begin{align}
E(\boldsymbol{\hat\beta}|\mathbf X) &amp; = \boldsymbol\beta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} E(\mathbf u | \mathbf X) &amp;&amp;(\mathbf u \text{ conditional on value of } \mathbf X) \\
E(\boldsymbol{\hat\beta}|\mathbf X) &amp; = \boldsymbol\beta &amp;&amp;(E(\mathbf u | \mathbf X) = 0)
\end{align}
\]</span></p>
<p>Now, we can use the law of iterated expectations (LIE) to conclude this proof:</p>
<p><span class="math display">\[
\begin{align}
E(\boldsymbol{\hat\beta}) &amp; = E(E(\boldsymbol{\hat\beta}|\mathbf X)) &amp;&amp; (\text{LIE: E(X) = E(E(X|Y))})\\
&amp; = E(\boldsymbol\beta) &amp;&amp; (\text{LIE: E(X) = E(E(X|Y))})\\
&amp; = \boldsymbol\beta &amp;&amp; (\text{expecation of a constant})
\end{align}
\]</span></p>
<p>Thus, OLS is unbiased under the 4 conditions above.</p>
<p><br></p>
</section>
<section id="deriving-ols-variance" class="level3">
<h3 class="anchored" data-anchor-id="deriving-ols-variance">Deriving OLS Variance</h3>
<p>To derive variance, let us add another condition: Homoscedasticity. This is when no matter the values of any explanatory variable, the error term variance is <strong>constant</strong> at <span class="math inline">\(\sigma^2\)</span>. The error term variance does not change based on the values of the explanatory variables:</p>
<p><span class="math display">\[
Var(\mathbf u | \mathbf X) = \sigma^2 \mathbf I_n = \begin{pmatrix}
\sigma^2 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \sigma^2 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; \dots &amp; \sigma^2
\end{pmatrix}
\]</span></p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Visualisation of Homoscedasticity
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>An easy way to identify homoscedasticity is to look at a residual plot (just the plot of all <span class="math inline">\(\widehat{u_i}\)</span>):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-1713529842.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%"></p>
</figure>
</div>
<p>Notice how the homoscedasticity residuals seem to have the same up-down variance, no matter the value of <span class="math inline">\(x\)</span>.</p>
<p>The heteroscedasticity residuals have a clear pattern - the up-down variance is smaller when <span class="math inline">\(x\)</span> is smaller, and the up-down variance is larger when <span class="math inline">\(x\)</span> is larger.</p>
<p>Essentially, if you see a pattern in the residual plot, it is likely heteroscedasticity.</p>
</div>
</div>
</div>
<p>Let us assume homoscedasticity. We want to find the variance of our estimator, <span class="math inline">\(Var(\boldsymbol{\hat\beta} | \mathbf X)\)</span>. Let us start off with our OLS solution. We can simplify as follows:</p>
<p><span class="math display">\[
\begin{align}
\boldsymbol{\hat\beta} &amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf y \\
&amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T}(\mathbf X \boldsymbol\beta + \mathbf u) &amp;&amp; (\text{plug in } \mathbf y = \mathbf X \boldsymbol\beta + \mathbf u) \\
&amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf X \boldsymbol\beta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u &amp;&amp;(\text{multiply out})\\
&amp; = \color{blue}{\mathbf I}\color{black}{\boldsymbol\beta} + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u &amp;&amp;( \ \color{blue}{(\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf X = \mathbf I}\color{black})\\
&amp; = \boldsymbol\beta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u &amp;&amp; (\text{identity property of } \mathbf I)
\end{align}
\]</span></p>
<p><span class="math display">\[
Var(\boldsymbol{\hat\beta} | \mathbf X) = Var(\boldsymbol\beta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u \ | \ \mathbf X)
\]</span></p>
<p><span class="math inline">\(\boldsymbol\beta\)</span> is a vector of fixed constants. <span class="math inline">\((\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u\)</span> can be imagined as a matrix of fixed constants, since we are conditioning the above variance on <span class="math inline">\(\mathbf X\)</span> (so for each <span class="math inline">\(\mathbf X\)</span>, the statement is fixed).</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mathematical Lemma
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>If <span class="math inline">\(\mathbf u\)</span> is an <span class="math inline">\(n\)</span> dimensional vector of random variables, <span class="math inline">\(\mathbf c\)</span> is an <span class="math inline">\(m\)</span> dimensional vector, and <span class="math inline">\(\mathbf B\)</span> is an <span class="math inline">\(n \times m\)</span> dimensional matrix with fixed constants, then the following is true:</p>
<p><span class="math display">\[
Var(\mathbf c + \mathbf{Bu}) = \mathbf B Var(\mathbf u)\mathbf B^T
\]</span></p>
<p>I will not prove this lemma here, but it is provable.</p>
</div>
</div>
</div>
<p>With the Lemma above, and with the definition of homoscedasticity, we can simplify:</p>
<p><span class="math display">\[
\begin{align}
Var(\boldsymbol{\hat\beta} | \mathbf X) &amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} Var(\mathbf u | \mathbf X) [(\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T}]^{-1} &amp;&amp; (\text{lemma})\\
&amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} Var(\mathbf u | \mathbf X) \color{blue}{\mathbf X (\mathbf X^\mathsf{T} \mathbf X)^{-1}} &amp;&amp; \color{black}( \ \color{blue}{[(\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T}]^{-1} = (\mathbf X^\mathsf{T} \mathbf X)^{-1}}\color{black})\\
&amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \color{blue}{\sigma^2 \mathbf I_n}\color{black}{ \mathbf X} (\mathbf X^\mathsf{T} \mathbf X)^{-1} &amp;&amp; (\color{blue}{Var(\mathbf u | \mathbf X) = \sigma^2 \mathbf I_n}\color{black}) \\
&amp; =  \color{red}{\sigma^2} \color{black} (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf I_n \mathbf X (\mathbf X^\mathsf{T} \mathbf X)^{-1} &amp;&amp; (\text{move scalar } \color{red}{\sigma^2}\color{black})\\
&amp; =  \sigma^2 (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T}  \mathbf X (\mathbf X^\mathsf{T} \mathbf X)^{-1} &amp;&amp; (\text{identity property of } \mathbf I_n)\\
&amp; =  \sigma^2 (\mathbf X^\mathsf{T} \mathbf X)^{-1} &amp;&amp; (\text{inverses } \mathbf X^\mathsf{T}  \mathbf X (\mathbf X^\mathsf{T} \mathbf X)^{-1} \text{ cancel})
\end{align}
\]</span></p>
<p>However, we do not actually know what <span class="math inline">\(\sigma^2\)</span> is. We can estimate it with <span class="math inline">\(\hat\sigma^2\)</span> (discussed <a href="#residual-standard-deviation">here</a>).</p>
<p><br></p>
</section>
<section id="gauss-markov-theorem" class="level3">
<h3 class="anchored" data-anchor-id="gauss-markov-theorem">Gauss-Markov Theorem</h3>
<p>The Gauss-Markov Theorem states that the OLS estimator is the <strong>best linear unbiased estimator</strong> (BLUE) - the unbiased linear estimator with the lowest variance, under 5 conditions:</p>
<ol type="1">
<li>Linearity (see unbiasedness conditions)</li>
<li>Random Sampling (…)</li>
<li>No Perfect Multicollinearity (…)</li>
<li>Zero-Conditional Mean (…)</li>
<li><strong>Homoscedasticity</strong> (the new condition).</li>
</ol>
<p>Proof</p>
<p><br></p>
</section>
<section id="heteroscedasticity-robust-standard-errors" class="level3">
<h3 class="anchored" data-anchor-id="heteroscedasticity-robust-standard-errors">Heteroscedasticity-Robust Standard Errors</h3>
<p><br></p>
</section>
<section id="asymptotic-consistency-of-ols" class="level3">
<h3 class="anchored" data-anchor-id="asymptotic-consistency-of-ols">Asymptotic Consistency of OLS</h3>
<p>OLS is an <a href="./quant1.html#asymptotically-consistent-estimators">asymptotically consistent estimator</a> of the relationship between any <span class="math inline">\(x_j\)</span> and <span class="math inline">\(y\)</span> under 4 conditions. These conditions are identical to the unbiasedness conditions EXCEPT condition 4, which is weakened from the original unbiasedness condition.</p>
<ol type="1">
<li><strong>Linearity</strong> (see unbiasedness)</li>
<li><strong>Random Sampling</strong> (…)</li>
<li><strong>No Perfect Multicolinearity</strong> (…)</li>
<li><strong>Zero Mean and Exogeneity</strong>: <span class="math inline">\(E(u_i) = 0\)</span>, and <span class="math inline">\(Cov(x_i, u_i) = 0\)</span>, which implies <span class="math inline">\(E(\mathbf x_i u_i) = 0\)</span>. This means that no regressor should be correlated with <span class="math inline">\(\mathbf u\)</span>. This is weaker than Zero-Conditional mean, since it means a function of regressors can be correlated with <span class="math inline">\(\mathbf u\)</span>.</li>
</ol>
<p>We need condition 3 to ensure <span class="math inline">\(\mathbf X^\mathsf{T} \mathbf X\)</span> is invertible, in order to have OLS estimates. Once we have OLS estimates (derivation above), we can manipulate it as following:</p>
<p><span class="math display">\[
\begin{align}
\boldsymbol{\hat\beta} &amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf y \\
&amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T}\color{blue}{(\mathbf X \boldsymbol\beta + \mathbf u)} &amp;&amp; \color{black}(\text{plug in } \color{blue}{\mathbf y = \mathbf X \boldsymbol\beta + \mathbf u}\color{black}) \\
&amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf X \boldsymbol\beta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u &amp;&amp;(\text{multiply out})\\
&amp; = \color{blue}{\mathbf I}\color{black}{\boldsymbol\beta} + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u &amp;&amp;( \ \color{blue}{(\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf X = \mathbf I}\color{black})\\
&amp; = \boldsymbol\beta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u &amp;&amp; (\text{identity property of } \mathbf I)
\end{align}
\]</span></p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Vector Notation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The following statements are true:</p>
<p><span class="math display">\[
\begin{split}
&amp; \mathbf X^\mathsf{T} \mathbf X = \sum\limits_{i=1}^n \mathbf x_i \mathbf x_i^\mathsf{T} \\
&amp; \mathbf X^\mathsf{T} \mathbf  u = \sum\limits_{i=1}^n \mathbf x_i u_i
\end{split}
\]</span></p>
</div>
</div>
</div>
<p>Using vector notation, <a href="./quant1.html#asymptotically-consistent-estimators">law of large numbers</a>, and zero-mean and exogeneity condition, we can simplify the above to:</p>
<p><span class="math display">\[
\begin{align}
\boldsymbol{\hat\beta} &amp; = \boldsymbol\beta + \left( \sum\limits_{i=1}^n \mathbf x_i \mathbf x_i^\mathsf{T} \right)^{-1} \left( \sum\limits_{i=1}^n \mathbf x_i \mathbf u \right) &amp;&amp; (\text{vector notation})\\
\boldsymbol{\hat\beta} &amp; = \boldsymbol\beta + \left( \frac{1}{n}\sum\limits_{i=1}^n \mathbf x_i \mathbf x_i^\mathsf{T} \right)^{-1} \left( \frac{1}{n}\sum\limits_{i=1}^n \mathbf x_i \mathbf u \right) &amp;&amp; ( \ \left(\frac{1}{n} \right)^{-1} \text{and } \frac{1}{n} \text{ cancel out}) \\
\text{plim} \boldsymbol{\hat\beta} &amp; = \boldsymbol\beta + \left( \text{plim} \frac{1}{n}\sum\limits_{i=1}^n \mathbf x_i \mathbf x_i^\mathsf{T} \right)^{-1} \left( \text{plim} \frac{1}{n}\sum\limits_{i=1}^n \mathbf x_i u_i \right) &amp;&amp; (\text{apply plim}) \\
\text{plim} \boldsymbol{\hat\beta} &amp; = \boldsymbol\beta + (E(\mathbf x_i \mathbf x_i^\mathsf{T}))^{-1}E(\mathbf x_i  u_i) &amp;&amp; (\text{law of large numbers})\\
\text{plim} \boldsymbol{\hat\beta} &amp; = \boldsymbol\beta &amp;&amp; (E(\mathbf x_i u_i) = 0)
\end{align}
\]</span></p>
<p>Thus, OLS is asymptotically consistent under the 4 conditions above.</p>
<p><br></p>
</section>
<section id="ols-as-a-conditional-expectation-function" class="level3">
<h3 class="anchored" data-anchor-id="ols-as-a-conditional-expectation-function">OLS as a Conditional Expectation Function</h3>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Conditional Expectation Functions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A <strong>conditional expectation function</strong> (CEF) says that the value of <span class="math inline">\(E(y)\)</span> depends on the value of <span class="math inline">\(x\)</span>. We notate a conditional expectation function as <span class="math inline">\(E(y|x)\)</span>. As we noted earlier, the linear regression model can be a conditional expectation function of <span class="math inline">\(E(y|x)\)</span>.</p>
<p>A <strong>best linear approximation</strong> of a conditional expectation function can take the following form:</p>
<p><span class="math display">\[
E(y_i|x_i) = b_0 + b_1x_i
\]</span></p>
<p>With parameters <span class="math inline">\(b_0, b_1\)</span> that minimise the mean squared errors (MSE).</p>
<p><span class="math display">\[
\begin{split}
MSE &amp; = E(y_i - E(y_i|x_i))^2 \\
&amp; = \frac{1}{n}\sum\limits_{i=1}^n( y_i - E(y_i|x_i))^2
\end{split}
\]</span></p>
</div>
</div>
</div>
<p>OLS is a best-linear approximation of the conditional expectation function. Suppose we have the conditional expectation function, and its mean squared errors:</p>
<p><span class="math display">\[
\begin{align}
E(y_i|x_i) &amp; = b_0 + b_1x_i \\
MSE &amp; = E(y_i - E(y_i|x_i))^2 \\
&amp; =  E(y_i - \beta_0 - \beta_1x_i)^2
\end{align}
\]</span></p>
<p>The first order conditions are (using chain rule and partial derivatives):</p>
<p><span class="math display">\[
\begin{split}
&amp; E(y_i - b_0 - b_1x_i) = 0 \\
&amp; E(x_i(y_i - b_0 - b_1x_i) = 0
\end{split}
\]</span></p>
<p>Now, recall our OLS minimisation conditions (simple linear regression):</p>
<p><span class="math display">\[
\begin{split}
&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
&amp; \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
\]</span></p>
<p>Since by definition, average/expectation is <span class="math inline">\(E(x) = \frac{1}{n} \sum x_i\)</span>, we can rewrite as:</p>
<p><span class="math display">\[
\begin{split}
&amp; n \times E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
&amp; n \times E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
\]</span></p>
<p>And since anything multiplied to a zero turns into zero, we can ignore the <span class="math inline">\(n\)</span> in the first order condition. Thus, our conditions are:</p>
<p><span class="math display">\[
\begin{split}
&amp; E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
&amp; E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
\]</span></p>
<p>Which as we can see, are the exact same minimisation conditions as the conditional expectation function. Thus, OLS is the best approximation of the conditional expectation function.</p>
<p>This property is very useful for causal inference, as it means OLS calculates the expected <span class="math inline">\(y\)</span>, which allows us to find causal effects by comparing the expected <span class="math inline">\(y\)</span> of the treatment and control groups (assuming the OLS estimator is unbiased).</p>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="regression-anatomy-and-controls" class="level1">
<h1><strong>Regression Anatomy and Controls</strong></h1>
<section id="controlling-for-regressors" class="level3">
<h3 class="anchored" data-anchor-id="controlling-for-regressors">Controlling for Regressors</h3>
<p>Take our multiple linear regression: <span class="math inline">\(y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_k x_{ki} + u_i\)</span>.</p>
<p>Let us say we are interested in <span class="math inline">\(x_1\)</span>. Let us make <span class="math inline">\(x_1\)</span> the outcome variable of a regression with explanatory variables <span class="math inline">\(x_2, ..., x_k\)</span>:</p>
<p><span class="math display">\[
x_{1i} = \gamma_0 + \gamma_1 x_{2i} + ... + \gamma_{k-1}x_{ki} + \widetilde{r_{1i}}
\]</span></p>
<p>The error term <span class="math inline">\(\widetilde{r_{1i}}\)</span> is the part of <span class="math inline">\(x_1\)</span> that cannot be explained by <span class="math inline">\(x_2, ..., x_k\)</span>.</p>
<p>Now, take the regression of with outcome variable <span class="math inline">\(y\)</span>, with all explanatory variables <u>except</u> <span class="math inline">\(x_1\)</span>:</p>
<p><span class="math display">\[
y_i = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \widetilde{y_i}
\]</span></p>
<p>The error term <span class="math inline">\(\widetilde{y_i}\)</span> is the part of <span class="math inline">\(y_i\)</span> that cannot be explained by <span class="math inline">\(x_2, ..., x_k\)</span>. That implies <span class="math inline">\(x_1\)</span> must be the one explaining <span class="math inline">\(\widetilde{y_i}\)</span>. But, <span class="math inline">\(x_1\)</span> may also correlated with <span class="math inline">\(x_2, ..., x_k\)</span>, and those correlated parts are already picked up in the regression coefficients of <span class="math inline">\(x_2, ..., x_k\)</span>. Thus, <span class="math inline">\(\widetilde{y_i}\)</span> must be explained by the part of <span class="math inline">\(x_1\)</span> that is uncorrelated with <span class="math inline">\(x_2, ..., x_k\)</span>, which we derived earlier as <span class="math inline">\(\widetilde{r_{1i}}\)</span>.</p>
<p>Thus, we can create another regression with explanatory variable <span class="math inline">\(\widetilde{x_{1i}}\)</span> and outcome variable <span class="math inline">\(\widetilde{y_i}\)</span>:</p>
<p><span class="math display">\[
\widetilde{y_i} = \alpha_0 + \alpha_1 \widetilde{r_{1i}} + u_i
\]</span></p>
<p>We plug <span class="math inline">\(\widetilde{y_i}\)</span> back into our regression of <span class="math inline">\(y_i\)</span> with explanatory variables <span class="math inline">\(x_2 ..., x_k\)</span>:</p>
<p><span class="math display">\[
\begin{align}
y_i &amp; = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \widetilde{y_i} \\
y_i &amp; = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \alpha_0 + \alpha_1 \widetilde{r_{1i}} + u_i &amp;&amp; (\text{plug in } \widetilde{y_i} = \alpha_0 + \alpha_1 \widetilde{r_{1i}} + u_i)\\
y_i  &amp; = \underbrace{(\delta_0 + \alpha_0)}_{\beta_0} + \underbrace{\alpha_1 \widetilde{r_{1i}}}_{\beta_1 x_{1i}} + \underbrace{\delta_1x_{2i}}_{\beta_2 x_{2i}} + ... + \underbrace{\delta_{k-1} x_{ki}}_{\beta_kx_{ki}} + \underbrace{u_i}_{u_i} &amp;&amp; (\text{rearrange})
\end{align}
\]</span></p>
<p>This new regression mirrors the original multiple linear regression. Importantly, we see <u>the estimate of <span class="math inline">\(\alpha_1\)</span> will be the same as <span class="math inline">\(\beta_1\)</span> in the original regression</u>. This coefficient explains the expected change in <span class="math inline">\(y\)</span>, given an increase in the part of <span class="math inline">\(x_1\)</span> uncorrelated with <span class="math inline">\(x_2, ..., x_k\)</span>.</p>
<p>So essentially, <u>we have <strong>partialed out</strong> the effect of the other explanatory variables, and only focus on the effect on <span class="math inline">\(y\)</span> of the uncorrelated part of <span class="math inline">\(x_1\)</span> (which is <span class="math inline">\(\widetilde{r_{1i}}\)</span>)</u>. This is what controlling for confounders is.</p>
<p><br></p>
</section>
<section id="partitioned-regression-model" class="level3">
<h3 class="anchored" data-anchor-id="partitioned-regression-model">Partitioned Regression Model</h3>
<p><br></p>
</section>
<section id="unbiasedness-of-regression-anatomy" class="level3">
<h3 class="anchored" data-anchor-id="unbiasedness-of-regression-anatomy">Unbiasedness of Regression Anatomy</h3>
<p><br></p>
</section>
<section id="variance-of-regression-anatomy" class="level3">
<h3 class="anchored" data-anchor-id="variance-of-regression-anatomy">Variance of Regression Anatomy</h3>
<p><br></p>
</section>
<section id="including-irrelevant-variables" class="level3">
<h3 class="anchored" data-anchor-id="including-irrelevant-variables">Including Irrelevant Variables</h3>
<p><br></p>
</section>
<section id="omitted-variable-bias" class="level3">
<h3 class="anchored" data-anchor-id="omitted-variable-bias">Omitted Variable Bias</h3>
<p><br></p>
<hr>
</section>
</section>
<section id="method-of-moments-estimator" class="level1">
<h1><strong>Method of Moments Estimator</strong></h1>
<section id="method-of-moments" class="level3">
<h3 class="anchored" data-anchor-id="method-of-moments">Method of Moments</h3>
<p>The Method of Moments Estimator is another estimator of the true value of populations in the parameter. The estimator defines key population <strong>moments</strong> of interest - which are the population parameters written in terms of expected value functions set equal to 0.</p>
<p>Then, the Method of Moments uses the sample equivalents of the population moments to estimate the population parameter. For example, to estimate the population mean, the Method of Moments uses the sample mean.</p>
<p>In order to define a method of moments for a set of parameters <span class="math inline">\(\theta_1, \dots, \theta_k\)</span>, we need to specify at least one population moment per parameter. Or in other words, we must have more than <span class="math inline">\(k\)</span> population moments.</p>
<p>Our population moments can be defined as the expected value of some function <span class="math inline">\(m(\theta; y)\)</span> that consists of both the variable <span class="math inline">\(y\)</span> and our unknown parameter <span class="math inline">\(\theta\)</span>. The expectation of the function <span class="math inline">\(m(\theta; y)\)</span> should equal 0.</p>
<p><span class="math display">\[
E(m(\theta; y)) = 0
\]</span></p>
<p>Our sample moments will be the sample analogues of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(y\)</span>, which are <span class="math inline">\(\hat\theta\)</span> and <span class="math inline">\(y_i\)</span>:</p>
<p><span class="math display">\[
\frac{1}{n}\sum\limits_{i=1}^n m(\hat\theta; y_i) = 0
\]</span></p>
<p>Method of moments estimators are asymptotically consistent, because of the law of large numbers.</p>
<p><br></p>
</section>
<section id="population-mean-estimator" class="level3">
<h3 class="anchored" data-anchor-id="population-mean-estimator">Population Mean Estimator</h3>
<p>Let us say that we have some random variable <span class="math inline">\(y\)</span>, with a true population mean <span class="math inline">\(\mu\)</span>. We want to estimate <span class="math inline">\(\mu\)</span>, but we only have a sample of the population.</p>
<p>How can we define <span class="math inline">\(\mu\)</span> in a moment of the form: <span class="math inline">\(E(m(\mu, y)) = 0\)</span>? Well, we know <span class="math inline">\(\mu\)</span> is the expectation of <span class="math inline">\(y\)</span>, so <span class="math inline">\(\mu = E(y)\)</span>. Since they are equal, <span class="math inline">\(\mu - E(y) = 0\)</span>. Thus, we can define the mean as a moment of the following condition:</p>
<p><span class="math display">\[
E(y - \mu) = 0
\]</span></p>
<p>The method of moments estimator uses the sample equivalent of the population moment. The sample equivalent of <span class="math inline">\(\mu\)</span>, is the sample mean <span class="math inline">\(\bar y\)</span>:</p>
<p><span class="math display">\[
E(y_i - \hat\mu) = \frac{1}{n}\sum\limits_{i=1}^n (y_i - \hat\mu) = 0
\]</span></p>
<p>With this equation, we can then solve for <span class="math inline">\(\hat\mu\)</span>:</p>
<p><span class="math display">\[
\begin{align}
0 &amp; = \frac{1}{n}\sum\limits_{i=1}^n (y_i - \hat\mu) \\
0 &amp; = \frac{1}{n}\sum\limits_{i=1}^ny_i - \frac{1}{n}\sum\limits_{i=1}^n \hat\mu  &amp;&amp; (\text{multiply out})\\
0 &amp; = \frac{1}{n}\sum\limits_{i=1}^ny_i - \frac{1}{n} n \hat\mu &amp;&amp;(\text{summation property of constant } \hat\mu)\\
0 &amp; = \bar y - \hat \mu &amp;&amp; (\text{definition of mean }\frac{1}{n}\sum\limits_{i=1}^ny_i = \bar y)\\
\hat\mu &amp; = \bar y &amp;&amp; (+\hat\mu\text{ to both sides})
\end{align}
\]</span></p>
<p>So, we see the method of moments estimates our true population mean <span class="math inline">\(\mu\)</span>, with the sample mean <span class="math inline">\(\bar y\)</span>. As a method of moments estimator, it is also asymptotically consistent.</p>
<p><br></p>
</section>
<section id="ols-as-a-method-of-moments-estimator" class="level3">
<h3 class="anchored" data-anchor-id="ols-as-a-method-of-moments-estimator">OLS as a Method of Moments Estimator</h3>
<p>OLS is a special case of the Method of Moments Estimator. Consider the bivariate regression model. The OLS estimator can be derived as a method of moments estimator, with 2 moments (expectation functions set equal to 0), one for each parameter (<span class="math inline">\(\beta_0, \beta_1\)</span>):</p>
<p><span class="math display">\[
\begin{split}
&amp; E(y-\beta_0 -\beta_1x) = 0 \\
&amp; E(x(y - \beta_0 - \beta_1 x)) = 0
\end{split}
\]</span></p>
<p>The estimates of these moments would use the sample equivalents: <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>.</p>
<p><span class="math display">\[
\begin{split}
&amp; E(y-\hat\beta_0 -\hat\beta_1x) = 0 \\
&amp; E(x(y - \hat\beta_0 - \hat\beta_1 x)) = 0
\end{split}
\]</span></p>
<p>Remember our OLS minimisation conditions:</p>
<p><span class="math display">\[
\begin{split}
&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
&amp; \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
\]</span></p>
<p>Since by definition, average/expectation is <span class="math inline">\(E(x) = \frac{1}{n} \sum x_i\)</span>, we can rewrite the OLS minimisation conditions as:</p>
<p><span class="math display">\[
\begin{split}
&amp; n \times E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
&amp; n \times E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
\]</span></p>
<p>And since anything multiplied to a zero turns into zero, we can ignore the <span class="math inline">\(n\)</span> in the first order condition, and only focus on the expected value part. Thus, our conditions are:</p>
<p><span class="math display">\[
\begin{split}
&amp; E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
&amp; E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
\]</span></p>
<p>Which as we can see, are the exact same minimisation conditions as the method of moments estimator. Thus, the OLS estimator is a special case of the Method of Moments estimator, and they produce the same coefficients. This is an important property for the instrumental variables method that will be covered later.</p>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>