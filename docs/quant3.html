<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Classical Least Squares Theory – Political Science &amp; Political Economy</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="mathjax-config.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./quant3.html">3 Classical Least Squares Theory</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Kevin’s PSPE Resources</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./games.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guide to Game Theory</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./math.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Essential Mathematics</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
 <span class="menu-text">Quantitative Methods (Causal Inference)</span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 Introductory Statistics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 Multiple Linear Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant3.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">3 Classical Least Squares Theory</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 Causal Frameworks</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 Randomised Controlled Trials</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 Selection on Observables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant7.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7 Instrumental Variables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant8.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8 Regression Discontinuity</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant9.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9 Differences-in-Differences</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
 <span class="menu-text">More Statistical Models</span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model1.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Logistic Regression Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model2.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Regression for Counts</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Supervised Learning Methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Unsupervised Learning Methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Factor Analysis Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model6.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Latent Trait Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model7.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Latent Class Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model9.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Structural Equation Models</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On This Page</h2>
   
  <ul>
  <li><a href="#ordinary-least-squares-estimator" id="toc-ordinary-least-squares-estimator" class="nav-link active" data-scroll-target="#ordinary-least-squares-estimator"><strong>Ordinary Least Squares Estimator</strong></a>
  <ul class="collapse">
  <li><a href="#deriving-the-estimator" id="toc-deriving-the-estimator" class="nav-link" data-scroll-target="#deriving-the-estimator">Deriving the Estimator</a></li>
  <li><a href="#projection-and-residual-maker" id="toc-projection-and-residual-maker" class="nav-link" data-scroll-target="#projection-and-residual-maker">Projection and Residual Maker</a></li>
  <li><a href="#orthogonal-projection-of-ols" id="toc-orthogonal-projection-of-ols" class="nav-link" data-scroll-target="#orthogonal-projection-of-ols">Orthogonal Projection of OLS</a></li>
  <li><a href="#error-covariance-matrix" id="toc-error-covariance-matrix" class="nav-link" data-scroll-target="#error-covariance-matrix">Error Covariance Matrix</a></li>
  <li><a href="#homoscedasticity-and-heteroscedasticity" id="toc-homoscedasticity-and-heteroscedasticity" class="nav-link" data-scroll-target="#homoscedasticity-and-heteroscedasticity">Homoscedasticity and Heteroscedasticity</a></li>
  </ul></li>
  <li><a href="#sample-properties-of-ols" id="toc-sample-properties-of-ols" class="nav-link" data-scroll-target="#sample-properties-of-ols"><strong>Sample Properties of OLS</strong></a>
  <ul class="collapse">
  <li><a href="#ols-as-an-unbiased-estimator" id="toc-ols-as-an-unbiased-estimator" class="nav-link" data-scroll-target="#ols-as-an-unbiased-estimator">OLS as an Unbiased Estimator</a></li>
  <li><a href="#deriving-variance" id="toc-deriving-variance" class="nav-link" data-scroll-target="#deriving-variance">Deriving Variance</a></li>
  <li><a href="#gauss-markov-theorem" id="toc-gauss-markov-theorem" class="nav-link" data-scroll-target="#gauss-markov-theorem">Gauss-Markov Theorem</a></li>
  <li><a href="#asymptotic-consistency-of-ols" id="toc-asymptotic-consistency-of-ols" class="nav-link" data-scroll-target="#asymptotic-consistency-of-ols">Asymptotic Consistency of OLS</a></li>
  </ul></li>
  <li><a href="#regression-anatomy-and-specification" id="toc-regression-anatomy-and-specification" class="nav-link" data-scroll-target="#regression-anatomy-and-specification"><strong>Regression Anatomy and Specification</strong></a>
  <ul class="collapse">
  <li><a href="#partitioned-regression-model" id="toc-partitioned-regression-model" class="nav-link" data-scroll-target="#partitioned-regression-model">Partitioned Regression Model</a></li>
  <li><a href="#omitted-variable-bias" id="toc-omitted-variable-bias" class="nav-link" data-scroll-target="#omitted-variable-bias">Omitted Variable Bias</a></li>
  </ul></li>
  <li><a href="#method-of-moments-estimator" id="toc-method-of-moments-estimator" class="nav-link" data-scroll-target="#method-of-moments-estimator"><strong>Method of Moments Estimator</strong></a>
  <ul class="collapse">
  <li><a href="#method-of-moments" id="toc-method-of-moments" class="nav-link" data-scroll-target="#method-of-moments">Method of Moments</a></li>
  <li><a href="#population-mean-estimator" id="toc-population-mean-estimator" class="nav-link" data-scroll-target="#population-mean-estimator">Population Mean Estimator</a></li>
  <li><a href="#ols-as-a-method-of-moments" id="toc-ols-as-a-method-of-moments" class="nav-link" data-scroll-target="#ols-as-a-method-of-moments">OLS as a Method of Moments</a></li>
  </ul></li>
  <li><a href="#maximum-likelihood-estimator" id="toc-maximum-likelihood-estimator" class="nav-link" data-scroll-target="#maximum-likelihood-estimator"><strong>Maximum Likelihood Estimator</strong></a>
  <ul class="collapse">
  <li><a href="#likelihood-functions" id="toc-likelihood-functions" class="nav-link" data-scroll-target="#likelihood-functions">Likelihood Functions</a></li>
  <li><a href="#gradient-descent-algorithms" id="toc-gradient-descent-algorithms" class="nav-link" data-scroll-target="#gradient-descent-algorithms">Gradient Descent Algorithms</a></li>
  <li><a href="#ols-as-a-maximum-likelihood-estimator" id="toc-ols-as-a-maximum-likelihood-estimator" class="nav-link" data-scroll-target="#ols-as-a-maximum-likelihood-estimator">OLS as a Maximum Likelihood Estimator</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Classical Least Squares Theory</h1>
<p class="subtitle lead">Chapter 3, Quantitative Methods (Causal Inference)</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Last chapter, we discussed the multiple linear regression model, and how it can help us measure relationships between explanatory and outcome variables.</p>
<p>This chapter introduces some key theory regarding the ordinary least squares estimator behind linear regression. Topics covered includes properties of estimators, the OLS estimator, and the Method of Moments estimator.</p>
<p>Use the right sidebar for quick navigation. This chapter is heavy on linear algebra, so consulting the linear algebra reference is useful.</p>
<hr>
<section id="ordinary-least-squares-estimator" class="level1">
<h1><strong>Ordinary Least Squares Estimator</strong></h1>
<section id="deriving-the-estimator" class="level3">
<h3 class="anchored" data-anchor-id="deriving-the-estimator">Deriving the Estimator</h3>
<p>Our <a href="./quant2.html">linear regression</a> model, and the fitted values <span class="math inline">\(\hat{\mathbf{y}}\)</span>, take the following form:</p>
<p><span class="math display">\[
\mathbf y = \mathbf X \boldsymbol\beta + \mathbf u, \qquad \hat{\mathbf y} = \mathbf X \hat{\boldsymbol\beta}
\]</span></p>
<p>OLS minimises the <a href="./quant2.html#estimation-process">sum of squared residuals</a> <span class="math inline">\(S(\hat{\boldsymbol\beta})\)</span> - the differences between the actual <span class="math inline">\(\mathbf y\)</span> and our predicted <span class="math inline">\(\hat{\mathbf y}\)</span>:</p>
<p><span class="math display">\[
\begin{align}
S(\hat{\boldsymbol\beta}) &amp; = (\mathbf y - \hat{\mathbf y})^\mathsf{T} (\mathbf y - \hat{\mathbf y})\\
&amp; = (\mathbf y - \color{blue}{\mathbf X \hat{\boldsymbol\beta}}\color{black})^\mathsf{T} (\mathbf y - \color{blue}{\mathbf{X} \hat{\boldsymbol\beta}}\color{black}) &amp;&amp; (\text{plug in } \color{blue}{\hat{\mathbf y}  = \mathbf X \hat{\boldsymbol\beta}}\color{black}) \\
&amp; = \mathbf y^\mathsf{T} \mathbf y - \hat{\boldsymbol\beta}^\mathsf{T} \mathbf X^\mathsf{T} \mathbf y - \mathbf y^\mathsf{T} \mathbf{X}\hat{\boldsymbol\beta} + \hat{\boldsymbol\beta}^\mathsf{T} \mathbf X^\mathsf{T} \mathbf{Xb} &amp;&amp; (\text{distribute out)} \\
&amp; = \mathbf y^\mathsf{T} \mathbf y - \color{blue}{2\hat{\boldsymbol\beta}^\mathsf{T} \mathbf X^\mathsf{T} \mathbf y}\color{black} + \hat{\boldsymbol\beta}^\mathsf{T} \mathbf X^\mathsf{T} \mathbf{X} \hat{\boldsymbol\beta} &amp;&amp;(\text{combine } \color{blue}{- \hat{\boldsymbol\beta}^\mathsf{T} \mathbf X^\mathsf{T} \mathbf y - \mathbf y^\mathsf{T} \mathbf{X}\hat{\boldsymbol\beta}}\color{black})
\end{align}
\]</span></p>
<p>Now, let us find the first order condition:</p>
<p><span class="math display">\[
\frac{\partial S(\hat{\boldsymbol\beta})}{\partial \hat{\boldsymbol\beta}} = -2\mathbf X^\mathsf{T} \mathbf y + 2 \mathbf X^\mathsf{T} \mathbf{X} \hat{\boldsymbol\beta} = 0
\]</span></p>
<p>When assuming <span class="math inline">\(\mathbf X^\mathsf{T} \mathbf X\)</span> is invertable (which is true if <span class="math inline">\(\mathbf X\)</span> is full rank), we can isolate <span class="math inline">\(\hat{\beta}\)</span> to find the solution to OLS:</p>
<p><span id="eq-ols"><span class="math display">\[
\begin{align}
-2\mathbf X^T\mathbf y + 2 \mathbf X^\mathsf{T} \mathbf X \boldsymbol{\hat{\beta}} &amp; = 0 \\
2 \mathbf X^\mathsf{T} \mathbf X \boldsymbol{\hat\beta} &amp; = 2\mathbf X^\mathsf{T} \mathbf y &amp;&amp; (+ 2\mathbf X^\mathsf{T} \mathbf y \text{ to both sides}) \\
\boldsymbol{\hat\beta} &amp; = (2\mathbf X^\mathsf{T} \mathbf X)^{-1} 2 \mathbf X^\mathsf{T} \mathbf y &amp;&amp; (\times (2\mathbf X^\mathsf{T} \mathbf X)^{-1} \text{ to both sides})\\
\boldsymbol{\hat\beta} &amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf y &amp;&amp;(\text{cancel out } 2^{-1}\times 2)
\end{align}
\tag{1}\]</span></span></p>
<p>Those are our coefficient solutions to OLS.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Alternative Derivation for Simple Linear Regression
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Currently, we are deriving the first order conditions for multiple linear regression using linear algebra.</p>
<p>For simple linear regression (with one explanatory variable), we can use summation notation. Recall our sum of squared residuals in summation form:</p>
<p><span class="math display">\[
SSR = S(\widehat{\beta_0}, \widehat{\beta_1})= \sum\limits_{i=1}^n(y_i - \widehat{\beta_0} - \widehat{\beta_1}x_i)^2
\]</span></p>
<p>We want to minimise the SSR in respect to both <span class="math inline">\(\widehat{\beta_0}\)</span> and <span class="math inline">\(\widehat{\beta_1}\)</span>. We can do this by taking the partial derivative in respect to both, and setting them equal to 0. We can find the partial derivative with chain rule and sum rule:</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial S(\widehat{\beta_0}, \widehat{\beta_1})}{\partial \widehat{\beta_0}} &amp; = \sum\limits_{i=1}^n(y_i - \widehat{\beta_0} - \widehat{\beta_1}x_i) = 0 \\
\frac{\partial S(\widehat{\beta_0}, \widehat{\beta_1})}{\partial \widehat{\beta_1}} &amp; = \sum\limits_{i=1}^nx_i(y_i - \widehat{\beta_0} - \widehat{\beta_1}x_i) = 0
\end{align}
\]</span></p>
<p>These conditions create a system of equations, which you can solve for the OLS solutions of <span class="math inline">\(\widehat{\beta_0}\)</span> and <span class="math inline">\(\widehat{\beta_1}\)</span>. I will not show it step by step, as it is tedious (and not that important). The OLS solutions are</p>
<p><span class="math display">\[
\begin{align}
\widehat{\beta_0} &amp; = \bar y - \widehat{\beta_1} \bar x \\
\widehat{\beta_1} &amp; = \frac{\sum_{i=1}^n(x_i - \bar x)(y_i - \bar y)}{\sum_{i=1}^n(x_i - \bar x)^2} = \frac{Cov(x, y)}{Var(y)}
\end{align}
\]</span></p>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="projection-and-residual-maker" class="level3">
<h3 class="anchored" data-anchor-id="projection-and-residual-maker">Projection and Residual Maker</h3>
<p>We can use the OLS solution from <a href="#eq-ols" class="quarto-xref">Equation&nbsp;1</a> to get our fitted values <span class="math inline">\(\hat{\mathbf y}\)</span>:</p>
<p><span id="eq-projection"><span class="math display">\[
\begin{align}
\hat{\mathbf y} &amp; = \mathbf X \hat{\boldsymbol\beta} \\
&amp; = \mathbf X \color{blue}{(\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf y} &amp;&amp; \color{black}(\text{plug in OLS solution } \color{blue}{\hat{\boldsymbol\beta} = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf y}) \\
&amp; = \color{red}{\mathbf P}\color{black}{\mathbf y} &amp;&amp; \text{(where } \color{red}{\mathbf P :=\mathbf X (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T}})
\end{align}
\tag{2}\]</span></span></p>
<p>Matrix <span class="math inline">\(\color{red}{\mathbf P}\)</span>, called the <strong>projection matrix</strong>, is a matrix operator that performs the <a href="./math.html#linear-mappings-and-combinations">linear mapping</a> <span class="math inline">\(\mathbf y \rightarrow \hat{\mathbf y}\)</span>.</p>
<p>We can also use the OLS solution from <a href="#eq-ols" class="quarto-xref">Equation&nbsp;1</a> to get our residuals <span class="math inline">\(\hat{\mathbf u}\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\hat{\mathbf u} &amp; = \mathbf y - \hat{\mathbf y} \\
&amp; = \mathbf y - \color{blue}{\mathbf P\mathbf y} &amp;&amp; \color{black}(\text{from equation (2) } \color{blue}{\hat{\mathbf y} = \mathbf P \mathbf y} \color{black}) \\
&amp; = (\mathbf I - \mathbf P)\mathbf y &amp;&amp; (\text{factor out } \mathbf y) \\
&amp; = \color{purple}{\mathbf M} \color{black}{\mathbf y} &amp;&amp; (\text{where } \color{purple}{\mathbf M = \mathbf I - \mathbf P} \color{black})
\end{align}
\]</span></p>
<p>Matrix <span class="math inline">\(\color{purple}{\mathbf M}\)</span>, called the <strong>residual maker</strong>, is a matrix operator that performs the <a href="./math.html#linear-mappings-and-combinations">linear mapping</a> <span class="math inline">\(\mathbf y \rightarrow \hat{\mathbf u}\)</span>.</p>
<p>Both <span class="math inline">\(\color{red}{\mathbf P}\)</span> and <span class="math inline">\(\color{purple}{\mathbf M}\)</span> are <a href="./math.html#types-of-matrices">symmetric matrices</a>: <span class="math inline">\(\mathbf P^\mathsf{T} = \mathbf P\)</span>. They are also both <a href="./math.html#types-of-matrices">idempotent matrices</a>: <span class="math inline">\(\mathbf {PP} = \mathbf P\)</span>. We can prove this second statement using the first (I will only do it for <span class="math inline">\(\mathbf P\)</span>, but the same applies for <span class="math inline">\(\mathbf M\)</span>:</p>
<p><span id="eq-pp"><span class="math display">\[
\begin{align}
\mathbf{PP} &amp; = \mathbf X (\mathbf X^\mathsf{T} \mathbf X)^{-1} \underbrace{\mathbf X^\mathsf{T} \mathbf X (\mathbf X^\mathsf{T} \mathbf X)^{-1}}_{= \ \mathbf I} \mathbf X^\mathsf{T} \\
&amp; = \mathbf X (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \\
&amp; = \mathbf P
\end{align}
\tag{3}\]</span></span></p>
<p><span class="math inline">\(\color{red}{\mathbf P}\)</span> and <span class="math inline">\(\color{purple}{\mathbf M}\)</span> are also <a href="./math.html#types-of-matrices">orthogonal</a> to each other - i.e.&nbsp;<span class="math inline">\(\mathbf P^\mathsf{T} \mathbf M = 0\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\mathbf P^\mathsf{T}\mathbf M &amp; = \mathbf {PM} &amp;&amp; (\because \mathbf P^\mathsf{T} = \mathbf P) \\
&amp; = \mathbf P(\color{blue}{\mathbf I-\mathbf P}\color{black}) &amp;&amp; (\because \color{blue}{\mathbf M = \mathbf I - \mathbf P}\color{black}) \\
&amp; = \mathbf P - \mathbf{PP} &amp;&amp; (\text{distribute out}) \\
&amp; = \mathbf P - \color{blue}{\mathbf P} &amp;&amp; (\because \text{equation (3)}) \\
&amp; = 0
\end{align}
\]</span></p>
<p>These two matrices will become very important throughout OLS theory.</p>
<p><br></p>
</section>
<section id="orthogonal-projection-of-ols" class="level3">
<h3 class="anchored" data-anchor-id="orthogonal-projection-of-ols">Orthogonal Projection of OLS</h3>
<p>We know that our fitted values <span class="math inline">\(\hat{\mathbf y}\)</span> are created as a linear combination of our explanatory variables <span class="math inline">\(\mathbf X\)</span>:</p>
<p><span class="math display">\[
\widehat{y_i} = \beta_0 + \beta_1x_{1i} + \dots + \beta_kx_{ki}
\]</span></p>
<p>That means, by the definition of <a href="./math.html#vector-spaces">vector spaces</a>, that our explanatory variable vectors <span class="math inline">\(\mathbf x_j\)</span> span a space that includes our fitted values vector <span class="math inline">\(\hat{\mathbf y}\)</span>.</p>
<p>So, what the matrix operator <span class="math inline">\(\color{red}{\mathbf P}\)</span> is doing is essentially taking our original data <span class="math inline">\(\mathbf y\)</span> values, and projecting it into the space spanned by our explanatory variables <span class="math inline">\(\mathbf X\)</span> (called the <strong>column space</strong>).</p>
<p>We can see in the figure below, our observed <span class="math inline">\(\mathbf y\)</span> vector is being projected onto the blue plane spanned by <span class="math inline">\(\mathbf X\)</span> to create our fitted values vector <span class="math inline">\(\hat{\mathbf y}\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-880030792.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>Residual maker matrix <span class="math inline">\(\color{purple}{\mathbf M}\)</span> projects <span class="math inline">\(\mathbf y\)</span> onto the space orthogonal to the column space of <span class="math inline">\(\mathbf X\)</span> to get our residuals <span class="math inline">\(\hat{\mathbf u}\)</span>. We can see this in the figure above, where the residuals vector (notated <span class="math inline">\(\mathbf e\)</span> in the figure) is orthogonal/perpendicular to the space of <span class="math inline">\(\mathbf X\)</span>.</p>
<p><br></p>
</section>
<section id="error-covariance-matrix" class="level3">
<h3 class="anchored" data-anchor-id="error-covariance-matrix">Error Covariance Matrix</h3>
<p>Aside from the population parameters <span class="math inline">\(\boldsymbol\beta\)</span>, there is another part of the linear model that needs to be estimated: the population <strong>covariance matrix</strong> of error terms <span class="math inline">\(u_1, \dots u_n\)</span>:</p>
<p><span class="math display">\[
\underbrace{Var(\mathbf u|\mathbf X)}_{\text{cov. matrix}} = \begin{pmatrix}
Var(u_1) &amp; Cov(u_1, u_2) &amp; Cov(u_1, u_3) &amp; \dots \\
Cov(u_2, u_1) &amp; Var(u_2) &amp; Cov(u_2, u_3) &amp; \dots \\
Cov(u_3, u_1) &amp; Cov(u_3, u_2) &amp; Var(u_3) &amp; \vdots \\
\vdots &amp; \vdots &amp; \dots &amp; \ddots
\end{pmatrix}
\]</span></p>
<p>Under the assumption of <strong>independence of observations</strong> (a key assumption of the linear model), the covariance elements should all equal 0. This assumption is also called <strong>no autocorrelation</strong>.</p>
<p>Thus, under this assumption, we have a diagonal matrix.</p>
<p><span class="math display">\[
\underbrace{Var(\mathbf u|\mathbf X)}_{\text{cov. matrix}} = \begin{pmatrix}
Var(u_1) &amp; 0 &amp; 0 &amp; \dots \\
0 &amp; Var(u_2) &amp; 0 &amp; \dots \\
0 &amp; 0 &amp; Var(u_3) &amp; \vdots \\
\vdots &amp; \vdots &amp; \dots &amp; \ddots
\end{pmatrix}
\]</span></p>
<p>We are not really going to discuss what happens when autocorrelation is present, as generally for many of our purposes, ruling out autocorrelation is okay. However, if you are interested in time series (common in economics), or spatial statistics, these are types of data that frequently have autocorrelation issues, and this creates further complications.</p>
<p><br></p>
</section>
<section id="homoscedasticity-and-heteroscedasticity" class="level3">
<h3 class="anchored" data-anchor-id="homoscedasticity-and-heteroscedasticity">Homoscedasticity and Heteroscedasticity</h3>
<p>Now, there are two possible forms of our covariance matrix of errors. <strong>Homoscedasticity</strong> assumes that every single unit <span class="math inline">\(i\)</span> has the same variance in error <span class="math inline">\(\sigma_i^2\)</span>. Or in other words, the error of <span class="math inline">\(u_i\)</span> does not depend on unit <span class="math inline">\(i\)</span>’s <span class="math inline">\(\mathbf X\)</span> values:</p>
<p><span id="eq-homo"><span class="math display">\[
Var(\mathbf u | \mathbf X) = \sigma^2 \mathbf I_n = \begin{pmatrix}
\sigma^2 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \sigma^2 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; \dots &amp; \sigma^2
\end{pmatrix}
\tag{4}\]</span></span></p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Visualisation of <span class="math inline">\(\sigma^2\)</span>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Below is a figure illustrating different residual standard deviations, with the same best-fit line.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-2649765153.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</div>
</div>
</div>
<p><span class="math inline">\(\sigma^2\)</span> is a population parameter. We can estimate it with the following unbiased estimator <span class="math inline">\(s^2 = \frac{\hat{\mathbf u}^\mathsf{T} \hat{\mathbf u}}{n-k-1}\)</span>. This is the reason we use a t-distribution in hypotheses tests - to account for the uncertainty of this estimator. We will show this later when deriving variance.</p>
<p><strong>Heteroscedasticity</strong> is when we do not believe the assumption of a constant variance for all units. Instead, we assume each unit <span class="math inline">\(i = 1, \dots, n\)</span> has their own variance <span class="math inline">\(\sigma^2_1, \dots, \sigma^2_n\)</span>:</p>
<p><span id="eq-hetero"><span class="math display">\[
Var(\mathbf u | \mathbf X) = \begin{pmatrix}
\sigma^2_1 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \sigma^2_2 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; \dots &amp; \sigma^2_n
\end{pmatrix}
\tag{5}\]</span></span></p>
<p>We can estimate each individual population parameter <span class="math inline">\(\sigma^2_i\)</span> with <span class="math inline">\(s^2_i = \hat{u_i}^2\)</span>.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Visualisation of Homoscedasticity
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>An easy way to identify homoscedasticity is to look at a residual plot (just the plot of all <span class="math inline">\(\widehat{u_i}\)</span>):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-1713529842.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%"></p>
</figure>
</div>
<p>Notice how the homoscedasticity residuals seem to have the same up-down variance, no matter the value of <span class="math inline">\(x\)</span>.</p>
<p>The heteroscedasticity residuals have a clear pattern - the up-down variance is smaller when <span class="math inline">\(x\)</span> is smaller, and the up-down variance is larger when <span class="math inline">\(x\)</span> is larger.</p>
<p>Essentially, if you see a pattern in the residual plot, it is likely heteroscedasticity.</p>
</div>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="sample-properties-of-ols" class="level1">
<h1><strong>Sample Properties of OLS</strong></h1>
<section id="ols-as-an-unbiased-estimator" class="level3">
<h3 class="anchored" data-anchor-id="ols-as-an-unbiased-estimator">OLS as an Unbiased Estimator</h3>
<p>OLS is an <a href="./quant1.html#finite-sample-properties-of-estimators">unbiased estimator</a> of the relationship between any <span class="math inline">\(x_j\)</span> and <span class="math inline">\(y\)</span> under 4 conditions:</p>
<ol type="1">
<li><strong>Linearity</strong> in parameters: the model of the population (data generating process) can be modelled as <span class="math inline">\(\mathbf y = \mathbf X \boldsymbol\beta + \mathbf u\)</span>.</li>
<li><strong>Random Sampling</strong>: the observations in our sample are randomly sampled.</li>
<li><strong>No Perfect Multicolinearity</strong>: There is no exact linear relationships between the regressors. This ensures that <span class="math inline">\(\mathbf X^\mathsf{T} \mathbf X\)</span> is invertible, which is required for the derivation of OLS.</li>
<li><strong>Zero Conditional Mean</strong>: <span class="math inline">\(E(\mathbf u|\mathbf X) = 0\)</span>. This implies that no <span class="math inline">\(x_j\)</span> is correlated with <span class="math inline">\(\mathbf u\)</span> (exogeneity), and no function of multiple regressors is correlated with <span class="math inline">\(\mathbf u\)</span>.</li>
</ol>
<p>Let us prove OLS is unbiased - i.e.&nbsp;<span class="math inline">\(E(\hat{\boldsymbol\beta}) = \boldsymbol\beta\)</span>. Let us manipulate our OLS solution:</p>
<p><span id="eq-simplify"><span class="math display">\[
\begin{align}
\boldsymbol{\hat\beta} &amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf y \\
&amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T}\color{blue}{(\mathbf X \boldsymbol\beta + \mathbf u)} &amp;&amp; \color{black}(\text{plug in } \color{blue}{\mathbf y = \mathbf X \boldsymbol\beta + \mathbf u}\color{black}) \\
&amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf X \boldsymbol\beta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u &amp;&amp;(\text{multiply out})\\
&amp; = \color{blue}{\mathbf I}\color{black}{\boldsymbol\beta} + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u &amp;&amp;( \ \color{blue}{(\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf X = \mathbf I}\color{black})\\
&amp; = \boldsymbol\beta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u &amp;&amp; (\text{identity property of } \mathbf I)
\end{align}
\tag{6}\]</span></span></p>
<p>Now, let us take the expectation of <span class="math inline">\(\boldsymbol{\hat\beta}\)</span> conditional on <span class="math inline">\(\mathbf X\)</span>. Remember condition 4, <span class="math inline">\(E(\mathbf u | \mathbf X) = 0\)</span>:</p>
<p><span class="math display">\[
\begin{align}
E(\boldsymbol{\hat\beta}|\mathbf X) &amp; = \boldsymbol\beta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} E(\mathbf u | \mathbf X) &amp;&amp;(\mathbf u \text{ conditional on value of } \mathbf X) \\
E(\boldsymbol{\hat\beta}|\mathbf X) &amp; = \boldsymbol\beta &amp;&amp;(E(\mathbf u | \mathbf X) = 0)
\end{align}
\]</span></p>
<p>Now, we can use the <a href="./quant1.html#expectation-and-variance">law of iterated expectations (LIE)</a> to conclude this proof:</p>
<p><span class="math display">\[
\begin{align}
E(\boldsymbol{\hat\beta}) &amp; = E(E(\boldsymbol{\hat\beta}|\mathbf X)) &amp;&amp; (\text{LIE: E(X) = E(E(X|Y))})\\
&amp; = E(\color{blue}{\boldsymbol\beta}) &amp;&amp; (\text{from above, plug in } \color{blue}{E(\boldsymbol{\hat\beta}|\mathbf X)  = \boldsymbol\beta}\color{black})\\
&amp; = \boldsymbol\beta &amp;&amp; (\text{expecation of a constant})
\end{align}
\]</span></p>
<p>Thus, OLS is unbiased under the 4 conditions above.</p>
<p><br></p>
</section>
<section id="deriving-variance" class="level3">
<h3 class="anchored" data-anchor-id="deriving-variance">Deriving Variance</h3>
<p>We want to find the variance of our estimator, <span class="math inline">\(Var(\boldsymbol{\hat\beta} | \mathbf X)\)</span>. First, let us start off where we left off in <a href="#eq-simplify" class="quarto-xref">Equation&nbsp;6</a> .</p>
<p><span class="math display">\[
\begin{align}
\boldsymbol{\hat\beta} &amp; = \boldsymbol\beta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u  \\
Var(\boldsymbol{\hat\beta} | \mathbf X) &amp; = Var(\boldsymbol\beta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u \ | \ \mathbf X)
\end{align}
\]</span></p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lemma: Variance
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>If <span class="math inline">\(\mathbf u\)</span> is an <span class="math inline">\(n\)</span> dimensional vector of random variables, <span class="math inline">\(\mathbf c\)</span> is an <span class="math inline">\(m\)</span> dimensional vector, and <span class="math inline">\(\mathbf B\)</span> is an <span class="math inline">\(n \times m\)</span> dimensional matrix with fixed constants, then the following is true:</p>
<p><span id="eq-lemma"><span class="math display">\[
Var(\mathbf c + \mathbf{Bu}) = \mathbf B Var(\mathbf u)\mathbf B^T
\tag{7}\]</span></span></p>
<p>I will not prove this lemma here, but it is provable.</p>
</div>
</div>
</div>
<p><span class="math inline">\(\boldsymbol\beta\)</span> is a vector of fixed constants. <span class="math inline">\((\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u\)</span> can be imagined as a matrix of fixed constants, since we are conditioning the above variance on <span class="math inline">\(\mathbf X\)</span> (so for each <span class="math inline">\(\mathbf X\)</span>, the statement is fixed). With the Lemma above, we can simplify:</p>
<p><span class="math display">\[
\begin{align}
Var(\boldsymbol{\hat\beta} | \mathbf X) &amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} Var(\mathbf u | \mathbf X) [(\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T}]^{-1} &amp;&amp; (\text{lemma})\\
&amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} Var(\mathbf u | \mathbf X) \color{blue}{\mathbf X (\mathbf X^\mathsf{T} \mathbf X)^{-1}} &amp;&amp; \color{black}( \ \color{blue}{[(\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T}]^{-1} = \mathbf X(\mathbf X^\mathsf{T} \mathbf X)^{-1}}\color{black})\\
\end{align}
\]</span></p>
<p>From here on, <a href="#homoscedasticity-and-heteroscedasticity">homoscedasticity and heteroscedasticity</a> matter. Let us first start by deriving variance with homoscedasticity, using the definition given by <a href="#eq-homo" class="quarto-xref">Equation&nbsp;4</a> :</p>
<p><span class="math display">\[
\begin{align}
Var(\hat{\boldsymbol\beta}|\mathbf X)&amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \color{blue}{\sigma^2 \mathbf I_n}\color{black}{ \mathbf X} (\mathbf X^\mathsf{T} \mathbf X)^{-1} &amp;&amp; (\color{blue}{Var(\mathbf u | \mathbf X) = \sigma^2 \mathbf I_n}\color{black}) \\
&amp; =  \color{red}{\sigma^2} \color{black} (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf I_n \mathbf X (\mathbf X^\mathsf{T} \mathbf X)^{-1} &amp;&amp; (\text{move scalar } \color{red}{\sigma^2}\color{black})\\
&amp; =  \sigma^2 (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T}  \mathbf X (\mathbf X^\mathsf{T} \mathbf X)^{-1} &amp;&amp; (\text{identity property of } \mathbf I_n)\\
&amp; =  \sigma^2 (\mathbf X^\mathsf{T} \mathbf X)^{-1} &amp;&amp; (\text{inverses } \mathbf X^\mathsf{T}  \mathbf X (\mathbf X^\mathsf{T} \mathbf X)^{-1} \text{ cancel})
\end{align}
\]</span></p>
<p>Now, let us calculate the variance for when heteroscedasticity is present, as defined by <a href="#eq-hetero" class="quarto-xref">Equation&nbsp;5</a> :</p>
<p><span class="math display">\[
Var(\hat{\boldsymbol\beta}|\mathbf X)  = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \color{blue}{\begin{pmatrix}
\sigma^2_1 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \sigma^2_2 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; \dots &amp; \sigma^2_n
\end{pmatrix}}\color{black}{ \mathbf X} (\mathbf X^\mathsf{T} \mathbf X)^{-1}
\]</span></p>
<p>To calculate both sets of standard errors (normal and robust), we estimate <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\sigma_i^2\)</span> as discussed <a href="#error-variance-and-homoscedasticity">previously</a>. The standard errors are then the square root of our estimated variances.</p>
<p><br></p>
</section>
<section id="gauss-markov-theorem" class="level3">
<h3 class="anchored" data-anchor-id="gauss-markov-theorem">Gauss-Markov Theorem</h3>
<p>The Gauss-Markov Theorem states that the OLS estimator is the <strong>best linear unbiased estimator</strong> (BLUE) - the unbiased linear estimator with the lowest variance, under 5 conditions: linearity, random sampling, no perfect multicollinearity, zero-conditional mean, and <a href="#homoscedasticity-and-heteroscedasticity">homoscedasticity</a>.</p>
<p>Any linear estimator takes the form <span class="math inline">\(\tilde{\boldsymbol\beta} = \mathbf{Cy}\)</span>. For example, the OLS estimator is of the form <span class="math inline">\(\boldsymbol{\hat\beta} = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf y\)</span>, which is the same as form as <span class="math inline">\(\mathbf{Cy}\)</span> if you define <span class="math inline">\(\mathbf C:= (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T}\)</span>. For any linear estimator <span class="math inline">\(\tilde{\boldsymbol\beta} = \mathbf{Cy}\)</span> to be unbiased, we need to assume <span class="math inline">\(\color{red}{\mathbf {CX} = \mathbf I}\)</span>.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof <span class="math inline">\(\mathbf{CX} = \mathbf I\)</span> For a Unbiased Linear Estimator
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For any linear estimator <span class="math inline">\(\tilde{\boldsymbol\beta} = \mathbf{Cy}\)</span> to be unbiased, we need to assume <span class="math inline">\(\color{red}{\mathbf {CX} = \mathbf I}\)</span>. The proof of this is as follows:</p>
<p>The proof of this is as follows:</p>
<p><span class="math display">\[
\begin{align}
\tilde{\boldsymbol\beta} &amp; = \mathbf C(\color{blue}{\mathbf X\boldsymbol\beta + \mathbf u} \color{black}) &amp;&amp; \text{plug in } \color{blue}{\mathbf y =\mathbf X\boldsymbol\beta + \mathbf u} \\
&amp; = \mathbf{CX} \boldsymbol\beta + \mathbf{Cu} &amp;&amp; \text{multiply out} \\
E(\tilde{\boldsymbol\beta}|\mathbf X)&amp; = E(\mathbf{CX} \boldsymbol\beta + \mathbf{Cu}) &amp;&amp; \text{take expectation} \\
&amp; = \mathbf{CX} \boldsymbol\beta + \mathbf C E(\mathbf u|\mathbf X) &amp;&amp; \text{take constants out of expectation} \\
&amp; = \mathbf{CX} \boldsymbol\beta &amp;&amp; \text{Zero-conditional mean}   \\
&amp; = \color{red}{\mathbf I}\color{black}{\boldsymbol\beta}  = \boldsymbol\beta &amp;&amp; \because \color{red}{\mathbf{CX} = \mathbf I} \\
E(\tilde{\boldsymbol\beta} &amp; = E(E(\tilde{\boldsymbol\beta}|\mathbf X)) &amp;&amp; \text{LIE: E(X) = E(E(X|Y)} \\
&amp; = E(\color{blue}{\boldsymbol\beta}) &amp;&amp; \because \color{blue}{E(\tilde{\boldsymbol\beta}|\mathbf X) = \boldsymbol\beta}\\
&amp; = \boldsymbol\beta &amp;&amp; \text{expectation of a constant}
\end{align}
\]</span></p>
<p>Thus, we have shown <span class="math inline">\(\color{red}{\mathbf {CX} = \mathbf I}\)</span> is a necessary condition for any linear estimator <span class="math inline">\(\tilde{\boldsymbol\beta} = \mathbf{Cy}\)</span> to be unbiased.</p>
</div>
</div>
</div>
<p>Now, let us calculate the variance of estimator <span class="math inline">\(\tilde{\boldsymbol\beta}\)</span>:</p>
<p><span class="math display">\[
\begin{align}
Var(\tilde{\boldsymbol\beta}|\mathbf X) &amp; = Var(\mathbf {Cy} | \mathbf X)\\
&amp; = Var(\mathbf C(\color{blue}{\mathbf X\boldsymbol\beta + \mathbf u} \color{black})| \mathbf X) &amp;&amp; (\text{plug in } \color{blue}{\mathbf y =\mathbf X\boldsymbol\beta + \mathbf u} \color{black}) \\
&amp; = Var(\mathbf{CX} \boldsymbol\beta + \mathbf{Cu}| \mathbf X) &amp;&amp; (\text{multiply out)} \\
&amp; = Var(\boldsymbol\beta + \mathbf{Cu} | \mathbf X) &amp;&amp; (\text{because }\color{red}{\mathbf {CX} = \mathbf I} \color{black})
\end{align}
\]</span></p>
<p>Using the above lemma in <a href="#eq-lemma" class="quarto-xref">Equation&nbsp;7</a> that we used for OLS variance, we get:</p>
<p><span class="math display">\[
\begin{align}
Var(\tilde{\boldsymbol\beta}|\mathbf X) &amp; = \mathbf CVar(\mathbf u|\mathbf X)\mathbf C^\mathsf{T} &amp;&amp; (\text{using lemma}) \\
&amp; = \mathbf C \color{blue}{\sigma^2\mathbf I} \color{black}{\mathbf C^\mathsf{T}} &amp;&amp; (\text{homoscedasticity } \color{blue}{Var(\mathbf u|\mathbf X) = \sigma^2 \mathbf I}) \\
&amp; = \sigma^2 \mathbf {CC}^\mathsf{T} &amp;&amp; (\text{identity property + rearrange})
\end{align}
\]</span></p>
<p>Now, we want to show that the variance of the OLS estimator <span class="math inline">\(\hat{\boldsymbol\beta}\)</span> (under homoscedasticity) is smaller than any linear estimator <span class="math inline">\(\tilde{\boldsymbol\beta}\)</span>. Let us find the difference between the variances of estimator <span class="math inline">\(\tilde{\boldsymbol\beta}\)</span> and <span class="math inline">\(\hat{\boldsymbol\beta}\)</span>. Note: since <span class="math inline">\(\color{red}{\mathbf{CX} = \mathbf I}\)</span>, the following is also true: <span class="math inline">\(\color{red}{\mathbf X^\mathsf{T} \mathbf C^\mathsf{T} = (\mathbf{CX})^\mathsf{T} = \mathbf I}\)</span>.</p>
<p><span class="math display">\[
\begin{align}
Var(\tilde{\boldsymbol\beta}|\mathbf X) - Var(\hat{\boldsymbol\beta} |\mathbf X) &amp; = \sigma^2\mathbf{CC}^\mathsf{T} - \sigma^2(\mathbf X^\mathsf{T}\mathbf X)^{-1} \\
&amp; = \sigma^2(\mathbf{CC}^\mathsf{T} - (\mathbf X^\mathsf{T}\mathbf X)^{-1}) &amp;&amp; (\text{factor out }\sigma^2) \\
&amp; = \sigma^2(\mathbf{CC}^\mathsf{T} - \color{red}{\mathbf{CX}}\color{black}(\mathbf X^\mathsf{T}\mathbf X)^{-1}\color{red}{\mathbf X^\mathsf{T} \mathbf C^\mathsf{T}}\color{black}) &amp;&amp; (\text{since }\color{red}{\mathbf X^\mathsf{T} \mathbf C^\mathsf{T} = \mathbf{CX} = \mathbf I} \color{black}) \\
&amp; = \sigma^2 \mathbf C (\mathbf I - \mathbf X(\mathbf X^\mathsf{T}\mathbf X)^{-1}\mathbf X^\mathsf{T})\mathbf C^\mathsf{T} &amp;&amp; (\text{factor out } \mathbf C, \mathbf C^\mathsf{T}) \\
&amp; = \sigma^2 \mathbf C\color{blue}{\mathbf M} \color{black}{\mathbf C}^\mathsf{T} &amp;&amp; \text{(residual maker matrix M)}
\end{align}
\]</span></p>
<p>Since <span class="math inline">\(\sigma^2 \mathbf {CM}\mathbf C^\mathsf{T}\)</span> is positive semi-definite (I will not prove this, but it is provable), we know that <span class="math inline">\(Var(\tilde{\boldsymbol\beta}|\mathbf X) &gt; Var(\hat{\boldsymbol\beta}|\mathbf X)\)</span>. Thus, OLS is BLUE under the Gauss-Markov Theorem.</p>
<p><br></p>
</section>
<section id="asymptotic-consistency-of-ols" class="level3">
<h3 class="anchored" data-anchor-id="asymptotic-consistency-of-ols">Asymptotic Consistency of OLS</h3>
<p>OLS is an <a href="./quant1.html#asymptotic-properties-of-estimators">asymptotically consistent estimator</a> of the relationship between any <span class="math inline">\(x_j\)</span> and <span class="math inline">\(y\)</span> under 4 conditions. These conditions are identical to the unbiasedness conditions EXCEPT condition 4, which is weakened from the original unbiasedness condition.</p>
<ol type="1">
<li><strong>Linearity</strong> (see unbiasedness)</li>
<li><strong>Random Sampling</strong> (…)</li>
<li><strong>No Perfect Multicolinearity</strong> (…)</li>
<li><strong>Zero Mean and Exogeneity</strong>: <span class="math inline">\(E(u_i) = 0\)</span>, and <span class="math inline">\(Cov(x_i, u_i) = 0\)</span>, which implies <span class="math inline">\(E(\mathbf x_i u_i) = 0\)</span>. This means that no regressor should be correlated with <span class="math inline">\(\mathbf u\)</span>. This is weaker than Zero-Conditional mean, since it means a function of regressors can be correlated with <span class="math inline">\(\mathbf u\)</span>.</li>
</ol>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lemma: Vector Notation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The following statements are true:</p>
<p><span class="math display">\[
\begin{split}
&amp; \mathbf X^\mathsf{T} \mathbf X = \sum\limits_{i=1}^n \mathbf x_i \mathbf x_i^\mathsf{T} \\
&amp; \mathbf X^\mathsf{T} \mathbf  u = \sum\limits_{i=1}^n \mathbf x_i u_i
\end{split}
\]</span></p>
</div>
</div>
</div>
<p>Let us start of where we left of from <a href="#eq-simplify" class="quarto-xref">Equation&nbsp;6</a>. Using vector notation, <a href="./quant1.html#asymptotically-consistent-estimators">law of large numbers</a>, and zero-mean and exogeneity condition:</p>
<p><span class="math display">\[
\begin{align}
\boldsymbol{\hat\beta} &amp; = \boldsymbol\beta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u  \\
\boldsymbol{\hat\beta} &amp; = \boldsymbol\beta + \left( \sum\limits_{i=1}^n \mathbf x_i \mathbf x_i^\mathsf{T} \right)^{-1} \left( \sum\limits_{i=1}^n \mathbf x_i \mathbf u \right) &amp;&amp; (\text{vector notation})\\
\boldsymbol{\hat\beta} &amp; = \boldsymbol\beta + \left( \frac{1}{n}\sum\limits_{i=1}^n \mathbf x_i \mathbf x_i^\mathsf{T} \right)^{-1} \left( \frac{1}{n}\sum\limits_{i=1}^n \mathbf x_i \mathbf u \right) &amp;&amp; ( \ \left(\frac{1}{n} \right)^{-1} \text{and } \frac{1}{n} \text{ cancel out}) \\
\text{plim} \boldsymbol{\hat\beta} &amp; = \boldsymbol\beta + \left( \text{plim} \frac{1}{n}\sum\limits_{i=1}^n \mathbf x_i \mathbf x_i^\mathsf{T} \right)^{-1} \left( \text{plim} \frac{1}{n}\sum\limits_{i=1}^n \mathbf x_i u_i \right) &amp;&amp; (\text{apply plim}) \\
\text{plim} \boldsymbol{\hat\beta} &amp; = \boldsymbol\beta + (E(\mathbf x_i \mathbf x_i^\mathsf{T}))^{-1}E(\mathbf x_i  u_i) &amp;&amp; (\text{law of large numbers})\\
\text{plim} \boldsymbol{\hat\beta} &amp; = \boldsymbol\beta &amp;&amp; (E(\mathbf x_i u_i) = 0)
\end{align}
\]</span></p>
<p>Thus, OLS is asymptotically consistent under the 4 conditions above.</p>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="regression-anatomy-and-specification" class="level1">
<h1><strong>Regression Anatomy and Specification</strong></h1>
<section id="partitioned-regression-model" class="level3">
<h3 class="anchored" data-anchor-id="partitioned-regression-model">Partitioned Regression Model</h3>
<p>We can split up matrix <span class="math inline">\(\mathbf X\)</span> into two matrices - <span class="math inline">\(\mathbf X_1\)</span> containing the regressors we care about, and <span class="math inline">\(\mathbf X_2\)</span> containing regressors we do not care about. <span class="math inline">\(\boldsymbol\beta\)</span> will be split in the same way. Our partitioned model is:</p>
<p><span class="math display">\[
\mathbf y = \mathbf X_1 \boldsymbol\beta_1 + \mathbf X_2 \boldsymbol\beta_2 + \mathbf u
\]</span></p>
<p>Recall our “residual maker” matrix <span class="math inline">\(\mathbf M\)</span>. First, note a unique property: <span class="math inline">\(\color{red}{\mathbf {MX} = 0}\)</span>. Now, let us define the residual making matrix for the second part of the regression <span class="math inline">\(\mathbf M_2\)</span>:</p>
<p><span class="math display">\[
\mathbf M_2 = \mathbf I - \mathbf X_2 (\mathbf X_2^\mathsf{T} \mathbf X_2) \mathbf X_2^\mathsf{T}
\]</span></p>
<p>Now, let us multiply both sides of our above partitioned model by <span class="math inline">\(\mathbf M_2\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\mathbf M_2 \mathbf y &amp; = \mathbf M_2(\mathbf X_1 \boldsymbol\beta_1 + \mathbf X_2 \boldsymbol\beta_2 + \mathbf u) \\
\mathbf M_2 \mathbf y &amp; = \mathbf M_2 \mathbf X_1 \boldsymbol\beta_1 + \mathbf M_2 \mathbf X_2 \boldsymbol\beta_2 + \mathbf M_2 \mathbf u &amp;&amp; (\text{multiply out}) \\
\mathbf M_2 \mathbf y &amp; = \mathbf M_2 \mathbf X_1 \boldsymbol\beta_1  + \mathbf M_2 \mathbf u &amp;&amp; ( \because \mathbf M_2 \mathbf X_2 =0, \because \color{red}{\mathbf {MX} = 0})
\end{align}
\]</span></p>
<p>Now, let us denote <span class="math inline">\(\tilde{\mathbf y} := \mathbf M_2 \mathbf y\)</span>, <span class="math inline">\(\tilde{\mathbf X}_1: = \mathbf M_2 \mathbf X_1\)</span>, and error <span class="math inline">\(\mathbf e := \mathbf M_2 \mathbf u\)</span>. Then we get the following regression equation and OLS coefficient estimates:</p>
<p><span class="math display">\[
\tilde{\mathbf y} = \tilde{\mathbf X}_1\boldsymbol\beta_1 + \mathbf e
\]</span></p>
<p><span class="math display">\[
\hat{\boldsymbol\beta}_1 = (\tilde{\mathbf X}_1^\mathsf{T} \tilde{\mathbf X}_1)^{-1}\tilde{\mathbf X}_1^\mathsf{T} \tilde{\mathbf y}
\]</span></p>
<p>Remember that <span class="math inline">\(\hat{\boldsymbol\beta}_1\)</span> is our coefficient estimates for <span class="math inline">\(\mathbf X_1\)</span>, the portion of <span class="math inline">\(\mathbf X\)</span> we are interested in. This is equivalent to the coefficient estimates had we not partitioned the model.</p>
<p>Notice how in the formula, we have <span class="math inline">\(\tilde{\mathbf X}_1\)</span>. What is <span class="math inline">\(\tilde{\mathbf X}_1 := \mathbf M_2 \mathbf X_1\)</span>? Well, we know that <span class="math inline">\(\mathbf M_2 \mathbf X_2 = 0\)</span>. That tells us that any part of <span class="math inline">\(\mathbf X_1\)</span> that was correlated to <span class="math inline">\(\mathbf X_2\)</span> also became 0. <u>Thus, <span class="math inline">\(\tilde{\mathbf X}_1\)</span> is the part of <span class="math inline">\(\mathbf X_1\)</span> that is uncorrelated with <span class="math inline">\(\mathbf X_2\)</span>.</u></p>
<p>What this essentially means is that the coefficient estimates of OLS <span class="math inline">\(\hat\beta_j\)</span> actually measure the effect on <span class="math inline">\(y\)</span> of the part of <span class="math inline">\(x_j\)</span> uncorrelated with the other explanatory variables <span class="math inline">\(x_1, \dots, x_k\)</span>. Essentially, we are <strong>partialling out</strong> the effect of other variables. <u>This is why we can “control” for other variables when focusing on the coefficient of one (or a few) variables</u>.</p>
<p><br></p>
</section>
<section id="omitted-variable-bias" class="level3">
<h3 class="anchored" data-anchor-id="omitted-variable-bias">Omitted Variable Bias</h3>
<p>From the regression anatomy theorem, we know that <span class="math inline">\(\hat\beta_j\)</span> is the relationship of <span class="math inline">\(y\)</span> and the part of <span class="math inline">\(x_j\)</span> that is uncorrelated with all the other explanatory variables. That implies that if we omit a variable that is correlated with both <span class="math inline">\(x_j\)</span> and <span class="math inline">\(y\)</span>, that we will get a different (biased) coefficient estimate. This is called <strong>omitted variable bias</strong>.</p>
<p>Suppose there is some variable <span class="math inline">\(z\)</span> that we have not included in a “short” regression. The actual, “true” regression of the population, would include this confounder <span class="math inline">\(z\)</span></p>
<p><span class="math display">\[
\underbrace{\mathbf y = \mathbf X \boldsymbol\beta + \mathbf u}_{\text{short regression}}
\qquad \underbrace{\mathbf y = \mathbf X \boldsymbol\beta + \mathbf z\boldsymbol\delta + \mathbf u}_{\text{true regression with z} }
\]</span></p>
<p>The <a href="#deriving-the-estimator">OLS estimate</a> of the “short regression” excluding confounder <span class="math inline">\(z\)</span> is:</p>
<p><span class="math display">\[
\begin{align}
\boldsymbol{\hat\beta} &amp; = (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf y \\
&amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} (\color{blue}{\mathbf X \boldsymbol\beta + \mathbf z\boldsymbol\delta + \mathbf u}\color{black}) &amp;&amp; (\text{plug in } \color{blue}{\mathbf y = \mathbf X \boldsymbol\beta + \mathbf z\boldsymbol\delta + \mathbf u}\color{black} )\\
&amp; = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf X \boldsymbol\beta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf z\boldsymbol\delta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u &amp;&amp; (\text{multiply out})\\
&amp; = \color{blue}{\mathbf I}\color{black}{\boldsymbol\beta} + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf z\boldsymbol\delta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u &amp;&amp; (\text{inverses } (\color{blue}{\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf X = \mathbf I}\color{black})\\
&amp; = \boldsymbol\beta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf z\boldsymbol\delta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf u &amp;&amp; (\text{identity property of } \mathbf I)
\end{align}
\]</span></p>
<p>Now, let us find the expected value of <span class="math inline">\(\boldsymbol{\hat\beta}\)</span>, which is conditional on <span class="math inline">\(\mathbf X, \mathbf z\)</span>, and simplify (using <a href="#ols-as-an-unbiased-estimator">zero conditional mean</a>):</p>
<p><span class="math display">\[
\begin{align}
E(\boldsymbol{\hat\beta}|\mathbf X, \mathbf z) &amp; = \boldsymbol\beta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf z\boldsymbol\delta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} E(\mathbf u | \mathbf X, \mathbf z) \\
&amp; = \boldsymbol\beta + (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf z\boldsymbol\delta &amp;&amp;(\because E(\mathbf u | \mathbf X, \mathbf z) = 0)
\end{align}
\]</span></p>
<p>Now, what if we had a regression of outcome variable being the confounder <span class="math inline">\(z\)</span>, on the explanatory variables <span class="math inline">\(\mathbf X\)</span>, such that <span class="math inline">\(\mathbf z = \mathbf X \boldsymbol\pi + \mathbf v\)</span>. Our OLS estimate would have the solution:</p>
<p><span class="math display">\[
\boldsymbol{\hat\pi} = (\mathbf X^\mathsf{T}\mathbf X)^{-1}\mathbf X^\mathsf{T} \mathbf z
\]</span></p>
<p>Now, we can plug <span class="math inline">\(\boldsymbol{\hat\pi}\)</span> into our expected value of <span class="math inline">\(\boldsymbol{\hat\beta}\)</span>. Assume our estimator <span class="math inline">\(\hat{\boldsymbol\pi}\)</span> is unbiased:</p>
<p><span class="math display">\[
\begin{align}
E(\boldsymbol{\hat\beta}|\mathbf X, \mathbf z)  &amp; = \boldsymbol\beta + \boldsymbol{\hat\pi \delta} \\
E(\boldsymbol{\hat\beta}) &amp; = E(E(\boldsymbol{\hat\beta}|\mathbf X, \mathbf z)) &amp;&amp; (\text{LIE: } E(X) = E(E(X|Y))\\
&amp; = E(\color{blue}{\boldsymbol\beta + \boldsymbol{\hat\pi \delta}}\color{black})  &amp;&amp; (\text{from the 1st line } \color{blue}{E(\boldsymbol{\hat\beta}|\mathbf X, \mathbf z) = \boldsymbol\beta + \boldsymbol{\hat\pi \delta}} \color{black}) \\
&amp; = \boldsymbol\beta + E(\boldsymbol{\hat\pi}) \boldsymbol \delta &amp;&amp; (\boldsymbol\beta, \boldsymbol\delta\text{ are constants})\\
&amp; = \boldsymbol\beta + \boldsymbol{\pi \delta} &amp;&amp; (\text{unbiased estimator } E(\boldsymbol{\hat\pi}) = \boldsymbol\pi)
\end{align}
\]</span></p>
<p>Thus, we can see by not including confounder <span class="math inline">\(z\)</span> in our “short regression”, the estimator is now biased by <span class="math inline">\(\boldsymbol{\hat\pi \delta}\)</span>. In the next chapter when we start discussing causality, we will see omitted confounders as a huge issue in our estimation.</p>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="method-of-moments-estimator" class="level1">
<h1><strong>Method of Moments Estimator</strong></h1>
<section id="method-of-moments" class="level3">
<h3 class="anchored" data-anchor-id="method-of-moments">Method of Moments</h3>
<p>The Method of Moments Estimator is another estimator of the true value of populations in the parameter. The estimator defines key population <strong>moments</strong> of interest - which are the population parameters written in terms of expected value functions set equal to 0.</p>
<p>Then, the Method of Moments uses the sample equivalents of the population moments to estimate the population parameter. For example, to estimate the population mean, the Method of Moments uses the sample mean.</p>
<p>In order to define a method of moments for a set of parameters <span class="math inline">\(\theta_1, \dots, \theta_k\)</span>, we need to specify at least one population moment per parameter. Or in other words, we must have more than <span class="math inline">\(k\)</span> population moments.</p>
<p>Our population moments can be defined as the expected value of some function <span class="math inline">\(m(\theta; y)\)</span> that consists of both the variable <span class="math inline">\(y\)</span> and our unknown parameter <span class="math inline">\(\theta\)</span>. The expectation of the function <span class="math inline">\(m(\theta; y)\)</span> should equal 0.</p>
<p><span class="math display">\[
E(m(\theta; y)) = 0
\]</span></p>
<p>Our sample moments will be the sample analogues of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(y\)</span>, which are <span class="math inline">\(\hat\theta\)</span> and <span class="math inline">\(y_i\)</span>:</p>
<p><span class="math display">\[
\frac{1}{n}\sum\limits_{i=1}^n m(\hat\theta; y_i) = 0
\]</span></p>
<p>Method of moments estimators are asymptotically consistent, because of the law of large numbers.</p>
<p><br></p>
</section>
<section id="population-mean-estimator" class="level3">
<h3 class="anchored" data-anchor-id="population-mean-estimator">Population Mean Estimator</h3>
<p>Let us say that we have some random variable <span class="math inline">\(y\)</span>, with a true population mean <span class="math inline">\(\mu\)</span>. We want to estimate <span class="math inline">\(\mu\)</span>, but we only have a sample of the population.</p>
<p>How can we define <span class="math inline">\(\mu\)</span> in a moment of the form: <span class="math inline">\(E(m(\mu, y)) = 0\)</span>? Well, we know <span class="math inline">\(\mu\)</span> is the expectation of <span class="math inline">\(y\)</span>, so <span class="math inline">\(\mu = E(y)\)</span>. Since they are equal, <span class="math inline">\(\mu - E(y) = 0\)</span>. Thus, we can define the mean as a moment of the following condition:</p>
<p><span class="math display">\[
E(y - \mu) = 0
\]</span></p>
<p>The method of moments estimator uses the sample equivalent of the population moment. The sample equivalent of <span class="math inline">\(\mu\)</span>, is the sample mean <span class="math inline">\(\bar y\)</span>:</p>
<p><span class="math display">\[
E(y_i - \hat\mu) = \frac{1}{n}\sum\limits_{i=1}^n (y_i - \hat\mu) = 0
\]</span></p>
<p>With this equation, we can then solve for <span class="math inline">\(\hat\mu\)</span>:</p>
<p><span class="math display">\[
\begin{align}
0 &amp; = \frac{1}{n}\sum\limits_{i=1}^n (y_i - \hat\mu) \\
0 &amp; = \frac{1}{n}\sum\limits_{i=1}^ny_i - \frac{1}{n}\sum\limits_{i=1}^n \hat\mu  &amp;&amp; (\text{multiply out})\\
0 &amp; = \frac{1}{n}\sum\limits_{i=1}^ny_i - \frac{1}{n} n \hat\mu &amp;&amp;(\text{summation property of constant } \hat\mu)\\
0 &amp; = \bar y - \hat \mu &amp;&amp; (\text{definition of mean }\frac{1}{n}\sum\limits_{i=1}^ny_i = \bar y)\\
\hat\mu &amp; = \bar y &amp;&amp; (+\hat\mu\text{ to both sides})
\end{align}
\]</span></p>
<p>So, we see the method of moments estimates our true population mean <span class="math inline">\(\mu\)</span>, with the sample mean <span class="math inline">\(\bar y\)</span>. As a method of moments estimator, it is also asymptotically consistent.</p>
<p><br></p>
</section>
<section id="ols-as-a-method-of-moments" class="level3">
<h3 class="anchored" data-anchor-id="ols-as-a-method-of-moments">OLS as a Method of Moments</h3>
<p>OLS is a special case of the Method of Moments Estimator. Consider the bivariate regression model. The OLS estimator can be derived as a method of moments estimator, with 2 moments (expectation functions set equal to 0), one for each parameter (<span class="math inline">\(\beta_0, \beta_1\)</span>):</p>
<p><span class="math display">\[
\begin{split}
&amp; E(y-\beta_0 -\beta_1x) = E(u) = 0 \\
&amp; E(x(y - \beta_0 - \beta_1 x)) = E(xu) = 0
\end{split}
\]</span></p>
<p>The estimates of these moments would use the sample equivalents: <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>.</p>
<p><span class="math display">\[
\begin{split}
&amp; E(y-\hat\beta_0 -\hat\beta_1x) = 0 \\
&amp; E(x(y - \hat\beta_0 - \hat\beta_1 x)) = 0
\end{split}
\]</span></p>
<p>Remember our OLS minimisation conditions:</p>
<p><span class="math display">\[
\begin{split}
&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
&amp; \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
\]</span></p>
<p>Since by definition, average/expectation is <span class="math inline">\(E(x) = \frac{1}{n} \sum x_i\)</span>, we can rewrite the OLS minimisation conditions as:</p>
<p><span class="math display">\[
\begin{split}
&amp; n \times E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
&amp; n \times E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
\]</span></p>
<p>And since anything multiplied to a zero turns into zero, we can ignore the <span class="math inline">\(n\)</span> in the first order condition, and only focus on the expected value part. Thus, our conditions are:</p>
<p><span class="math display">\[
\begin{split}
&amp; E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
&amp; E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
\]</span></p>
<p>Which as we can see, are the exact same minimisation conditions as the method of moments estimator. Thus, the OLS estimator is a special case of the Method of Moments estimator, and they produce the same coefficients. This is an important property for the instrumental variables method that will be covered later.</p>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="maximum-likelihood-estimator" class="level1">
<h1><strong>Maximum Likelihood Estimator</strong></h1>
<section id="likelihood-functions" class="level3">
<h3 class="anchored" data-anchor-id="likelihood-functions">Likelihood Functions</h3>
<p><br></p>
</section>
<section id="gradient-descent-algorithms" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent-algorithms">Gradient Descent Algorithms</h3>
<p><br></p>
</section>
<section id="ols-as-a-maximum-likelihood-estimator" class="level3">
<h3 class="anchored" data-anchor-id="ols-as-a-maximum-likelihood-estimator">OLS as a Maximum Likelihood Estimator</h3>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>