<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>The Classical Linear Model – Political Science &amp; Political Economy</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="mathjax-config.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./clm.html">1 The Classical Linear Model</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Kevin’s PSPE Resources</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
 <span class="menu-text">Quantitative Methods</span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clm.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">1 The Classical Linear Model</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 The Generalised Linear Model</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./causal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 Causal Frameworks</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 Randomised Controlled Trials</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./soo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 Selection on Observables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./iv.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 Instrumental Variables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rd.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7 Regression Discontinuity</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./did.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8 Differences-in-Differences</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Further Statistical Methods</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./predict.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Forecasting and Prediction Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unsupervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Unsupervied Learning Methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./factor.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Factor Analysis Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./latent.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Latent Trait and Class Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sem.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Structural Equation Models</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./games.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guide to Game Theory</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./math.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Background Mathematics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Background Statistics</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On This Page</h2>
   
  <ul>
  <li><a href="#the-linear-model" id="toc-the-linear-model" class="nav-link active" data-scroll-target="#the-linear-model"><strong>The Linear Model</strong></a>
  <ul class="collapse">
  <li><a href="#model-specification" id="toc-model-specification" class="nav-link" data-scroll-target="#model-specification">Model Specification</a></li>
  <li><a href="#ordinary-least-squares-estimator" id="toc-ordinary-least-squares-estimator" class="nav-link" data-scroll-target="#ordinary-least-squares-estimator">Ordinary Least Squares Estimator</a></li>
  <li><a href="#orthogonal-projection-of-ols" id="toc-orthogonal-projection-of-ols" class="nav-link" data-scroll-target="#orthogonal-projection-of-ols">Orthogonal Projection of OLS</a></li>
  </ul></li>
  <li><a href="#interpreting-the-model" id="toc-interpreting-the-model" class="nav-link" data-scroll-target="#interpreting-the-model"><strong>Interpreting the Model</strong></a>
  <ul class="collapse">
  <li><a href="#regression-anatomy-and-controlling" id="toc-regression-anatomy-and-controlling" class="nav-link" data-scroll-target="#regression-anatomy-and-controlling">Regression Anatomy and Controlling</a></li>
  <li><a href="#interpretation-of-coefficients" id="toc-interpretation-of-coefficients" class="nav-link" data-scroll-target="#interpretation-of-coefficients">Interpretation of Coefficients</a></li>
  <li><a href="#goodness-of-fit-with-r-squared" id="toc-goodness-of-fit-with-r-squared" class="nav-link" data-scroll-target="#goodness-of-fit-with-r-squared">Goodness of Fit with R-Squared</a></li>
  </ul></li>
  <li><a href="#classical-least-squares-theory" id="toc-classical-least-squares-theory" class="nav-link" data-scroll-target="#classical-least-squares-theory"><strong>Classical Least Squares Theory</strong></a>
  <ul class="collapse">
  <li><a href="#the-classical-assumptions" id="toc-the-classical-assumptions" class="nav-link" data-scroll-target="#the-classical-assumptions">The Classical Assumptions</a></li>
  <li><a href="#ols-as-an-unbiased-estimator" id="toc-ols-as-an-unbiased-estimator" class="nav-link" data-scroll-target="#ols-as-an-unbiased-estimator">Unbiasedness of OLS</a></li>
  <li><a href="#variance-of-the-ols-estimator" id="toc-variance-of-the-ols-estimator" class="nav-link" data-scroll-target="#variance-of-the-ols-estimator">Variance of the OLS Estimator</a></li>
  <li><a href="#gauss-markov-theorem" id="toc-gauss-markov-theorem" class="nav-link" data-scroll-target="#gauss-markov-theorem">Gauss-Markov Theorem</a></li>
  <li><a href="#asymptotic-consistency-of-ols" id="toc-asymptotic-consistency-of-ols" class="nav-link" data-scroll-target="#asymptotic-consistency-of-ols">Asymptotic Consistency of OLS</a></li>
  <li><a href="#weakening-spherical-errors" id="toc-weakening-spherical-errors" class="nav-link" data-scroll-target="#weakening-spherical-errors">Robust Standard Errors</a></li>
  </ul></li>
  <li><a href="#statistical-inference" id="toc-statistical-inference" class="nav-link" data-scroll-target="#statistical-inference"><strong>Statistical Inference</strong></a>
  <ul class="collapse">
  <li><a href="#normality-and-standard-errors" id="toc-normality-and-standard-errors" class="nav-link" data-scroll-target="#normality-and-standard-errors">Normality and Standard Errors</a></li>
  <li><a href="#hypothesis-testing" id="toc-hypothesis-testing" class="nav-link" data-scroll-target="#hypothesis-testing">Hypothesis Testing</a></li>
  <li><a href="#f-tests" id="toc-f-tests" class="nav-link" data-scroll-target="#f-tests">F-Tests</a></li>
  </ul></li>
  <li><a href="#model-specification-issues" id="toc-model-specification-issues" class="nav-link" data-scroll-target="#model-specification-issues"><strong>Model Specification Issues</strong></a>
  <ul class="collapse">
  <li><a href="#omitted-variable-bias" id="toc-omitted-variable-bias" class="nav-link" data-scroll-target="#omitted-variable-bias">Omitted Variable Bias</a></li>
  <li><a href="#heterogeneity-and-interactions" id="toc-heterogeneity-and-interactions" class="nav-link" data-scroll-target="#heterogeneity-and-interactions">Heterogeneity and Interactions</a></li>
  <li><a href="#polynomial-transformations" id="toc-polynomial-transformations" class="nav-link" data-scroll-target="#polynomial-transformations">Polynomial Transformations</a></li>
  <li><a href="#logarithmic-transformations" id="toc-logarithmic-transformations" class="nav-link" data-scroll-target="#logarithmic-transformations">Logarithmic Transformations</a></li>
  <li><a href="#categorical-explanatory-variables" id="toc-categorical-explanatory-variables" class="nav-link" data-scroll-target="#categorical-explanatory-variables">Categorical Explanatory Variables</a></li>
  <li><a href="#linear-probability-model" id="toc-linear-probability-model" class="nav-link" data-scroll-target="#linear-probability-model">Linear Probability Model</a></li>
  </ul></li>
  <li><a href="#implementation-in-r" id="toc-implementation-in-r" class="nav-link" data-scroll-target="#implementation-in-r"><strong>Implementation in R</strong></a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">The Classical Linear Model</h1>
<p class="subtitle lead">Chapter 1, Quantitative Methods</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Before we start with causal inference, we need to understand relationships between variables (correlations). This chapter inroduces the <strong>classical linear model</strong>, its mechanics, and how we can achieve unbiased estimates of relationships. This chapter discusses the classical linear models, the properties of the ordinary least squares estimator, statistical inference in a regression setting, and model selection.</p>
<p>Use the right sidebar for quick navigation. R-code is provided at the bottom.</p>
<hr>
<section id="the-linear-model" class="level1">
<h1><strong>The Linear Model</strong></h1>
<section id="model-specification" class="level3">
<h3 class="anchored" data-anchor-id="model-specification">Model Specification</h3>
<p>There is some random outcome variable <span class="math inline">\(Y_i\)</span>, and <span class="math inline">\(p\)</span> explanatory variables <span class="math inline">\(X_{i1}, X_{i2}, \dots, X_{ip}\)</span>. The population linear model is specified as a <a href="./stats.html#conditional-distributions">conditional expectation function</a>:</p>
<p><span class="math display">\[
\E(Y_i|X_i) = \beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip}
\]</span></p>
<p>Where <span class="math inline">\(\beta_0, \dots, \beta_p\)</span> are population parameters to be estimated. We can also specify linear regression not as the expected <span class="math inline">\(Y\)</span> as above, but for each observation of <span class="math inline">\(Y_i\)</span>:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1X_{i1} + \dots + \beta_pX_{ip} + \eps_i \ = \  \beta_0 + \sum\limits_{j=1}^p \beta_jX_{ij} + \eps_i
\]</span></p>
<p>Where <span class="math inline">\(\eps_i\)</span> is the error term representing the variation in <span class="math inline">\(Y_i\)</span> that is not explained by the <span class="math inline">\(p\)</span> explanatory variables: either a missing variable in our model, or some random noise. This model implies for each observation/individual <span class="math inline">\(i=1,\dots ,n\)</span> in the population, with values <span class="math inline">\((y_i, x_{i1} \dots, x_{ip})\)</span> has a regression equation:</p>
<p><span class="math display">\[
\begin{align}
y_1 = &amp; \ \beta_0 + \beta_1x_{11} + \dots + \beta_px_{1p} + \eps_1 \\
y_2 = &amp; \ \beta_0 + \beta_1x_{21} + \dots + \beta_px_{2p} + \eps_2 \\
&amp; \qquad \vdots \qquad \qquad \vdots \\
y_n = &amp; \ \beta_0 + \beta_1x_{n1} + \dots + \beta_px_{np} + \eps_n
\end{align}
\]</span></p>
<p>We can write this system of regression equations in linear algebra form:</p>
<p><span class="math display">\[
y = X\beta + \eps \quad \iff \quad \begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix}  =
\begin{pmatrix}1 &amp; x_{11} &amp; \dots &amp; x_{1p} \\1 &amp; x_{21} &amp; \dots &amp; x_{2p} \\\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\1 &amp; x_{n1} &amp; \dots &amp; x_{np}\end{pmatrix}
\begin{pmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_p\end{pmatrix}
+ \begin{pmatrix}\eps_1 \\ \eps_2 \\ \vdots \\ \eps_n\end{pmatrix}
\]</span></p>
<p>Our goal is to estimate <span class="math inline">\(\beta\)</span> to get our estimates <span class="math inline">\(\hat\beta\)</span>, which allows us to create predicted fitted-values model <span class="math inline">\(\hat y = X\hat\beta\)</span>.</p>
<p><br></p>
</section>
<section id="ordinary-least-squares-estimator" class="level3">
<h3 class="anchored" data-anchor-id="ordinary-least-squares-estimator">Ordinary Least Squares Estimator</h3>
<p>To <a href="./stats.html#estimators-and-sampling-distributions">estimate</a> the population parameters <span class="math inline">\(\beta_0, \dots, \beta_p\)</span>, we use our sample data and try to find the values <span class="math inline">\(\hat\beta_0, \dots, \hat\beta_p\)</span> that <strong>minimise the square sum of residuals</strong> (SSR): <span class="math inline">\(\sum(Y_i - \hat Y_i)^2\)</span> .We will define the SSR as function <span class="math inline">\(S(\hat\beta)\)</span>:</p>
<p><span class="math display">\[
\begin{align}
S(\hat\beta) &amp;  = (y - \hat y)^\top (y - \hat y) &amp;&amp; (\Sigma(Y - \hat Y_i)^2 \text{ in linear algebra})\\
&amp; = (y - \color{blue}{X \hat\beta}\color{black} )^\top (y - \color{blue}{X \hat\beta}\color{black})  &amp;&amp; (\because \color{blue}{\hat y = X \hat\beta}\color{black})\\
&amp; = y^\top y - \hat\beta^\top X^\top y - y^\top X \hat\beta +  \hat\beta^\top X^\top X \hat\beta &amp;&amp; (\text{distribute out}) \\
&amp; = y^\top y \ \color{blue}{-  2 \hat\beta^\top X^\top y} \color{black}  +  \underbrace{\hat\beta^\top X^\top X \hat\beta}_{\text{quadratic}} &amp;&amp; (\because \color{blue}{- \hat\beta^\top X^\top y - y^\top X \hat\beta = - 2 \hat\beta^\top X^\top y} \color{black})
\end{align}
\]</span></p>
<p>Now, let us take the gradient to find the first order condition:</p>
<p><span class="math display">\[
\frac{\partial S(\hat\beta)}{\partial \hat\beta} = -2 X^\top y + 2 X^\top X \hat\beta = 0
\]</span></p>
<p>When assuming <span class="math inline">\(X^\top X\)</span> is invertable (which is true if <span class="math inline">\(X\)</span> is full rank), we can isolate <span class="math inline">\(\hat\beta\)</span> to find the solution to OLS:</p>
<p><span id="eq-ols"><span class="math display">\[
\begin{align}
-2 X^\top y + 2 X^\top X \hat\beta &amp; = 0 \\
2 X^\top X \hat\beta &amp; = 2 X^\top y &amp;&amp; ( + 2X^\top y \text{ to both sides}) \\
\hat\beta &amp; = (2X^\top X)^{-1} -2 X^\top y &amp;&amp; (\times (2X^\top X)^{-1} \text{ to both sides}) \\
\hat\beta &amp; = (X^\top X)^{-1} X^\top y &amp;&amp; (2^{-1}, 2 \text{ cancel out})
\end{align}
\tag{1}\]</span></span></p>
<p>Vector <span class="math inline">\(\hat\beta\)</span> is our coefficient estimates derived from OLS. With <span class="math inline">\(\hat\beta\)</span>, we can calculate <strong>predictions</strong> of <span class="math inline">\(y\)</span> with <span class="math inline">\(\hat y = X \hat\beta\)</span>.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Alternative Derivation for Simple Linear Regression
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For simple linear regression (with one explanatory variable), we can use summation notation instead of linear regression. Our SSR in summation form is:</p>
<p><span class="math display">\[
SSR = S(\hat\beta_0, \hat\beta_1)= \sum\limits_{i=1}^n(Y_i - \hat\beta_0 - \hat\beta_1X_i)^2
\]</span></p>
<p>We want to minimise the SSR in respect to both <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>. We can do this by finding our first order conditions:</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial S(\hat\beta_0, \hat\beta_1)}{\partial \hat\beta_0} &amp; = \sum\limits_{i=1}^n(Y_i - \hat\beta_0 - \hat\beta_1X_i) = 0 \\
\frac{\partial S(\hat\beta_0, \hat\beta_1)}{\partial \hat\beta_1} &amp; = \sum\limits_{i=1}^n X_i(Y_i - \hat\beta_0 - \hat\beta_1X_i) = 0 \\
\end{align}
\]</span></p>
<p>These conditions create a system of equations, which you can solve for the OLS solutions of <span class="math inline">\(\widehat{\beta_0}\)</span> and <span class="math inline">\(\widehat{\beta_1}\)</span>. I will not show it step by step, as it is tedious (and not that important). The OLS solutions are</p>
<p><span class="math display">\[
\begin{align}
\hat\beta_0 &amp; = \bar Y - \widehat{\beta_1} \bar X \\
\hat\beta_1 &amp; = \frac{\sum_{i=1}^n(X_i - \bar X)(Y_i - \bar Y)}{\sum_{i=1}^n(X_i - \bar X)^2} = \frac{Cov(X_i, Y_i)}{\V Y_i}
\end{align}
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof OLS Estimates are Equal with <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(\E(Y_i|X_i)\)</span>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Before, we specified the linear model in terms of both <span class="math inline">\(Y_i\)</span> and the conditional expectation function <span class="math inline">\(\E(Y_i|X_i)\)</span>. However, I only derived OLS estimates in respect to <span class="math inline">\(Y_i\)</span>. For those interested, this is proof the OLS estimates of both are equivalent.</p>
<p>Best-approximation of a conditional expectation function is defined by the lowest mean-squared error (MSE). Let us prove OLS on <span class="math inline">\(Y_i\)</span> gets the same <span class="math inline">\(\beta_0, \dots, \beta_p\)</span> as the best linear approximation of <span class="math inline">\(\E(Y_i|X_i)\)</span>. Take this very simple CEF and its MSE:</p>
<p><span class="math display">\[
\begin{align}
\E(Y_i|X_i) &amp; = b_0 + b_1X_i \\
MSE &amp; = \E(Y_i - \E(Y_i|X_i))^2 \\
&amp; =  \E(Y_i - (\color{blue}{b_0 + b_1X_i}\color{black}))^2  &amp;&amp; (\because \color{blue}{\E(Y_i|X_i) = b_0 + b_1X_i}\color{black})\\
&amp; = \E(Y_i - b_0 - b_1 X_i) &amp;&amp; \text{(distribute negative sign)}
\end{align}
\]</span></p>
<p>The first order conditions are (using chain rule and partial derivatives):</p>
<p><span class="math display">\[
\begin{split}
&amp; \E(Y_i - b_0 - b_1X_i) = 0 \\
&amp; \E(X_i(Y_i - b_0 - b_1X_i) = 0
\end{split}
\]</span></p>
<p>Now, recall our OLS minimisation conditions (simple linear regression, presented in the extra info box above). Since by definition, average/expectation is <span class="math inline">\(\E(x) = \frac{1}{n} \sum x_i\)</span>, we can rewrite as:</p>
<p><span class="math display">\[
\begin{split}
&amp; \sum\limits_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) = \ n \times \E(Y_i - \hat\beta_0 - \hat\beta_1X_i) = 0 \\
&amp; \sum\limits_{i=1}^n X_i (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) = \  n \times \E(X_i(Y_i - \hat\beta_0 - \hat\beta_1X_i)) = 0
\end{split}
\]</span></p>
<p>And since anything multiplied to a zero turns into zero, we can ignore the <span class="math inline">\(n\)</span> in the first order condition. Thus, we can see, are the exact same minimisation conditions as the conditional expectation function.</p>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="orthogonal-projection-of-ols" class="level3">
<h3 class="anchored" data-anchor-id="orthogonal-projection-of-ols">Orthogonal Projection of OLS</h3>
<p>We can use the OLS solution from <a href="#eq-ols" class="quarto-xref">Equation&nbsp;1</a> to get our fitted values <span class="math inline">\(\hat{y}\)</span> and residuals <span class="math inline">\(\hat\eps\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\hat y &amp; = X\hat\beta = X \color{blue}{(X^\top X)^{-1}X^\top y} &amp;&amp; \color{black}(\because \color{blue}{\hat\beta = (X^\top X)^{-1}X^\top y} \color{black}) \\
&amp; = \color{red}{P}\color{black}y &amp;&amp; (\because \color{red}{P:= X(X^\top X)^{-1}X^\top}) \\
\hat\eps &amp; = y - \hat y  = y - \color{blue}{Py} &amp;&amp; \color{black}( \because \color{blue}{\hat y = Py}\color{black}) \\
&amp; = (I-P)y &amp;&amp; (\text{factor out y}) \\
&amp; = \color{purple}{M}\color{black}y &amp;&amp; (\because \color{purple}{M:= I - P}\color{black})
\end{align}
\]</span></p>
<p>Matrix <span class="math inline">\(\color{red}{P}\)</span>, called the <strong>projection matrix</strong>, is a matrix operator that performs the <a href="./math.html#linear-mappings-and-combinations">linear mapping</a> <span class="math inline">\(y \rightarrow \hat{ y}\)</span>. Matrix <span class="math inline">\(\color{purple}{M}\)</span>, called the <strong>residual maker</strong>, is a matrix operator that performs the <a href="./math.html#linear-mappings-and-combinations">linear mapping</a> <span class="math inline">\(y \rightarrow \hat{\eps}\)</span>.</p>
<p>Fitted values <span class="math inline">\(\hat{y}\)</span> are a linear combination of our explanatory variables. That implies our explanatory variable vectors <span class="math inline">\(x_1, x_2, \dots, x_p\)</span> span a <a href="./math.html#vector-spaces-and-matrix-rank">vector space</a> that includes our fitted values vector <span class="math inline">\(\hat{y}\)</span>. Thus, what <span class="math inline">\(\color{red}{ P}\)</span> is doing is taking our original vector <span class="math inline">\(y\)</span>, and projecting it into the space spanned by our <span class="math inline">\(X\)</span> (called the <strong>column space</strong>). In the figure below, our observed <span class="math inline">\(y\)</span> vector is being projected onto the blue plane spanned by <span class="math inline">\(X\)</span> to create our <span class="math inline">\(\hat{y}\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-880030792.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:45.0%"></p>
</figure>
</div>
<p>Residual maker <span class="math inline">\(\color{purple}{M}\)</span> projects <span class="math inline">\(y\)</span> onto the space orthogonal to the column space of <span class="math inline">\(X\)</span> to get our residuals <span class="math inline">\(\hat{\eps}\)</span>. We can see in the figure the residuals vector (notated <span class="math inline">\(\mathbf e\)</span> in the figure) is orthogonal/perpendicular to the space of <span class="math inline">\(\mathbf X\)</span>.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Matrix Properties of <span class="math inline">\(P\)</span> and <span class="math inline">\(M\)</span>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Both <span class="math inline">\(\color{red}{P}\)</span> and <span class="math inline">\(\color{purple}{M}\)</span> are <a href="./math.html#types-of-matrices">symmetric matrices</a>: <span class="math inline">\(P^\top = P, \ M^\top = M\)</span>. They are also both <a href="./math.html#types-of-matrices">idempotent matrices</a>: <span class="math inline">\(PP = P, \ MM = M\)</span>. We can prove this second statement using the first (I will only do it for <span class="math inline">\(P\)</span>, but the same applies for <span class="math inline">\(M\)</span>:</p>
<p><span class="math display">\[
\begin{align}
PP &amp; = X(X^\top X)^{-1} \underbrace{X^\top X(X^\top X)^{-1}}_{= I} X^\top \\
&amp; = X(X^\top X)^{-1} X^\top = P
\end{align}
\]</span></p>
<p><span class="math inline">\(\color{red}{ P}\)</span> and <span class="math inline">\(\color{purple}{ M}\)</span> are also <a href="./math.html#types-of-matrices">orthogonal</a> to each other - i.e.&nbsp;<span class="math inline">\(P^\top M = 0\)</span>:</p>
<p><span class="math display">\[
\begin{align}
P^\top M &amp; = \color{blue}{P}\color{black}M &amp;&amp; (\because \color{blue}{P^\top = P}\color{black}) \\
&amp; = P(\color{blue}{I-P}\color{black}) &amp;&amp; (\because \color{blue}{M:= I - P}\color{black}) \\
&amp; = P - PP &amp;&amp; \text{(distribute out)} \\
&amp; = P - \color{blue}{P} &amp;&amp; \color{black}(\because \color{blue}{PP = P}\color{black}) \\
&amp; = 0
\end{align}
\]</span></p>
</div>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="interpreting-the-model" class="level1">
<h1><strong>Interpreting the Model</strong></h1>
<section id="regression-anatomy-and-controlling" class="level3">
<h3 class="anchored" data-anchor-id="regression-anatomy-and-controlling">Regression Anatomy and Controlling</h3>
<p>We can split up matrix <span class="math inline">\(X\)</span> into two matrices - <span class="math inline">\(X_1\)</span> containing the regressors we care about, and <span class="math inline">\(X_2\)</span> containing regressors we do not care about. Vector <span class="math inline">\(\beta\)</span> will be split in the same way. Our partitioned model is:</p>
<p><span class="math display">\[
y = X_1 \beta_1 + X_2 \beta_2 + \eps
\]</span></p>
<p>Recall “<a href="#orthogonal-projection-of-ols">residual maker</a>” matrix <span class="math inline">\(M\)</span>. First, note a unique property: <span class="math inline">\(\color{red}{MX = 0}\)</span>. Now, let us define the residual making matrix for the second part of the regression <span class="math inline">\(M_2\)</span>:</p>
<p><span class="math display">\[
M_2 = I - X_2 (X_2^\top X_2)^{-1}X_2^\top
\]</span></p>
<p>Now, let us multiply both sides of our above partitioned model by <span class="math inline">\(M_2\)</span>:</p>
<p><span class="math display">\[
\begin{align}
M_2 y &amp; = M2(X_1\beta_1 + X_2\beta_2 + \eps) \\
M_2 y &amp; = M_2X_1 \beta_1 + M_2 X_2 \beta_2 + M_2 \eps &amp;&amp; \text{(multiply out)} \\
M_2 y &amp; = M_2 X_1 \beta_1 + M_2 \eps &amp;&amp; (\because M_2X_2 = 0, \ \because \color{red}{MX = 0}\color{black})
\end{align}
\]</span></p>
<p>Now, let us define <span class="math inline">\(\tilde{y} := M_2 y\)</span>, <span class="math inline">\(\tilde{X}_1: = M_2 X_1\)</span>, and error <span class="math inline">\(\tilde\eps := M_2 \eps\)</span>. Then we get the following regression equation and OLS coefficient estimates:</p>
<p><span class="math display">\[
\tilde y = \tilde X_1 \beta_1 + \tilde\eps
\]</span></p>
<p><span class="math display">\[
\hat\beta_1 = (\tilde X_1^\top \tilde X_1)^{-1}\tilde X_1 ^\top \tilde y
\]</span></p>
<p>Remember that vector <span class="math inline">\(\hat{\beta}_1\)</span> is our coefficient estimates for <span class="math inline">\(X_1\)</span>, the portion of <span class="math inline">\(X\)</span> we are interested in. This is equivalent to the coefficient estimates had we not partitioned the model.</p>
<p>Notice how in the formula, we have <span class="math inline">\(\tilde{X}_1 := M_2 X_1\)</span>. We know that <span class="math inline">\(M_2 X_2 = 0\)</span>. That tells us that any part of <span class="math inline">\(X_1\)</span> that was correlated to <span class="math inline">\(X_2\)</span> also became 0. <u>Thus, <span class="math inline">\(\tilde{X}_1\)</span> is the part of <span class="math inline">\(X_1\)</span> that is uncorrelated with <span class="math inline">\(X_2\)</span>.</u> Essentially, we are <strong>partialling out</strong> the effect of other variables. <u>This is why we can “control” for other variables when conducting multiple regression.</u></p>
<p><br></p>
</section>
<section id="interpretation-of-coefficients" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-of-coefficients">Interpretation of Coefficients</h3>
<p>Above, we showed OLS coefficients partial out (control) for the other control variables in the regression. Here, we formalise the interpretations. I define <span class="math inline">\(\hat\beta_j \in \{\hat\beta_1, \dots, \hat\beta_p\}\)</span>, multiplied to <span class="math inline">\(X_{ij} \in \{X_{i1}, \dots, X_{ip}\}\)</span>. <span class="math inline">\(\hat\beta_0\)</span> is the intercept.</p>
<p>I assume here that <span class="math inline">\(Y_i\)</span> is continuous. For interpretations for non-linear <span class="math inline">\(Y_i\)</span>, see the <a href="#linear-probability-model">linear probability model</a> section below. For categorical <span class="math inline">\(X_{ij}\)</span>, see the <a href="#categorical-explanatory-variables">categorical explanatory variables</a> section.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 46%">
<col style="width: 49%">
</colgroup>
<tbody>
<tr class="odd">
<td></td>
<td><strong>Continuous</strong> <span class="math inline">\(X_{ij}\)</span></td>
<td><strong>Binary</strong> <span class="math inline">\(X_{ij}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat\beta_j\)</span></td>
<td>For every one unit increase in <span class="math inline">\(X_{ij}\)</span>, there is an expected <span class="math inline">\(\hat\beta_j\)</span> unit change in <span class="math inline">\(Y_i\)</span>, holding all other explanatory variables constant.</td>
<td>There is a <span class="math inline">\(\hat\beta_j\)</span> unit difference in <span class="math inline">\(Y_i\)</span> between category <span class="math inline">\(X_{ij} = 1\)</span> and category <span class="math inline">\(X_{ij} = 0\)</span>, holding all other explanatory variables constant.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\hat\beta_0\)</span></td>
<td>When all explanatory variables equal 0, the expected value of <span class="math inline">\(Y_i\)</span> is <span class="math inline">\(\hat\beta_0\)</span>.</td>
<td>For category <span class="math inline">\(X_{ij} = 0\)</span>, the expected value of <span class="math inline">\(Y_i\)</span> is <span class="math inline">\(\hat\beta_0\)</span> (when all other explanatory variables equal 0).</td>
</tr>
</tbody>
</table>
<p>Note: these interpretations are not causal effects, just correlations. We also have not discussed if the actual estimates of <span class="math inline">\(\hat\beta_j\)</span> are reliable (which will be covered in the classical least squares theory below).</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Standardised Interpretations
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Sometimes, unit change is not very useful - as it depends on how the variable is measured. For example, what does a 5 unit change in democracy mean? Is that big, small? It is hard to tell.</p>
<p>Instead, we can look at the change in standard deviations. For a one standard deviation <span class="math inline">\(\sigma_X\)</span> increase in <span class="math inline">\(X_{ij}\)</span>, there is an expected <span class="math inline">\(\frac{\beta_j\sigma_X}{\sigma_Y}\)</span>-standard deviation change in <span class="math inline">\(Y_i\)</span>. The proof is provided below.</p>
<p><strong>Proof</strong>: For simplicity, let us use a simple linear regression <span class="math inline">\(\E(Y_i|X_i) = \beta_0 + \beta_1 X_i\)</span>:</p>
<p><span class="math display">\[
\begin{align}
&amp; \E \left(\frac{Y_i}{\sigma_Y} | X_i = x + \sigma_X \right ) - \E \left(\frac{Y_i}{\sigma_Y} | X_i = x \right ) \\
&amp; = \frac{\E(Y_i|X_i = x+ \sigma_X)}{\sigma_Y} - \frac{\E(Y_i|X_i = x)}{\sigma_Y} &amp;&amp;\text{(property of expectation)} \\
&amp; = \frac{\E(Y_i|X_i = x+ \sigma_X) - \E(Y_i|X_i = x)}{\sigma_Y} &amp;&amp; \text{(combine into 1 fraction)}\\
&amp; = \frac{\beta_0 + \beta_1(x+\sigma_X) - [\beta_0 + \beta_1(x)]}{\sigma_Y} &amp;&amp; \text{(plug in regression models)}\\
&amp; = \frac{\beta_1\sigma_X}{\sigma_Y} &amp;&amp; \text{(cancel and simplify)}
\end{align}
\]</span></p>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="goodness-of-fit-with-r-squared" class="level3">
<h3 class="anchored" data-anchor-id="goodness-of-fit-with-r-squared">Goodness of Fit with R-Squared</h3>
<p>Recall our fitted values equation, shown <a href="#orthogonal-projection-of-ols">previously</a>, can be rewritten with the projection matrix <span class="math inline">\(P\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\hat y = X(X^\top X)^{-1} X^\top y \ = \ \color{blue}{P}\color{black}y &amp;&amp; (\because \color{blue}{P := X(X^\top X)^{-1} X^\top})
\end{align}
\]</span></p>
<p>We are interested in is how well our model <span class="math inline">\(Py\)</span> explains the actual <span class="math inline">\(y\)</span>. The <a href="./math.html#vector-algebra">scalar product</a> <span class="math inline">\(y^\top Py\)</span> describes the shadow the actual <span class="math inline">\(y\)</span> casts on our projected model. We can divide it by <span class="math inline">\(y^\top y\)</span>, which is the “maximum” shadow possible (perfect shadow) to create a value between 0 and 1. This ratio is <span class="math inline">\(R^2\)</span>.</p>
<p><span class="math display">\[
R^2 = \frac{y^\top Py}{y^\top y}
\]</span></p>
<p>The total amount of variation in <span class="math inline">\(y\)</span> is called the total sum of squares (SST). The part of <span class="math inline">\(y\)</span> we cannot explain is the SSR that we minimised for OLS esimtation. That implies that the remaining part SST-SSR is the variation in <span class="math inline">\(y\)</span> that our model explains, called the sum of explained squares (SSE). <span class="math inline">\(R^2\)</span> can be though of the ratio of explained variation in <span class="math inline">\(y\)</span> by our model to the total variation in <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
R^2 = \frac{SSE}{SST} = \frac{SST - SSR}{SST} = 1 - \frac{SSR}{SST} = 1 - \frac{\sum (Y_i - \hat Y_i)^2}{\sum(Y_i - \bar Y)^2}
\]</span></p>
<p>R-Squared (<span class="math inline">\(R^2\)</span>) measures the proportion of variation in <span class="math inline">\(y\)</span> that is explained by our explanatory variables. R-Squared is always between 0 and 1 (0%-100%). Higher values indicate our model better explains the variation in <span class="math inline">\(y\)</span>.</p>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="classical-least-squares-theory" class="level1">
<h1><strong>Classical Least Squares Theory</strong></h1>
<section id="the-classical-assumptions" class="level3">
<h3 class="anchored" data-anchor-id="the-classical-assumptions">The Classical Assumptions</h3>
<p>1) <strong>Linearity in Parameters</strong>. This means that the linear model must be able to be written in the form <span class="math inline">\(y = X\beta + \eps\)</span>. This <u>does not mean the best-fit line must be linear</u> (as we will explore later).</p>
<p>2) <strong>Independent and Identically Distributed (i.i.d.)</strong>. Essentially, this means any two observations <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are sampled from the same random variable distribution with the same probabilities.</p>
<p>3) <strong>No Perfect Multicolinearity</strong>. This means that no explanatory variables <span class="math inline">\(X_{i1}, \dots, X_{ip}\)</span> can be written as an exact linear combination of other explanatory variables in the model. This is needed for the OLS formula.</p>
<p>4) <strong>Zero Conditional Mean</strong> <span class="math inline">\(\E(\eps|X) = 0\)</span>. This can be broken down into two parts. First, <span class="math inline">\(\E(\eps) = 0\)</span>: This is always met since if it is not 0, you can adjust <span class="math inline">\(\beta_0\)</span> until it is 0. More importantly, the assumption of <strong>exogeneity</strong> <span class="math inline">\(\E(X^\top \eps) = 0\)</span>. This means that all regressors <span class="math inline">\(X_{ij}\)</span>, and any combination of regressors, should be uncorrelated with the error term <span class="math inline">\(\eps\)</span>.</p>
<p>5) <strong>Spherical Errors</strong>. This is an assumption made on the variance-covariance matrix of the error terms <span class="math inline">\(\eps_i\)</span>:</p>
<p><span id="eq-homo"><span class="math display">\[
\underbrace{\V(\eps|X)}_{\mathrm{cov. \ matrix}} = \begin{pmatrix}
\V\eps_1 &amp; cov(\eps_1, \eps_2) &amp; cov(\eps_1, \eps_3) &amp; \dots \\
cov(\eps_2, \eps_1) &amp; \V\eps_2 &amp; cov(\eps_2, \eps_3) &amp; \dots \\
cov(\eps_3, \eps_1) &amp; cov(\eps_3, \eps_2) &amp; \V\eps_3 &amp; \vdots \\
\vdots &amp; \vdots &amp; \dots &amp; \ddots
\end{pmatrix} = \sigma^2 I_n = \begin{pmatrix}
\sigma^2 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \sigma^2 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; \dots &amp; \sigma^2
\end{pmatrix}
\tag{2}\]</span></span></p>
<p>This assumption can also be broken into two parts. <strong>No</strong> <strong>Autocorrelation</strong> is the assumption that all error terms are uncorrelated <span class="math inline">\(cov(\eps_i, \eps_j) = 0\)</span>, and is directly a result of the earlier i.d.d. assumption. The second part is <strong>Homoscedasticity</strong>, which says that all the variances of each error term <span class="math inline">\(\V(\eps_i)\)</span> is constant at some value <span class="math inline">\(\sigma^2\)</span>, which implies <span class="math inline">\(X\)</span> has no impact on the variance of <span class="math inline">\(\eps_i\)</span>.</p>
<p>If homoscedasticity is violated but ‘no autocorrleation’ is still met, we have heteroscedasticity. We will discuss heteroscedasticity later below, and we will not worry too much about autocorrelation here.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Visualising Homoscedasticity
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>An easy way to identify homoscedasticity is to look at a residual plot (just the plot of all <span class="math inline">\(\hat\eps_i\)</span>):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-1713529842.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></p>
</figure>
</div>
<p>Notice how the homoscedasticity residuals seem to have the same up-down variance, no matter the value of <span class="math inline">\(X_i\)</span>.</p>
<p>The heteroscedasticity (when homoscedasticity is violated) residuals have a clear pattern - the up-down variance is smaller when <span class="math inline">\(X_i\)</span> is smaller, and the up-down variance is larger when <span class="math inline">\(X_i\)</span> is larger. Essentially, if you see a pattern in the residual plot, it is likely heteroscedasticity.</p>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="ols-as-an-unbiased-estimator" class="level3">
<h3 class="anchored" data-anchor-id="ols-as-an-unbiased-estimator">Unbiasedness of OLS</h3>
<p>OLS is an <a href="./stats.html#finite-sample-properties-of-estimators">unbiased estimator</a> of the relationship between any <span class="math inline">\(X_{ij}\)</span> and <span class="math inline">\(Y_i\)</span> under the first 4 <a href="#the-classical-assumptions">classical assumptions</a>: linearity, i.i.d., no perfect multicollinearity, and zero-conditional mean.</p>
<p>Let us prove OLS is unbiased - i.e.&nbsp;<span class="math inline">\(\E\hat\beta = \beta\)</span> under the 4 classical assumptions. Let us manipulate our OLS solution:</p>
<p><span id="eq-simplify"><span class="math display">\[
\begin{align}
\hat\beta &amp; = (X^\top X)^{-1} X^\top y \\
&amp; = (X^\top X)^{-1} X^\top(\color{blue}{X\beta + \eps}\color{black}) &amp;&amp; (\because \color{blue}{y = X\beta + \eps}\color{black}) \\
&amp; = \underbrace{(X^\top X)^{-1} X^\top X}_{= \ I}\beta + (X^\top X)^{-1}X^\top \eps &amp;&amp; \text{(multiply out)} \\
&amp; = \beta + (X^\top X)^{-1}X^\top \eps
\end{align}
\tag{3}\]</span></span></p>
<p>Now, let us take the expectation of <span class="math inline">\(\hat\beta\)</span> conditional on <span class="math inline">\(X\)</span>. Remember condition 4, <span class="math inline">\(\E(\eps | X) = 0\)</span>:</p>
<p><span class="math display">\[
\E(\hat\beta | X) = \beta + (X^\top X)^{-1} \underbrace{\E(\eps | X)}_{= \ 0} \  = \ \beta
\]</span></p>
<p>Now, we can use the <a href="./stats.html#expectation-and-variance">law of iterated expectations (LIE)</a> to conclude this proof:</p>
<p><span class="math display">\[
\begin{align}
\E \hat\beta &amp; = \E(\E(\hat\beta|X)) &amp;&amp; (\because \mathrm{LIE}) \\
&amp; = \E(\color{blue}{\beta}\color{black}) &amp;&amp; (\because \color{blue}{\E(\hat\beta | X = \beta)}\color{black}) \\
&amp; = \beta &amp;&amp; \text{(expectation of a constant)}
\end{align}
\]</span></p>
<p>Thus, OLS is unbiased under the 4 conditions above. This is extremely desirable, as in causal inference (which is a major goal in the social sciences), we will want unbiased estimators that can accurately find the causal effects of one variable on another. In fact, most of the methods we cover in later chapters will be about trying to satisfy these conditions.</p>
<p><br></p>
</section>
<section id="variance-of-the-ols-estimator" class="level3">
<h3 class="anchored" data-anchor-id="variance-of-the-ols-estimator">Variance of the OLS Estimator</h3>
<p>We want to find the variance of our estimator’s sampling distribution, <span class="math inline">\(\V(\hat\beta | X)\)</span>, under all 5 of the <a href="#the-classical-assumptions">classical assumptions</a>. First, let us start off where we left off in <a href="#eq-simplify" class="quarto-xref">Equation&nbsp;3</a>:</p>
<p><span class="math display">\[
\begin{align}
&amp; \hat\beta = \beta + (X^\top X)^{-1} X^\top \eps \\
&amp; \V(\hat\beta | X) = \V(\beta + (X^\top X)^{-1} X^\top \eps)
\end{align}
\]</span></p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lemma: Property of Variance
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Lemma</strong>: If <span class="math inline">\(\eps\)</span> is an <span class="math inline">\(n\)</span> dimensional vector of random variables, <span class="math inline">\(c\)</span> is an <span class="math inline">\(m\)</span> dimensional vector, and <span class="math inline">\(B\)</span> is an <span class="math inline">\(n \times m\)</span> dimensional matrix with fixed constants, then the following is true (I will not prove this lemma here, but it is provable):</p>
<p><span id="eq-lemma"><span class="math display">\[
\V(c + B\eps) =  B \V(\eps) B^\top
\tag{4}\]</span></span></p>
</div>
</div>
</div>
<p><span class="math inline">\(\beta\)</span> is a vector of fixed constants. <span class="math inline">\((X^\top X)^{-1} X^\top \eps\)</span> can be imagined as a matrix of fixed constants, since we are conditioning the variance on <span class="math inline">\(X\)</span> (so for each <span class="math inline">\(X\)</span>, it is fixed). With the Lemma above:</p>
<p><span id="eq-var"><span class="math display">\[
\begin{align}
\V (\hat\beta | X) &amp; = (X^\top X)^{-1}X^\top \V(\eps|X) [(X^\top X)^{-1}X^\top]^{-1} &amp;&amp; \text{(lemma)} \\
&amp; = (X^\top X)^{-1}X^\top \V(\eps|X) \color{blue}{X(X^\top X)^{-1}} &amp;&amp; \color{black}(\because \color{blue}{[(X^\top X)^{-1}X^\top]^{-1} = X(X^\top X)^{-1}} \color{black})
\end{align}
\tag{5}\]</span></span></p>
<p>Now, assuming spherical errors in <a href="#eq-homo" class="quarto-xref">Equation&nbsp;2</a>, we can conclude the derivation.</p>
<p><span class="math display">\[
\begin{align}
\V (\hat\beta | X) &amp; = (X^\top X)^{-1}X^\top \color{blue}{\sigma^2I_n}\color{black}{X} (X^\top X)^{-1} &amp;&amp; (\because \color{blue}{\V(\eps|X) = \sigma^2 I_n}\color{black}) \\
&amp; = \color{blue}{\sigma^2}\color{black}{\underbrace{(X^\top X)^{-1}X^\top X}_{= \ I}(X^\top X)^{-1}} &amp;&amp; \text{(rearrange and simplify)} \\
&amp; = \sigma^2 (X^\top X)^{-1}
\end{align}
\]</span></p>
<p>This is the variance of the sampling distribution of <span class="math inline">\(\hat\beta\)</span>. We will use this for the Gauss-Markov theorem (below), and also statistical inference later.</p>
<p><br></p>
</section>
<section id="gauss-markov-theorem" class="level3">
<h3 class="anchored" data-anchor-id="gauss-markov-theorem">Gauss-Markov Theorem</h3>
<p>The Gauss-Markov Theorem states that if all 5 <a href="#the-classical-assumptions">classical assumptions</a> are met, the OLS estimator is the <strong>best linear unbiased estimator</strong> (BLUE) - the unbiased linear estimator with the lowest variance. Any linear estimator takes the form <span class="math inline">\(\tilde{\beta} = Cy\)</span>, including OLS. For any linear estimator <span class="math inline">\(\tilde{\beta} = Cy\)</span> to be unbiased, we need to assume <span class="math inline">\(\color{red}{CX = I}\)</span>.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof <span class="math inline">\(CX = I\)</span> For a Unbiased Linear Estimator
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For any linear estimator <span class="math inline">\(\tilde{\beta} = Cy\)</span> to be unbiased, we need to assume <span class="math inline">\(\color{red}{CX = I}\)</span>. The proof of this is as follows:</p>
<p><span class="math display">\[
\begin{align}
\tilde\beta =  C &amp; (\color{blue}{C\beta + \eps}\color{black}) &amp;&amp; (\because \color{blue}{y = X\beta + \eps}\color{black}) \\
=  C &amp; X\beta + C\eps &amp;&amp; \text{(multiply out)} \\
\E(\tilde\beta | X) &amp; = \E(C X\beta + C\eps) \\
&amp; = CX\beta + C \underbrace{\E(\eps | X)}_{= \ 0} &amp;&amp; \text{(take constants out of exp.)} \\
&amp; = CX\beta \\
&amp; = \color{red}{I}\color{black}\beta = \beta &amp;&amp; (\because \color{red}{CX = I}\color{black}) \\
\E \tilde\beta &amp; = \E( \E(\tilde\beta|X)) &amp;&amp; \text{(law of iterated expect.)} \\
&amp; = \E(\color{blue}{\beta}\color{black}) &amp;&amp; (\because \color{blue}{\E(\tilde\beta|X) = \beta}\color{black}) \\
&amp; = \beta &amp;&amp; \text{(expect. of a constant)}
\end{align}
\]</span></p>
<p>Thus, we have shown <span class="math inline">\(\color{red}{CX = I}\)</span> is a necessary condition for any linear estimator <span class="math inline">\(\tilde{\beta} = Cy\)</span> to be unbiased.</p>
</div>
</div>
</div>
<p>Now, let us calculate the variance of <span class="math inline">\(\tilde{\beta}\)</span>, taking into consideration the lemma (<a href="#eq-lemma" class="quarto-xref">Equation&nbsp;4</a>) used in the OLS variance:</p>
<p><span class="math display">\[
\begin{align}
\V(\tilde\beta | X) &amp; = \V(Cy|X) \\
&amp; = \V(C(\color{blue}{X\beta + \eps}\color{black})|X) &amp;&amp; (\because \color{blue}{y = X\beta + \eps}\color{black}) \\
&amp; = \V(\underbrace{CX}_{= I}\beta + C\eps | X) &amp;&amp; \text{(multiply out)} \\
&amp; = \V(\beta + C\eps | X) \\
&amp; = C \V(\eps | X) C^\top &amp;&amp; \text{(using lemma)} \\
&amp; = C \color{blue}{\sigma^2 I_n} \color{black} C^\top &amp;&amp; (\mathrm{homoscedasticity} \ \color{blue}{\V(\eps|X) = \sigma^2 I_n}\color{black}) \\
&amp; = \sigma^2 CC^\top &amp;&amp; \text{(rearrange and simplify)}
\end{align}
\]</span></p>
<p>Now, we want to show that the variance of the OLS estimator <span class="math inline">\(\hat{\beta}\)</span> (under homoscedasticity) is smaller than any linear estimator <span class="math inline">\(\tilde{\beta}\)</span>. Let us find the difference between the variances of estimator <span class="math inline">\(\tilde{\beta}\)</span> and <span class="math inline">\(\hat{\beta}\)</span>. Note: since <span class="math inline">\(\color{red}{CX = I}\)</span>, the following is also true: <span class="math inline">\(\color{red}{ X^\top C^\top = (CX)^\top = I}\)</span>.</p>
<p><span class="math display">\[
\begin{align}
\V(\tilde\beta | X)  - \V(\hat\beta|X) &amp; = \sigma^2 CC^\top - \sigma^2 (X^\top X)^{-1} \\
&amp; = \sigma^2(CC^\top - (X^\top X)^{-1}) &amp;&amp; (\text{factor out }\sigma^2) \\
&amp; = \sigma^2(CC^\top - \color{red}{CX}\color{black}(X^\top X)^{-1} \color{red}{X^\top C^\top}\color{black}) &amp;&amp; (\because \color{red}{X^\top C^\top = CX = I}\color{black}) \\
&amp; = \sigma^2 C(I - X(X^\top X)^{-1} X^\top) C^\top &amp;&amp; (\text{factor out }C, C^\top) \\
&amp; = \sigma^2 C \color{blue}{M}\color{black}C^\top &amp;&amp; (\text{residual maker matrix } \color{blue}{M}\color{black})
\end{align}
\]</span></p>
<p>Since <span class="math inline">\(\sigma^2 CM C^\top\)</span> is positive semi-definite (I will not prove this, but it is provable with the properties of <span class="math inline">\(M\)</span> introduced earlier), we know that <span class="math inline">\(V(\tilde{\beta}| X) &gt; V(\hat{\beta}| X)\)</span>. Thus, OLS is BLUE under the Gauss-Markov Theorem.</p>
<p><br></p>
</section>
<section id="asymptotic-consistency-of-ols" class="level3">
<h3 class="anchored" data-anchor-id="asymptotic-consistency-of-ols">Asymptotic Consistency of OLS</h3>
<p>We know OLS is unbiased under the first 4 <a href="#the-classical-assumptions">classical assumptions</a>: linearity, i.i.d., no perfect multicollinearity, and zero-conditional mean. OLS is also an <a href="./stats.html#asymptotic-properties-of-estimators">asymptotically consistent estimator</a> of <span class="math inline">\(\beta_j\)</span> under the first 3 classical assumptions, and one weakened version of the zero-conditional mean.</p>
<p>For asymptotic consistency, we replace zero-conditional mean with <strong>zero-mean and exogeneity</strong>: <span class="math inline">\(\E(\eps_i) = 0\)</span>, and <span class="math inline">\(Cov(x_i, \eps_i) = 0\)</span>, which implies <span class="math inline">\(E(X_i \eps_i) = 0\)</span>. This means that no regressor should be correlated with <span class="math inline">\(\eps\)</span>. This is weaker than Zero-Conditional mean, since it means a function of regressors can be correlated with <span class="math inline">\(\eps_i\)</span>.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lemma: Vector Notation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The following statements are true (with <span class="math inline">\(x_i\)</span> being a vector and <span class="math inline">\(\eps_i\)</span> being a scalar):</p>
<p><span class="math display">\[
\begin{split}
&amp; X^\top X = \sum\limits_{i=1}^n x_i x_i^\top\\
&amp;  X^\top \mathbf \eps = \sum\limits_{i=1}^n x_i \eps_i
\end{split}
\]</span></p>
</div>
</div>
</div>
<p>Let us start of where we left of from <a href="#eq-simplify" class="quarto-xref">Equation&nbsp;3</a>. Using vector notation, we can simplify:</p>
<p><span class="math display">\[
\begin{align}
\hat\beta &amp; = \beta + (X^\top X)^{-1} X^\top \eps \\
&amp; = \beta \left( \sum\limits_{i=1}^n x_i x_i^\top \right)^{-1} \left( \sum\limits_{i=1}^n x_i \eps_i \right) &amp;&amp; \text{(vector notation)} \\
&amp; = \beta + \left( \frac{1}{n}\sum\limits_{i=1}^n x_i x_i^\top \right)^{-1} \left( \frac{1}{n} \sum\limits_{i=1}^n x_i \eps_i \right) &amp;&amp; (\left(\frac{1}{n}\right)^{-1}, \frac{1}{n} \text{ cancel out})
\end{align}
\]</span></p>
<p>Now, we apply probability limits to both sides, and then use the law of large numbers and zero-conditional mean and exogeniety condition to simplify:</p>
<p><span class="math display">\[
\begin{align}
\mathrm{plim}\hat\beta &amp; = \beta + \left( \mathrm{plim} \frac{1}{n}\sum\limits_{i=1}^n x_i x_i^\top \right)^{-1} \left( \mathrm{plim}\frac{1}{n} \sum\limits_{i=1}^n x_i \eps_i \right) \\
&amp; = \beta + (\E(x_i x_i^\top))^{-1} \underbrace{\E(x_i \eps_i)}_{= 0} = \beta &amp;&amp; \text{(law of large numbers)}
\end{align}
\]</span></p>
<p>Thus, OLS is asymptotically consistent under the 4 conditions above. Note that it is possible for OLS to be consistent but biased (if we only meet the weakened zero-mean and exogeneity condition, and not the full zero-conditional mean condition). However, asymptotic consistency is still valuable if we have large sample sizes.</p>
<p><br></p>
</section>
<section id="weakening-spherical-errors" class="level3">
<h3 class="anchored" data-anchor-id="weakening-spherical-errors">Robust Standard Errors</h3>
<p>So far, we have assumed that the <a href="#the-classical-assumptions">classical assumptions</a> are met. However, this is often not the case, especially with the final assumption: spherical errors (homoscedasticity + no autocorrelation).</p>
<p><strong>Heteroscedasticity</strong> is when homoscedasticity is violated - which implies each <span class="math inline">\(i\)</span> (based on its <span class="math inline">\(X\)</span> values) has its own error: <span class="math inline">\(\V(\eps_i|X) = \sigma_i^2\)</span>. We still assume no autocorrelation.</p>
<p><span class="math display">\[
\V(\eps| X) = \Omega = \begin{pmatrix}
\sigma^2_1 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \sigma^2_2 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; \dots &amp; \sigma^2_n
\end{pmatrix}
\]</span></p>
<p>What are the implications of heteroscedasticity? First, heteroscedasticity <u><strong>does not</strong> bias OLS</u>, as the unbiasedness proof only relies on the first 4 classical assumptions. Second, heteroscedasticity means OLS is <u>no longer BLUE</u> - i.e.&nbsp;there exists are more efficient unbiased linear estimator (the generalised least squares estimator, which we will not cover since it is rarely used).</p>
<p>Third, and most relevant to us, is that the OLS variance formula (and standard errors) are no longer valid. Instead, we have to use the Huber-White Standard Errors (also called <strong>robust standard errors</strong>). This is because if we recall from <a href="#eq-var" class="quarto-xref">Equation&nbsp;5</a>, we originally simplified this equation using the homoscedasticity assumption. Instead, for robust standard errors, we start from <a href="#eq-var" class="quarto-xref">Equation&nbsp;5</a>, and plug in our error covariance matrix <span class="math inline">\(\Omega\)</span> from above:</p>
<p><span class="math display">\[
\V(\hat{\beta}| X)  = (X^\top X)^{-1} X^\top \color{blue}{\begin{pmatrix}
\sigma^2_1 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \sigma^2_2 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; \dots &amp; \sigma^2_n
\end{pmatrix}}\color{black} X ( X^\top X)^{-1}
\]</span></p>
<p>The other assumption of spherical errors, autocorrelation, is a common problem in time-series and data that has spatial elements, and just like heteroscedasticity, this means that OLS is no longer BLUE (the generalised least squares will be BLUE), and that we will need special autocorrelation + heteroscedasticity (HAC) robust standard errors.</p>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="statistical-inference" class="level1">
<h1><strong>Statistical Inference</strong></h1>
<section id="normality-and-standard-errors" class="level3">
<h3 class="anchored" data-anchor-id="normality-and-standard-errors">Normality and Standard Errors</h3>
<p>We need to know the form of the <span class="math inline">\(\hat\beta\)</span> sampling distribution for statistical tests. If our sample size is sufficiently large, we can invoke the central limit theorem, which says that the sampling distribution of <span class="math inline">\(\hat\beta\)</span> is approximately normal if our sample size <span class="math inline">\(n\)</span> is large enough.</p>
<p>Or, we can impose a condition of normality of the error terms: <span class="math inline">\(\eps | X \sim \mathcal N(0, \sigma^2 I)\)</span>. This will ensure the sampling distribution of <span class="math inline">\(\hat\beta\)</span> is normally distributed. (Note, the mean 0 and variance <span class="math inline">\(\sigma^2 I\)</span> come from the <a href="#the-classical-assumptions">classical assumptions</a> of zero-conditional mean and spherical errors). However, this condition is very restrictive, so we typically rely on the reliable central limit theorem.</p>
<p>Now, we will need the standard error of the sampling distribution. We know the standard error is the square root of the variance of the sampling distribution, which we <a href="#variance-of-the-ols-estimator">derived</a> (assuming the classical assumptions are met) as:</p>
<p><span class="math display">\[
\V(\hat\beta | X) = \sigma^2 (X^\top X)^{-1}
\]</span></p>
<p>We do not know the value of <span class="math inline">\(\sigma^2\)</span> as it is a population parameter. So, we estimate it with an unbiased estimator <span class="math inline">\(s^2\)</span>:</p>
<p><span class="math display">\[
\sigma^2 \approx s^2 = \frac{\hat{\eps}^\top \hat{\eps}}{n-k-1}
\]</span></p>
<p>While this estimator is unbiased, the estimator <span class="math inline">\(s^2\)</span> has variance. The implication of this is that we can no longer use the standard normal distribution for our sampling distribution, and instead, use the t-distribution, which has fatter tails and a lower peak to account for this variance in <span class="math inline">\(s^2\)</span>.</p>
<p>If we believe <strong>heteroscedasticity is violated</strong> (which we assume by default), we should use the <a href="#weakening-spherical-errors">heteroscedasticity variance</a> of OLS:</p>
<p><span class="math display">\[
\V(\hat{\beta}| X)  = (X^\top X)^{-1} X^\top \Omega\color{black} X ( X^\top X)^{-1}
\]</span></p>
<p>The <span class="math inline">\(\sigma^2_i\)</span> in the <span class="math inline">\(\Omega\)</span> matrix can be estimated with <span class="math inline">\(s_i^2 = \hat\eps_i^2\)</span>. The robust standard errors can be derived by taking the square root. <u>We typically use robust-standard errors in causal inference by default, unless we can prove homoscedasticity.</u></p>
<p><br></p>
</section>
<section id="hypothesis-testing" class="level3">
<h3 class="anchored" data-anchor-id="hypothesis-testing">Hypothesis Testing</h3>
<p>With our (robust) standard errors, we can run <a href="./stats.html#intuition-of-hypothesis-testing">hypothesis tests</a> on our coefficients to see if we have a relationship between two variables <span class="math inline">\(X_{ij}\)</span> and <span class="math inline">\(Y_i\)</span>. Our typical hypotheses are:</p>
<ul>
<li><span class="math inline">\(H_0 : \beta_j = 0\)</span> (i.e.&nbsp;there is no relationship between <span class="math inline">\(X_{ij}\)</span> and <span class="math inline">\(Y_i\)</span>).</li>
<li><span class="math inline">\(H_1:\beta_j ≠ 0\)</span> (i.e.&nbsp;there is a relationship between <span class="math inline">\(X_{ij}\)</span> and <span class="math inline">\(Y_i\)</span>).</li>
</ul>
<p>First, we calculate the t-test statistic, where <span class="math inline">\(H_0\)</span> is the value set in the null hypothesis (typically 0):</p>
<p><span class="math display">\[
t = \frac{\hat\beta_j - H_0}{\widehat{se}(\hat\beta_j)}
\]</span></p>
<p>Now, we consult a t-distribution of <span class="math inline">\(n-k-1\)</span> degrees of freedom. We find points <span class="math inline">\(t\)</span> and <span class="math inline">\(-t\)</span> on our t-distribution, and highlight the areas under the curve further away from the 0 at these two points. In the figure below, <span class="math inline">\(t = 2.228\)</span>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-1533818238.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:45.0%"></p>
</figure>
</div>
<p>The area highlighted, divided by the entire area under the curve, is the p-value. The p-value we get is the probability of getting a test statistic equally or more extreme than the one we got, given the null hypothesis is true.</p>
<ul>
<li>If <span class="math inline">\(p&lt;0.05\)</span>, we believe the probability of a null hypothesis is low enough, such that we reject the null hypothesis (that there is no relationship between <span class="math inline">\(X_{ij}\)</span> and <span class="math inline">\(Y_i\)</span>), and conclude our alternate hypothesis (that there is a relationship between <span class="math inline">\(X_{ij}\)</span> and <span class="math inline">\(Y_i\)</span>).</li>
<li>If <span class="math inline">\(p &gt; 0.05\)</span>, we cannot reject the null hypothesis, and cannot reject there is no relationship between <span class="math inline">\(X_{ij}\)</span> and <span class="math inline">\(Y_i\)</span>.</li>
</ul>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Confidence Intervals
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The 95% <a href="./stats.html#confidence-intervals">confidence intervals</a> of coefficients have the following bounds:</p>
<p><span class="math display">\[
(\hat\beta_j - 1.96 \widehat{se}(\hat\beta_j), \ \ \hat\beta_j + 1.96 \widehat{se}(\hat\beta_j))
\]</span></p>
<ul>
<li>The 1.96 is an approximation assuming a normal distribution. The actual confidence intervals (calculated by computers) will use a t-distribution of <span class="math inline">\(n-k-1\)</span>, which will result in a slightly different multiplicative factor.</li>
</ul>
<p>The confidence interval means that under repeated sampling and estimating <span class="math inline">\(\hat\beta_j\)</span>, 95% of the confidence intervals that we construct will include the true <span class="math inline">\(\beta_j\)</span> value in the population.</p>
<p>If the confidence interval contains 0, we cannot conclude a relationship between <span class="math inline">\(X_{ij}\)</span> and <span class="math inline">\(Y_i\)</span>, as 0 is a plausible value of <span class="math inline">\(\beta_j\)</span>. These results will always match those of the t-test.</p>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="f-tests" class="level3">
<h3 class="anchored" data-anchor-id="f-tests">F-Tests</h3>
<p>F-tests are used to test more than one coefficient at a time. For example, you might want to test if two variables or coefficients are jointly significant. The utility of this will become more clear when we talk about categorical explanatory variables and polynomial transformations. Our hypotheses in a F-test will be:</p>
<ul>
<li><span class="math inline">\(M_0 : Y_i = \beta_0 + \sum\limits_{j=1}^g \beta_{j} X_{ij} + \eps_i\)</span> (the smaller null model with <span class="math inline">\(g\)</span> variables).</li>
<li><span class="math inline">\(M_a : Y_i = \beta_0 + \sum\limits_{j=1}^g \beta_{j} X_{ij} + \sum\limits_{j=g+1}^p \beta_{j} X_{ij} + \eps_i\)</span> (the bigger model with the original <span class="math inline">\(g\)</span> variables + additional variables up to <span class="math inline">\(p\)</span>).</li>
</ul>
<p>F-tests compare the <a href="#goodness-of-fit-with-r-squared">R-squared</a> of the two models through the F-statistic:</p>
<p><span class="math display">\[
F = \frac{(SSR_0 - SSR_a) / (k_a - k_0)}{SSR_a /(n - k_a - 1)}
\]</span></p>
<p>We then consult a F-distribution with <span class="math inline">\(k_a - k_0\)</span> and <span class="math inline">\(n-k_a - 1\)</span> degrees of freedom, obtaining a p-value (in the same way as the t-test). The p-value we get is the probability of getting a test statistic equally or more extreme than the one we got, given the null hypothesis is true.</p>
<ul>
<li>If <span class="math inline">\(p&lt;0.05\)</span>, the we believe the probability of the null hypothesis is low enough, such that we reject the null hypothesis (that <span class="math inline">\(M_0\)</span> is the better model), and conclude our alternate hypothesis (that <span class="math inline">\(M_a\)</span> is a better model). This also means the extra coefficients in <span class="math inline">\(M_a\)</span> are jointly statistically significant.</li>
<li>If <span class="math inline">\(p &gt; 0.05\)</span>, we cannot reject the null hypothesis, and cannot reject that <span class="math inline">\(M_0\)</span> is a better model. Thus, the extra coefficients in <span class="math inline">\(M_a\)</span> are jointly not statistically significant.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="model-specification-issues" class="level1">
<h1><strong>Model Specification Issues</strong></h1>
<section id="omitted-variable-bias" class="level3">
<h3 class="anchored" data-anchor-id="omitted-variable-bias">Omitted Variable Bias</h3>
<p>Suppose there is some variable <span class="math inline">\(Z_i\)</span> that we have not included in a “short” regression. The actual, “true” regression of the population, would include this confounder <span class="math inline">\(Z_i\)</span></p>
<p><span class="math display">\[
\underbrace{y = X\beta + \eps}_{\text{short regression}}
\qquad \underbrace{y = X\beta + z\delta + \eps}_{\text{true regression with z} }
\]</span></p>
<p>We can find the expected value of the <a href="#ordinary-least-squares-estimator">OLS estimate</a> of the “short regression” excluding confounder <span class="math inline">\(Z_i\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\hat\beta &amp; = (X^\top X)^{-1}X^\top y \\
&amp; = (X^\top X)^{-1}X^\top(\color{blue}{X\beta + z\delta + \eps}\color{black}) &amp;&amp; (\because \color{blue}{y = X\beta + z\delta + \eps}\color{black}) \\
&amp; = \underbrace{(X^\top X)^{-1}X^\top X}_{= \ I}\beta + (X^\top X)^{-1}X^\top z\delta + (X^\top X)^{-1} X^\top \eps &amp;&amp; \text{(multiply out)} \\
&amp; = \beta + (X^\top X)^{-1}X^\top z\delta + (X^\top X)^{-1} X^\top \eps \\
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
\E(\hat\beta | X, z) &amp; = \beta + (X^\top X)^{-1}X^\top z \delta + (X^\top X)^{-1} X^\top \underbrace{\E(\eps | X, z)}_{= 0} \\
&amp; = \beta + (X^\top X)^{-1}X^\top z \delta
\end{align}
\]</span></p>
<p>Now, imagine a regression of outcome variable being the confounder <span class="math inline">\(z\)</span>, on the explanatory variables <span class="math inline">\(X\)</span>, such that <span class="math inline">\(z = X\eta + u\)</span>. Our OLS estimate would of <span class="math inline">\(\eta\)</span> would be <span class="math inline">\(\hat\eta = (X^\top X)^{-1} X^\top z\)</span>. Now, we can plug <span class="math inline">\(\hat\eta\)</span> into our expected value of <span class="math inline">\(\hat\beta\)</span>. Assume our estimator <span class="math inline">\(\hat{\eta}\)</span> is unbiased:</p>
<p><span class="math display">\[
\begin{align}
\E(\hat\beta | X, z) &amp; = \beta + (X^\top X)^{-1}X^\top z \delta\\
&amp; = \beta + \color{blue}{\hat\eta}\color{black}\delta &amp;&amp; (\because \color{blue}{\hat\eta = (X^\top X)^{-1} X^\top z }\color{black}) \\
\E\hat\beta &amp; = \E(\E(\hat\beta|X, z)) &amp;&amp; \text{(law of iterated expect.)} \\
&amp; = E(\color{blue}{\beta + \hat\eta \delta}\color{black}) &amp;&amp; (\because \color{blue}{\E(\hat\beta|X, z) = \beta + \hat\eta\delta} \color{black}) \\
&amp; = \beta + \E\hat\eta \ \delta &amp;&amp; \text{(take out constants from exp.)} \\
&amp; = \beta + \eta\delta &amp;&amp; (\text{unbiased estimator } \E\hat\eta = \eta)
\end{align}
\]</span></p>
<p>Thus, we can see by not including confounder <span class="math inline">\(z\)</span> in our “short regression”, the estimator is now <strong>biased</strong> by <span class="math inline">\(\hat\eta \delta\)</span>. In later chapters when we start discussing causality, we will see omitted confounders as a huge issue in our estimation.</p>
<p><br></p>
</section>
<section id="heterogeneity-and-interactions" class="level3">
<h3 class="anchored" data-anchor-id="heterogeneity-and-interactions">Heterogeneity and Interactions</h3>
<p><strong>Heterogeneity</strong> is when we believe the magnitude of the relationship between <span class="math inline">\(X_{i1}\)</span> and <span class="math inline">\(Y_i\)</span> in the population is affected by another variable <span class="math inline">\(X_{i2}\)</span>, called the <strong>moderating</strong> variable. An <strong>interaction</strong> between <span class="math inline">\(X_{i1}\)</span> and the moderating variable <span class="math inline">\(X_{i2}\)</span> means they are multiplied in the regression equation:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \underbrace{\beta_3 X_{i1} X_{i2}}_{\text{interaction}}
\]</span></p>
<p>In an interaction, <span class="math inline">\(\hat\beta_0\)</span> is still the expected <span class="math inline">\(Y_i\)</span> when all explanatory variables equal 0. The other coefficient’s interpretations are:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 47%">
<col style="width: 43%">
</colgroup>
<tbody>
<tr class="odd">
<td></td>
<td><strong>Binary</strong> <span class="math inline">\(X_{i2}\)</span></td>
<td><strong>Continuous</strong> <span class="math inline">\(X_{i2}\)</span></td>
</tr>
<tr class="even">
<td><strong>Binary</strong> <span class="math inline">\(X_{i1}\)</span></td>
<td><p>When <span class="math inline">\(X_{i2} = 0\)</span>, the effect of <span class="math inline">\(X_{i1}\)</span> (going from 0 to 1) on <span class="math inline">\(Y_i\)</span> is <span class="math inline">\(\hat\beta_1\)</span>.</p>
<p>When <span class="math inline">\(X_{i2} = 1\)</span>, the effect of <span class="math inline">\(X_{i1}\)</span> (going from 0 to 1) on <span class="math inline">\(Y_i\)</span> is <span class="math inline">\(\hat\beta_1 + \hat\beta_3\)</span>.</p></td>
<td>The effect of <span class="math inline">\(X_{i1}\)</span> (going from 0 to 1) on <span class="math inline">\(Y_i\)</span> is <span class="math inline">\(\hat\beta_1 + \hat\beta_3 X_{i2}\)</span>.</td>
</tr>
<tr class="odd">
<td><strong>Continuous</strong> <span class="math inline">\(X_{i1}\)</span></td>
<td><p>When <span class="math inline">\(X_{i2} = 0\)</span>, for every increase in one unit of <span class="math inline">\(X_{i1}\)</span>, there is an expected <span class="math inline">\(\widehat{\beta_1}\)</span> unit change in <span class="math inline">\(Y_i\)</span>.</p>
<p>When <span class="math inline">\(X_{i2} = 1\)</span>, for every increase in one unit of <span class="math inline">\(X_{i1}\)</span>, there is an expected <span class="math inline">\(\hat\beta_1+ \hat\beta_3\)</span> change in <span class="math inline">\(Y_i\)</span>.</p></td>
<td>For every increase of one unit in <span class="math inline">\(X_{i1}\)</span>, there is an expected <span class="math inline">\(\hat\beta_1 + \hat\beta_3 X_{i2}\)</span> change in <span class="math inline">\(Y_i\)</span>.</td>
</tr>
</tbody>
</table>
<p>The hypothesis test for <span class="math inline">\(\hat\beta_3\)</span> tests the null hypothesis that there is no interaction in the population between <span class="math inline">\(X_{i1}\)</span> and <span class="math inline">\(X_{i2}\)</span>. If our coefficient <span class="math inline">\(\hat\beta_3\)</span> is statistically significant, we can conclude that this null is wrong, and there indeed is an interaction. If <span class="math inline">\(\hat\beta_3\)</span> is insignificant, we fail to reject the null, and can drop the interaction effect from our regression.</p>
<p><br></p>
</section>
<section id="polynomial-transformations" class="level3">
<h3 class="anchored" data-anchor-id="polynomial-transformations">Polynomial Transformations</h3>
<p>Sometimes the relationship between two variables in the population is not a straight linear line. We want to accurately reflect this non-linear relationship in our regression to mantain accurate <span class="math inline">\(\hat\beta\)</span> estimates. The most common form of <strong>polynomial transformation</strong> is the quadratic transformation:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1X_i + \beta_2 X_i^2 + \eps_i
\]</span></p>
<p>Note that while <span class="math inline">\(X_i\)</span> is non-linear, the actual regression is still linear in <u>parameters</u>. We can see this because it can still be written as <span class="math inline">\(y = X\beta + \eps\)</span> when you consider <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_i^2\)</span> to be two different explanatory variables.</p>
<p>Our estimated <span class="math inline">\(\hat\beta_0\)</span> remains the expected value of <span class="math inline">\(Y_i\)</span> when all explanatory variables equal 0. Unfortunately, the <span class="math inline">\(\hat\beta_1\)</span> and <span class="math inline">\(\hat\beta_2\)</span> coefficients are not directly interpretable.</p>
<ul>
<li><span class="math inline">\(\hat\beta_2\)</span>’s sign can tell us if the best-fit parabola opens upward or downward.</li>
<li>The significance of <span class="math inline">\(\hat\beta_2\)</span> also indicates if the quadratic term is statistically significant. If it is not, we can remove the transformation.</li>
</ul>
<p>We can interpret two things about the quadratic transformation:</p>
<ul>
<li>For every one unit increase in <span class="math inline">\(X_i\)</span>, there is an expected <span class="math inline">\(\hat\beta_1 + 2 \hat\beta_2X_i\)</span> unit increase in <span class="math inline">\(Y_i\)</span>.</li>
<li>The minimum/maximum point in the best-fit parabola occurs at <span class="math inline">\(X_i = - \hat\beta_1/2 \hat\beta_2\)</span>.</li>
</ul>
<p><br></p>
</section>
<section id="logarithmic-transformations" class="level3">
<h3 class="anchored" data-anchor-id="logarithmic-transformations">Logarithmic Transformations</h3>
<p><strong>Logarithmic</strong> transformations are often used to change skewed variables into normally distributed variables. Often, skewed variables will violate homoscedasticity, which means the <a href="#gauss-markov-theorem">Gauss-Markov theorem</a> of OLS being BLUE no longer applies. By applying a logarithmic transformation, you can meet homoscedasticity and retain the BLUE properties of OLS. We have 3 types of logarithmic transformations:</p>
<ul>
<li>Linear-Log model: <span class="math inline">\(Y_i = \beta_0 + \beta_1 \log X_i + \eps_i\)</span>.</li>
<li>Log-Linear model: <span class="math inline">\(\log Y_i = \beta_0 + \beta_1 X_i + \eps_i\)</span>.</li>
<li>Log-Log model: <span class="math inline">\(\log Y_i = \beta_0 + \beta_1 \log X_i + \eps_i\)</span>.</li>
</ul>
<p>The coefficient interpretations become a little more complex with logarithms:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 46%">
<col style="width: 45%">
</colgroup>
<tbody>
<tr class="odd">
<td></td>
<td><span class="math inline">\(X_i\)</span></td>
<td><span class="math inline">\(\log (X_i)\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(Y_i\)</span></td>
<td><p>Linear Model:</p>
<p>When <span class="math inline">\(X_i\)</span> increases by one unit, there is an expected <span class="math inline">\(\hat\beta_1\)</span> unit change in <span class="math inline">\(Y_i\)</span>.</p></td>
<td><p>Linear-Log Model:</p>
<p>When <span class="math inline">\(x\)</span> increases by 10%, there is an expected <span class="math inline">\(0.096 \hat\beta_1\)</span> unit change in <span class="math inline">\(Y_i\)</span>.</p></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\log (Y_i)\)</span></td>
<td><p>Log-Linear Model:</p>
<p>For every one unit increase in <span class="math inline">\(X_i\)</span>, the expected <span class="math inline">\(Y_i\)</span> is multiplied by <span class="math inline">\(e^{\hat\beta_1}\)</span>.</p></td>
<td><p>Log-Log Model:</p>
<p>Multiplying <span class="math inline">\(X_i\)</span> by <span class="math inline">\(e\)</span> will multiply the expected value of <span class="math inline">\(Y_i\)</span> by <span class="math inline">\(e^{\hat\beta_1}\)</span>.x</p></td>
</tr>
</tbody>
</table>
<p><br></p>
</section>
<section id="categorical-explanatory-variables" class="level3">
<h3 class="anchored" data-anchor-id="categorical-explanatory-variables">Categorical Explanatory Variables</h3>
<p>Take an explanatory variable <span class="math inline">\(X_i\)</span>, which has <span class="math inline">\(g\)</span> number of categories <span class="math inline">\(1, \dots, g\)</span>. To include <span class="math inline">\(X_i\)</span> in our regression, we would create <span class="math inline">\(g-1\)</span> dummy (binary) variables, to create the following regression model:</p>
<p><span class="math display">\[
\E(Y_i|X_i) = \beta_0 + \sum\limits_{j=1}^{g-1} \beta_j X_{ij}
\]</span></p>
<ul>
<li>Categories <span class="math inline">\(1, \dots, g-1\)</span> get there own binary variable <span class="math inline">\(X_{i1}, \dots, X_{ig-1}\)</span>.</li>
<li>Category <span class="math inline">\(g\)</span> (the reference category) does not get its own variable. We can change which category we wish to be the reference.</li>
</ul>
<p>Interpretation is as follows (category <span class="math inline">\(j\)</span> is any one of category <span class="math inline">\(1, \dots, g-1\)</span>).</p>
<ul>
<li><span class="math inline">\(\beta_j\)</span> is the difference in expected <span class="math inline">\(Y_i\)</span> between category <span class="math inline">\(j\)</span> and the reference category <span class="math inline">\(g\)</span>.</li>
<li><span class="math inline">\(\beta_0\)</span> is the expected <span class="math inline">\(Y_i\)</span> of the reference category <span class="math inline">\(g\)</span>.</li>
<li>Thus, category <span class="math inline">\(j\)</span> has an expected <span class="math inline">\(Y_i\)</span> of <span class="math inline">\(\beta_0 + \beta_j\)</span>.</li>
</ul>
<p>To test the categorical variable’s statistical significance, we will need to test all the coefficients together. The most common way is to do it with a f-test of multiple coefficients.</p>
<p>One type of categorical explanatory variable is a fixed effect. This is done in panel or clustered data, where your categorical variable is the cluster variable. We will explore this more near the end of the course on differences-in-differences.</p>
<p><br></p>
</section>
<section id="linear-probability-model" class="level3">
<h3 class="anchored" data-anchor-id="linear-probability-model">Linear Probability Model</h3>
<p>The standard linear model assumes a continuous <span class="math inline">\(Y_i\)</span> variable. However, we can adapt the linear model to fit binary <span class="math inline">\(Y_i\)</span> variables. When <span class="math inline">\(Y_i\)</span> is binary and only has values <span class="math inline">\(Y_i \in \{0, 1\}\)</span>, our linear model is actually no longer a predictor of <span class="math inline">\(Y_i\)</span>, since our regression will output values that are not 0 and 1.</p>
<p>Instead, our linear model will now predict the <strong>probability</strong> of unit <span class="math inline">\(i\)</span> having <span class="math inline">\(Y_i = 1\)</span>. The is due to the <a href="#conditional-expectation-function">conditional expectation interpretation of regression</a>, and the expectation of the <a href="./stats.html#bernoulli-and-binomial-distribution">binomial distribution</a>:</p>
<p><span class="math display">\[
\begin{align}
\E(Y_i|X_i) &amp; = \underbrace{0 \times \P(Y_i = 0|X_i) \ + \ \P(Y_i = 1|X_i)}_{\text{a weighted avg. formula}} \\
&amp; = \P(Y_i=1|X_i)
\end{align}
\]</span></p>
<p>Thus, we can rewrite our linear model with the primary outcome being <span class="math inline">\(\P(Y_i = 1|X_i)\)</span>. This model is called the <strong>linear probability model</strong>:</p>
<p><span class="math display">\[
\P(Y_i = 1|X_i) = \beta_0 + \beta_1X_{i1} + \dots + \beta_pX_{ip} + \eps_i
\]</span></p>
<p>Our interpretations of coefficients also slightly change.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 46%">
<col style="width: 48%">
</colgroup>
<tbody>
<tr class="odd">
<td></td>
<td><strong>Continuous</strong> <span class="math inline">\(X_{ij}\)</span></td>
<td><strong>Binary</strong> <span class="math inline">\(X_{ij}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat\beta_j\)</span></td>
<td>For every one unit increase in <span class="math inline">\(X_{ij}\)</span>, there is an expected <span class="math inline">\(\hat\beta_j \times 100\)</span> percentage point change in the probability of a unit being in category <span class="math inline">\(Y_i=1\)</span>, holding all other explanatory variables constant.</td>
<td>There is a <span class="math inline">\(\hat\beta_j\times 100\)</span> percentage point difference in the probability of a unit being in category <span class="math inline">\(Y_i=1\)</span> between category <span class="math inline">\(X_{ij} = 1\)</span> and category <span class="math inline">\(X_{ij} = 0\)</span>, holding all other explanatory variables constant.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\widehat{\beta_0}\)</span></td>
<td>When all explanatory variables equal 0, the expected probability of a unit being in category <span class="math inline">\(Y_i=1\)</span> is <span class="math inline">\(\hat\beta_0 \times 100\)</span></td>
<td>For category <span class="math inline">\(X_{ij} = 0\)</span>, the expected probability of a unit being in category <span class="math inline">\(Y_i=1\)</span> is <span class="math inline">\(\hat\beta_j \times 100\)</span> (when all other explanatory variables equal 0).</td>
</tr>
</tbody>
</table>
<p>The main downside to the linear probability model relates to prediction - i.e.&nbsp;<span class="math inline">\(\P(Y-i = 1|X_i)\)</span> as a probability should be restricted between 0 and 1, however, the linear model does not guarantee this restriction. We will introduce methods to deal with this in the next chapter.</p>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="implementation-in-r" class="level1">
<h1><strong>Implementation in R</strong></h1>
<p>You will need package <em>fixest</em> and <em>estimatr</em>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(fixest)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(estimatr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Regression with normal standard errors can be done with the <em>lm()</em> function:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x3, <span class="at">data =</span> mydata)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Regression with robust standard errors can be done with the <em>feols()</em> function or <em>lm_robust()</em> function:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># feols</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">feols</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x3, <span class="at">data =</span> mydata, <span class="at">se =</span> <span class="st">"hetero"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># lm robust</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm_robust</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x3, <span class="at">data =</span> mydata)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Output will include coefficients, standard errors, p-values, and more.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Binary and Categorical Variables
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>You can include binary and categorical variables by using the <em>as.factor()</em> function:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">feols</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> <span class="fu">as.factor</span>(x2) <span class="sc">+</span> x3, <span class="at">data =</span> mydata, <span class="at">se =</span> <span class="st">"hetero"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You can do the same for <span class="math inline">\(y\)</span> or <span class="math inline">\(x\)</span>. Just remember, <span class="math inline">\(y\)</span> cannot be a categorical variable (use multinomial logsitic regression instead).</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Fixed Effects
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>You can include one-way fixed effects by adding a | after your regression formula in <em>feols()</em>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">feols</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x3 <span class="sc">|</span> cluster,</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> mydata, <span class="at">se =</span> <span class="st">"hetero"</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You can add two-way fixed effects as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">feols</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x3 <span class="sc">|</span> unit <span class="sc">+</span> year,</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> mydata, <span class="at">se =</span> <span class="st">"hetero"</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interaction Effects
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Two interact two variables, use * between them. This will automatically include both the interaction term, and the two variables by themselves.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">feols</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2<span class="sc">*</span>x3, <span class="at">data =</span> mydata, <span class="at">se =</span> <span class="st">"hetero"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If for some reason, you only want the interaction term, but not the variables by themselves, you can use a colon : between the two variables:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">feols</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2<span class="sc">:</span>x3, <span class="at">data =</span> mydata, <span class="at">se =</span> <span class="st">"hetero"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Polynomial Transformations
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>To conduct a polynomial transformation, you can use the <em>I()</em> function. The second argument is the degree of the polynomial:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">feols</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> <span class="fu">I</span>(x2, <span class="dv">3</span>), <span class="at">data =</span> mydata, <span class="at">se =</span> <span class="st">"hetero"</span>) <span class="co">#cubic for x2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Logarithmic Transformations
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The best way to do a logarithmic transformation is to create a new variable that is the log of the variable you want to transform using the <em>log()</em> function, before you even start the regression:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>mydata<span class="sc">$</span>x1_log <span class="ot">&lt;-</span> <span class="fu">log</span>(mydata<span class="sc">$</span>x1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-15-contents" aria-controls="callout-15" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Confidence Intervals
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-15" class="callout-15-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>To find the confidence intervals for coefficients, first estimate the model with <em>lm()</em> or <em>feols()</em> as shown previously, then use the <em>confint()</em> command:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-16-contents" aria-controls="callout-16" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
F-Tests
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-16" class="callout-16-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>To run a f-test, use the <em>anova()</em> command, and input your two different models, with the null model going first.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(model1, model2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note: F-tests only work with models that are run with homoscedastic standard errors. Robust standard errors will not work.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-17-contents" aria-controls="callout-17" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
LaTeX Regression Tables
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-17" class="callout-17-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>You can use the <em>texreg</em> package to make nice regression tables automatically.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(texreg)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The syntax for <em>texreg()</em> is as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">texreg</span>(<span class="at">l =</span> <span class="fu">list</span>(model1, model2, model3),</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>       <span class="at">custom.model.names =</span> <span class="fu">c</span>(<span class="st">"model 1"</span>, <span class="st">"model 2"</span>, <span class="st">"model 3"</span>),</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>       <span class="at">custom.coef.names =</span> <span class="fu">c</span>(<span class="st">"intercept"</span>, <span class="st">"x1"</span>, <span class="st">"x2"</span>),</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>       <span class="at">digits =</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You can replace <em>texreg()</em> with <em>screenreg()</em> if you want a nicer regression table in the R-console.</p>
<p>Note: you must have the same amount of model names as total models in your texreg, and you must have the same amount of coeficient names as the total amount of coefficients in all of your models.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-18-contents" aria-controls="callout-18" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prediction
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-18" class="callout-18-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We can use the <em>predict()</em> function to generate fitted value predictions in R:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>my_predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, <span class="at">newdata =</span> my_new_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><em>my_new_data</em> is a dataframe with a bunch of explanatory variable values (for every explanatory variable) for a collection of observations, that you wish to predict <span class="math inline">\(\hat y\)</span> for.</p>
</div>
</div>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>