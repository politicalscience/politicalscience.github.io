<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Least Squares Theory – Kevin's PSPE Resources</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./causal.html" rel="next">
<link href="./mle.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="mathjax-config.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./random.html">Theoretical Statistics</a></li><li class="breadcrumb-item"><a href="./ols.html"><span class="chapter-title">Least Squares Theory</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Kevin’s PSPE Resources</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Theoretical Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./random.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Random Variables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./inference.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Statistical Inference</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mle.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Maximum Likelihood</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ols.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Least Squares Theory</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./causal.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Theory of Causation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Applied Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./thecauses.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Applied Causal Inference</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Generalised Linear Model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./multivariate.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Multivariate Methods</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Game Theory</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./games.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Guide to Game Theory</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Mathematics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linear.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Linear Algebra</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./calc.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Calculus and Probability</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Changing Soon</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./identify.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Quasi-Experimental Methods</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul class="collapse">
  <li><a href="#the-classical-linear-model" id="toc-the-classical-linear-model" class="nav-link active" data-scroll-target="#the-classical-linear-model">The Classical Linear Model</a></li>
  <li><a href="#ordinary-least-squares" id="toc-ordinary-least-squares" class="nav-link" data-scroll-target="#ordinary-least-squares">Ordinary Least Squares</a></li>
  <li><a href="#ols-estimator-properties" id="toc-ols-estimator-properties" class="nav-link" data-scroll-target="#ols-estimator-properties">OLS Estimator Properties</a></li>
  <li><a href="#gauss-markov-theorem" id="toc-gauss-markov-theorem" class="nav-link" data-scroll-target="#gauss-markov-theorem">Gauss-Markov Theorem</a></li>
  <li><a href="#ols-and-non-spherical-errors" id="toc-ols-and-non-spherical-errors" class="nav-link" data-scroll-target="#ols-and-non-spherical-errors">OLS and Non-Spherical Errors</a></li>
  <li><a href="#generalised-least-squares" id="toc-generalised-least-squares" class="nav-link" data-scroll-target="#generalised-least-squares">Generalised Least Squares</a></li>
  <li><a href="#feasible-generalised-least-squares" id="toc-feasible-generalised-least-squares" class="nav-link" data-scroll-target="#feasible-generalised-least-squares">Feasible Generalised Least Squares</a></li>
  <li><a href="#instrumental-variables-estimator" id="toc-instrumental-variables-estimator" class="nav-link" data-scroll-target="#instrumental-variables-estimator">Instrumental Variables Estimator</a></li>
  <li><a href="#statistical-inference" id="toc-statistical-inference" class="nav-link" data-scroll-target="#statistical-inference">Statistical Inference</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./random.html">Theoretical Statistics</a></li><li class="breadcrumb-item"><a href="./ols.html"><span class="chapter-title">Least Squares Theory</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Least Squares Theory</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In the previous chapter, we talked about the Maximum Likelihood Estimator. However, MLE can be computationally difficult to calculate, often without analytical solutions, requiring iterative algorithms.</p>
<p>In this chapter, we introduce a series of “Least-Squares” estimators, that for a specific type of model (linear model), have fixed solutions derivable through minimisation. First, we introduce the classical linear model and its assumptions. Then, we discuss the most common least-square estimator: ordinary least squares (OLS). Finally, we discuss extensions to OLS (generalised least squares, instrumental variables) for when the assumptions needed for OLS are violated.</p>
<p><br></p>
<section id="the-classical-linear-model" class="level2">
<h2 class="anchored" data-anchor-id="the-classical-linear-model">The Classical Linear Model</h2>
<p>A class of estimators called <strong>least squares</strong> estimators is another way to estimate population parameters <span class="math inline">\(\theta\)</span> besides MLE. However, this class of estimators can only be applied in one specific model: the classical linear model.</p>
<p>Let us say there are individuals <span class="math inline">\(t = 1, 2, \dots, n\)</span> in the population. Each individual’s <span class="math inline">\(Y\)</span> value is determined by a set of random variables <span class="math inline">\(Y_1, \dots, Y_n\)</span>. Any individual random variable <span class="math inline">\(Y_t\)</span> is data generating process defined as <span class="math inline">\(Y_t \sim \set D(\mu_Y, \sigma^2_Y)\)</span>, where <span class="math inline">\(\set D\)</span> represents any distributional form, <span class="math inline">\(\mu_Y\)</span> is the mean of the random variable, and <span class="math inline">\(\sigma^2_Y\)</span> is the variance of the random variable.</p>
<p>The classical linear model is a specification that the mean <span class="math inline">\(\mu_Y\)</span> of <span class="math inline">\(Y_t\)</span> is linearly determined by a set of explanatory variables <span class="math inline">\(\set X = \{X_1, \dots, X_p\}\)</span>:</p>
<p><span class="math display">\[
\E(Y_t|\set X_t) = \beta_0 + \beta_1 X_{1t} + \beta_2 X_{2t} + \dots + \beta_p X_{pt}
\]</span></p>
<p>Where <span class="math inline">\(\beta_0, \dots, \beta_p\)</span> are a set of population parameters (that need to be estimated) that determine how <span class="math inline">\(\mu_Y\)</span> changes in respect to explanatory variables <span class="math inline">\(\set X\)</span>.</p>
<p>You will frequently see the linear model represented in another form:</p>
<p><span class="math display">\[
Y_t = \underbrace{\beta_0 + \beta_1 X_{1t} + \dots + \beta_p X_{pt}}_{\E(Y_t | \set X_t)} + \eps_t, \quad \eps_t \sim \set D(\mu_{\eps_t} = 0, \sigma^2_{\eps_t})
\]</span></p>
<p>Where the <strong>error term</strong> <span class="math inline">\(\eps_t\)</span> represents the variance/randomness in our data generating process, and the rest of the model represents <span class="math inline">\(\mu_y\)</span>.</p>
<p>We know that this data generating process applies for random variables <span class="math inline">\(Y_1, Y_2, \dots, Y_n\)</span>. To represent all random variables together, we use the matrix representation of the linear model:</p>
<p><span class="math display">\[
\b y = \b{X\beta} + \b\eps \iff \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} =
\begin{pmatrix} 1 &amp; x_{11} &amp; \dots &amp; x_{1p} \\
1 &amp; x_{21} &amp; \dots &amp; x_{2p} \\
\vdots &amp; \vdots &amp; \dots &amp; \vdots \\
1 &amp; x_{n1} &amp; \dots &amp; x_{np}\end{pmatrix}
\begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p\end{pmatrix} +
\begin{pmatrix} \eps_1 \\ \eps_2 \\ \vdots \\ \eps_n \end{pmatrix}
\]</span></p>
<p>The <strong>classical linear model</strong> has a set of <strong>five</strong> <strong>assumptions</strong>:</p>
<div class="tabset-margin-container"></div><div id="assumptions" class="panel-tabset nav-pills">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true" aria-current="page">Linearity</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">i.i.d.</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" role="tab" aria-controls="tabset-1-3" aria-selected="false">Linear Independence</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-4" role="tab" aria-controls="tabset-1-4" aria-selected="false">Exogeneity</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-5-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-5" role="tab" aria-controls="tabset-1-5" aria-selected="false">Spherical Errors</a></li></ul>
<div id="assumptions" class="tab-content nav-pills">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div id="def-linearity" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.1 (Linearity in Parameters)</strong></span> A model is linear in parameters if it can be written in the form <span class="math inline">\(\b y = \b{X\beta} + \b \eps\)</span>. Or in other words, all parameters <span class="math inline">\(\beta_0, \dots, \beta_p\)</span> must be either added or subtracted from each other.</p>
</div>
<p>It is important to note that linearity in parameters is not equal to linearity in the classical sense. For example, the model</p>
<p><span class="math display">\[
Y_t = \beta_0 + \beta_1X_t + \beta_2 X_t^2 + \beta_3X_t^3 + \eps_t
\]</span></p>
<p>would be considered linear in parameters because all parameters <span class="math inline">\(\beta_0, \dots, \beta_p\)</span> are added together, not multiplied. It does not matter if the <span class="math inline">\(X\)</span> are linear or not.</p>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div id="def-iid" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.2 (Independent and Identically Distributed)</strong></span> A series of random variables <span class="math inline">\(Y_1, Y_2, \dots, Y_n\)</span> is considered to be independent and identically distributed if each random variable is independent (<a href="random.html#def-independence" class="quarto-xref">definition&nbsp;<span>1.6</span></a>), and each random variable has the exact same distribution.</p>
</div>
<p>This assumption is often not met in its purest form - since when we randomly sample <span class="math inline">\(Y_t\)</span> without replacement, technically the next observation <span class="math inline">\(Y_{t+1}\)</span> will have slightly different distribution, since we took one observation out already.</p>
<p>This assumption is also frequently violated in time-series data, where <span class="math inline">\(Y_t\)</span> at time <span class="math inline">\(t\)</span> is likely to have an effect on <span class="math inline">\(Y_{t+1}\)</span> at time <span class="math inline">\(t+1\)</span>.</p>
</div>
<div id="tabset-1-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-3-tab">
<div id="def-linearindependence" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.3 (Linear Independence)</strong></span> Also called non-perfect multicollinearity, this assumption means that no explanatory variables <span class="math inline">\(X_1, \dots, X_p\)</span> can be perfectly correlated with any other explanatory variable, or perfectly correlated with any linear combination of other explanatory variables.</p>
</div>
<p>This assumption is required for estimation to be possible with the ordinary least squares estimator, as it requires matrix <span class="math inline">\(\b X\)</span> to be full rank which allows <span class="math inline">\(\b X^\top \b X\)</span> to be invertable.</p>
</div>
<div id="tabset-1-4" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-4-tab">
<div id="def-strictexog" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.4 (Strict Exogeneity)</strong></span> Strict Exogeneity is <span class="math inline">\(\E(\b\eps | \b X) = 0\)</span>. This implies that <span class="math inline">\(\E(\b X^\top \b\eps) = 0\)</span>, which means all regressors <span class="math inline">\(X_1, \dots, X_p\)</span> should be uncorrelated with the error terms <span class="math inline">\(\eps\)</span>, and any linear combination of <span class="math inline">\(X_1, \dots, X_p\)</span> should be uncorrelated with the error term.</p>
</div>
<p>There is also a weaker form of exogeneity, called weak exogeneity:</p>
<div id="def-weakexog" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.5 (Weak Exogeneity)</strong></span> Weak exogeneity is defined as <span class="math inline">\(\E(\b x_t \eps_t) = 0\)</span>. Weak exogeneity only requires that regressors <span class="math inline">\(X_1, \dots, X_p\)</span> individually are uncorrelated with the error term. Weak exogeneity allows for combinations of <span class="math inline">\(X_1, \dots, X_p\)</span> to to be correlated with <span class="math inline">\(\eps\)</span>, which is not allowed under strict exogeneity.</p>
</div>
</div>
<div id="tabset-1-5" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-5-tab">
<div id="def-sphericalerrors" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.6 (Spherical Errors)</strong></span> The spherical errors assumption states that the covariance matrix of errors <span class="math inline">\(\eps_t\)</span> takes the form:</p>
<p><span class="math display">\[
\V(\b\eps|\b X) = \sigma^2 \b I_n = \begin{pmatrix}
\sigma^2 &amp; 0 &amp; 0 &amp; \dots \\
0 &amp; \sigma^2 &amp; 0 &amp; \dots  \\
0 &amp; 0&amp; \sigma^2 &amp; \vdots \\
\vdots &amp; \vdots &amp; \dots &amp; \ddots
\end{pmatrix}
\]</span></p>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Variance-Covariance Matrix of Errors
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The variance-covariance matrix of errors <span class="math inline">\(\eps_t\)</span> takes the form:</p>
<p><span class="math display">\[
\V(\b\eps | \b X) = \b\Omega = \begin{pmatrix}
\V \eps_1 &amp; Cov(\eps_1, \eps_2) &amp; Cov(\eps_1, \eps_3) &amp; \dots \\
Cov(\eps_2, \eps_1) &amp; \V \eps_2 &amp; Cov(\eps_2, \eps_3) &amp; \dots \\
Cov(\eps_3, \eps_1) &amp; Cov(\eps_3, \eps_2) &amp; \V\eps_3 &amp; \vdots \\
\vdots &amp; \vdots &amp; \dots &amp; \ddots
\end{pmatrix}
\]</span></p>
</div>
</div>
</div>
<p>Spherical errors implies two things:</p>
<ol type="1">
<li><strong>No Autocorrelation</strong> means that the covariance of any two error terms <span class="math inline">\(Cov(\eps_i, \eps_j) = 0\)</span>. This is reflected in spherical errors with all the 0’s in the non-diagonal positions.</li>
<li><strong>Homoscedasticity</strong> means that every <span class="math inline">\(\eps_t\)</span> for any observation <span class="math inline">\(t\)</span> has the same variance <span class="math inline">\(\sigma^2\)</span>. The variance of the error <span class="math inline">\(\eps_t\)</span> does not depend on the values of <span class="math inline">\(X_1, \dots, X_p\)</span>. If this condition is violated, as in each observation <span class="math inline">\(t\)</span> has their own <span class="math inline">\(\sigma^2_t\)</span>, we call this homoscedasticity.</li>
</ol>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="ordinary-least-squares" class="level2">
<h2 class="anchored" data-anchor-id="ordinary-least-squares">Ordinary Least Squares</h2>
<p>OLS is an estimation process that finds the values <span class="math inline">\(\hat{\b\beta}\)</span> by the <span class="math inline">\(\hat{\b\beta}\)</span> values that minimise the <strong>sum of squared residuals</strong> (SSR) (also called the sum of squared errors), the difference between <span class="math inline">\(\b y\)</span> and predicted <span class="math inline">\(\hat{\b y}\)</span> squared, where <span class="math inline">\(\hat{\b y}\)</span> is defined as <span class="math inline">\(\hat{\b y} = \b X \hat{\b\beta}\)</span>.</p>
<div id="def-ssr" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 4.7 (Sum of Squared Residuals)</strong></span> We will define the SSR by function <span class="math inline">\(S(\hat{\b\beta})\)</span>:</p>
<p><span class="math display">\[
S(\hat{\b\beta}) = \sum\limits_{i=1}^n (Y_i - \hat Y_i)^2 \ = \ (\b y - \hat{\b y})^\top (\b y - \hat{\b y})
\]</span></p>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why Sum of Squared Residuals?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>One common question is why are we squaring the residuals (difference between <span class="math inline">\(\b y\)</span> and <span class="math inline">\(\hat{\b y}\)</span>). The main reason is that squaring gets rid of negative and positive residuals, which might cancel each other out. We do not care about the direction of residuals/errors, only the magnitude.</p>
<p>The reason we choose to square the residuals, and not to use absolute value, is because of a variety of unique properties of OLS (unbiasedness, variance, efficeincy) that we will explore throughout this chapter.</p>
</div>
</div>
</div>
<p>We know that predicted <span class="math inline">\(\hat{\b y} = \b X \hat{\b \beta}\)</span>. Thus, let us plug that into <span class="math inline">\(S(\hat{\b\beta})\)</span>, and simplify to get:</p>
<p><span class="math display">\[
\begin{align}
S(\hat{\b\beta}) &amp; = (\b y - \b X \hat{\b\beta})^\top (\b y - \b X \hat{\b\beta}) \\
&amp; = \b y^\top \b y - \hat{\b\beta}^\top \b X^\top \b y - \b y^\top \b X \hat{\b\beta} +  \hat{\b\beta}^\top \b{X^\top X} \hat{\b\beta} \\
&amp; =  \b y^\top \b y - 2 \hat{\b\beta}^\top \b{X^\top y} + \hat{\b\beta}^\top \b{X^\top X} \hat{\b\beta}
\end{align}
\]</span></p>
<p>Now, we want to maximise in respect to <span class="math inline">\(\hat{\b\beta}\)</span>, so let us take the gradient of function <span class="math inline">\(S\)</span> in respect to <span class="math inline">\(\hat{\b\beta}\)</span>, and set it equal to 0. Then, we can solve for <span class="math inline">\(\hat{\b\beta}\)</span> to get our parameter estimates:</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial S}{\partial \hat{\b\beta}} = -2 \b{X^\top y} + 2 &amp; \b{X^\top X} \hat{\b\beta} = 0 \\
2 \b{X^\top X}\hat{\b\beta} &amp; = 2 \b{X^\top y} \\
\hat{\b\beta} &amp; = (2 \b{X^\top X})^{-1}2 \b{X^\top y} \\
\hat{\b\beta} &amp; =  (2^{-1})2(\b{X^\top X})^{-1}\b{X^\top y} \\
\hat{\b\beta} &amp; =  (\b{X^\top X})^{-1}\b{X^\top y}
\end{align}
\]</span></p>
<div id="def-ols" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.8 (OLS Estimate)</strong></span> The OLS estimate of parameters <span class="math inline">\(\b\beta\)</span> is:</p>
<p><span class="math display">\[
\hat{\b\beta}_{\mathrm{OLS}} =  (\b{X^\top X})^{-1}\b{X^\top y}
\]</span></p>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Non-Matrix Derivation for Simple Linear Regression
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For simple linear regression, our sum of squared errors (<a href="#def-ssr" class="quarto-xref">definition&nbsp;<span>4.7</span></a>) is:</p>
<p><span class="math display">\[
S(\hat\beta_0, \hat\beta_1) = \sum\limits_{t=1}^n (Y_t - \hat\beta_0 - \hat\beta_1 X_t)^2
\]</span></p>
<p>Our first order conditions by taking the partial derivative in respect to <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are:</p>
<p><span class="math display">\[
\begin{align}
&amp; \frac{\partial S}{\partial \hat\beta_0} = \sum\limits_{i=1}^n (Y_t -\hat\beta_0 = \hat\beta X_t) = 0 \\
&amp; \frac{\partial S}{\partial \hat\beta_1} = \sum\limits_{i=1}^n X_t (Y_t -\hat\beta_0 = \hat\beta X_t) = 0 \\
\end{align}
\]</span></p>
<p>And the final solutions for <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> after solving this system of equations is:</p>
<p><span class="math display">\[
\begin{align}
&amp; \hat\beta_0 = \bar Y - \hat\beta_1 \bar X \\
&amp; \hat\beta_1 = \frac{\sum (X_t -\bar X)(Y_t -\bar Y)}{\sum (X_t - \bar X)^2} = \frac{Cov(X, Y)}{\V Y}
\end{align}
\]</span></p>
</div>
</div>
</div>
<p>OLS has some unique properties, that we will explore in the tabs below.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset nav-pills">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">P and M Matrix</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">OLS Projection</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-3" role="tab" aria-controls="tabset-2-3" aria-selected="false">R-Squared</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-4" role="tab" aria-controls="tabset-2-4" aria-selected="false">Regression Anatomy</a></li></ul>
<div class="tab-content nav-pills">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<p>Recall our predictions <span class="math inline">\(\hat{\b y} = \b X \hat{\b\beta}\)</span>. Using our OLS solution, we can find</p>
<p><span class="math display">\[
\hat{\b y} = \b X (\b{X^\top X})^{-1} \b{X^\top y} = \color{red}{\b P}\color{black}{\b y}
\]</span></p>
<div id="def-projectmatrix" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.9 (Projection Matrix)</strong></span> Let us define the projection matrix <span class="math inline">\(\b P\)</span> as:</p>
<p><span class="math display">\[
\color{red}{\b P}\color{black} := \b X(\b{X^\top X})^{-1} \b X^\top
\]</span></p>
<p>Matrix <span class="math inline">\(\color{red}{\b P}\)</span> is symmetrical as in <span class="math inline">\(\b P^\top = \b P\)</span>, and is idempotent as in <span class="math inline">\(\b{PP} = \b P\)</span>.</p>
</div>
<p>We can see that projection matrix <span class="math inline">\(\color{red}{\b P}\)</span> performs a linear mapping of <span class="math inline">\(\b y \rightarrow \hat{\b y}\)</span>, hence why it is called the “projection” matrix.</p>
<p><br></p>
<p>Now, let us look at our residuals <span class="math inline">\(\hat{\b \eps} = \b y - \hat{\b y}\)</span>. By plogging in <span class="math inline">\(\b y = \b{Py}\)</span>, we can get:</p>
<p><span id="eq-calcresiduals"><span class="math display">\[
\hat{\b\eps} = \b y - \color{red}{\b P} \color{black}{\b y} = (\b I - \color{red}{\b P}\color{black})\b y = \color{purple}{\b M} \color{black}{\b y}
\tag{4.1}\]</span></span></p>
<div id="def-residmatrix" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.10 (Residual Maker Matrix)</strong></span> Let us define the residual maker matrix <span class="math inline">\(\b M\)</span> as:</p>
<p><span class="math display">\[
\color{purple}{\b M}\color{black} := \b I - \b X(\b{X^\top X})^{-1} \b X^\top = \b I - \color{red}{\b P}
\]</span></p>
<p>Matrix <span class="math inline">\(\color{purple}{\b M}\)</span> is symmetrical as in <span class="math inline">\(\b M^\top = \b M\)</span>, and is idempotent as in <span class="math inline">\(\b{MM} = \b M\)</span>. Residual maker <span class="math inline">\(\b M\)</span> is also orthogonal to <span class="math inline">\(\b P\)</span> and <span class="math inline">\(\b X\)</span>, implying <span class="math inline">\(\b{PX} = 0\)</span> and <span class="math inline">\(\b{MX} = 0\)</span>.</p>
</div>
<p>We can see that residual maker matrix <span class="math inline">\(\color{purple}{\b M}\)</span> performs a linear mapping of <span class="math inline">\(\b y \rightarrow \hat{\b\eps}\)</span>, thus “making” the residuals <span class="math inline">\(\hat{\b\eps}\)</span>.</p>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<p>From the previous tab, we see projection matrix <span class="math inline">\(\color{red}{\b P}\)</span> performs a linear mapping of <span class="math inline">\(\b y \rightarrow \hat{\b y}\)</span>, hence why it is called the “projection” matrix. We can see that residual maker matrix <span class="math inline">\(\color{purple}{\b M}\)</span> performs a linear mapping of <span class="math inline">\(\b y \rightarrow \hat{\b\eps}\)</span>, thus “making” the residuals <span class="math inline">\(\hat{\b\eps}\)</span>.</p>
<p>We know that our predictsion <span class="math inline">\(\hat{\b y}\)</span> are a linear combination of our explanatory variable vectors <span class="math inline">\(X_1, \dots, X_p\)</span>, since <span class="math inline">\(\hat{\b y} = \b X \hat{\b\beta}\)</span>. This means that projection matrix <span class="math inline">\(\color{red}{\b P}\)</span> projects <span class="math inline">\(\b y\)</span> into <span class="math inline">\(\hat{\b y}\)</span> which is in the space spanned by our <span class="math inline">\(\b X\)</span> (called the column space of <span class="math inline">\(\b X\)</span>).</p>
<p>This is visualised in the figure below, where observed vector <span class="math inline">\(\b y\)</span> is projected into the blue space of regressors <span class="math inline">\(\b X\)</span> to create vector <span class="math inline">\(\hat{\b y}\)</span>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-880030792.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>We can see as well that residual maker <span class="math inline">\(\color{purple}{\b M}\)</span> projects vector <span class="math inline">\(\b y\)</span> into vector <span class="math inline">\(\b\eps\)</span>, which is in the space orthogonal/perpendicular to the column space of <span class="math inline">\(\b X\)</span>. This is why our condition of strict exogeneity (<a href="#def-strictexog" class="quarto-xref">definition&nbsp;<span>4.4</span></a>) is required - there should be no correlation between <span class="math inline">\(\b X\)</span> and <span class="math inline">\(\b \eps\)</span> as they are orthogonal by design.</p>
</div>
<div id="tabset-2-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-3-tab">
<p>This idea of projection from the last tabe means we can measure the fit of our model with the correlation/overlap between original vector <span class="math inline">\(\b y\)</span> and our predicted values vector <span class="math inline">\(\hat{\b y} = \b{Py}\)</span>.</p>
<div id="def-rsquared" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.11 (R-Squared)</strong></span> <span class="math inline">\(R^2\)</span> is a metric to measure the fit of our model.</p>
<p><span class="math display">\[
R^2 = \frac{\overbrace{\b{y^\top Py}}^{\text{model}}}{\underbrace{\b{y^\top y}}_{\V Y}} \ = \ 1-\frac{\overbrace{\sum(Y_t - \hat Y_t)^2}^{\text{SSR}}}{\underbrace{\sum (Y_t - \bar Y)^2}_{\V Y}}
\]</span></p>
<p><span class="math inline">\(R^2\)</span> is always between 0 and 1, and measures the proportion of variance our model with explanatory variables <span class="math inline">\(X_1, \dots, X_p\)</span> explains the variation in <span class="math inline">\(Y\)</span>.</p>
</div>
<p>The first definition of <span class="math inline">\(R^2\)</span> works because a dot-product of vectors measures their overlap/shadow on each other - so <span class="math inline">\(\b{y^\top Py}\)</span> measures the overlap between our actual <span class="math inline">\(\b y\)</span> and our model <span class="math inline">\(\b{Py}\)</span>. This is divided by the maximum possible shadow of <span class="math inline">\(\b y\)</span> on itself, to ensure a <span class="math inline">\(R^2\)</span> between 0 and 1.</p>
<p>The second definition of <span class="math inline">\(R^2\)</span> works because we know the SSR is the part of <span class="math inline">\(Y\)</span> that is not explained by our model (the error). Thus, SSR divided by the variance in <span class="math inline">\(Y\)</span> is the porportion of the variance in <span class="math inline">\(Y\)</span> not explained by our model. Thus, the remaining part must be explained by our model, so we do 1 minus SSR/Variance.</p>
</div>
<div id="tabset-2-4" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-4-tab">
<p>A partitioned regression model is when we split up our matrices/vectors in our classical linear model <span class="math inline">\(\b y = \b{X\beta} + \b\eps\)</span>. Let us say we only care about some of the explanatory variables in our data generating process. We can split matrix <span class="math inline">\(\b X\)</span> into two: <span class="math inline">\(\b X_1\)</span> contains the explanatory variables we care about, and <span class="math inline">\(\b X_2\)</span> contains the other explanatory variables. Similarly, we divide <span class="math inline">\(\b\beta\)</span>. We get:</p>
<p><span class="math display">\[
\b y = \b X_1 \b\beta_1 + \b X_2 \b\beta_2 + \b\eps
\]</span></p>
<p>Recall the residual maker matrix <span class="math inline">\(\b M\)</span> (<a href="#def-residmatrix" class="quarto-xref">definition&nbsp;<span>4.10</span></a>), and recall how <span class="math inline">\(\b M\)</span> is orthogonal to <span class="math inline">\(\b X\)</span>, meaning <span class="math inline">\(\b{MX} = 0\)</span>. Now, let us consider only the residual maker matrix of the second part of our partitioned model <span class="math inline">\(\b M_2\)</span>, which means <span class="math inline">\(\b M_2 \b X_2 = 0\)</span>. Let us take our partitioned regression model, and pre-multiply <span class="math inline">\(\b M_2\)</span> to both sides:</p>
<p><span class="math display">\[
\begin{align}
\b M_2 \b y &amp; = \b M_2(\b X_1 \b\beta_1 + \b X_2 \b\beta_2 + \b\eps) \\
\b M_2 \b y &amp; = \b M_2 \b X_1 \b\beta_1 + \b M_2 \b X_2 \b\beta_2 + \b M_2 \b\eps
\end{align}
\]</span></p>
<p>Now recall that <span class="math inline">\(\b M_2 \b X_2 = 0\)</span>. That means we can simplify the above to</p>
<p><span class="math display">\[
\b M_2 \b y = \b M_2 \b X_1 \b\beta_1  + \b M_2 \b\eps
\]</span></p>
<p>Now, let us define <span class="math inline">\(\tilde{\b y} := \b M_2 \b y\)</span>, <span class="math inline">\(\tilde{\b X_1} := \b M_2 \b X_1\)</span>, and error <span class="math inline">\(\tilde{\b\eps} = \b M_2 \b\eps\)</span>. We can rewrite as</p>
<p><span class="math display">\[
\tilde{\b y} = \tilde{\b X_1}\b\beta_1 + \tilde{\b \eps}
\]</span></p>
<p>Using <a href="#def-ols" class="quarto-xref">definition&nbsp;<span>4.8</span></a>, we know the OLS estimate of <span class="math inline">\(\hat{\b \beta_1}\)</span> is:</p>
<p><span class="math display">\[
\hat{\b\beta}_1 = (\tilde{\b X_1^\top} \tilde{\b X_1}) \tilde{\b X_1^\top} \tilde{\b y}
\]</span></p>
<p><span class="math inline">\(\hat{\b\beta}_1\)</span> is equal to our normal OLS estimates without partitioning the model. Notice how we have <span class="math inline">\(\tilde{\b X_1} := \b M_2 \b X_1\)</span> in our formula. Well, we know <span class="math inline">\(\b M_2 \b X_2 = 0\)</span>. That means that any part of <span class="math inline">\(\b X_1\)</span> that was correlated to <span class="math inline">\(\b X_2\)</span> became 0, after it was multiplied by <span class="math inline">\(\b M_2\)</span>. Thus, <span class="math inline">\(\tilde{\b X_1}\)</span> is the part of <span class="math inline">\(\b X_1\)</span> that is uncorrelated with <span class="math inline">\(\b X_2\)</span>.</p>
<div id="thm-regressionanatomy" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.1 (Regression Anatomy Theorem)</strong></span> Our individual parameter estimates <span class="math inline">\(\hat\beta_j \in \{\hat\beta_1, \dots, \hat\beta_p \}\)</span> are the relationship between <span class="math inline">\(Y\)</span> and the part of <span class="math inline">\(X_j \in \{X_1, \dots, X_p\}\)</span> uncorrelated with the other explanatory variables.</p>
</div>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="ols-estimator-properties" class="level2">
<h2 class="anchored" data-anchor-id="ols-estimator-properties">OLS Estimator Properties</h2>
<p>We know that an estimator has two finite sample properties: unbiasedness (<a href="inference.html#def-unbiased" class="quarto-xref">definition&nbsp;<span>2.4</span></a>) and variance (<a href="inference.html#def-varest" class="quarto-xref">definition&nbsp;<span>2.5</span></a>), and has the asymptotic property of consistency (<a href="inference.html#def-consistency" class="quarto-xref">definition&nbsp;<span>2.6</span></a>). Before we explore these properties, let us first transform our OLS estimates (<a href="#def-ols" class="quarto-xref">definition&nbsp;<span>4.8</span></a>) into a form more useful for showing these properties of OLS.</p>
<p>Let us start with our OLS solution (<a href="#def-ols" class="quarto-xref">definition&nbsp;<span>4.8</span></a>), and plug in our original model <span class="math inline">\(\b y = \b{X\beta} + \b\eps\)</span> into where <span class="math inline">\(\b y\)</span> is in the OLS solution:</p>
<p><span id="eq-olsimplify"><span class="math display">\[
\begin{align}
\hat{\b\beta} &amp; = (\b{X^\top X})^{-1} \b{X^\top y} \\
&amp; = (\b{X^\top X})^{-1} \b X^\top (\b{X\beta} + \b\eps) \\
&amp; = \underbrace{(\b{X^\top X})^{-1}\b{X^\top X}}_{\text{inverses cancel}}\b\beta + (\b{X^\top X})^{-1} \b{X^\top \eps} \\
&amp; = \b\beta + (\b{X^\top X})^{-1}\b{X^\top \eps}
\end{align}
\tag{4.2}\]</span></span></p>
<p>Now, we are ready to explore the properties of the ordinary least squares estimator: unbiasedness, variance, and consistency.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset nav-pills">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true">Unbiasedness</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-2" role="tab" aria-controls="tabset-3-2" aria-selected="false">Variance</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-3" role="tab" aria-controls="tabset-3-3" aria-selected="false">Consistency</a></li></ul>
<div class="tab-content nav-pills">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<div id="thm-olsunbiased" class="theorem" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Theorem 4.2 (OLS is an Unbiased Estimator)</strong></span> The Ordinary Least Squares estimator is an unbiased estimator under the <a href="#assumptions">assumptions</a> of linearity, i.i.d., linear independence, and strict exogeneity.</p>
<p><span class="math display">\[
\E \hat{\b\beta} = \b\beta
\]</span></p>
<p>Note that the assumption of spherical errors is <u>not</u> needed for the unbiasedness of OLS.</p>
</div>
<p><br></p>
<p><strong>Proof</strong>: Let us take the expectation of <span class="math inline">\(\hat{\b\beta}\)</span> (from <a href="#eq-olsimplify" class="quarto-xref">eq.&nbsp;<span>4.2</span></a>) conditional on <span class="math inline">\(\b X\)</span> (remember that <span class="math inline">\(\b X\)</span> and <span class="math inline">\(\b \beta\)</span> are fixed constants, so they are not affected by the expectation). We can use the strict exogeneity assumption (<a href="#def-strictexog" class="quarto-xref">definition&nbsp;<span>4.4</span></a>) to simplify:</p>
<p><span class="math display">\[
\E(\hat{\b\beta}|\b X) = \b\beta + (\b{X^\top X})^{-1} \underbrace{\E (\b \eps | \b X)}_{= \ 0} = \b\beta
\]</span></p>
<p>Now, we know <span class="math inline">\(\E(\hat{\b\beta} | \b X)\)</span>. We can deduce <span class="math inline">\(\E(\hat{\b\beta})\)</span> using the law of iterated expectations (<a href="random.html#thm-lie" class="quarto-xref">theorem&nbsp;<span>1.3</span></a>), and plugging in <span class="math inline">\(\E(\hat{\b\beta} | \b X) = \b\beta\)</span>:</p>
<p><span class="math display">\[
\E(\hat{\b\beta}) = \E[\E(\hat{\b\beta}|\b X)] = \E[\b\beta] = \b\beta
\]</span></p>
<p>The final step is because the expectation of a constant <span class="math inline">\(\b\beta\)</span> (the fixed true population value) is the constant itself. Thus, we have proven OLS is an unbiased.</p>
</div>
<div id="tabset-3-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-2-tab">
<div id="thm-varols" class="theorem" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Theorem 4.3 (Variance of the OLS Estimator)</strong></span> The variance of the OLS estimator under the <a href="#assumptions">assumptions</a> of linearity, i.i.d., linear independence, strict exogeneity, and spherical errors, is given by</p>
<p><span class="math display">\[
\V(\hat{\b\beta}|\b X) = \sigma^2 (\b{X^\top X})^{-1}
\]</span></p>
</div>
<p><br></p>
<p><strong>Proof:</strong> Let us start where we left off from <a href="#eq-olsimplify" class="quarto-xref">eq.&nbsp;<span>4.2</span></a>. This tells us the variance of the OLS estimator is:</p>
<p><span class="math display">\[
\V(\hat{\b\beta}|\b X) = \V(\b\beta + (\b{X^\top X})^{-1} \b{X^\top \eps})
\]</span></p>
<p>We know that <span class="math inline">\(\b\beta\)</span> is a vector of fixed true population values. <span class="math inline">\((\b{X^\top X})^{-1} \b X^\top\)</span> can also be considered a fixed constant matrix because we are conditioning our variance on <span class="math inline">\(\b X\)</span>. Thus, we can use <a href="random.html#thm-variance" class="quarto-xref">theorem&nbsp;<span>1.2</span></a> to rewrite the above as</p>
<p><span class="math display">\[
\V(\hat{\b\beta}|\b X) = (\b{X^\top X})^{-1}\b X^\top \V(\b\eps| \b X)[(\b{X^\top X})^{-1}\b X^\top]^{-1}
\]</span></p>
<p>With the properties of matrix inverses and transposes, we can determine that <span class="math inline">\([(\b{X^\top X})^{-1}\b X^\top]^{-1}\)</span> is equivalent to <span class="math inline">\(\b X(\b{X^\top X})^{-1}\)</span>. Thus, plugging this in, we get</p>
<p><span class="math display">\[
\V(\hat{\b\beta}|\b X) = (\b{X^\top X})^{-1}\b X^\top \V(\b\eps| \b X) \b X(\b{X^\top X})^{-1}
\]</span></p>
<p>Now, according to the assumption of spherical errors (<a href="#def-sphericalerrors" class="quarto-xref">definition&nbsp;<span>4.6</span></a>), we know that <span class="math inline">\(\V(\b \eps| \b X) = \sigma^2 \b I_n\)</span>. Thus, let us plug that into our equation to get</p>
<p><span class="math display">\[
\V(\hat{\b\beta}|\b X) = (\b{X^\top X})^{-1}\b X^\top \sigma^2 \b I_n \b X(\b{X^\top X})^{-1}
\]</span></p>
<p>Since <span class="math inline">\(\b I_n\)</span> is the identity matrix, it cancels out. For notation simplicity, we can move the scalar <span class="math inline">\(\sigma^2\)</span> to the front, and simplify:</p>
<p><span class="math display">\[
\begin{align}
\V(\hat{\b\beta}|\b X) &amp; = \sigma^2 \underbrace{(\b{X^\top X})^{-1}\b X^\top \b X}_{\text{inverses cancel}}(\b{X^\top X})^{-1} \\
&amp; = \sigma^2 (\b{X^\top X})^{-1}
\end{align}
\]</span></p>
<p>Thus, we have proved that the variance of the OLS estimator is as the theorem above.</p>
</div>
<div id="tabset-3-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-3-tab">
<div id="thm-olsconsistent" class="theorem" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Theorem 4.4 (Consistency of OLS)</strong></span> The Ordinary Least Squares estimator is asymptotically consistent (<a href="inference.html#def-consistency" class="quarto-xref">definition&nbsp;<span>2.6</span></a>), under the <a href="#assumptions">assumptions</a> of linearity, i.i.d., linear independence, weak exogeneity, and spherical errors. Mathematically:</p>
<p><span class="math display">\[
\mathrm{plim}\hat{\b\beta} = \b\beta
\]</span></p>
</div>
<p><br></p>
<p><strong>Proof</strong>: Let us start off from <a href="#eq-olsimplify" class="quarto-xref">eq.&nbsp;<span>4.2</span></a>. We can rewrite our matrix notation in the form of vectors, scalars, and summation:</p>
<p><span class="math display">\[
\begin{align}
\hat{\b\beta} &amp; = \b\beta + (\b{X^\top X})^{-1}\b{X^\top\eps} \\
&amp; = \b\beta + \left(\sum\limits_{t=1}^n \b x_t \b x_t^\top\right)^{-1} \left(\sum\limits_{t=1}^n\b x_t \eps_t \right)
\end{align}
\]</span></p>
<p>Now, let us do a little algebra trick as follows:</p>
<p><span class="math display">\[
\hat{\b\beta} = \b\beta + \left(\frac{1}{n}\sum\limits_{t=1}^n \b x_t \b x_t^\top\right)^{-1} \left(\frac{1}{n}\sum\limits_{t=1}^n\b x_t \eps_t \right)
\]</span></p>
<p>The reason we can do this is because the first <span class="math inline">\(\frac{1}{n}\)</span> is inversed as <span class="math inline">\(\frac{1}{n}^{-1}\)</span>, so this cancels out the second one, maintaining the equality of our equation.</p>
<p>Now, we want to prove <span class="math inline">\(\mathrm{plim}\hat{\b\beta} =\b\beta\)</span>, so let us take the probability limit of both sides.</p>
<p><span class="math display">\[
\mathrm{plim}\hat{\b\beta} = \mathrm{plim} \b\beta + \left( \mathrm{plim} \frac{1}{n}\sum\limits_{t=1}^n \b x_t \b x_t^\top \right)^{-1} \left( \mathrm{plim}\frac{1}{n}\sum\limits_{t=1}^n\b x_t \eps_t \right)
\]</span></p>
<p>We know that the probability limit of a constant is itself, so <span class="math inline">\(\mathrm{plim} \b\beta = \b\beta\)</span>, since <span class="math inline">\(\b\beta\)</span> is a constant of true population parameters. Look at the other two terms on the right. They take the form of sample averages <span class="math inline">\(\frac{1}{n}\sum\)</span>. Using the law of of large numbers (<a href="inference.html#thm-lawoflargenumbers" class="quarto-xref">theorem&nbsp;<span>2.1</span></a>), we can simplify to:</p>
<p><span class="math display">\[
\mathrm{plim}\hat{\b\beta} = \b\beta + (\E(\b x_t \b x_t^\top))^{-1} \underbrace{\E(\b x_t \eps_t)}_{=0} = \b\beta
\]</span></p>
<p>And we know <span class="math inline">\(\E(\b x_t \eps_t) = 0\)</span> because of the condition of <strong>weak</strong> <strong>exogeneity</strong> (<a href="#def-weakexog" class="quarto-xref">definition&nbsp;<span>4.5</span></a>). Thus, we have proved that OLS is asymptotically consistent.</p>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="gauss-markov-theorem" class="level2">
<h2 class="anchored" data-anchor-id="gauss-markov-theorem">Gauss-Markov Theorem</h2>
<p>You might ask, what is so special about Ordinary Least Squares, and why should we use this estimator? The answer lies in the Gauss-Markov Theorem.</p>
<div id="thm-gaussmarkov" class="theorem" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Theorem 4.5 (Gauss-Markov)</strong></span> If all of the <a href="#assumptions">assumptions</a> of linearity, i.i.d., no perfect multicollinearity, strict exogeneity, and spherical errors are all met, then the Ordinary Least Squares estimator is the <strong>best linear unbiased estimator</strong> (BLUE) - the unbiased linear estimator with the lowest variance of any other unbiased estimator.</p>
<p>Formally, if <span class="math inline">\(\hat{\b\beta}\)</span> is the OLS estimator, and <span class="math inline">\(\tilde{\b\beta}\)</span> is any other linear unbiased estimator, then</p>
<p><span class="math display">\[
\V(\hat{\b\beta}|\b X) ≤ \V(\tilde{\b\beta} | \b X)
\]</span></p>
</div>
<p><br></p>
<p>Any linear estimator <span class="math inline">\(\tilde{\b\beta}\)</span> must be in the form <span class="math inline">\(\tilde{\b\beta} = \b{Cy}\)</span>, where <span class="math inline">\(\b C\)</span> is some linear mapping. For example, using projection matrix <span class="math inline">\(\b P\)</span> (<a href="#def-projectmatrix" class="quarto-xref">definition&nbsp;<span>4.9</span></a>), OLS can be written as <span class="math inline">\(\hat{\b\beta} = \b{Py}\)</span>. Before we prove the Gauss-Markov theorem, we need a lemma about any unbiased linear estimator.</p>
<div id="lem-unbiasedlin" class="theorem lemma" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Lemma 4.1</strong></span> For any linear estimator <span class="math inline">\(\tilde{\b\beta} = \b{Cy}\)</span> to be unbiased, <span class="math inline">\(\b{CX} = \b I\)</span>.</p>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof of Lemma 3.1
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Proof</strong>: Let us start off with our linear estimator <span class="math inline">\(\tilde{\b\beta} = \b{Cy}\)</span>, and plug in the true linear model <span class="math inline">\(\b y = \b{X\beta} + \b\eps\)</span> into our linear estimator:</p>
<p><span class="math display">\[
\tilde{\b\beta} = \b C (\b{X\beta} + \b\eps) = \b{CX\beta} + \b{C\eps}
\]</span></p>
<p>Now, let us find the expected value of this estimator conditional on <span class="math inline">\(\b X\)</span>. Remember that the expected values of constants (like <span class="math inline">\(\b C\)</span>, <span class="math inline">\(\b \beta\)</span>, and <span class="math inline">\(\b X\)</span> since we are conditioning on <span class="math inline">\(\b X\)</span>) are the constants themselves.</p>
<p><span class="math display">\[
\begin{align}
\E(\tilde{\b\beta}|\b X) &amp; = \E(\b{CX\beta} + \b{C\eps}) \\
&amp; = \b{CX\beta} + \b C \E(\b\eps| \b X)
\end{align}
\]</span></p>
<p>From the strict exogeneity assumption (<a href="#def-strictexog" class="quarto-xref">definition&nbsp;<span>4.4</span></a>), we know <span class="math inline">\(\E(\b\eps | \b X) = 0\)</span>, so we can simplify to</p>
<p><span class="math display">\[
\E(\tilde{\b\beta}|\b X) = \b{CX\beta}
\]</span></p>
<p>And using the law of iterated expectations (<a href="random.html#thm-lie" class="quarto-xref">theorem&nbsp;<span>1.3</span></a>), we can find <span class="math inline">\(\E\tilde{\b\beta}\)</span>:</p>
<p><span class="math display">\[
\E\tilde{\b\beta} = \E[\E(\tilde{\b\beta}|\b X)] = \E[\b{CX\beta}] = \b{CX\beta}
\]</span></p>
<p>For unbiasedness (<a href="inference.html#def-unbiased" class="quarto-xref">definition&nbsp;<span>2.4</span></a>), we know <span class="math inline">\(\E\tilde{\b\beta} = \b\beta\)</span>. The only way <span class="math inline">\(\b{CX\beta}\)</span> will equal <span class="math inline">\(\b\beta\)</span> is if <span class="math inline">\(\b{CX} = \b I\)</span>. Thus, for any linear unbiased estimator, the lemma <span class="math inline">\(\b{CX} = \b I\)</span> must hold.</p>
</div>
</div>
</div>
<p>With this lemma, now let us prove Gauss-Markov. First, let us calculate the variance of unbiased linear estimator <span class="math inline">\(\tilde{\b\beta}\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\V(\tilde{\b\beta} | \b X) &amp; = \V(\b{Cy} | \b X) \\
&amp; = \V(\b C( \b{X\beta} + \b \eps)| \b X) \\
&amp; = \V(\b{CX\beta} + \b{C\eps} | \b X)
\end{align}
\]</span></p>
<p>And since we know from <a href="#lem-unbiasedlin" class="quarto-xref">Lemma&nbsp;<span>4.1</span></a> that <span class="math inline">\(\b{CX = I}\)</span>, we can get</p>
<p><span class="math display">\[
\V(\tilde{\b\beta} | \b X) = \V(\b\beta + \b{C\eps} | \b X)
\]</span></p>
<p>We know that <span class="math inline">\(\b\beta\)</span> is a vector of fixed constants (the true population values). We also know <span class="math inline">\(\b C\)</span> is some fixed constant matrix (that depends on <span class="math inline">\(\b X\)</span>, but we are conditioning on <span class="math inline">\(\b X\)</span>). Thus, we can use <a href="random.html#thm-variance" class="quarto-xref">theorem&nbsp;<span>1.2</span></a> to rewrite the above as</p>
<p><span class="math display">\[
\V(\tilde{\b\beta} | \b X) = \b C\V(\b\eps | \b X) \b C^\top
\]</span></p>
<p>Now, according to the assumption of spherical errors (<a href="#def-sphericalerrors" class="quarto-xref">definition&nbsp;<span>4.6</span></a>), we know that <span class="math inline">\(\V(\b \eps| \b X) = \sigma^2 \b I_n\)</span>. Thus, let us plug that into our equation to get</p>
<p><span id="eq-unbiasedlinearvar"><span class="math display">\[
\begin{align}
\V(\tilde{\b\beta} | \b X) &amp; = \b C \sigma^2 \b I_n \b C^\top \\
&amp; = \sigma^2 \b{CC^\top}
\end{align}
\tag{4.3}\]</span></span></p>
<p>Now we have the variance of estimator <span class="math inline">\(\tilde{\b\beta}\)</span>. To prove Gauss-Markov, we need to show that the variance of <span class="math inline">\(\tilde{\b\beta}\)</span> is greater than the variance of <span class="math inline">\(\hat{\b\beta}\)</span>. For this to be true,</p>
<p><span class="math display">\[
\V(\tilde{\b\beta}|\b X) - \V(\tilde{\b\beta}| \b X) ≥ 0
\]</span></p>
<p>We can plug in the variance of <span class="math inline">\(\tilde{\b\beta}\)</span> from <a href="#eq-unbiasedlinearvar" class="quarto-xref">eq.&nbsp;<span>4.3</span></a>, and the variance of OLS <span class="math inline">\(\hat{\b\beta}\)</span> from <a href="#thm-varols" class="quarto-xref">theorem&nbsp;<span>4.3</span></a>:</p>
<p><span class="math display">\[
\begin{align}
\sigma^2 \b{CC^\top} - \sigma^2 (\b{X^\top X})^{-1} &amp; ≥ 0 \\
\sigma^2 (\b{CC^\top} - (\b{X^\top X})^{-1}) &amp; ≥ 0
\end{align}
\]</span></p>
<p>We know from <a href="#lem-unbiasedlin" class="quarto-xref">Lemma&nbsp;<span>4.1</span></a> that <span class="math inline">\(\b{CX} = \b I\)</span>, which through the properties of tranposes, also implies that <span class="math inline">\(\b{X^\top C^\top} = (\b{CX})^\top = \b I\)</span>. Multipling by <span class="math inline">\(\b I\)</span> doesn’t change anything, so we can insert a <span class="math inline">\(\b{CX}\)</span> and <span class="math inline">\(\b{X^\top C^\top}\)</span> into our equation above to get</p>
<p><span class="math display">\[
\sigma^2 (\b{CC^\top} - \b{CX} (\b{X^\top X})^{-1}\b{X^\top C^\top}) ≥ 0
\]</span></p>
<p>Factoring out <span class="math inline">\(\b C\)</span> and <span class="math inline">\(\b C^\top\)</span>, and remembering our residual maker <span class="math inline">\(\b M\)</span> (<a href="#def-residmatrix" class="quarto-xref">definition&nbsp;<span>4.10</span></a>),</p>
<p><span class="math display">\[
\begin{align}
\sigma^2 \b C(\b I - \b X(\b{X^\top X}^{-1}\b X^\top) \b C^\top &amp; ≥ 0 \\
\sigma^2 \b{CMC} &amp; ≥ 0
\end{align}
\]</span></p>
<p>We know <span class="math inline">\(\sigma^2\)</span>, the variance of the error term, must be positive. <span class="math inline">\(\b{CMC}\)</span> is also a positive semi-definite matrix (behaves like a positive number). The proof is provided below.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof <span class="math inline">\(CMC\)</span> is Positive Semi-Definite
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>To show <span class="math inline">\(\b {CMC}\)</span> is positive semi-definite, the following must be true for every vector <span class="math inline">\(\b z\)</span>:</p>
<p><span class="math display">\[
\b{z^\top CMC^\top z} ≥ 0
\]</span></p>
<p>Remember that from <a href="#def-residmatrix" class="quarto-xref">definition&nbsp;<span>4.10</span></a> that <span class="math inline">\(\b M\)</span> is symmetric and idempotent. This implies that <span class="math inline">\(\b M = \b{MM} = \b M^\top\)</span>. Thus, plugging this in, we get</p>
<p><span class="math display">\[
\underbrace{\b{z^\top CM}}_{\b w^\top} \underbrace{\b{M^\top C^\top z}}_{\b w} = \b{w^\top w} = \sum\limits_{i=1}^n w_i^2 ≥ 0
\]</span></p>
<p>Which is true since the square of any number cannot be negative. Thus, <span class="math inline">\(\b{CMC}\)</span> is positive semi-definite, and behaves like a positive number.</p>
</div>
</div>
</div>
<p>This property means that OLS produces the best estimates for any linear model, which makes it very popular in statistics (especially considering many statistical models are linear).</p>
<p><br></p>
</section>
<section id="ols-and-non-spherical-errors" class="level2">
<h2 class="anchored" data-anchor-id="ols-and-non-spherical-errors">OLS and Non-Spherical Errors</h2>
<p>For the classical linear model, one of the assumptions was spherical errors (<a href="#def-sphericalerrors" class="quarto-xref">definition&nbsp;<span>4.6</span></a>). This was an assumption made on the variance-covariance matrix of error term <span class="math inline">\(\eps_t\)</span>.</p>
<p>The spherical errors assumption can thus be violated in two ways. First, is <strong>conditional heteroscedasticity</strong>, where only homoscedasticity is violated, but no autocorrleation still holds. The variance-covariance matrix of errors will take the form:</p>
<p><span class="math display">\[
\V(\b\eps|\b X) = \b\Omega= \begin{pmatrix}
\sigma^2_1 &amp; 0 &amp; 0 &amp; \dots \\
0 &amp; \sigma^2_2 &amp; 0 &amp; \dots  \\
0 &amp; 0&amp; \sigma^2_3 &amp; \vdots \\
\vdots &amp; \vdots &amp; \dots &amp; \ddots
\end{pmatrix}
\]</span></p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Identifying Heteroscedasticity
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-1713529842.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></p>
</figure>
</div>
<p>The above is a residual plot of OLS residuals <span class="math inline">\(\hat\eps_i\)</span> against some explanatory variable <span class="math inline">\(X\)</span>. Notice how for homoscedasticity, the variance of the error terms (how spread out they are up-down wise) is constant for any value of <span class="math inline">\(X\)</span>.</p>
<p>For heteroscedasticity, we can clearly see that the residual variance is smaller for some <span class="math inline">\(X\)</span> values, and larger for other <span class="math inline">\(X\)</span> values. If you see a pattern in your residual plot, it is likely heteroscedasticity.</p>
</div>
</div>
</div>
<p>The second way spherical errors can be violated is with <strong>autocorrelation</strong>, where both of the assumptions of spherical errors are violated.</p>
<p>What is the impact of violating spherical errors?</p>
<ol type="1">
<li>OLS estimates <strong>remain unbiased</strong>. This is because our OLS unbiasedness proof (<a href="#thm-olsunbiased" class="quarto-xref">theorem&nbsp;<span>4.2</span></a>) does not depend on the spherical errors assumption.</li>
<li>Our derived OLS variance is <strong>incorrect</strong>. This is because our variance formula (<a href="#thm-varols" class="quarto-xref">theorem&nbsp;<span>4.3</span></a>) depends on the spherical errors assumption.</li>
<li>OLS is no longer the best linear unbiased estimator - the linear unbiased estimator with the lowest variance. This is because the Gauss-Markov Theorem (<a href="#thm-gaussmarkov" class="quarto-xref">theorem&nbsp;<span>4.5</span></a>) depends on the spherical errors assumption. Thus, there are other linear unbiased estimators with lower variance.</li>
</ol>
<p>Since OLS remains unbiased, we can still use OLS as an estimator. We just have to correct our OLS variance calculations to account for the fact that spherical errors is not met. There are three different variance formulas used for different forms of non-spherical errors.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset nav-pills">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-4-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-1" role="tab" aria-controls="tabset-4-1" aria-selected="true">Heteroscedasticity</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-4-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-2" role="tab" aria-controls="tabset-4-2" aria-selected="false">Clustered</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-4-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-3" role="tab" aria-controls="tabset-4-3" aria-selected="false">Heteroscedasticity + Autocorrelation</a></li></ul>
<div class="tab-content nav-pills">
<div id="tabset-4-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-4-1-tab">
<div id="thm-heterovar" class="theorem" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Theorem 4.6 (Heteroscedasticity Variance of OLS)</strong></span> The variance of the OLS estimator under heteroscedasticity is given by</p>
<p><span class="math display">\[
\V(\hat{\b\beta}|\b X) = (\b{X^\top X})^{-1}\b X^\top \begin{pmatrix}
\sigma^2_1 &amp; 0 &amp; 0 &amp; \dots \\
0 &amp; \sigma^2_2 &amp; 0 &amp; \dots  \\
0 &amp; 0&amp; \sigma^2_3 &amp; \vdots \\
\vdots &amp; \vdots &amp; \dots &amp; \ddots
\end{pmatrix} \b X(\b{X^\top X})^{-1}
\]</span></p>
</div>
<p><br></p>
<p><strong>Proof:</strong> Let us start where we left off from <a href="#eq-olsimplify" class="quarto-xref">eq.&nbsp;<span>4.2</span></a> during the proof of OLS properties. This tells us the variance of the OLS estimator is:</p>
<p><span class="math display">\[
\V(\hat{\b\beta}|\b X) = \V(\b\beta + (\b{X^\top X})^{-1} \b{X^\top \eps})
\]</span></p>
<p>We know that <span class="math inline">\(\b\beta\)</span> is a vector of fixed true population values. <span class="math inline">\((\b{X^\top X})^{-1} \b X^\top\)</span> can also be considered a fixed constant matrix because we are conditioning our variance on <span class="math inline">\(\b X\)</span>. Thus, we can use <a href="random.html#thm-variance" class="quarto-xref">theorem&nbsp;<span>1.2</span></a> to rewrite the above as</p>
<p><span class="math display">\[
\V(\hat{\b\beta}|\b X) = (\b{X^\top X})^{-1}\b X^\top \V(\b\eps| \b X)[(\b{X^\top X})^{-1}\b X^\top]^{-1}
\]</span></p>
<p>With the properties of matrix inverses and transposes, we can determine that <span class="math inline">\([(\b{X^\top X})^{-1}\b X^\top]^{-1}\)</span> is equivalent to <span class="math inline">\(\b X(\b{X^\top X})^{-1}\)</span>. Thus, plugging this in, we get</p>
<p><span class="math display">\[
\V(\hat{\b\beta}|\b X) = (\b{X^\top X})^{-1}\b X^\top \V(\b\eps| \b X) \b X(\b{X^\top X})^{-1}
\]</span></p>
<p>Now, we can replace <span class="math inline">\(\V(\b\eps|\b X)\)</span> with the heteroscedasticity variance matrix of errors:</p>
<p><span class="math display">\[
\V(\hat{\b\beta}|\b X) = (\b{X^\top X})^{-1}\b X^\top \begin{pmatrix}
\sigma^2_1 &amp; 0 &amp; 0 &amp; \dots \\
0 &amp; \sigma^2_2 &amp; 0 &amp; \dots  \\
0 &amp; 0&amp; \sigma^2_3 &amp; \vdots \\
\vdots &amp; \vdots &amp; \dots &amp; \ddots
\end{pmatrix} \b X(\b{X^\top X})^{-1}
\]</span></p>
<p>And thus, we have proven the OLS variance under heteroscedasticity. When we actually write the formula, we will typically replace the matrix with <span class="math inline">\(\b\Omega\)</span>.</p>
</div>
<div id="tabset-4-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-4-2-tab">
<p>Clustered standard errors are when you have done clustered sampling for your observations. For example, you randomly sample 100 people from 100 different villages.</p>
<p>The errors of observations belonging to the same cluster (say village) might exhibit correlation, while errors of observtations from distinct clusters are assumed to be uncorrelated.</p>
<div id="def-clusteredse" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 4.12 (Clustered Sampling Variance of OLS)</strong></span> The variance of the OLS estimator under clustered sampling is given by</p>
<p><span class="math display">\[
\V(\hat{\b\beta}|\b X) = (\b{X^\top X})^{-1}\b X^\top \begin{pmatrix}
\b\Sigma_1 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \b\Sigma_2 &amp; &amp; 0 \\
\vdots &amp; 0 &amp; \ddots &amp; 0 \\
0 &amp; 0&amp; \dots &amp; \b\Sigma_G
\end{pmatrix} \b X(\b{X^\top X})^{-1}
\]</span></p>
<p>Where <span class="math inline">\(\b\Sigma_1, \dots \b\Sigma_G\)</span> are intracluster covariance-variance error matrices, that exhibit autocorrelation within each cluster, but the different clusters are not autocorrelated.</p>
</div>
<p><br></p>
<p><strong>Proof:</strong> Let us start off by taking the same steps as the heteroscedasticity proof, until we got to this point:</p>
<p><span class="math display">\[
\V(\hat{\b\beta}|\b X) = (\b{X^\top X})^{-1}\b X^\top \V(\b\eps| \b X) \b X(\b{X^\top X})^{-1}
\]</span></p>
<p>Now, we can replace <span class="math inline">\(\V(\b\eps|\b X)\)</span> with the clustered sampling variance matrix of errors:</p>
<p><span class="math display">\[
\V(\hat{\b\beta}|\b X) = (\b{X^\top X})^{-1}\b X^\top \begin{pmatrix}
\b\Sigma_1 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \b\Sigma_2 &amp; &amp; 0 \\
\vdots &amp; 0 &amp; \ddots &amp; 0 \\
0 &amp; 0&amp; \dots &amp; \b\Sigma_G
\end{pmatrix} \b X(\b{X^\top X})^{-1}
\]</span></p>
<p>And thus, we have proven the OLS variance under heteroscedasticity. When we actually write the formula, we will typically replace the matrix with <span class="math inline">\(\b\Omega\)</span>.</p>
</div>
<div id="tabset-4-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-4-3-tab">
<p>This type of standard errors is quite complicated, so we usually trust the computer to compute it for us.</p>
<div id="def-hacse" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.13 (HAC Variance of OLS)</strong></span> Heteroscedasticity and Autocorrelation (HAC) variance is given by:</p>
<p><span class="math display">\[
\V(\hat{\b\beta}|\b X) = (\b{X^\top X})^{-1}\b X^\top \b\Omega \b X(\b{X^\top X})^{-1}
\]</span></p>
<p>Where <span class="math inline">\(\b\Omega\)</span> is the variance-covariance matrix of the error term <span class="math inline">\(\eps_t\)</span>.</p>
</div>
<p>Obviously, we probably do not know the form of <span class="math inline">\(\b\Omega\)</span> ins our population. We can estimate <span class="math inline">\(\b X^\top \b\Omega \b X\)</span> with the Newey-West Estimator:</p>
<p><span class="math display">\[
\b X^\top \b\Omega \b X = \frac{1}{n}\sum\limits_{t=1}^n \eps_t^2 \b x_t \b x_t^\top + \frac{1}{n}\sum\limits_{\ell = 1}^L \sum\limits_{t = \ell + 1}^n w_\ell\eps_t\eps_{t-\ell}(\b x_t \b x_{t-\ell}^\top + \b x_{t-\ell} \b x_t^\top)
\]</span></p>
<p>Where <span class="math inline">\(w_\ell\)</span> is given by:</p>
<p><span class="math display">\[
w_\ell = 1 - \frac{\ell}{L + 1}
\]</span></p>
<p>Where <span class="math inline">\(L\)</span> specifies the maximum lag considered for the control of autocorrelation (so how many previous residuals from previous time periods are correlated with <span class="math inline">\(\eps_t\)</span>). A common choice for <span class="math inline">\(L\)</span> is <span class="math inline">\(n^{1/4}\)</span>.</p>
<p>We will not prove this, as it is a very complicated estimator.</p>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="generalised-least-squares" class="level2">
<h2 class="anchored" data-anchor-id="generalised-least-squares">Generalised Least Squares</h2>
<p>We mentioned that if spherical errors (<a href="#def-sphericalerrors" class="quarto-xref">definition&nbsp;<span>4.6</span></a>) is violated, OLS is no longer the linear unbiased estimator with the least variance. Instead, another estimator, the Generalised Least Squares estimator, is the best linear unbiased estimator.</p>
<p>In the generalised least squares estimator, we assume that the variance-covariance is</p>
<p><span id="eq-glsvarform"><span class="math display">\[
\V(\b\eps | \b X) = \E(\b{\eps\eps^\top}) = \sigma^2 \b\Omega
\tag{4.4}\]</span></span></p>
<p>Where <span class="math inline">\(\sigma^2\)</span> is an unknown scalar constant, but <span class="math inline">\(\b\Omega\)</span> is a known matrix that is equivalent to the population variance-covariance matrix of errors. The variance is equivalent to <span class="math inline">\(\E(\b{\eps\eps^\top})\)</span> because we assume by strict exogeneity (<a href="#def-strictexog" class="quarto-xref">definition&nbsp;<span>4.4</span></a>) that <span class="math inline">\(\E(\b\eps) = 0\)</span>.</p>
<p>Let us define a matrix <span class="math inline">\(\b\Omega^{-1/2}\)</span>, which will be the inverse of the square root of <span class="math inline">\(\b\Omega\)</span>. This means that the following should be true:</p>
<p><span id="eq-glsproperty"><span class="math display">\[
\b\Omega^{-1/2} \ \b\Omega \ {\b\Omega^{-1/2}}^\top = \b I
\tag{4.5}\]</span></span></p>
<p>We multipy <span class="math inline">\(\b\Omega^{-1/2}\)</span> to all terms of model <span class="math inline">\(\b y = \b{X\beta} + \b\eps\)</span> to get a transformed model</p>
<p><span id="eq-glstransformation"><span class="math display">\[
\underbrace{\b\Omega^{-1/2}}_{\b y^*} \b y = \underbrace{\b\Omega^{-1/2} \b X}_{\b X^*} \b \beta + \underbrace{\b\Omega^{-1/2} \b \eps}_{\b \eps^*}
\tag{4.6}\]</span></span></p>
<p>This transformed model meets spherical errors, which we can prove by plugging in the definition of <span class="math inline">\(\b\eps^*\)</span> from above, and the definition of <span class="math inline">\(\E(\b{\eps\eps^\top})\)</span> from <a href="#eq-glsvarform" class="quarto-xref">eq.&nbsp;<span>4.4</span></a>:</p>
<p><span class="math display">\[
\begin{align}
\V (\b\eps^* | \b X) &amp; = \E(\b\eps^* \b\eps^{*\top}) \\
&amp; = \E(\b\Omega^{-1/2} \b \eps \b\eps^\top {\b\Omega^{-1/2}}^\top) \\
&amp; = \b\Omega^{-1/2} \E(\b{\eps \eps^\top}) \b\Omega^{-1/2} \\
&amp; = \b\Omega^{-1/2} \sigma^2 \b\Omega \b\Omega^{-1/2}
\end{align}
\]</span></p>
<p>And by moving scalar <span class="math inline">\(\sigma^2\)</span> to the front, and using the property from <a href="#eq-glsproperty" class="quarto-xref">eq.&nbsp;<span>4.5</span></a>, we get:</p>
<p><span class="math display">\[
\V (\b\eps^* | \b X) = \sigma^2 \underbrace{\b\Omega^{-1/2} \b\Omega \b\Omega^{-1/2}}_{\b I} = \sigma^2 \b I
\]</span></p>
<p>Thus proving this transformed model meets the spherical errors assumption (<a href="#def-sphericalerrors" class="quarto-xref">definition&nbsp;<span>4.6</span></a>). Thus, we can use OLS on this transformed model, and it will be the best linear unbiased estimator. Our OLS estimator (<a href="#def-ols" class="quarto-xref">definition&nbsp;<span>4.8</span></a>) of the transformed model will be:</p>
<p><span class="math display">\[
\hat{\b\beta} = (\b X^{*\top} \b X^*)^{-1} \b X^{*\top} \b y^*
\]</span></p>
<p>And if we plug in our definitions of <span class="math inline">\(\b y^*\)</span>, and <span class="math inline">\(\b X^*\)</span> from <a href="#eq-glstransformation" class="quarto-xref">eq.&nbsp;<span>4.6</span></a>, we can get</p>
<p><span class="math display">\[
\hat{\b\beta} = \left[(\b\Omega^{-1/2} \b X)^\top (\b\Omega^{-1/2} \b X) \right]^{-1} (\b\Omega^{-1/2} \b X) (\b\Omega^{-1/2} \b y)
\]</span></p>
<p>And using the properties of matrix transposes, and that <span class="math inline">\(\b\Omega^{-1/2} \b\Omega^{-1/2} = \b\Omega^{-1}\)</span>, we can get</p>
<p><span class="math display">\[
\begin{align}
\hat{\b\beta} &amp; = [\b X^\top \b\Omega^{-1/2} \b\Omega^{-1/2} \b X]^{-1} \b X^\top \b\Omega^{-1/2} \b\Omega^{-1/2} \b y \\
&amp; = (\b X^\top \b\Omega^{-1} \b X)^{-1} \b X^\top \b\Omega^{-1} \b y
\end{align}
\]</span></p>
<div id="def-gls" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 4.14 (Generalised Least Squares Estimator)</strong></span> The GLS estimator is</p>
<p><span class="math display">\[
\hat{\b\beta} = (\b X^\top \b\Omega^{-1} \b X)^{-1} \b X^\top \b\Omega^{-1} \b y
\]</span></p>
<p>Where <span class="math inline">\(\b\Omega\)</span> is the population variance-covariance matrix of errors. The variance is</p>
<p><span class="math display">\[
\V\hat{\b\beta} = (\b X^\top \b\Omega^{-1} \b X)^{-1}
\]</span></p>
</div>
<p><br></p>
<p>The obvious issue is that we do not generally know the form of <span class="math inline">\(\b\Omega\)</span>. This means the theoretical GLS estimator is often not feasible. Instead, we will use another estimator, called the Feasible Generalised Least Squares (FGLS) estimator.</p>
<p>The only times when GLS is feasible is when we are confident we have a specific form of autocorrelation (such as AR(1), MA(1), which we will cover in the stochastic processes chapter). This is because the covariance-variance matrix is known for these processes, so we can directly use them in GLS.</p>
<p><br></p>
</section>
<section id="feasible-generalised-least-squares" class="level2">
<h2 class="anchored" data-anchor-id="feasible-generalised-least-squares">Feasible Generalised Least Squares</h2>
<p>The issue with GLS is that we do not know the form of <span class="math inline">\(\b\Omega\)</span>. Thus, the feasible generalised least squares estimator estimates <span class="math inline">\(\hat{\b\Omega}\)</span>, before estimating the GLS estimator.</p>
<p>There are a few ways we can go about doings this, including the Cochrane-Orcutt Estimator, the Weighted Least Squares estimator, and the 2-stage GLS estimator:</p>
<div class="tabset-margin-container"></div><div class="panel-tabset nav-pills">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-5-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-1" role="tab" aria-controls="tabset-5-1" aria-selected="true">Cochrane-Orcutt</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-5-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-2" role="tab" aria-controls="tabset-5-2" aria-selected="false">Weighted Least Squares</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-5-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-3" role="tab" aria-controls="tabset-5-3" aria-selected="false">2-Stage FGLS</a></li></ul>
<div class="tab-content nav-pills">
<div id="tabset-5-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-5-1-tab">
<p>The Cochrane-Orcutt Estimator is a way to estimate GLS assuming autocorrelation of a specific form (Autoregressive 1). Let us start off with a linear model:</p>
<p><span class="math display">\[
Y_t = \beta_0 + \beta_1 X_{t}  + \eps_t
\]</span></p>
<p>Let us say some autocorrelation is present - which means the error term <span class="math inline">\(\eps_t\)</span> is related to some other error term of another observation. More specifically, let us assume an autoregressive order 1 autocorrelation, which means that <span class="math inline">\(\eps_t\)</span> is correlated with the error term of the time period before, <span class="math inline">\(\eps_{t-1}\)</span>. We can model this as:</p>
<p><span class="math display">\[
\eps_t = \rho \eps_{t-1} + u_t
\]</span></p>
<p>Where <span class="math inline">\(\rho\)</span> is the coefficient describing the correlation between <span class="math inline">\(\eps_t\)</span> and <span class="math inline">\(\eps_{t-1}\)</span>, and <span class="math inline">\(u_t\)</span> is the error term of this smaller model that is the part of <span class="math inline">\(\eps_t\)</span> that is not explained by <span class="math inline">\(\eps_{t-1}\)</span>.</p>
<p>Thus, our true linear model is:</p>
<p><span class="math display">\[
Y_t = \beta_0 + \beta_1 X_{t} + \rho \eps_{t-1} + u_t
\]</span></p>
<p>If we could get the <span class="math inline">\(\rho\eps_{t-1}\)</span> term out of this equation, we will no longer have any autocorrelation, since <span class="math inline">\(u_t\)</span> is not correlated/explained by past error terms.</p>
<p>Consider the linear model for <span class="math inline">\(Y_{t-1}\)</span>:</p>
<p><span class="math display">\[
Y_{t-1} = \beta_0 + \beta_1 X_{t-1}+ \eps_{t-1}
\]</span></p>
<p>Now, let us multiply both sides (every term) with parameter <span class="math inline">\(\rho\)</span>:</p>
<p><span class="math display">\[
\rho Y_{t-1} = \rho\beta_0 + \rho\beta_1 X_{t-1} + \rho\eps_{t-1}
\]</span></p>
<p>Now, let us subtract our model for <span class="math inline">\(\rho Y_{t-1}\)</span> from our original <span class="math inline">\(Y_t\)</span>:</p>
<p><span class="math display">\[
\begin{array}{ccccccc}
Y_t &amp; = &amp; \beta_0 &amp; + &amp; \beta_1X_t &amp; + &amp; \rho\eps_{t-1} + u_t \\
\rho Y_{t-1} &amp; = &amp; \rho\beta_0 &amp; + &amp; \rho\beta_1X_{t-1} &amp; + &amp; \rho\eps_{t-1} \\
\hline
Y_t - \rho Y_{t-1} &amp; = &amp; \beta_0(1-\rho) &amp; + &amp; \beta_1(X_t - \rho X_{t-1}) &amp; + &amp; u_t
\end{array}
\]</span></p>
<p>Now we can see we have a new transformed model with only error term <span class="math inline">\(u_t\)</span> which is not autocorrelated with <span class="math inline">\(t-1\)</span>.</p>
<p><span class="math display">\[
\underbrace{Y_t - \rho Y_{t-1}}_{Y_t^*} = \underbrace{\beta_0(1-\rho)}_{\beta_0^*} + \beta_1 \underbrace{(X_t - \rho X_{t-1})}_{X_t^*} + \underbrace{u_t}_{\eps_t^*}
\]</span></p>
<p>Which we can rewrite more simply as:</p>
<p><span class="math display">\[
Y_t^* = \beta_0^* + \beta_1 X_t^* + \eps_t^*
\]</span></p>
<p>Since this model no longer has autocorrelation and now meets spherical errors, we can use the OLS estimator on this transformed model, and this will be the best linear unbiased estimator.</p>
</div>
<div id="tabset-5-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-5-2-tab">
<p>The weighted least squares (WLS) estimator is a FGLS when we have only conditional heteroscedasticity, and we believe that conditional heteroscedasticity to be in a specific form:</p>
<p><span id="eq-wlsvariance"><span class="math display">\[
\V(\eps_t|\b X) = \sigma^2 \  \Omega(X_t)
\tag{4.7}\]</span></span></p>
<p>Where <span class="math inline">\(\sigma^2\)</span> is some constant (can be 1) and <span class="math inline">\(\Omega(X_t)\)</span> is some function of <span class="math inline">\(X_t\)</span> that explained the difference in error variance between individuals.</p>
<p>Now, consider the variance of this (modified) error term that is the original error <span class="math inline">\(\eps_t\)</span> divided by the square root <span class="math inline">\(\Omega(X_t)\)</span>, which is a function of <span class="math inline">\(X_t\)</span>:</p>
<p><span class="math display">\[
\V \left( \frac{1}{\sqrt{\Omega(X_t)}}\eps_t \biggr| X_t \right)
\]</span></p>
<p>Using <a href="random.html#thm-variance" class="quarto-xref">theorem&nbsp;<span>1.2</span></a>, we know <span class="math inline">\(\V(cu) = c^2 \V(u)\)</span> if <span class="math inline">\(c\)</span> is a constant and <span class="math inline">\(u\)</span> is a random variable. This function also applies to a function <span class="math inline">\(a(x)\)</span> where <span class="math inline">\(\V(a(x) u) = a(x)^2 \V(u)\)</span>. Using this, we can determine the variance of the modified error term is equal to</p>
<p><span class="math display">\[
\V \left( \frac{1}{\sqrt{\Omega(X_t)}}\eps_t \biggr| X_t \right) = \left(\frac{1}{\sqrt{\Omega(X_t)}}\right)^2 \V(\eps_t | X_t)
\]</span></p>
<p>And from <a href="#eq-wlsvariance" class="quarto-xref">eq.&nbsp;<span>4.7</span></a>, we can plug in <span class="math inline">\(\V(\eps_t|X_t) = \sigma^2 \ \Omega(X_t)\)</span> to get</p>
<p><span class="math display">\[
\V \left( \frac{1}{\sqrt{\Omega(X_t)}}\eps_t \biggr| X_t \right) = \frac{1}{\Omega(X_t)}\sigma^2 \ \Omega(X_t) = \sigma^2
\]</span></p>
<p>What does this tell us? Well it tells us the modified error term <span class="math inline">\(\frac{1}{\sqrt{\Omega(X_i)}} \eps_t\)</span> has a variance of constant <span class="math inline">\(\sigma^2\)</span> for all units <span class="math inline">\(i\)</span>, which does not dependent on <span class="math inline">\(X_i\)</span>. What does this mean? Well, our modified error term is now meeting homoscedasticity (<a href="#def-sphericalerrors" class="quarto-xref">definition&nbsp;<span>4.6</span></a>)!</p>
<p>However, we obviously cannot just divide the error term by <span class="math inline">\(1/\sqrt{\Omega(X_t)}\)</span> - that changes our linear model. What we can though do is divide every term of our linear model:</p>
<p><span class="math display">\[
\frac{Y_t}{\sqrt{\Omega(X_t)}} = \beta_0\left(\frac{1}{\sqrt{\Omega(X_t)}}\right) + \beta_1 \left(\frac{X_{t1}}{\sqrt{\Omega(X_t)}}\right) + \dots +  \frac{\eps_t}{\sqrt{\Omega(X_t)}}
\]</span></p>
<p>And since we divide both side by <span class="math inline">\(1/\sqrt{\Omega(X_t)}\)</span>, and our model is conditional on individual <span class="math inline">\(t\)</span> (see all the subscripts), that means this model is still “equivalent” to our original linear model.</p>
<p>Thus, the idea of weighted least squares is to “transform” our heteroscedastic linear model into one that meets homoscedasitcity. We can then just use OLS on our new homoscedastic regression, and since homoscedsaticity is met, Gauss-Markov (<a href="#thm-gaussmarkov" class="quarto-xref">theorem&nbsp;<span>4.5</span></a>) is met, and our estimator is once again the best linear unbiased estimator.</p>
</div>
<div id="tabset-5-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-5-3-tab">
<p>The previous two methods assumed we know some information about the variance-covariance errors matrix (is there only autocorrelation, or only heteroscedasticity, etc.).</p>
<p>If we do not konw anything, we can still do feasible GLS with a 2-stage process.</p>
<p>We typically produce a estimate <span class="math inline">\(\hat{\b\Omega}\)</span> by first running an OLS regression, in which we will obtain the residuals <span class="math inline">\(\hat\eps_i\)</span>. These can be used to estimate the structure of <span class="math inline">\(\b\Omega\)</span>, producing <span class="math inline">\(\hat{\b\Omega}\)</span>. Then, using this estimate <span class="math inline">\(\hat{\b\Omega}\)</span>, we can run feasible GLS, and obtain an estimator with less error.</p>
<p>However, this specific form has many drawbacks. When we estimate <span class="math inline">\(\hat{\b\Omega}\)</span> with OLS (or any other method), we of course have some imprecision in our estimates. Econometricians have shown that the 2-stage feasible GLS estimator often is far worse than the hypothetical perfect GLS. Very often, feasible GLS will actually result in larger variances of estimates.</p>
<p>There is also some risk with 2-stage feasible GLS. Often, heteroscedasticity and autocorrelation occur in our estimated OLS models not because the population actually has heteroscedasticity or autocorrelation, but rather, our original linear model is missing some explanatory variables which causes other violations in our classical linear model, such as exogeneity violations. This mispecified nature will not only make FGLS even more imprecise, but also has the potential to bias FGLS estimates.</p>
<p>Thus, 2-stage FGLS is not super popular in most applied statistician’s toolkit, and the default tends to be sticking to OLS with either robust standard errors or Heteroscedasticity-and-autocorrelation (HAC) robust standard errors.</p>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="instrumental-variables-estimator" class="level2">
<h2 class="anchored" data-anchor-id="instrumental-variables-estimator">Instrumental Variables Estimator</h2>
<p>One of the assumptions in the classical model is exogeneity (<a href="#def-strictexog" class="quarto-xref">definition&nbsp;<span>4.4</span></a>). This assumption is critical in the proofs of OLS unbiasedness and asymptotic consistency. This implies that when exogeneity is violated, our estimates of <span class="math inline">\(\hat\beta\)</span> become unrealiable.</p>
<p>The <strong>instrumental variables estimator</strong> is a solution to this issue. The idea is to find a third variable (or more) <span class="math inline">\(Z\)</span>, that does meet this condition of exogeneity:</p>
<p><span id="eq-instrumentexog"><span class="math display">\[
\E(\b{Z^\top \eps}) = 0
\tag{4.8}\]</span></span></p>
<p>and we will have no exogeneity if <span class="math inline">\(Z\)</span> is not correlated with the error term. We then use these instruments <span class="math inline">\(Z\)</span> to predict <span class="math inline">\(X\)</span>, which will get us the parts of <span class="math inline">\(X\)</span> that are explained by <span class="math inline">\(Z\)</span> (and thus, uncorrelated with the error term). Then, we can use that exogenous part of <span class="math inline">\(X\)</span> to estimate the relationship with <span class="math inline">\(Y\)</span>. However, this hinges on <span class="math inline">\(Z\)</span> meeting that moments condition.</p>
<div id="def-instrumentassumptions" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 4.15 (Assumptions of Instruments)</strong></span> For instrument(s) <span class="math inline">\(Z\)</span> to meet the moment condition <span class="math inline">\(\E(\b{Z^\top \eps}) = 0\)</span>, the following facts must be true:</p>
<ol type="1">
<li><span class="math inline">\(Z\)</span> must be exogenous/ignorable, i.e.&nbsp;<span class="math inline">\(Cov(Z, \eps) = 0\)</span>.</li>
<li><span class="math inline">\(Z\)</span> must be relevant, i.e.&nbsp;<span class="math inline">\(Cov(Z, X) ≠ 0\)</span>.</li>
<li><span class="math inline">\(Z\)</span> must meet the exclusions restriction (which is implied by exogenous). This means that <span class="math inline">\(Z\)</span> cannot have an independent effect on <span class="math inline">\(Y\)</span>, outside of its impact on <span class="math inline">\(Y\)</span> through <span class="math inline">\(X\)</span>.</li>
</ol>
</div>
<p><br></p>
<p>Let us derive the IV estimator (and an alternative IV estimator called 2SLS), and explore the asymptotic properties of this estimator.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset nav-pills">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-6-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-6-1" role="tab" aria-controls="tabset-6-1" aria-selected="true">IV Estimator</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-6-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-6-2" role="tab" aria-controls="tabset-6-2" aria-selected="false">2SLS</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-6-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-6-3" role="tab" aria-controls="tabset-6-3" aria-selected="false">Asymptotics of IV</a></li></ul>
<div class="tab-content nav-pills">
<div id="tabset-6-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-6-1-tab">
<p>So we know what an instrument <span class="math inline">\(Z\)</span> is, and the requirements for <span class="math inline">\(Z\)</span> to be a valid instrument. But how does <span class="math inline">\(Z\)</span> help us estimate the effect between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>?</p>
<p>Our moment condition and sample equivalent for instrumental variables is:</p>
<p><span class="math display">\[
\E(\b{Z^\top \eps}) \approx \b Z^\top \b\eps = 0
\]</span></p>
<p>Now, we know that <span class="math inline">\(\b\eps = \b y - \hat{\b y}\)</span> and <span class="math inline">\(\hat{\b y} = \b{X\hat\beta}\)</span>, so let us plug that in:</p>
<p><span class="math display">\[
\begin{align}
\b Z^\top (\b y - \hat{\b y}) &amp; = 0 \\
\b Z^\top (\b y - \b{X \hat\beta}) &amp; = 0 \\
\b{Z^\top y} - \b{Z^\top X \hat\beta} &amp; = 0
\end{align}
\]</span></p>
<p>And now, solving for <span class="math inline">\(\hat{\b\beta}\)</span> with matrix inversion, we get:</p>
<p><span class="math display">\[
\begin{align}
- \b{Z^\top X \hat\beta} &amp; = - \b{Z^\top y} \\
\b{Z^\top X \hat\beta} &amp; = \b{Z^\top y} \\
\hat{\b\beta} &amp; = (\b{Z^\top X})^{-1} \b{Z^\top y}
\end{align}
\]</span></p>
<div id="def-ivestimator" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 4.16 (Instrumental Variables Estimator)</strong></span> The IV estimator produces the estimates:</p>
<p><span class="math display">\[
\hat{\b\beta}^* = (\b{Z^\top X})^{-1} \b{Z^\top y}
\]</span></p>
</div>
<p><br></p>
<p>Note that the IV estimator is <strong>biased</strong> in small sample sizes, and only asymptotically consistent (proof provided in the third tab). Thus, we should be careful when using IV in small sample sizes.</p>
<p>It can also be shown that the asymptotic variance of the instrumental variables estimator, under the assumption of spherical errors is</p>
<p><span class="math display">\[
\V \hat{\b\beta}^* = \sigma^2(\b{Z^\top X})^{-1} \b{Z^\top Z} (\b{X^\top Z})^{-1}
\]</span></p>
<p>Although the proof of this is beyond the scope of this chapter. There is also a more complex derivation for robust standard errors with instrumental variables estimation.</p>
</div>
<div id="tabset-6-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-6-2-tab">
<p>While sometimes we do compute the instrumental variables estimator as shown in <a href="#def-ivestimator" class="quarto-xref">definition&nbsp;<span>4.16</span></a>, often, we use another estimator, the two-stage-least-squares (2SLS) estimator.</p>
<p>The 2SLS estimator is based on the intuitive interpretation of instrumental variables: that we use only the part of <span class="math inline">\(X\)</span> explained by <span class="math inline">\(Z\)</span> (which should be endogenous), and estimate that part of <span class="math inline">\(X\)</span>’s relationship with <span class="math inline">\(Y\)</span>.</p>
<p>The two stage least squares estimator follows this exact procedure. In the first stage, we find the part of <span class="math inline">\(X\)</span> that is explained by <span class="math inline">\(Z\)</span>, which we call <span class="math inline">\(\hat X_t\)</span>:</p>
<p><span class="math display">\[
\hat X_t = \hat\beta_0 + \hat\beta_1 Z_t
\]</span></p>
<p>We do this by running a linear OLS model with <span class="math inline">\(X_t\)</span> as the outcome variable, and <span class="math inline">\(Z_t\)</span> as the explanatory variable. Our predicted <span class="math inline">\(\hat X_t\)</span> will be the part of <span class="math inline">\(X\)</span> explained by instrument <span class="math inline">\(Z\)</span>.</p>
<p>Then, for the second stange, we take this <span class="math inline">\(\hat X_t\)</span>, and use it in a model with <span class="math inline">\(Y\)</span> as follows:</p>
<p><span class="math display">\[
Y_t = \delta_0 + \delta_1 \hat X_t + \eps_t
\]</span></p>
<p>How is 2SLS in these two stages equal to instrumental variables estimator we derived in <a href="#def-ivestimator" class="quarto-xref">definition&nbsp;<span>4.16</span></a>? Our estimates (in matrix form) for the second stage would be:</p>
<p><span id="eq-2slshat"><span class="math display">\[
\hat{\b\beta}_{2SLS} = (\hat{\b X}^\top \hat{\b X})^{-1} \hat{\b X}^\top \b y
\tag{4.9}\]</span></span></p>
<p>Where <span class="math inline">\(\hat{\b X}\)</span> is given as the fitted values of the first stage:</p>
<p><span class="math display">\[
\hat{\b X} = \b Z \hat{\b \delta} = \b Z (\b{Z^\top Z})^{-1}\b{Z^\top X}
\]</span></p>
<p>We can plug <span class="math inline">\(\hat{\b X}\)</span> into <a href="#eq-2slshat" class="quarto-xref">eq.&nbsp;<span>4.9</span></a> to get:</p>
<p><span class="math display">\[
\hat{\b\beta}_{2SLS} = [(\b Z (\b{Z^\top Z})^{-1}\b{Z^\top X})^\top (\b Z (\b{Z^\top Z})^{-1}\b{Z^\top X})]^{-1}(\b Z (\b{Z^\top Z})^{-1}\b{Z^\top X})^\top \b y
\]</span></p>
<p>Using the properties of matrix transposes, we get:</p>
<p><span class="math display">\[
\begin{align}
\hat{\b\beta}_{2SLS} &amp; = [(\b X^\top \b Z (\b{Z^\top Z})^{-1} \b Z^\top) (\b Z (\b{Z^\top Z})^{-1}\b{Z^\top X})]^{-1}(\b X^\top \b Z (\b{Z^\top Z})^{-1} \b Z^\top)\b y \\
&amp; = [\b X^\top \b Z \underbrace{(\b{Z^\top Z})^{-1} \b Z^\top \b Z}_{\text{inverses cancel}} (\b{Z^\top Z})^{-1}\b{Z^\top X}]^{-1}\b X^\top \b Z (\b{Z^\top Z})^{-1} \b Z^\top\b y \\
&amp; = (\underbrace{\b X^\top \b Z (\b{Z^\top Z})^{-1}}_{\text{cancel}}\b{Z^\top X})^{-1}\underbrace{\b X^\top \b Z (\b{Z^\top Z})^{-1}}_{\text{cancel}} \b Z^\top\b y \\
&amp; = \underbrace{(\b{Z^\top X})^{-1} \b{Z^\top y}}_{\text{equal to IV}}
\end{align}
\]</span></p>
<p>And the second to last step is possible because one of the highlighted “cancel” parts is within an inverse and the other isn’t, so they cancel. Thus, we have shown 2SLS is equivalent to the instrumental variables estimator.</p>
</div>
<div id="tabset-6-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-6-3-tab">
<p>Despite being biased, the instrumental variables estimator is an asymptotically consistent estimator of <span class="math inline">\(\b\beta\)</span> even with exogeneity between <span class="math inline">\(X\)</span> and <span class="math inline">\(\eps\)</span> violated (as long as <span class="math inline">\(Z\)</span> is exogenous).</p>
<div id="thm-ivconsistency" class="theorem" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Theorem 4.7 (IV Consistency)</strong></span> The IV estimator is asymptotically consistent, meaning</p>
<p><span class="math display">\[
\mathrm{plim}\hat{\b\beta}^* = \b\beta
\]</span></p>
</div>
<p><br></p>
<p><strong>Proof</strong>: Let us first re-arrange the IV estimator by plugging in the original model <span class="math inline">\(\b y = \b{X\beta} + \b\eps\)</span>, and simplify:</p>
<p><span class="math display">\[
\begin{align}
\hat{\b\beta}^* &amp; = (\b{Z^\top X})^{-1} \b{Z^\top y} \\
&amp; = (\b{Z^\top X})^{-1} \b{Z^\top}(\b{X\beta} + \b \eps) \\
&amp; = \underbrace{(\b{Z^\top X})^{-1} \b{Z^\top X}}_{\text{inverses cancel}}\b\beta + (\b{Z^\top X})^{-1} \b{Z^\top\eps} \\
&amp; = \b\beta + (\b{Z^\top X})^{-1} \b{Z^\top\eps}
\end{align}
\]</span></p>
<p>We can rewrite our matrices in the form of vectors, scalars, and summation:</p>
<p><span class="math display">\[
\begin{align}
\hat{\b\beta}^* &amp; = \b\beta + (\b{Z^\top X})^{-1} \b{Z^\top\eps} \\
&amp; = \b\beta + \left(\sum\limits_{t=1}^n \b z_t \b x_t^\top \right)^{-1} \left(\sum\limits_{t=1}^n \b z_t \eps_t \right)
\end{align}
\]</span></p>
<p>Now, let us do a little algebra trick as follows:</p>
<p><span class="math display">\[
\hat{\b\beta}^* = \b\beta + \left(\frac{1}{n}\sum\limits_{t=1}^n \b z_t \b x_t^\top \right)^{-1} \left(\frac{1}{n}\sum\limits_{t=1}^n \b z_t \eps_t \right)
\]</span></p>
<p>The reason we can do this is because the first <span class="math inline">\(\frac{1}{n}\)</span> is inversed as <span class="math inline">\(\frac{1}{n}^{-1}\)</span>, so this cancels out the second one, maintaining the equality of our equation.</p>
<p>Now, we want to prove <span class="math inline">\(\mathrm{plim}\hat{\b\beta}^* = \b\beta\)</span>, so let us take the probability limit of both sides:</p>
<p><span class="math display">\[
\mathrm{plim}\hat{\b\beta}^* = \mathrm{plim}\b\beta + \left(\mathrm{plim}\frac{1}{n}\sum\limits_{t=1}^n \b z_t \b x_t^\top \right)^{-1} \left(\mathrm{plim}\frac{1}{n}\sum\limits_{t=1}^n \b z_t \eps_t \right)
\]</span></p>
<p>We know the probability limit of a constant is itself. Look at the other two terms on the right: they take the form of sample averages <span class="math inline">\(\frac{1}{n}\sum\)</span>. Using the law of large numbers (<a href="inference.html#thm-lawoflargenumbers" class="quarto-xref">theorem&nbsp;<span>2.1</span></a>), we can simplify to:</p>
<p><span class="math display">\[
\mathrm{plim}\hat{\b\beta}^*  = \b\beta + (\E(\b z_t \b x_t^\top))^{-1} \underbrace{\E(\b z_t \eps_t)}_{= \ 0} = \b\beta
\]</span></p>
<p>And we know <span class="math inline">\(\E(\b z_t \eps_t) = 0\)</span> because this is the same condition as the moment condition <span class="math inline">\(\E(\b{Z^\top \eps}) = 0\)</span>, just written in terms of individual observations. Thus, the Instrumental variables estimator is asymptotically consistent.</p>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="statistical-inference" class="level2">
<h2 class="anchored" data-anchor-id="statistical-inference">Statistical Inference</h2>
<p>Standard errors are by definition, the square root of the variance of the estimator, which we derived for both OLS under spherical errors, and OLS under non-spherical errors.</p>
<p>There is an issue though: <span class="math inline">\(\sigma^2\)</span> is the population variance of error term <span class="math inline">\(\eps_i\)</span>, and appears in the OLS variance under spherical errors. But we don’t know this population value. Thus, we will need an estimator <span class="math inline">\(s^2\)</span> that will estimate <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[
s^2 = \frac{\b{\hat\eps^\top \hat\eps}}{n - p-1} = \frac{\sum_{t=1}^n \hat\eps_t^2}{n-p-1}
\]</span></p>
<p>Where <span class="math inline">\(\hat{\b\eps}\)</span> are equal to <span class="math inline">\(\b y - \hat{\b y}\)</span>, and can be calculated with residual maker <span class="math inline">\(\b M\)</span> as shown in <a href="#eq-calcresiduals" class="quarto-xref">eq.&nbsp;<span>4.1</span></a>. <span class="math inline">\(n\)</span> is the size of our sample, and <span class="math inline">\(p\)</span> is the number of explanatory variables we have. We will not prove it here, but this is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span></p>
<p>For OLS variance in conditional heteroscedasticity (robust), we have the unknown population term <span class="math inline">\(\sigma^2_i\)</span>, which we estimate with <span class="math inline">\(s^2_i\)</span>:</p>
<p><span class="math display">\[
\sigma^2_i \approx s^2_i = \hat\eps_i^2
\]</span></p>
<p>However, our estimate <span class="math inline">\(s^2\)</span> and <span class="math inline">\(s_i^2\)</span> has an implication - every estimator has variance and uncertainty.</p>
<p>Under the central limit theorem (<a href="inference.html#thm-clt" class="quarto-xref">theorem&nbsp;<span>2.2</span></a>), our standardised sampling distribution of <span class="math inline">\(\hat\beta_j\)</span> should be normally distributed. However, because we are estimating <span class="math inline">\(\sigma^2\)</span> with <span class="math inline">\(s^2\)</span>, this uncertainty in estimates <span class="math inline">\(s^2\)</span> means we cannot use the normal distribution as given by the central limit theorem. Instead, we use a t-distribution to account for the uncertainty.</p>
<p>Once we have our correct standard errors, we can conduct hypothesis testing. There are two main hypothesis tests: the t-test for single parameters, and the f-test for multiple parameters or comparing models:</p>
<div class="tabset-margin-container"></div><div class="panel-tabset nav-pills">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-7-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-7-1" role="tab" aria-controls="tabset-7-1" aria-selected="true">T-Test</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-7-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-7-2" role="tab" aria-controls="tabset-7-2" aria-selected="false">F-Test</a></li></ul>
<div class="tab-content nav-pills">
<div id="tabset-7-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-7-1-tab">
<p>The t-test is a way to test single parameters in our linear model. Typically, our hypothesis are the following:</p>
<ul>
<li><span class="math inline">\(H_0: \beta_j = 0\)</span>, which states that there is no relationship between <span class="math inline">\(X_j\)</span> and <span class="math inline">\(Y\)</span>.</li>
<li><span class="math inline">\(H_1: \beta_j ≠ 0\)</span>, which states that there is a relationship between <span class="math inline">\(X_j\)</span> and <span class="math inline">\(Y\)</span>.</li>
</ul>
<div id="def-ttest" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 4.17 (T-Test)</strong></span> In our t-test, our test statistic is the t-statistic.</p>
<p><span class="math display">\[
t = \frac{\hat\beta_j - \text{null value}}{se(\hat\beta_j)}
\]</span></p>
<p>And we will conduct a hypothesis test using the <span class="math inline">\(t\)</span>-distribution with <em>degrees of freedom</em> of <span class="math inline">\(n-p-1\)</span>.</p>
</div>
<p><br></p>
<p>We then calculate a p-value. The procedure of conducting a hypothesis test was outlined in the <a href="#inference.qmd#hypothesis-testing">last chapter</a>. If our p-value is statistically significant, we can conclude there is a significant relationship between <span class="math inline">\(X_j\)</span> and <span class="math inline">\(Y\)</span>.</p>
</div>
<div id="tabset-7-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-7-2-tab">
<p>We can also do a statistical inference test with multiple coefficients at a time with the <strong>F-test</strong>. The F-test compares two different models, a null model with less parameters, and a alternate model with all the parameters in the null and some more parameters:</p>
<ul>
<li><p><span class="math inline">\(M_0 : Y_t = \beta_0 + \sum\limits_{j=1}^g \beta_j X_{tj} + \eps_t\)</span> (the smaller null model with <span class="math inline">\(g\)</span> parameters).</p></li>
<li><p><span class="math inline">\(M_a : Y_t = \beta_0 + \sum\limits_{j=1}^g \beta_j X_{tj} + \sum\limits_{j=g+1}^p \beta_j X_{tj} \eps_t\)</span> (the bigger model with the original <span class="math inline">\(g\)</span> parameters in the null + additional parameters up to <span class="math inline">\(p\)</span>).</p></li>
</ul>
<p>Recall <span class="math inline">\(R^2\)</span> (<a href="#def-rsquared" class="quarto-xref">definition&nbsp;<span>4.11</span></a>), which is a measure of fit for our models. F-tests compare the <span class="math inline">\(R^2\)</span> of the alternate model to the null model.</p>
<div id="def-ftest" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 4.18 (F-test)</strong></span> F-tests are used to test a smaller model <span class="math inline">\(M_0\)</span> and larger model <span class="math inline">\(M_a\)</span> model. The F-test statistic is given by</p>
<p><span class="math display">\[
F = \frac{(SSR_0 - SSR_a) / (p_a - p_0)}{SSR_a /(n-p_A - 1)}
\]</span></p>
<p>Where <span class="math inline">\(SSR\)</span> represents the sum of squared residuals, <span class="math inline">\(p\)</span> represents the number of parameters in each model, and <span class="math inline">\(n\)</span> is the sample size (should be the same between both <span class="math inline">\(M_0\)</span> and <span class="math inline">\(M_a\)</span>).</p>
<p>We then consult a F-distribution with <span class="math inline">\(p_1 - p_0\)</span> and <span class="math inline">\(n - p_a - 1\)</span> degrees of freedom.</p>
</div>
<p><br></p>
<p>We then calculate a p-value. The procedure of conducting a hypothesis test was outlined in the <a href="#inference.qmd#hypothesis-testing">last chapter</a>.</p>
<p>If our result is statistically significant, then the alternative model <span class="math inline">\(M_a\)</span> is a statistically significantly better model than <span class="math inline">\(M_0\)</span>, and the extra parameters in <span class="math inline">\(M_a\)</span> are jointly significant.</p>
</div>
</div>
</div>
<p><br></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./mle.html" class="pagination-link" aria-label="Maximum Likelihood">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Maximum Likelihood</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./causal.html" class="pagination-link" aria-label="Theory of Causation">
        <span class="nav-page-text"><span class="chapter-title">Theory of Causation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>