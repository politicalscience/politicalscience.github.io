<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>The Generalised Linear Model – Political Science &amp; Political Economy</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="mathjax-config.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./glm.html">2 The Generalised Linear Model</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Kevin’s PSPE Resources</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
 <span class="menu-text">Quantitative Methods</span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 The Classical Linear Model</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">2 The Generalised Linear Model</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./causal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 Causal Frameworks</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 Randomised Controlled Trials</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./soo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 Selection on Observables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./iv.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 Instrumental Variables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rd.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7 Regression Discontinuity</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./did.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8 Differences-in-Differences</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Further Statistical Methods</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./predict.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Forecasting and Prediction Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unsupervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Unsupervied Learning Methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./factor.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Factor Analysis Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./latent.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Latent Trait and Class Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sem.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Structural Equation Models</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./games.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guide to Game Theory</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./math.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Background Mathematics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Background Statistics</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On This Page</h2>
   
  <ul>
  <li><a href="#glms-and-maximum-likelihood" id="toc-glms-and-maximum-likelihood" class="nav-link active" data-scroll-target="#glms-and-maximum-likelihood"><strong>GLMs and Maximum Likelihood</strong></a>
  <ul class="collapse">
  <li><a href="#the-generalised-linear-model" id="toc-the-generalised-linear-model" class="nav-link" data-scroll-target="#the-generalised-linear-model">The Generalised Linear Model</a></li>
  <li><a href="#maximum-likelihood-estimation" id="toc-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#maximum-likelihood-estimation">Maximum Likelihood Estimation</a></li>
  <li><a href="#score-function-and-expectation" id="toc-score-function-and-expectation" class="nav-link" data-scroll-target="#score-function-and-expectation">Score Function and Expectation</a></li>
  <li><a href="#information-matrix-and-variance" id="toc-information-matrix-and-variance" class="nav-link" data-scroll-target="#information-matrix-and-variance">Fischer Information Matrix</a></li>
  <li><a href="#asymptotics-and-cramér-rao" id="toc-asymptotics-and-cramér-rao" class="nav-link" data-scroll-target="#asymptotics-and-cramér-rao">Asymptotics and Cramér-Rao</a></li>
  <li><a href="#ols-as-a-mle" id="toc-ols-as-a-mle" class="nav-link" data-scroll-target="#ols-as-a-mle">OLS as a MLE</a></li>
  <li><a href="#newton-and-fischer-algorithms" id="toc-newton-and-fischer-algorithms" class="nav-link" data-scroll-target="#newton-and-fischer-algorithms">Newton and Fischer Algorithms</a></li>
  <li><a href="#information-criterion-statistics" id="toc-information-criterion-statistics" class="nav-link" data-scroll-target="#information-criterion-statistics">Information Criterion Statistics</a></li>
  </ul></li>
  <li><a href="#logistic-regression-model" id="toc-logistic-regression-model" class="nav-link" data-scroll-target="#logistic-regression-model"><strong>Logistic Regression Model</strong></a>
  <ul class="collapse">
  <li><a href="#model-specification" id="toc-model-specification" class="nav-link" data-scroll-target="#model-specification">Model Specification</a></li>
  <li><a href="#fitted-probabilities-and-applications" id="toc-fitted-probabilities-and-applications" class="nav-link" data-scroll-target="#fitted-probabilities-and-applications">Fitted Probabilities and Applications</a></li>
  <li><a href="#interpretation-with-odds-ratios" id="toc-interpretation-with-odds-ratios" class="nav-link" data-scroll-target="#interpretation-with-odds-ratios">Interpretation with Odds Ratios</a></li>
  </ul></li>
  <li><a href="#ordinal-and-multinomial-models" id="toc-ordinal-and-multinomial-models" class="nav-link" data-scroll-target="#ordinal-and-multinomial-models"><strong>Ordinal and Multinomial Models</strong></a>
  <ul class="collapse">
  <li><a href="#ordinal-logistic-regression" id="toc-ordinal-logistic-regression" class="nav-link" data-scroll-target="#ordinal-logistic-regression">Ordinal Logistic Regression</a></li>
  <li><a href="#multinomial-logistic-regression" id="toc-multinomial-logistic-regression" class="nav-link" data-scroll-target="#multinomial-logistic-regression">Multinomial Logistic Regression</a></li>
  </ul></li>
  <li><a href="#negative-binomial-regression" id="toc-negative-binomial-regression" class="nav-link" data-scroll-target="#negative-binomial-regression"><strong>Negative Binomial Regression</strong></a>
  <ul class="collapse">
  <li><a href="#model-specification-1" id="toc-model-specification-1" class="nav-link" data-scroll-target="#model-specification-1">Model Specification</a></li>
  <li><a href="#interpreting-coefficients" id="toc-interpreting-coefficients" class="nav-link" data-scroll-target="#interpreting-coefficients">Interpreting Coefficients</a></li>
  <li><a href="#poisson-regression" id="toc-poisson-regression" class="nav-link" data-scroll-target="#poisson-regression">Poisson Regression</a></li>
  </ul></li>
  <li><a href="#statistical-inference" id="toc-statistical-inference" class="nav-link" data-scroll-target="#statistical-inference"><strong>Statistical Inference</strong></a>
  <ul class="collapse">
  <li><a href="#hypothesis-testing" id="toc-hypothesis-testing" class="nav-link" data-scroll-target="#hypothesis-testing">Hypothesis Testing</a></li>
  <li><a href="#confidence-intervals" id="toc-confidence-intervals" class="nav-link" data-scroll-target="#confidence-intervals">Confidence Intervals</a></li>
  <li><a href="#likelihood-ratio-test" id="toc-likelihood-ratio-test" class="nav-link" data-scroll-target="#likelihood-ratio-test">Likelihood Ratio Test</a></li>
  </ul></li>
  <li><a href="#implementation-in-r" id="toc-implementation-in-r" class="nav-link" data-scroll-target="#implementation-in-r"><strong>Implementation in R</strong></a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">The Generalised Linear Model</h1>
<p class="subtitle lead">Chapter 2, Quantitative Methods</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In the last chapter, we discussed the classical linear model. However, the classical linear model has a few limitations, which are addressed by the generalised linear model (GLM). In this chapter, we explore the generalised linear model, including logistic and negative binomial regression, and the maximum likelihood estimator used to estimate GLMs.</p>
<p>Use the right sidebar for quick navigation. R-code is provided at the bottom.</p>
<hr>
<section id="glms-and-maximum-likelihood" class="level1">
<h1><strong>GLMs and Maximum Likelihood</strong></h1>
<section id="the-generalised-linear-model" class="level3">
<h3 class="anchored" data-anchor-id="the-generalised-linear-model">The Generalised Linear Model</h3>
<p>There are some limitations to the <a href="./clm.html">classical linear model</a>. Consider the <a href="./clm.html#linear-probability-model">linear probability model</a>:</p>
<p><span class="math display">\[
\P(Y_i = 1|X_i)= \pi_i = \beta_0 + \beta_1X_{i1} + \dots + \beta_pX_{ip} + \eps_i
\]</span></p>
<p>The classical assumptions assume homoscedasticity. However, probability of a binary event is given by the bernoulli distribution, for which <span class="math inline">\(\V \pi_i = \pi_i(1-\pi_i)\)</span>, which is clearly a function of outcome <span class="math inline">\(\pi_i\)</span>, meaning it is homoscedastic. And more importantly, the linear model will predict probabilities <span class="math inline">\(\P(Y_i = 1 | X_i)\)</span> that are higher than 1 and less than 0, which, if we know the rules of probability, is nonsensical. This nonsensical outcome issue is also present in count and rate outcomes.</p>
<p>The generalised linear model allows us to “transform” the outcome variable (either <span class="math inline">\(Y_i\)</span> or <span class="math inline">\(\pi_i\)</span> when dealing with probabilities) through a link function <span class="math inline">\(g(\cdot)\)</span>, while maintaining the linear structure of the rest of the linear model. This allows us to deal with other distributions and avoid nonsensical predictions.</p>
<p>Of course, we have the <strong>Classical Linear Model</strong>, which is considered a GLM that has no link function:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1X_{i1} + \beta_p X_{ip} + \eps_i \ = \ x_i^\top\beta + \eps_i
\]</span></p>
<p>The <strong>Logistic Regression Model</strong> is a model that deals with probabilities <span class="math inline">\(\pi_i\)</span>, just like the linear probability model. It solves the limitations of the linear probability model through a link function:</p>
<p><span class="math display">\[
\log \left( \frac{\pi_i}{1 - \pi_i} \right) = \beta_0 + \beta_1X_{i1} + \dots + \beta_p X_{ip} \ = \ x_i^\top\beta
\]</span></p>
<p>The <strong>Negative Binomial Model</strong> (and a special case called the Poisson model) is a model that deals with count and rate data (only positive values), where <span class="math inline">\(\lambda_i\)</span> is the <span class="math inline">\(\E Y_i\)</span> for a negative binomial distribution.</p>
<p><span class="math display">\[
\log \lambda_i = \beta_0 + \beta_1X_{i1} + \dots + \beta_p X_{ip} \ = \ x_i^\top \beta
\]</span></p>
<p>We will explore each of these models in more detail later.</p>
<p><br></p>
</section>
<section id="maximum-likelihood-estimation" class="level3">
<h3 class="anchored" data-anchor-id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h3>
<p>All GLMs are estimated with an estimator called the maximum likelihood estimator (MLE). This is also one of the most used estimators in statistics for a bunch of other methods, so it is useful to understand.</p>
<p>We have a set of population parameters in the vector <span class="math inline">\(\theta\)</span> we want to estimate. This set of parameters determines our population distribution of <span class="math inline">\(Y_i\)</span>, which we can describe with some <a href="./stats.html#probability-density-functions">probability density function</a> <span class="math inline">\(\varphi(Y_i, \theta)\)</span>. For example, in linear regression, our population <span class="math inline">\(y\)</span> is determined by the population parameters <span class="math inline">\(\beta_0, \dots, \beta_p\)</span>.</p>
<p>When we are estimating parameters, we will have sample data with <span class="math inline">\(n\)</span> number of observations, with each observation <span class="math inline">\(i\)</span> having its own <span class="math inline">\(Y_i\)</span> value. Thus, our sample looks something like <span class="math inline">\((y_1, \dots, y_n)\)</span>. Based on the probability density function <span class="math inline">\(\varphi(\cdot)\)</span> of the population <span class="math inline">\(Y_i\)</span>, the probability of getting our observed <span class="math inline">\(y_1\)</span> in our sample from the population data is <span class="math inline">\(\varphi(y_1, \theta)\)</span>, and the probability of getting <span class="math inline">\(y_i\)</span> is <span class="math inline">\(\varphi(y_i, \theta)\)</span>.</p>
<p>We know by the rules of probability, that the <a href="./math.html#basics-of-probability">probability of multiple independent events</a> is the product of their probabilities. Thus, the probability that we get a specific sample with <span class="math inline">\(y\)</span> values <span class="math inline">\((y_1, \dots ,y_n)\)</span>, based on the value of our population parameters <span class="math inline">\(\boldsymbol\theta\)</span> is given by the <strong>likelihood function</strong> <span class="math inline">\(L\)</span>:</p>
<p><span class="math display">\[
\begin{align}
L(\theta, y) &amp; = \varphi(y_1;\theta) \times \varphi(y_2;\theta) \times \dots \times \varphi(y_n;\theta) = \prod\limits_{i=1}^n \varphi(y_i, \theta) \\
&amp; = \varphi(y, \theta), \  \text{where this is a joint probability PDF with vector } y
\end{align}
\]</span></p>
<p>We want to find values of <span class="math inline">\(\theta\)</span> that make it the highest probability we observe our sample <span class="math inline">\(y_1, \dots ,y_n\)</span>. This is done by maximising the likelihood function <span class="math inline">\(L(\cdot)\)</span>. However, maximising the likelihood function <span class="math inline">\(L(\cdot)\)</span> is very difficult. Luckily, we can use the log of the likelihood function <span class="math inline">\(\ell(\cdot)\)</span>, which retains the same maximum/minimum points as <span class="math inline">\(L(\cdot)\)</span> why being easier to work with.</p>
<p><span id="eq-loglike"><span class="math display">\[
\begin{align}
\ell (\theta; y)&amp; = \log L(\theta, y)  \\
&amp; = \log \left(\prod_{i=1}^n \varphi(y_i; \theta) \right) \\
&amp; = \log[\varphi(y_1, \boldsymbol\theta) \times \varphi(y_2, \boldsymbol\theta) \times \dots \times \varphi(y_n, \boldsymbol\theta)] &amp;&amp; \text{(expand product notation)} \\
&amp; = \log[\varphi(y_1, \boldsymbol\theta)] + \log [\varphi(y_2, \boldsymbol\theta)] + \dots + \log[\varphi(y_n, \boldsymbol\theta)] &amp;&amp; \text{(property of logs)} \\
&amp; = \sum\limits_{i=1}^n \log[\varphi(y_i, \boldsymbol\theta)] &amp;&amp; \text{(condense into sum)}
\end{align}
\tag{1}\]</span></span></p>
<p><br></p>
</section>
<section id="score-function-and-expectation" class="level3">
<h3 class="anchored" data-anchor-id="score-function-and-expectation">Score Function and Expectation</h3>
<p>The gradient of <span class="math inline">\(\ell(\theta; y)\)</span> with respect to vector <span class="math inline">\(\theta\)</span> is known as the <strong>score function</strong> <span class="math inline">\(s(\theta, y)\)</span>:</p>
<p><span class="math display">\[
s(\theta;y) = \frac{\partial}{\partial \theta} \ell(\theta; y) = \frac{\partial}{\partial \theta}
\sum\limits_{i=1}^n \log[\varphi(y_i; \theta)]
\]</span></p>
<p>The <span class="math inline">\(\hat\theta\)</span> values of MLE are the <span class="math inline">\(\theta\)</span> that solve <span class="math inline">\(s(\theta; y) = 0\)</span>. If we have <span class="math inline">\(p\)</span> parameters, there will be <span class="math inline">\(p\)</span> partial derivatives within the gradient.</p>
<p>Let us define the true parameter value in the population as <span class="math inline">\(\theta_0\)</span>, which implies the score function of <span class="math inline">\(\theta_0\)</span> is <span class="math inline">\(s(\theta_0; Y_i)\)</span>, where <span class="math inline">\(Y_i\)</span> is the random outcome variable. Vector <span class="math inline">\(y\)</span>, our sample, is a realisation of this random variable - since we are sampling for our observed <span class="math inline">\(y\)</span>, our sampled <span class="math inline">\(y\)</span> will change with a different sample. Thus, the score function is also a random variable with expectation and variance.</p>
<p>The expectation of the score function in respect to random <span class="math inline">\(Y_i\)</span>, evaluated at true population parameter value <span class="math inline">\(\theta_0\)</span> is:</p>
<p><span class="math display">\[
\begin{align}
\E[s(\theta_0; Y_i)] &amp; = \int s(\theta_0;y) \overbrace{\varphi(y; \theta_0)}^{\P \ \mathrm{ of\ sample \ y}}dy &amp;&amp; (\text{definition of continuous } \E) \\
&amp; = \int \left[ \frac{\partial}{\partial\theta} \ell(\theta; y)\right]\varphi(y \theta_0)dy &amp;&amp; \text{(plug in score function)} \\
&amp; = \int\left[\frac{\partial}{\partial\theta} \varphi(y; \theta) \right] \varphi (y; \theta_0)dy &amp;&amp; \text{(plug in } \ell \text{, y is vector so no summation needed)} \\
&amp; = \int \frac{\frac{\partial}{\partial\theta} \varphi(y; \theta_0)}{\varphi(y; \theta_0)}\varphi(y; \theta_0) &amp;&amp; (\because [\log(u(x))]' = u'(x)/u(x) \ ) \\
&amp; = \int \frac{\partial}{\partial\theta} \varphi(y; \theta_0) &amp;&amp; \text{(cancel out)} \\
&amp; = \frac{\partial}{\partial\theta} \int \varphi(y; \theta_0) &amp;&amp; \text{(can flip deriv. and anti-deriv.)} \\
&amp; = \frac{\partial}{\partial\theta} 1 = 0 &amp;&amp; \text{(indef int. of PDF = 1)}
\end{align}
\]</span></p>
<p>Thus, the expectation of the score at true population parameter <span class="math inline">\(\theta_0\)</span> in respect to the random variable vector <span class="math inline">\(y\)</span> is 0.</p>
<p><br></p>
</section>
<section id="information-matrix-and-variance" class="level3">
<h3 class="anchored" data-anchor-id="information-matrix-and-variance">Fischer Information Matrix</h3>
<p>With the expectation from above, we can also find the variance-covariance matrix of the score function at <span class="math inline">\(\theta_0\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\V s(\theta_0; y) &amp; = \E [(s(\theta_0; y) - \E(s(\theta_0; y))^2 ] &amp;&amp; (\because \V Z = \E[Z - \E Z]) \\
&amp; = \E[(s(\theta_0; y) - 0)^2] &amp;&amp; (\because \E [s(\theta_0 ; y)] = 0) \\
&amp; = \E\left [\frac{\partial \ell (\theta_0; y)}{\partial \theta} \frac{\partial \ell (\theta_0; y)}{\partial \theta^\top} \right] &amp;&amp; \text{(plug in and square } s(\theta_0, y)) \\
&amp; = \E\left[ -\frac{\partial^2 \ell (\theta_0; y)}{\partial\theta\partial \theta^\top}\right] \equiv \mathcal I(\theta_0)
\end{align}
\]</span></p>
<p>Where <span class="math inline">\(\mathcal I (\theta_0)\)</span> is also known as the <strong>expected fisher information matrix</strong>. This is also the matrix of second derivatives of the log-likelihood, meaning this is the negative of the Hessian matrix of the log-likelihood.</p>
<p>We can also get the observed equivalent of our parameter estimate <span class="math inline">\(\theta\)</span>, called the <strong>observed information matrix</strong>, which does not involve expectation (which can be difficult):</p>
<p><span class="math display">\[
I(\theta;y) = -\frac{\partial^2 \ell(\theta; y)}{\partial \theta \partial \theta^\top}
\]</span></p>
<p>Through complex math beyond this course, we can show that the asymptotic variance of any MLE estimate <span class="math inline">\(\hat\theta\)</span>, <span class="math inline">\(\V \hat\theta\)</span>, is the inverse of the information matrix:</p>
<p><span class="math display">\[
\V (\hat\theta) = \mathcal I(\hat\theta)^{-1}
\]</span></p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Variance of an Estimator
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We have some true population parameter <span class="math inline">\(\theta\)</span> that we are estimating with a sample. Our sample will produce some estimate <span class="math inline">\(\hat\theta_n\)</span>.</p>
<p>Imagine we take another sample from the population: we will get a slightly different estimate. Taking <span class="math inline">\(N\)</span> samples, we get estimates <span class="math inline">\(\hat\theta_1, \hat\theta_2, \dots, \hat\theta_N\)</span>. Thus, our single sample estimate <span class="math inline">\(\hat\theta_n\)</span> is actually a realisation of a random variable, drawn from the sampling distribution <span class="math inline">\(\hat\theta_1, \dots, \hat\theta_N\)</span>. The best guess of the value of <span class="math inline">\(\hat\theta_n\)</span> is the expected value <span class="math inline">\(\E \hat\theta_n\)</span> of the distribution.</p>
<p>However, our sampling distribution of <span class="math inline">\(\hat\theta_1, \dots, \hat\theta_N\)</span> can also be described by the variance - how spread out the distribution is. Even if our estimator is unbiased, if it has a very high variance (spread), that means any specific realisation <span class="math inline">\(\hat\theta_n\)</span> might be far away from the expected value. Thus, we want a low-variance estimator.</p>
</div>
</div>
</div>
<p>When we have finite (but sufficiently large samples), this is our variance estimate. We can also use the observed information matrix <span class="math inline">\(I(\hat\theta)^{-1}\)</span>.</p>
<p><br></p>
</section>
<section id="asymptotics-and-cramér-rao" class="level3">
<h3 class="anchored" data-anchor-id="asymptotics-and-cramér-rao">Asymptotics and Cramér-Rao</h3>
<p>Through complex proofs beyond the scope of this chapter, we can determine that asymptotically as <span class="math inline">\(n \rightarrow ∞\)</span>, the distribution of the MLE estimates <span class="math inline">\(\hat\theta\)</span> becomes:</p>
<p><span class="math display">\[
\hat\theta \sim \mathcal N(\theta_0, \mathcal I(\theta_0)^{-1})
\]</span></p>
<p>This tells us two things. First, maximum likelihood estimates are asymptotically consistent, since the asymptotic distribution has an expectation <span class="math inline">\(\E \hat\theta = \theta_0\)</span>.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Asymptotic Consistency of an Estimator
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>An estimator is asymptotically consistency, if as we increase the sample size <span class="math inline">\(n\)</span> towards <span class="math inline">\(∞\)</span>, the sampling distribution will become more and more concentrated around the true population <span class="math inline">\(\theta\)</span>. At <span class="math inline">\(n = ∞\)</span>, our sampling distribution collapses to just the true population value <span class="math inline">\(\theta\)</span>. Mathematically:</p>
<p><span class="math display">\[
\P(|\hat\theta_n - \theta) &gt; \epsilon) \rightarrow 0, \ \mathrm{as} \ n \rightarrow ∞
\]</span></p>
<p>This essentially means that the probability that the difference between our sample estimate <span class="math inline">\(\hat\theta_n\)</span> and <span class="math inline">\(\theta\)</span> is greater than some arbitrarily small value <span class="math inline">\(\epsilon\)</span> becomes 0, as our sample size approaches infinity.</p>
<p>Note: estimators can be biased but consistent. This means they are biased at small sample sizes, but as sample size increases, they are consistent.</p>
</div>
</div>
</div>
<p>Second, we know that the asymptotic variance of MLE estimates is <span class="math inline">\(\mathcal I(\theta_0)^{-1}\)</span>. This is notable because of the <strong>Cramér-Rao bound</strong>. The Cramér-Rao bound states that the variance of any unbiased estimator <span class="math inline">\(\hat\theta\)</span> is bounded by the reciprocal of the Fisher Information matrix:</p>
<p><span class="math display">\[
\V \hat\theta_n ≥ \frac{1}{\mathcal I(\theta)}
\]</span></p>
<p>We will not prove this here since it is technical. However, this is the lowest possible variance of any unbiased estimator. As we see above, the asymptotic variance of the MLE <span class="math inline">\(\hat\theta\)</span> is exactly <span class="math inline">\(1/\mathcal I(\theta)\)</span>. Thus, this tells us there is no other asymptotically consistent (unbiased) estimator that has a lower variance than the MLE.</p>
<p>However, do note that the MLE is biased in finite-samples (but the bias becomes small in large-samples). This becomes an issue when we are dealing with some applications of causal inference, such as fixed effects.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Unbiased and Biased Estimators
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We have some true population parameter <span class="math inline">\(\theta\)</span> that we are estimating with a sample. Our sample will produce some estimate <span class="math inline">\(\hat\theta_n\)</span>.</p>
<p>Imagine we take another sample from the population: we will get a slightly different estimate. Taking <span class="math inline">\(N\)</span> samples, we get estimates <span class="math inline">\(\hat\theta_1, \hat\theta_2, \dots, \hat\theta_N\)</span>. Thus, our single sample estimate <span class="math inline">\(\hat\theta_n\)</span> is actually a realisation of a random variable, drawn from the sampling distribution <span class="math inline">\(\hat\theta_1, \dots, \hat\theta_N\)</span>. The best guess of the value of <span class="math inline">\(\hat\theta_n\)</span> is the expected value <span class="math inline">\(\E \hat\theta_n\)</span> of the distribution.</p>
<p>An unbiased estimator is when <span class="math inline">\(\E\hat\theta_n = \theta\)</span>, or in other words, the expected sample estimate when drawn from the sampling distribution is equal to the true population value <span class="math inline">\(\theta\)</span>. We want an unbiased estimator, because that means any sample estimate <span class="math inline">\(\hat\theta_n\)</span> is expected to equal the true population parameter.</p>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="ols-as-a-mle" class="level3">
<h3 class="anchored" data-anchor-id="ols-as-a-mle">OLS as a MLE</h3>
<p>Earlier, we noted that all linear models are estimated with a Maximum Likelihood Estimator. This includes the classical linear model.</p>
<p>OLS is a MLE under the classical assumptions, and assuming <span class="math inline">\(Y_i\)</span> is normally distributed at <span class="math inline">\(Y_i \sim \mathcal N(X\beta, \sigma^2)\)</span> (where <span class="math inline">\(\mu = \E(Y_i|X_i) = x_i^\top\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span> as the homoscedastic variance of <span class="math inline">\(\eps_i\)</span>). By the PDF of a normal distribution, we can express the PDF of <span class="math inline">\(Y_i\)</span>:</p>
<p><span class="math display">\[
f(z) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(z-\mu)^2}{2\sigma^2}} \quad \implies \quad f(Y_i|\beta, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(Y_i - x_i^\top\beta)^2}{2\sigma^2}}
\]</span></p>
<p>Now using <a href="#eq-loglike" class="quarto-xref">Equation&nbsp;1</a>, the log-likelihood function <span class="math inline">\(\ell\)</span> of our sample <span class="math inline">\((y_1, \dots, y_n)\)</span> for linear regression is:</p>
<p><span class="math display">\[
\begin{align}
\ell(\beta, \sigma^2; y_i)
&amp; = \sum\limits_{i=1}^n \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} e^{\left( -\frac{1}{2 \sigma^2}(y_i - x_i^\top \beta)^2\right)} \right) \\
&amp; = \sum\limits_{i=1}^n \log (1) - \log (\sqrt{2\pi\sigma^2})   + \log\left( e^{\left( -\frac{1}{2 \sigma^2}(y_i - x_i^\top\beta )^2\right)}\right) &amp;&amp; \text{(prop. of logs)} \\
&amp; = \sum\limits_{i=1}^n 0 - \frac{1}{2}\log ({2\pi\sigma^2})  + \left( -\frac{1}{2 \sigma^2}(y_i - x_i^\top\beta)^2\right) &amp;&amp; \text{(prop. of logs)} \\
&amp; = -\frac{n}{2} \log (2\pi\sigma^2)  -\frac{1}{2 \sigma^2}\sum\limits_{i=1}^n(y_i - x_i^\top\beta)^2 &amp;&amp; \text{(prop. of sums)}
\end{align}
\]</span></p>
<p>Look at the right part of the above equation. We can see that is the SSR. Thus, we can rewrite:</p>
<p><span class="math display">\[
\ell(\beta, \sigma^2; y) = -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}(y-X\beta)^\top (y-X\beta)
\]</span></p>
<p>And if we take the gradient in respect to <span class="math inline">\(\beta\)</span>, we can see we get the same first order condition as OLS:</p>
<p><span class="math display">\[
\frac{\partial \ell}{\partial \beta} = -2X^\top y + 2 X^\top X\beta = 0
\]</span></p>
<p>With the same first order condition, we will get the same <span class="math inline">\(\beta\)</span> estimate as in OLS. Thus, we can see under the condition of normality of <span class="math inline">\(Y_i\)</span>, MLE is equivalent to OLS.</p>
<p><br></p>
</section>
<section id="newton-and-fischer-algorithms" class="level3">
<h3 class="anchored" data-anchor-id="newton-and-fischer-algorithms">Newton and Fischer Algorithms</h3>
<p>We need to solve <span class="math inline">\(s(\theta; y) = 0\)</span> to find our MLE estimates <span class="math inline">\(\hat\theta\)</span>. However, there is often no closed form/analytical solution like OLS. Thus, we must use iterated algorithms to solve for <span class="math inline">\(\hat\theta\)</span>.</p>
<p>One version is the <strong>Newton Algorithm</strong> (also called the newton-raphson algorithm). Suppose <span class="math inline">\(\theta\)</span> is a scalar (only one parameter). For values of <span class="math inline">\(\theta\)</span> that are close to the true population <span class="math inline">\(\theta_0\)</span>, the first-order taylor series expansion of <span class="math inline">\(s(\theta)\)</span> about <span class="math inline">\(\theta_0\)</span> states:</p>
<p><span class="math display">\[
s(\theta; y) \approx s(\theta_0; y) + s'(\theta_0;y)(\theta - \theta_0)
\]</span></p>
<p>Where <span class="math inline">\(s'(\theta_0; y)\)</span> is the first derivative of the score function <span class="math inline">\(s(\theta; y)\)</span> evaluated at <span class="math inline">\(\theta = \theta_0\)</span>. When dealing with a vector <span class="math inline">\(\theta\)</span>, the equivalent first derivative is the hessian matrix of <span class="math inline">\(\ell(\cdot)\)</span>, or the negative of the <a href="#information-matrix-and-variance">observed information matrix</a>:</p>
<p><span class="math display">\[
\frac{\partial s(\theta; y)}{\partial \theta} = \frac{\partial^2 \ell(\theta; y)}{\partial \theta \partial \theta^\top} = - I(\theta)
\]</span></p>
<p>This gives us the following first-order taylor series expansion of <span class="math inline">\(s(\theta)\)</span>, and when we solve <span class="math inline">\(s(\theta; y) = 0\)</span> to maximimse, we can solve for <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
\begin{align}
s(\theta; y) &amp; \approx s(\theta_0; y) - I(\theta_0)(\theta - \theta_0) \\
0 &amp; \approx s(\theta_0; y) - I(\theta_0)(\theta - \theta_0) &amp;&amp; (\because s(\theta;y) = 0) \\
I(\theta_0)(\theta - \theta_0) &amp; \approx s(\theta_0; y)  &amp;&amp; (+I(\theta_0)(\theta - \theta_0) \text{ to both sides})\\
\theta - \theta_0 &amp; \approx I(\theta_0)^{-1}s(\theta_0; y) &amp;&amp; (\times I(\theta_0)^{-1} \text{ to both sides}) \\
\theta &amp; \approx \theta_0 + \approx I(\theta_0)^{-1}s(\theta_0; y) &amp;&amp; (+\theta_0 \text{ to both sides})
\end{align}
\]</span></p>
<p>Since <span class="math inline">\(\theta_0\)</span> is unknown, we cannot use this formula directly. Thus, we use an iterative procedure. We start with some initial value <span class="math inline">\(\theta^{(0)}\)</span> that is randomly chosen. Then, we “update” for a new value <span class="math inline">\(\theta^{(1)}\)</span>:</p>
<p><span class="math display">\[
\theta^{(1)} = \theta^{(0)} + I (\theta^{(0)})^{-1} s(\theta^{(0)}; y)
\]</span></p>
<p>And we do the same updating for <span class="math inline">\(\theta^{(2)}\)</span> with <span class="math inline">\(\theta^{(1)}\)</span>, and <span class="math inline">\(\theta^{(m+1)}\)</span> using <span class="math inline">\(\theta^{(m)}\)</span>, until the algorithm converges and the differences between <span class="math inline">\(\theta^{(m+1)}\)</span> using <span class="math inline">\(\theta^{(m)}\)</span> becomes minimal (under some pre-specified threshold of “tolerance”). An alternative is <strong>fisher-scoring</strong>, which does the same thing but with the <a href="#information-matrix-and-variance">expected fisher information matrix</a> <span class="math inline">\(\mathcal I(\theta)\)</span> instead of the observed <span class="math inline">\(I(\theta)\)</span> when <span class="math inline">\(\mathcal I(\theta)\)</span> is not too difficult to compute. This is the method used for most GLMs.</p>
<p><br></p>
</section>
<section id="information-criterion-statistics" class="level3">
<h3 class="anchored" data-anchor-id="information-criterion-statistics">Information Criterion Statistics</h3>
<p>Recall that the likelihood function <span class="math inline">\(L(\cdot)\)</span> is the probability of observing a particular sample given the parameters <span class="math inline">\(\theta\)</span>, or in other words, <span class="math inline">\(\P (y | \theta)\)</span>.</p>
<p>This property of <span class="math inline">\(L(\cdot)\)</span> also allows us to compare models between each other. For example, let us say we have two models of the same outcome variable <span class="math inline">\(Y_i\)</span>, one model with parameters <span class="math inline">\(\theta_1\)</span>, and another with parameters <span class="math inline">\(\theta_2\)</span> (perhaps one has more parameters/explanatory variables, etc). The model with a higher likelihood <span class="math inline">\(L(\theta; y)\)</span> is the model that is considered the better fit.</p>
<p>Thus, the likelihood value <span class="math inline">\(L(\theta; y)\)</span> allows us to compare the explanatory power of models, like <span class="math inline">\(R^2\)</span> does for the classical linear model. This will become useful when we discuss likelihood ratio tests.</p>
<p>A group of statistics, called <strong>information criterion (IC) statistics</strong>, use the likelihood <span class="math inline">\(L(\cdot)\)</span> to compare different models. The idea behind these statistics is to not only reward higher likelihoods <span class="math inline">\(L\)</span>, but also reward simplicty of models with less parameters.</p>
<p>The most commonly used is <strong>Akaike’s Information Criterion (AIC)</strong>. The formula for AIC is as follows:</p>
<p><span class="math display">\[
AIC = -2 \log L + 2p
\]</span></p>
<ul>
<li>Where <span class="math inline">\(L\)</span> is the likelihood of the model in question evaluated as <span class="math inline">\(L(\theta ; y)\)</span>, and <span class="math inline">\(p\)</span> is the number of parameters in the model.</li>
</ul>
<p>The lower the AIC is, the better the model is considered. There are also alternative IC statistics, such as the Bayesian Information Criterion (BIC).</p>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="logistic-regression-model" class="level1">
<h1><strong>Logistic Regression Model</strong></h1>
<section id="model-specification" class="level3">
<h3 class="anchored" data-anchor-id="model-specification">Model Specification</h3>
<p>The logistic model is a model for binary outcome variables <span class="math inline">\(Y_i\)</span> distribute according to the bernoulli distribution. Specifically, it models the outcome <span class="math inline">\(\E(Y_i |X_i) = \P(Y_i = 1|X_i) = \pi_i\)</span> like the <a href="./clm.html#linear-probability-model">linear probability model</a>. As a GLM, logistic regression applies a link function to the outcome <span class="math inline">\(\pi_i\)</span>. The model is specified:</p>
<p><span class="math display">\[
\log \left( \frac{\pi_i}{1-\pi_i}\right) = \beta_0 + \sum\limits_{j=1}^n \beta_j X_{ij} \ = \ x_i^\top \beta
\]</span></p>
<p>However, this form of the model with <span class="math inline">\(\log(\pi_i/1-\pi_i))\)</span> (called the log-odds) is not very useful - after all, we care about <span class="math inline">\(\E(Y_i |X_i) = \P(Y_i = 1|X_i) = \pi_i\)</span>. Thus, through algebra manipulation, we can isolate <span class="math inline">\(\pi_i\)</span> on the left for a model of probabilities <span class="math inline">\(\pi_i\)</span> (proof provided below).</p>
<p>Start with the original log-odds model, and take <span class="math inline">\(e\)</span> to the power of both sides to get rid of the natural log:</p>
<p><span id="eq-odds"><span class="math display">\[
\log \left( \frac{\pi_i}{1-\pi_i}\right) = x_i^\top\beta \quad \implies \quad \frac{\pi_i}{1-\pi_i} = e^{x_i^\top \beta}
\tag{2}\]</span></span></p>
<p>The left eqution is a model for the odds (we will explore this more later). Now, we just solve for <span class="math inline">\(\pi_i\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\pi_i &amp; = (1- \pi_i)e^{x_i^\top \beta} &amp;&amp; (\times (1-\pi_i) \text{ to both sides}) \\
\pi_i &amp; = e^{x_i^\top \beta} - \pi_i e^{x_i^\top \beta} &amp;&amp; \text{(multiply out)} \\
\pi_i + \pi_i e^{x_i^\top \beta} &amp; = e^{x_i^\top \beta} &amp;&amp; (+\pi_i e^{x_i^\top \beta} \text{ to both sides}) \\
\pi_i(1 + e^{x_i^\top \beta}) &amp; = e^{x_i^\top \beta} &amp;&amp; (\text{factor out }\pi_i) \\
\end{align}
\]</span></p>
<p>Finally, dividing both sides by <span class="math inline">\((1+e^{x_i^\top \beta})\)</span> gets us the model for the outcome <span class="math inline">\(\pi_i\)</span>:</p>
<p><span class="math display">\[
\E(Y_i |X_i) = \P(Y_i = 1|X_i) = \pi_i = \frac{e^{x_i^\top \beta}}{1+ e^{x_i^\top \beta}}
\]</span></p>
<p>In which we can estimate parameter vector <span class="math inline">\(\beta\)</span> with MLE as explained above. If you graph this function of <span class="math inline">\(\pi_i\)</span>, you will see that <span class="math inline">\(\pi_i\)</span> always stays between 0 and 1.</p>
<p><br></p>
</section>
<section id="fitted-probabilities-and-applications" class="level3">
<h3 class="anchored" data-anchor-id="fitted-probabilities-and-applications">Fitted Probabilities and Applications</h3>
<p>Once we have estimated parameters in vector <span class="math inline">\(\beta\)</span>, we can get fitted probabilities <span class="math inline">\(\hat\pi_i\)</span>:</p>
<p><span class="math display">\[
\hat\pi_i = \frac{e^{x_i^\top \hat\beta}}{1+ e^{x_i^\top \hat\beta}}
\]</span></p>
<p>The fact that predicted <span class="math inline">\(\hat\pi_i\)</span> is always between 0 and 1 makes the logistic model significantly better for the prediction of probabilities than the linear probability model. In fact, if our goal is prediction, we will almost always prefer the logistic model over the classical linear model.</p>
<p>We can also do <strong>classification</strong> - i.e.&nbsp;predicting if unit <span class="math inline">\(i\)</span> should be <span class="math inline">\(\hat Y_i =1\)</span> or <span class="math inline">\(\hat Y_i = 0\)</span>. Typically if predicted <span class="math inline">\(\hat\pi_i &gt; 0.5\)</span>, we assign unit <span class="math inline">\(i\)</span> to <span class="math inline">\(\hat Y_i = 1\)</span>.</p>
<p>The form of the logistic regression is confusing - it is not a simple linear line. Below is the graph of a simple logistic regression with only one explanatory variable, and how changing coefficients <span class="math inline">\(\beta_0 = \alpha\)</span> and <span class="math inline">\(\beta_1\)</span> affect the curve of <span class="math inline">\(\pi_i\)</span>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-1638897592.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
<p>We can see that the curves are always between 0 and 1. We also see that <span class="math inline">\(\beta_1\)</span> affects the relationship between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(\pi_i\)</span>. However, the value of <span class="math inline">\(\beta_1\)</span> is not directly interpretable unlike linear regression. However, there is a way to interpret a modified version of the coefficients, called the odds ratios (see below).</p>
<p><br></p>
</section>
<section id="interpretation-with-odds-ratios" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-with-odds-ratios">Interpretation with Odds Ratios</h3>
<p>Before we define odds ratios and how they can help with interpretation, we first need to define the <strong>odds</strong> of an event <span class="math inline">\(A\)</span>. This is defined as the probability of <span class="math inline">\(A\)</span> occuring, divided by the probability of <span class="math inline">\(A\)</span> not occuring. We can apply the same logic to <span class="math inline">\(\P (Y_i = 1) = \pi_i\)</span>:</p>
<p><span class="math display">\[
\mathrm{odds}_A = \frac{\P(A)}{1-\P(A)} \quad \implies \quad \mathrm{odds}_{Y_i = 1} = \frac{\pi_i}{1 - \pi_i}
\]</span></p>
<p>In <a href="#eq-odds" class="quarto-xref">Equation&nbsp;2</a>, we also showed the logistic regression can be written with the outcome of odds of <span class="math inline">\(Y_i = 1\)</span>.</p>
<p><strong>Odds Ratios</strong> are the ratio of two odds. For example, let us say we have two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. The odds ratio is:</p>
<p><span class="math display">\[
OR = \frac{\mathrm{odds}_{A}}{\mathrm{odds}_{B}} = \frac{\P(A)/[1-\P(A)]}{\P(B)/[1-\P(B)]}
\]</span></p>
<p>We know that the logistic model measures <span class="math inline">\(\pi_i = \P(Y_i = 1 | X_i)\)</span>. This implies that <span class="math inline">\(\pi_i\)</span> depends on the value of <span class="math inline">\(X_i\)</span>. That also means the odds depend on <span class="math inline">\(X_i\)</span>. This allows us to compare how the odds change when we change <span class="math inline">\(X_i\)</span>.</p>
<p>For example, let us say we have the odds when <span class="math inline">\(X_i = x\)</span>, and <span class="math inline">\(X_i = x+1\)</span> for a simple logistic regression. Using <a href="#eq-odds" class="quarto-xref">Equation&nbsp;2</a>, we can calculate the odds and odds ratio of <span class="math inline">\(Y_i\)</span> given these two different values of <span class="math inline">\(X_i\)</span>:</p>
<p><span class="math display">\[
\begin{align}
&amp; \mathrm{odds}_{Y_i = 1 |X_i = x} = \left(\frac{\pi_i}{1-\pi_i} \biggr|X_i = x \right) = e^{\beta_0 + \beta_1X_i} = e^{\beta_0 + \beta_1(x)}\\
&amp; \mathrm{odds}_{Y_i = 1 |X_i = x+1} = \left(\frac{\pi_i}{1-\pi_i} \biggr|X_i = x+1 \right) = e^{\beta_0 + \beta_1X_i} = e^{\beta_0 + \beta_1(x+1)} \\
&amp; \frac{\mathrm{odds}_{Y_i = 1|X_i = x+1}}{\mathrm{odds}_{Y_i = 1|X_i = x}} = \frac{e^{\beta_0 + \beta_1(x+1)}}{e^{\beta_0 + \beta_1(x)}} = \frac{e^{\beta_0 + \beta_1x+\beta_1}}{e^{\beta_0 + \beta_1x}} = \frac{e^{\beta_0}e^{\beta_1x}e^{\beta_1}}{e^{\beta_0} e^{\beta_1x}} = e^{\beta_1}
\end{align}
\]</span></p>
<p>Thus, when increasing <span class="math inline">\(X_i\)</span> by one unit, the odds of an observation being in category <span class="math inline">\(Y_i = 1\)</span> multiply by <span class="math inline">\(e^{\beta_1}\)</span>. A multiplicative increase also implies percentage change.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Percentage Change Interpretations
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We can also interpret odds ratios as percentage changes in odds (because that is what a multiplicative change in):</p>
<ul>
<li>If <span class="math inline">\(e^{\beta_1} &gt; 1\)</span>, then a one unit increase in <span class="math inline">\(X_i\)</span> is associated with a <span class="math inline">\((e^{\beta_1} - 1)\times 100\)</span> percent increase in the odds of an observation being in category <span class="math inline">\(Y_i = 1\)</span>.</li>
<li>If <span class="math inline">\(e^{\beta_1} &lt; 1\)</span>, then a one unit increase in <span class="math inline">\(X_i\)</span> is associated with a <span class="math inline">\((1-e^{\beta_1} )\times 100\)</span> percent decrease in the odds of an observation being in category <span class="math inline">\(Y_i = 1\)</span>.</li>
<li>If <span class="math inline">\(e^{\beta_1} = 1\)</span>, then there is no relationship between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span>.</li>
</ul>
</div>
</div>
</div>
<p>An important point (that is often a mistake) is that odds are not probabilities. A doubling of odds is not equivalent to a doubling of probability. This is a common mistake in interpretation.</p>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="ordinal-and-multinomial-models" class="level1">
<h1><strong>Ordinal and Multinomial Models</strong></h1>
<section id="ordinal-logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="ordinal-logistic-regression">Ordinal Logistic Regression</h3>
<p>What if we have an ordinal outcome <span class="math inline">\(Y_i \in \{0, 1, 2, \dots \}\)</span>. The <strong>ordinary logistic regression model</strong> is a set of <span class="math inline">\(c-1\)</span> binary logistic regressions that calculate cumulative probabilities, where <span class="math inline">\(c\)</span> is the total number of categories.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ordinal Models as a set of Binary Models
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Ordinal logistic model is a set of <span class="math inline">\(c-1\)</span> binary models, where <span class="math inline">\(c\)</span> is the number of categories. For example, take <span class="math inline">\(c=4\)</span>:</p>
<ol type="1">
<li>The first model is a binary of category 1 vs.&nbsp;categories 2-4.</li>
<li>The second model is a binary of categories 1-2 vs.&nbsp;categories 3-4.</li>
<li>The third model is a binary of categories 1-3 vs.&nbsp;category 4.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-1556270979.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>Where the categories highlighted in yellow are considered to be <span class="math inline">\(Y_i = 1\)</span> for that specific binary regression, and the green are considered <span class="math inline">\(Y_i =0\)</span> for the specific binary regression.</p>
<p>We can see that the yellow increases from just category 1 in the first regression, to categories 1-3 in the last regression. This tells us that these are actually measuring the <strong>cumulative</strong> probabilities <span class="math inline">\(\gamma_j\)</span> (for example, the final regression is predicting the probability of being in either category <span class="math inline">\(j ≤ 3\)</span>, compared to <span class="math inline">\(j = 4\)</span>).</p>
</div>
</div>
</div>
<p>The ordinal logistic regression model is a model for the cumulative probability <span class="math inline">\(\gamma_j\)</span> of an outcome <span class="math inline">\(Y_i\)</span> being in a certain category <span class="math inline">\(j\)</span> or below, for categories <span class="math inline">\(j = 1, \dots, c-1\)</span>:</p>
<p><span class="math display">\[
\gamma_j = \P(Y_i ≤ j) = \frac{e^{\beta_{0j} - (\beta_1X_{i1} + \dots + \beta_p X_{ip})}}{1 + e^{\beta_{0j} - (\beta_1X_{i1} + \dots + \beta_p X_{ip})}}
\]</span></p>
<p>Notice how for each <span class="math inline">\(j\)</span>, there is a different intercept <span class="math inline">\(\beta_{0j}\)</span> value. However, the other coefficients <span class="math inline">\(\beta_1, \dots, \beta_p\)</span> do not depend on <span class="math inline">\(j\)</span>, and are consistent across all <span class="math inline">\(j\)</span>. What this means is that the <span class="math inline">\(c-1\)</span> binary logistic regressions are “parallel”, since they have the same coefficients, but are shifted by intercepts. This is called the <strong>proportional odds</strong> assumption, and assumes all of the binary comparisons must have the same correlation between <span class="math inline">\(X_{ij}\)</span> and <span class="math inline">\(Y_i\)</span>. If you believe this is not reasonable to assume, you should use the multinomial model below.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Visualisation of Proportional Odds
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Imagine we have <span class="math inline">\(c=4\)</span> categories. The 3 binary model’s predicted cumulative probabilities are displayed below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-3226102876.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%"></p>
</figure>
</div>
<p>Notice how all three are parallel, since they have the same <span class="math inline">\(\beta_1, \dots, \beta_p\)</span> coefficient values. They are only shifted due to <span class="math inline">\(\beta_{0j}\)</span> depending on the model <span class="math inline">\(j\)</span>.</p>
</div>
</div>
</div>
<p>Interpretation of the coefficients <span class="math inline">\(\beta_1, \dots, \beta_p\)</span> use <a href="#interpretation-with-odds-ratios">odds ratios</a> as discussed above. An increase in one unit of <span class="math inline">\(X_I\)</span> increases the odds of a unit being in a higher category of <span class="math inline">\(Y_i\)</span> by a multiplicative increase of <span class="math inline">\(e^{\beta_j}\)</span>.</p>
<p>Predictions for probabilities can be calculated with the above equation but substituting in our predicted <span class="math inline">\(\hat\beta\)</span> from maximum likelihood estimation. Of course, the model is specified towards cumulative probabilities <span class="math inline">\(\gamma_j = \P(Y_i ≤ j)\)</span>. For the probability of a specific category <span class="math inline">\(\P(Y_i = j)\)</span>:</p>
<ul>
<li>The probability of the first category <span class="math inline">\(j=1\)</span> is <span class="math inline">\(\P (Y_i = 1) =\hat\gamma_1\)</span>.</li>
<li>The probability the last category <span class="math inline">\(j =c\)</span> is 1 minus the cumulative probability of the second-to-last category: <span class="math inline">\(\P(Y_i = c) = 1 - \gamma_{c-1}\)</span>.</li>
<li>The probability of any category <span class="math inline">\(j\)</span> in the middle is that category <span class="math inline">\(j\)</span>’s cumulative probability minus the cumulative probability of <span class="math inline">\(j-1\)</span>: <span class="math inline">\(\P(Y_i = j) = \gamma_j - \gamma_{j-1}\)</span>.</li>
</ul>
<p><br></p>
</section>
<section id="multinomial-logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="multinomial-logistic-regression">Multinomial Logistic Regression</h3>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="negative-binomial-regression" class="level1">
<h1><strong>Negative Binomial Regression</strong></h1>
<section id="model-specification-1" class="level3">
<h3 class="anchored" data-anchor-id="model-specification-1">Model Specification</h3>
<p>Talk about negative binomial distribution.</p>
<p>Include model for rates, and fitted probabilities.</p>
<p><br></p>
</section>
<section id="interpreting-coefficients" class="level3">
<h3 class="anchored" data-anchor-id="interpreting-coefficients">Interpreting Coefficients</h3>
<p><br></p>
</section>
<section id="poisson-regression" class="level3">
<h3 class="anchored" data-anchor-id="poisson-regression">Poisson Regression</h3>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="statistical-inference" class="level1">
<h1><strong>Statistical Inference</strong></h1>
<section id="hypothesis-testing" class="level3">
<h3 class="anchored" data-anchor-id="hypothesis-testing">Hypothesis Testing</h3>
<p><br></p>
</section>
<section id="confidence-intervals" class="level3">
<h3 class="anchored" data-anchor-id="confidence-intervals">Confidence Intervals</h3>
<p>add odds ratios</p>
<p><br></p>
</section>
<section id="likelihood-ratio-test" class="level3">
<h3 class="anchored" data-anchor-id="likelihood-ratio-test">Likelihood Ratio Test</h3>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="implementation-in-r" class="level1">
<h1><strong>Implementation in R</strong></h1>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>