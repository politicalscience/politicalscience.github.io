<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>The Generalised Linear Model – Political Science &amp; Political Economy</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="mathjax-config.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./glm.html">2 The Generalised Linear Model</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Kevin’s PSPE Resources</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
 <span class="menu-text">Quantitative Methods</span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 The Classical Linear Model</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">2 The Generalised Linear Model</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./causal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 Causal Frameworks</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 Randomised Controlled Trials</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./soo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 Selection on Observables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./iv.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 Instrumental Variables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rd.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7 Regression Discontinuity</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./did.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8 Differences-in-Differences</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Further Statistical Methods</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./predict.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Forecasting and Prediction Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unsupervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Unsupervied Learning Methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./factor.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Factor Analysis Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./latent.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Latent Trait and Class Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sem.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Structural Equation Models</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./games.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guide to Game Theory</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./math.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Background Mathematics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Background Statistics</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On This Page</h2>
   
  <ul>
  <li><a href="#generalised-linear-model" id="toc-generalised-linear-model" class="nav-link active" data-scroll-target="#generalised-linear-model"><strong>Generalised Linear Model</strong></a>
  <ul class="collapse">
  <li><a href="#limitations-of-the-classical-model" id="toc-limitations-of-the-classical-model" class="nav-link" data-scroll-target="#limitations-of-the-classical-model">Limitations of the Classical Model</a></li>
  <li><a href="#the-generalised-linear-model" id="toc-the-generalised-linear-model" class="nav-link" data-scroll-target="#the-generalised-linear-model">The Generalised Linear Model</a></li>
  </ul></li>
  <li><a href="#maximum-likelihood-estimator" id="toc-maximum-likelihood-estimator" class="nav-link" data-scroll-target="#maximum-likelihood-estimator"><strong>Maximum Likelihood Estimator</strong></a>
  <ul class="collapse">
  <li><a href="#likelihood-function" id="toc-likelihood-function" class="nav-link" data-scroll-target="#likelihood-function">Likelihood Function</a></li>
  <li><a href="#log-likelihood-and-score-functions" id="toc-log-likelihood-and-score-functions" class="nav-link" data-scroll-target="#log-likelihood-and-score-functions">Log-Likelihood and Score Functions</a></li>
  <li><a href="#gradient-descent-algorithms" id="toc-gradient-descent-algorithms" class="nav-link" data-scroll-target="#gradient-descent-algorithms">Gradient Descent Algorithms</a></li>
  <li><a href="#properties-of-the-mle" id="toc-properties-of-the-mle" class="nav-link" data-scroll-target="#properties-of-the-mle">Properties of the MLE</a></li>
  <li><a href="#ols-as-a-maximum-likelihood-estimator" id="toc-ols-as-a-maximum-likelihood-estimator" class="nav-link" data-scroll-target="#ols-as-a-maximum-likelihood-estimator">OLS as a Maximum Likelihood Estimator</a></li>
  <li><a href="#information-criterion-statistics" id="toc-information-criterion-statistics" class="nav-link" data-scroll-target="#information-criterion-statistics">Information Criterion Statistics</a></li>
  </ul></li>
  <li><a href="#logistic-regression-model" id="toc-logistic-regression-model" class="nav-link" data-scroll-target="#logistic-regression-model"><strong>Logistic Regression Model</strong></a>
  <ul class="collapse">
  <li><a href="#model-specification" id="toc-model-specification" class="nav-link" data-scroll-target="#model-specification">Model Specification</a></li>
  <li><a href="#interpretation-and-odds-ratios" id="toc-interpretation-and-odds-ratios" class="nav-link" data-scroll-target="#interpretation-and-odds-ratios">Interpretation and Odds Ratios</a></li>
  <li><a href="#ordinal-logistic-regression" id="toc-ordinal-logistic-regression" class="nav-link" data-scroll-target="#ordinal-logistic-regression">Ordinal Logistic Regression</a></li>
  <li><a href="#multinomial-logistic-regression" id="toc-multinomial-logistic-regression" class="nav-link" data-scroll-target="#multinomial-logistic-regression">Multinomial Logistic Regression</a></li>
  </ul></li>
  <li><a href="#negative-binomial-regression" id="toc-negative-binomial-regression" class="nav-link" data-scroll-target="#negative-binomial-regression"><strong>Negative Binomial Regression</strong></a>
  <ul class="collapse">
  <li><a href="#model-specification-1" id="toc-model-specification-1" class="nav-link" data-scroll-target="#model-specification-1">Model Specification</a></li>
  <li><a href="#interpreting-coefficients" id="toc-interpreting-coefficients" class="nav-link" data-scroll-target="#interpreting-coefficients">Interpreting Coefficients</a></li>
  <li><a href="#poisson-regression" id="toc-poisson-regression" class="nav-link" data-scroll-target="#poisson-regression">Poisson Regression</a></li>
  </ul></li>
  <li><a href="#statistical-inference" id="toc-statistical-inference" class="nav-link" data-scroll-target="#statistical-inference"><strong>Statistical Inference</strong></a>
  <ul class="collapse">
  <li><a href="#hypothesis-testing" id="toc-hypothesis-testing" class="nav-link" data-scroll-target="#hypothesis-testing">Hypothesis Testing</a></li>
  <li><a href="#confidence-intervals" id="toc-confidence-intervals" class="nav-link" data-scroll-target="#confidence-intervals">Confidence Intervals</a></li>
  <li><a href="#likelihood-ratio-test" id="toc-likelihood-ratio-test" class="nav-link" data-scroll-target="#likelihood-ratio-test">Likelihood Ratio Test</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">The Generalised Linear Model</h1>
<p class="subtitle lead">Chapter 1, Quantitative Methods</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In the last chapter, we discussed the classical linear model. However, the classical linear model has a few limitations, which are addressed by the generalised linear model (GLM). In this chapter, we explore the generalised linear model, including logistic and negative binomial regression, and the maximum likelihood estimator used to estimate GLMs.</p>
<p>Use the right sidebar for quick navigation. R-code is provided at the bottom.</p>
<hr>
<section id="generalised-linear-model" class="level1">
<h1><strong>Generalised Linear Model</strong></h1>
<section id="limitations-of-the-classical-model" class="level3">
<h3 class="anchored" data-anchor-id="limitations-of-the-classical-model">Limitations of the Classical Model</h3>
<p>Consider the <a href="./clm.html#linear-probability-model">linear probability model</a> introduced at the end of last chapter:</p>
<p><span class="math display">\[
\P(Y_i = 1|X_i)= \pi_i = \beta_0 + \beta_1X_{i1} + \dots + \beta_pX_{ip} + \eps_i
\]</span></p>
<p>There are two main issues with using the classical model for probabilities:</p>
<ol type="1">
<li>The classical assumptions assume homoscedasticity. However, probability of a binary event is given by the bernoulli distribution, for which <span class="math inline">\(\V \pi_i = \pi_i(1-\pi_i)\)</span>, which is clearly a function of outcome <span class="math inline">\(\pi_i\)</span>.</li>
<li>And more importantly, the linear model will predict probabilities <span class="math inline">\(\P(Y_i = 1 | X_i)\)</span> that are higher than 1 and less than 0, which, if we know the rules of probability, is nonsensical.</li>
</ol>
<p>Similar limitations apply to the classical model when applied to count data (counting the times something occurs) and rate data. This makes the classical linear model unsuited for accurate predictions.</p>
<p>The generalised linear model aims to solve this by not requiring the response variable to just be <span class="math inline">\(Y_i\)</span> (or <span class="math inline">\(\pi_i\)</span> for probabilities). Instead, we can “transform” the response variable through a link function <span class="math inline">\(g(\cdot)\)</span> which will allow the linear model to be applied to other types of data and outcome variables.</p>
<p><br></p>
</section>
<section id="the-generalised-linear-model" class="level3">
<h3 class="anchored" data-anchor-id="the-generalised-linear-model">The Generalised Linear Model</h3>
<p>As noted above, the generalised linear model allows us to “transform” the outcome variable through a link function <span class="math inline">\(g(\cdot)\)</span>, while maintaining the linear structure of the rest of the linear model. There are several of these generalised linear models that we will explore.</p>
<p>Of course, we have the <strong>Classical Linear Model</strong>, which is considered a GLM that has no link function:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1X_{i1} + \beta_p X_{ip} + \eps_i \ = \ x_i^\top\beta + \eps_i
\]</span></p>
<p>The <strong>Logistic Regression Model</strong> is a model that deals with probabilities <span class="math inline">\(\pi_i\)</span>, just like the linear probability model. It solves the limitations of the linear probability model through a link function:</p>
<p><span class="math display">\[
\log \left( \frac{\pi_i}{1 - \pi_i} \right) = \beta_0 + \beta_1X_{i1} + \dots + \beta_p X_{ip} \ = \ x_i^\top\beta
\]</span></p>
<p>Where <span class="math inline">\(\pi_i\)</span> is the probabiltiy of success <span class="math inline">\(p\)</span> for a binomial distribution. The <strong>Probit Model</strong> is another model that deals with probabilities.</p>
<p><span class="math display">\[
\Phi^{-1}(\pi_i) = \beta_0 + \beta_1X_{i1} + \dots + \beta_p X_{ip} \ = \ x_i^\top\beta
\]</span></p>
<p>Where <span class="math inline">\(\Phi\)</span> is the cumulative density function of the standard normal distribution. We will not explore probit in detail, since it produces almost identical results as logistic regressions with worse interpretability.</p>
<p>The <strong>Negative Binomial Model</strong> is a model that deals with count and rate data (only positive values):</p>
<p><span class="math display">\[
\log \lambda_i = \beta_0 + \beta_1X_{i1} + \dots + \beta_p X_{ip} \ = \ x_i^\top \beta
\]</span></p>
<p>Where <span class="math inline">\(\lambda_i\)</span> is the <span class="math inline">\(\E Y_i\)</span> for a negative binomial distribution. A specific version of this regression with a more restrictive distribution assumption is called the <strong>Poisson model</strong>.</p>
<p>We will explore each of these models in more detail later.</p>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="maximum-likelihood-estimator" class="level1">
<h1><strong>Maximum Likelihood Estimator</strong></h1>
<section id="likelihood-function" class="level3">
<h3 class="anchored" data-anchor-id="likelihood-function">Likelihood Function</h3>
<p>All generalised linear models are estimated with an estimator called the maximum likelihood estimator (MLE). This estimator is probably the most widely used in statistics, applied to many methods beyond the generalised linear model.</p>
<p>We have a set of population parameters in the vector <span class="math inline">\(\theta\)</span> we want to estimate. This set of parameters determines our population distribution of <span class="math inline">\(Y_i\)</span>, which we can describe with some <a href="./stats.html#probability-density-functions">probability density function</a> <span class="math inline">\(\varphi(y, \theta)\)</span>. For example, in linear regression, our population <span class="math inline">\(y\)</span> is determined by the population parameters <span class="math inline">\(\beta_0, \dots, \beta_k\)</span>.</p>
<p>When we are estimating parameters, we will have sample data with <span class="math inline">\(n\)</span> number of observations, with each observation <span class="math inline">\(i\)</span> having its own <span class="math inline">\(Y_i\)</span> value. Thus, our sample looks something like <span class="math inline">\((y_1, \dots, y_n)\)</span>.</p>
<p>Based on the probability density function of the population <span class="math inline">\(Y_i\)</span>, the probability of getting our observed <span class="math inline">\(y_1\)</span> in our sample from the population data is <span class="math inline">\(\varphi(y_1, \theta)\)</span>, and the probability of getting observation <span class="math inline">\(y_i\)</span> is <span class="math inline">\(\varphi(y_i, \theta)\)</span>.</p>
<p>We know by the rules of probability, that the <a href="./math.html#basics-of-probability">probability of multiple independent events</a> is the product of their probabilities. Thus, the probability that we get a specific sample with <span class="math inline">\(y\)</span> values <span class="math inline">\((y_1, \dots ,y_n)\)</span>, based on the value of our population parameters <span class="math inline">\(\boldsymbol\theta\)</span> is given by the <strong>likelihood function</strong> <span class="math inline">\(L\)</span>:</p>
<p><span class="math display">\[
\begin{align}
L(\theta, y_1, \dots y_n) &amp; = \varphi(y_1;\theta) \times \varphi(y_2;\theta) \times \dots \times \varphi(y_n;\theta) \\
&amp; = \prod\limits_{i=1}^n \varphi(y_i, \theta) \\
\end{align}
\]</span></p>
<p>We want to find some values of <span class="math inline">\(\boldsymbol\theta\)</span> that make it the highest probability we observe our sample <span class="math inline">\(y_1, \dots ,y_n\)</span>. This is done by maximising the likelihood function <span class="math inline">\(L(\cdot)\)</span>. Those parameters <span class="math inline">\(\theta\)</span> that maximise <span class="math inline">\(L\)</span> will become our estimates for our statistical model.</p>
<p><br></p>
</section>
<section id="log-likelihood-and-score-functions" class="level3">
<h3 class="anchored" data-anchor-id="log-likelihood-and-score-functions">Log-Likelihood and Score Functions</h3>
<p>Maximising the likelihood function <span class="math inline">\(L(\cdot)\)</span> is very difficult, because of the product notation. Luckily, we can use the log of the likelihood function <span class="math inline">\(\ell(\cdot)\)</span>, which retains the same maximum/minimum points as <span class="math inline">\(L(\cdot)\)</span>. Using the properties of logarithms, we can also rewrite this log-likelihood function in terms of summation notation, making maximisation far easier.</p>
<p><span class="math display">\[
\begin{align}
\log L(\boldsymbol\theta, y_1, \dots y_n) &amp; = \log \left(\prod_{i=1}^n \varphi(y_i; \theta) \right) \\
&amp; = \log[\varphi(y_1, \boldsymbol\theta) \times \varphi(y_2, \boldsymbol\theta) \times \dots \times \varphi(y_n, \boldsymbol\theta)] &amp;&amp; \text{(expand product notation)} \\
&amp; = \log[\varphi(y_1, \boldsymbol\theta)] + \log [\varphi(y_2, \boldsymbol\theta)] + \dots + \log[\varphi(y_n, \boldsymbol\theta)] &amp;&amp; \text{(property of logs)} \\
&amp; = \sum\limits_{i=1}^n \log[\varphi(y_i, \boldsymbol\theta)] &amp;&amp; \text{(condense into sum)}
\end{align}
\]</span></p>
<p>Thus, our goal in MLE is to maximise the log-likelihood function (denoted <span class="math inline">\(\ell\)</span>) to obtain our estimates <span class="math inline">\(\hat\theta\)</span>:</p>
<p><span id="eq-loglike"><span class="math display">\[
\hat\theta = \max\limits_{\theta} \ell(\theta; y_i) = \max\limits_\theta \sum\limits_{i=1}^n \log[\varphi(y_i \theta)]
\tag{1}\]</span></span></p>
<p>The vector of first derivatives of <span class="math inline">\(\ell(\theta; y)\)</span> with respect to vector <span class="math inline">\(\theta\)</span> is known as the <strong>score function</strong> <span class="math inline">\(s(\theta, y)\)</span>:</p>
<p><span class="math display">\[
s(\theta;y) = \frac{\partial}{\partial \theta} \ell(\theta; y) = \sum\limits_{i=1}^n \log[\varphi(y_i \theta)]\frac{\partial}{\partial \theta}
\]</span></p>
<p>The <span class="math inline">\(\hat\theta\)</span> values of MLE are the <span class="math inline">\(\theta\)</span> that solve <span class="math inline">\(s(\theta; y) = 0\)</span>. In many cases, the score is not computable, so we will need to rely on other computer techniques and algorithms, such as gradient descent, which we will cover below.</p>
<p><br></p>
</section>
<section id="gradient-descent-algorithms" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent-algorithms">Gradient Descent Algorithms</h3>
<p>In some more complex models, we cannot mathematically find the minimum of the log-likelihood function. So instead, we resort to a series of computer algorithms called gradient descent. The algorithm takes the following form:</p>
<ol type="1">
<li>We randomly choose values of <span class="math inline">\(\boldsymbol\theta\)</span> to start (let us notate the chosen as <span class="math inline">\(\boldsymbol\theta^*\)</span>), and calculate the likelihood <span class="math inline">\(L\)</span> with those chosen at <span class="math inline">\(\boldsymbol\theta^*\)</span>.</li>
<li>We then slightly shift the values of <span class="math inline">\(\boldsymbol\theta^*\)</span> upwards and downwards, calculating all the likelihoods. We see in which shift-direction does the likelihood <span class="math inline">\(L\)</span> increase the most.</li>
<li>Once we determine the direction that <span class="math inline">\(L\)</span> increases the most, we shift int hat direction to a new <span class="math inline">\(\boldsymbol\theta'\)</span> value. Once again, we slightly shift values of <span class="math inline">\(\boldsymbol\theta'\)</span> upwards and downwards, and see which shift-direction does the likelihood <span class="math inline">\(L\)</span> increase the most.</li>
<li>We keep repeating this process of moving in the direction that increases <span class="math inline">\(L\)</span> the most, shifting around in all directions at that point, and once again moving in the direction that increases <span class="math inline">\(L\)</span> the most.</li>
<li>We stop when we are at some point <span class="math inline">\(\boldsymbol\theta^!\)</span> where all directions of shits decrease <span class="math inline">\(L\)</span>. We are “at the top of the mountain”, and that becomes our estimate.</li>
</ol>
<p>This is a very simple gradient descent algorithm. You might point out that this algorithm only works if there is one global maximum, and no local maxima (since we would stop the algorithm at a local extrema if this were the case). This usually is not an issue since in common regression models (linear, logistic, poisson), their is only one global maximum. The figure below shows this:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-3075379377.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%"></p>
</figure>
</div>
<p>A solution for this problem of local maxima (which becomes more an issue with machine learning models) is to basically do the estimation algorithm multiple times, each time starting at some random <span class="math inline">\(\boldsymbol\theta^*\)</span>. Then, we find the time with the largest <span class="math inline">\(L\)</span>. This will in theory help us determine which are local and global maxima.</p>
<p><br></p>
</section>
<section id="properties-of-the-mle" class="level3">
<h3 class="anchored" data-anchor-id="properties-of-the-mle">Properties of the MLE</h3>
<p>Let us find the variance of the score function <span class="math inline">\(s(\theta_0, y)\)</span>, where <span class="math inline">\(\theta_0\)</span> is the value of the parameter <span class="math inline">\(\theta\)</span> when <span class="math inline">\(s(\theta ; y_i) = 0\)</span> is solved, and <span class="math inline">\(y\)</span> is the vector of our observed values of <span class="math inline">\(Y_i\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\V s(\theta_0; y_i) &amp; = \E [(s(\theta_0; y) - \E(s(\theta_0; y))^2 ] &amp;&amp; (\because \V Z = \E[Z - \E Z]) \\
&amp; = \E[(s(\theta_0; y) - 0)^2] &amp;&amp; (\because \E [s(\theta_0 ; y)] = 0) \\
&amp; = \E\left [\frac{\partial \ell (\theta_0; y)}{\partial \theta} \frac{\partial \ell (\theta_0; y)}{\partial \theta^\top} \right] &amp;&amp; \text{(plug in and square } s(\theta_0, y)) \\
&amp; = \E\left[ -\frac{\partial^2 \ell (\theta_0; y)}{\partial\theta\partial \theta^\top}\right] \equiv \mathcal I(\theta_0)
\end{align}
\]</span></p>
<p>Where <span class="math inline">\(\mathcal I (\theta_0)\)</span> is also known as the <strong>expected fisher information matrix</strong>. Through complex math that I will not do here, we can show that asymptotically as <span class="math inline">\(n \rightarrow ∞\)</span>, the distribution of the MLE estimates <span class="math inline">\(\hat\theta\)</span> becomes:</p>
<p><span class="math display">\[
\hat\theta \sim \mathcal N(\theta_0, \mathcal I(\theta_0)^{-1})
\]</span></p>
<p>This tells us two things. First, maximum likelihood estimates are asymptotically consistent, since the asymptotic distribution has an expectation <span class="math inline">\(\E \hat\theta = \theta_0\)</span>.</p>
<p>Second, this tells us the asumptotic variance of the MLE. To calculate the variance, we can estimate it with the expected fisher information matrix of our estimate <span class="math inline">\(\hat\theta\)</span>:</p>
<p><span class="math display">\[
\V \hat\theta = \mathcal I(\hat\theta)^{-1}
\]</span></p>
<p>It can also be proven with the Cramer-Rao lower bound that the MLE is the asymptotic consistent estimator with the lowest asymptotic variance. However, do note that the MLE is biased in finite-samples (but the bias becomes small in large-samples). This becomes an issue when we are dealing with some applications of causal inference, such as fixed effects.</p>
<p><br></p>
</section>
<section id="ols-as-a-maximum-likelihood-estimator" class="level3">
<h3 class="anchored" data-anchor-id="ols-as-a-maximum-likelihood-estimator">OLS as a Maximum Likelihood Estimator</h3>
<p>Earlier, we noted that all linear models are estimated with a Maximum Likelihood Estimator. This includes the classical linear model, as OLS is a MLE under classical conditions.</p>
<p>We know in the classical model, <span class="math inline">\(\mu = E(y|x) = x_i^\top \beta\)</span>. Let us plug that into the <a href="quant1.qmd#the-normal-distribution">probability density function of a normal distribution</a> to get the PDF of <span class="math inline">\(Y_i\)</span> in a simple linear regression. By <a href="#eq-loglike" class="quarto-xref">Equation&nbsp;1</a>, the log-likelihood function <span class="math inline">\(\ell\)</span> of our sample for linear regression is:</p>
<p><span class="math display">\[
\begin{align}
\ell(\beta, \sigma^2; y_i)
&amp; = \sum\limits_{i=1}^n \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} e^{\left( -\frac{1}{2 \sigma^2}(y_i - x_i^\top \beta)^2\right)} \right) \\
&amp; = \sum\limits_{i=1}^n \log (1) - \log (\sqrt{2\pi\sigma^2})   + \log\left( e^{\left( -\frac{1}{2 \sigma^2}(y_i - x_i^\top\beta )^2\right)}\right) &amp;&amp; \text{(prop. of logs)} \\
&amp; = \sum\limits_{i=1}^n 0 - \frac{1}{2}\log ({2\pi\sigma^2})  + \left( -\frac{1}{2 \sigma^2}(y_i - x_i^\top\beta)^2\right) &amp;&amp; \text{(prop. of logs)} \\
&amp; = -\frac{n}{2} \log (2\pi\sigma^2)  -\frac{1}{2 \sigma^2}\sum\limits_{i=1}^n(y_i - x_i^\top\beta)^2 &amp;&amp; \text{(prop. of sums)}
\end{align}
\]</span></p>
<p>Now, let us take the derivative in respect to vector <span class="math inline">\(\beta\)</span> to solve for our score function:</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial \ell}{\partial \beta} &amp; = 0 + \frac{1}{\sigma^2}\sum\limits_{i=1}^n(y_i - x_i^\top\beta)x_i &amp;&amp; \text{(by chain and power rule)} \\
&amp; = \frac{1}{\sigma^2} \left[ \sum\limits_{i=1}^n y_i x_i - \sum\limits_{i=1}^n x_i x_i^\top \beta\right] &amp;&amp; \text{(multiply out, prop. of sums)}\\
&amp; = \frac{1}{\sigma^2} (X^\top y - (X^\top X)\beta) &amp;&amp; \text{(vector to matrix notation)}
\end{align}
\]</span></p>
<p>And now set the score function equal to 0 and solve for <span class="math inline">\(\beta\)</span>. We can ignore the <span class="math inline">\(1/\sigma^2\)</span> out front since if the rest equals 0, the score function equals 0.</p>
<p><span class="math display">\[
\begin{align}
0 &amp; = X^\top y - (X^\top X)\beta \\
(&amp; X^\top X)\beta = X^\top y &amp;&amp; (+(X^\top X)\beta \text{ to both sides}) \\
\beta &amp; = (X^\top X)^{-1} X^\top y &amp;&amp; (\times (X^\top X)^{-1} \text{ to both sides})
\end{align}
\]</span></p>
<p>And we can see, our MLE estimates are exactly the same form as the OLS estimates.</p>
<p><br></p>
</section>
<section id="information-criterion-statistics" class="level3">
<h3 class="anchored" data-anchor-id="information-criterion-statistics">Information Criterion Statistics</h3>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="logistic-regression-model" class="level1">
<h1><strong>Logistic Regression Model</strong></h1>
<section id="model-specification" class="level3">
<h3 class="anchored" data-anchor-id="model-specification">Model Specification</h3>
<p>Include definition of <span class="math inline">\(\pi_i\)</span>., and odds <span class="math inline">\(pi_i / (1- \pi_i)\)</span>.</p>
<p>Include regression writeen in terms of <span class="math inline">\(\pi_i\)</span>. (in in for box, show derivation).</p>
<p>Show fitted values. plot.</p>
<p><br></p>
</section>
<section id="interpretation-and-odds-ratios" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-and-odds-ratios">Interpretation and Odds Ratios</h3>
<p><br></p>
</section>
<section id="ordinal-logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="ordinal-logistic-regression">Ordinal Logistic Regression</h3>
<p><br></p>
</section>
<section id="multinomial-logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="multinomial-logistic-regression">Multinomial Logistic Regression</h3>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="negative-binomial-regression" class="level1">
<h1><strong>Negative Binomial Regression</strong></h1>
<section id="model-specification-1" class="level3">
<h3 class="anchored" data-anchor-id="model-specification-1">Model Specification</h3>
<p>Talk about negative binomial distribution.</p>
<p>Include model for rates, and fitted probabilities.</p>
<p><br></p>
</section>
<section id="interpreting-coefficients" class="level3">
<h3 class="anchored" data-anchor-id="interpreting-coefficients">Interpreting Coefficients</h3>
<p><br></p>
</section>
<section id="poisson-regression" class="level3">
<h3 class="anchored" data-anchor-id="poisson-regression">Poisson Regression</h3>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="statistical-inference" class="level1">
<h1><strong>Statistical Inference</strong></h1>
<section id="hypothesis-testing" class="level3">
<h3 class="anchored" data-anchor-id="hypothesis-testing">Hypothesis Testing</h3>
<p><br></p>
</section>
<section id="confidence-intervals" class="level3">
<h3 class="anchored" data-anchor-id="confidence-intervals">Confidence Intervals</h3>
<p>add odds ratios</p>
<p><br></p>
</section>
<section id="likelihood-ratio-test" class="level3">
<h3 class="anchored" data-anchor-id="likelihood-ratio-test">Likelihood Ratio Test</h3>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>