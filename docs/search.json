[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kevin’s PSPE Resources",
    "section": "",
    "text": "Use the left sidebar for navigation between topics. Use the right sidebar to navigate within a topic. There is also a search bar on the left sidebar if you need it.\nThis collection is best used with a computer, as the layout is more optimised. For mobile users, there should be a button on the top-left to access the navigation sidebar."
  },
  {
    "objectID": "index.html#navigating-the-collection",
    "href": "index.html#navigating-the-collection",
    "title": "Kevin’s PSPE Resources",
    "section": "",
    "text": "Use the left sidebar for navigation between topics. Use the right sidebar to navigate within a topic. There is also a search bar on the left sidebar if you need it.\nThis collection is best used with a computer, as the layout is more optimised. For mobile users, there should be a button on the top-left to access the navigation sidebar."
  },
  {
    "objectID": "index.html#further-resources",
    "href": "index.html#further-resources",
    "title": "Kevin’s Political Science and Political Economy Resources",
    "section": "Further Resources",
    "text": "Further Resources\nMathematics:\n\nDavid Siegel’s Youtube Channel (Mathematics for Political Scientists)\nKhan Academy\n\nQuantitative Methods (Causal Inference):\n\nBen Lambert’s Youtube Channel\nBurkey Academy\nMY457 Causal Inference for Experimental and Observational Studies\nAngrist and Pischke: Mastering Metrics (undergrad level applied econometrics)\nAngrist and Pishke: Mostly Harmless Econometrics (graduate level applied econometrics)\nWooldridge: Introductory Econometrics (the classic)\nStock and Watson: Introduction to Econometrics (another classic)\nImbens and Rubin: Causal Inference for Statistics, Social, and Biomedical Sciences\nHuntington-Klein: The Effect"
  },
  {
    "objectID": "index.html#about-the-creator",
    "href": "index.html#about-the-creator",
    "title": "Kevin’s Political Science and Political Economy Resources",
    "section": "About the Creator",
    "text": "About the Creator\nThis collection was created by Kevin.\n\nk.l.li1@lse.ac.uk\nhttps://kevinli03.github.io"
  },
  {
    "objectID": "games.html",
    "href": "games.html",
    "title": "Guide to Game Theory",
    "section": "",
    "text": "Use the right sidebar for easy navigation.\n\n\nBasics of Game Theory\n\nGames and Notations\nGames consist of:\n\nA set of players \\(i = 1, \\dots, N\\)\nFor each player \\(i\\), a set of actions \\(a_i\\) and strategies \\(s_i\\) or \\(\\sigma_i\\).\nAn action profile of all player’s actions \\(a = (a_1, \\dots, a_N)\\), also notated \\(a = (a_i, a_{-i})\\), where \\(a_{-i}\\) is the actions of all other players not player \\(i\\).\nA strategy profile of all player’s strategies \\(s = (s_1, \\dots, s_N)\\), also notated \\(s = (s_i, s_{-i})\\).\nFor each player \\(i\\), preferences over the action profiles, specified by a payoff function \\(u_i(s_i, s_{-i})\\).\n\n\n\n\n\n\n\nStrategies vs. Actions\n\n\n\n\n\nStrategies and Actions are two different things.\nAn action is something you do on a particular moment a player has to make a move.\n\nFor example, in chess, an action might be move the knight to some square.\n\nA strategy is a complete plan of action - i.e. a plan of all the actions you will take in the game, for all the possible alternate scenarios that could occur in the game.\n\nFor example, in chess, a strategy would be a complete plan of what actions to take, for every possible response of your opponent, for every possible move throughout the entire game.\n\nFor static-complete information games, these two are often equivalent, but it is important to make this distinction for dynamic and incomplete information games.\n\n\n\n\n\n\nPreference Theory\nLet us define \\(X\\) as a set of alternatives that some agent \\(i\\) can achieve. Alternatives are both mutually exclusive and exhaustive.\n\n\n\n\n\n\nMutually Exclusive and Exhaustive Alternatives\n\n\n\n\n\nMutually exclusive means that if you end up with one alternative, you cannot end up with any other alternative at the same time.\n\nFor example, if I get a 95 on my exam, I cannot possibly also get a 85 on the same exam. Having gotten one score on my exam, the other alternatives cannot be obtained.\n\nExhaustive means \\(X\\) consists of all possibilities - the set of all possible outcomes.\n\nFor example, the potential scores on an exam are 0 - 100, so the set \\(X\\) includes all of these potential scores. There are no potential outcomes I can get outside of \\(X\\).\n\n\n\n\nA preference relation is a relationship between two elements of \\(X\\):\n\nIf \\(x \\succ y\\), the agent strictly prefers \\(x\\) to \\(y\\) - i.e. when given the choice, they will always choose \\(x\\).\nIf \\(x \\sim y\\), the agent is indifferent between \\(x\\) and \\(y\\).\nIf \\(x \\succsim y\\), the agent weakly prefers \\(x\\) to \\(y\\) - i.e. the agent either prefers \\(x\\) to \\(y\\), or is indifferent, but never prefers \\(y\\) to \\(x\\).\n\nPreferences are assumed to be rational if they are complete and transitive.\n\n\n\n\n\n\nComplete and Transitive Preferences\n\n\n\n\n\nComplete preferences means for all \\(x, y \\in X\\), either \\(x \\succsim y\\) or \\(y \\succsim x\\).\n\nFor any two alternatives, the agent always has some type of preference relation (never “no opinion”).\nThis does not mean they cannot be indifferent (that is a preference relation). It just means that if you ask the agent how they feel between \\(x\\) and \\(y\\), they must be able to say they prefer one, or are indifferent, and not say “I don’t have an opinion”.\n\nTransitive preferences means that for \\(x, y, z \\in X\\), if \\(x \\succsim y\\), and \\(y \\succsim z\\), then \\(x \\succsim z\\).\n\nNote: group preferences are not always transitive (condercet’s paradox), and thus not rational. But for individual agent purposes, we generally assume to this to be true.\n\n\n\n\n\n\n\nUtility Functions and Maximal Set\nA utility function is a mapping \\(u: X \\rightarrow \\mathbb R\\), such that for any \\(x, y \\in X\\), \\(u(x) ≥ u(y)\\) if and only if \\(x \\succsim y\\). Or in other words, the utility function should match the preference relations.\n\n\n\n\n\n\nNotes on Utility Functions\n\n\n\n\n\nThe mapping of utility functions is often ordinal, not continuous.\n\nFor example, let us say \\(u(x) = 2\\) and \\(u(y) = 1\\). If \\(u\\) is ordinal, we can say that \\(x\\) is preferred to \\(y\\). But, we cannot say \\(x\\) is preferred twice as much as \\(y\\), since for ordinal, the distance means nothing - only the magnitude.\n\nAny rational set of preferences (both complete and transitive) can be represented by a utility function.\n\nThe proof is beyond the scope of this lesson. But we can quickly show why if either are violated, they cannot be represented.\nIf the preferences are incomplete, we cannot assign a number utility to the alternative that the individual is unsure about/does not have an opinion on.\nIf the preferences are not transitive, we cannot create a utility function, because numbers \\(\\mathbb R\\) (which the utility function uses as a mapping) are transitive, and you cannot map something non-transitive into something transitive.\n\n\n\n\nWhat is the best choice from \\(X\\) that an agent can make (rationality)?\nWe define the “best choice” as outcomes/alternatives included in the maximal set of \\(X\\), notated \\(M(\\succsim, X)\\):\n\\[\n\\begin{split}\nM(\\succsim, X) & = \\{ x \\in X : x \\succsim y, \\ \\forall \\ y \\in X \\} \\\\\n& = \\arg \\max\\limits_{x \\in X} \\{u(x) \\}\n\\end{split}\n\\]\nOr in more intuitive terms, the inputs \\(x \\in X\\) that make the utility function \\(u \\in \\mathbb R\\) achieve its maximum value, are the elements of \\(x\\) in the maximal set \\(M(\\succsim, X)\\).\n\n\n\n\n\n\nStatic Games of Perfect Information\n\nDominant Strategy\nA strategy \\(s_i^D\\) is a strictly dominant strategy for player \\(i\\) if it gives a higher payoff than any other strategy of player \\(i\\), no matter the choice of strategy of other players.\n\\[\n\\underbrace{u_i(s_i^D, s_{-i})}_{S_i^D \\text{ utility} } &gt; \\underbrace{u_i (s_i, s_{-i})}_{\\text{utility of others}} \\quad \\forall \\ s_{-i}, \\ \\forall s_i ≠ s_i^D\n\\]\nA strategy \\(s_i^1\\) that always produces a lower payoff than \\(s_i^2\\) is said to be strictly dominated.\n\n\n\n\n\n\nWeakly Dominant Strategies\n\n\n\n\n\nA strategy \\(s_i^D\\) is weakly dominant for player \\(i\\) if it gives an equal or higher payoff than any other strategy of player \\(i\\), no matter the choice of strategy of other players.\n\\[\n\\underbrace{u_i(s_i^D, s_{-i})}_{S_i^D \\text{ utility}} ≥ \\underbrace{u_i (s_i, s_{-i})}_{\\text{utility of others}} \\quad \\forall \\ s_{-i}, \\ \\forall s_i ≠ s_i^D\n\\]\nA strategy \\(s_i^1\\) that always produces a lower or equal payoff than \\(s_i^2\\) is said to be weakly dominated.\n\n\n\n\n\n\n\n\n\nEliminating Dominated Strategies\n\n\n\n\n\nA strictly dominated strategy can be eliminated, as it will never be played. A strictly dominated strategy will never form a part of any equilibrium (pure or mixed).\nA weakly dominated strategy cannot be eliminated. It can be a best response in certain cases.\n\nHowever, if you have multiple equilibria, some that have weakly dominated strategies, you can eliminate some of them as they are often considered non-pivotal or nonsensical, which narrows the amount of equilibrium you have.\n\n\n\n\nA strategy profile \\(s^D\\) is a dominant strategy equilibrium, if every player is playing a dominant strategy. Formally, a strategy profile \\(s^D = (s_1^D, \\dots, s_N^D)\\) is a dominant strategy equilibrium, if \\(s_i^D\\) is a dominant strategy for all \\(i\\):\n\\[\n\\underbrace{u_i(s_i^D, s_{-i})}_{S_i^D \\text{ utility} } &gt; \\underbrace{u_i (s_i, s_{-i})}_{\\text{utility of others}} \\quad \\forall \\ s_{-i}, \\ \\forall s_i ≠ s_i^D, \\ \\forall \\ i\n\\]\nDominant Strategy Equilibrium, if it exists, is always unique. However, it often does not exist.\n\n\n\n\n\n\nExample: Prisoner’s Dilemma\n\n\n\n\n\nTake the classic game: the prisoner’s dilemma. The matrix representation is as follows:\n\\[\n\\begin{matrix}\n& C_2 & D_2 \\\\\nC_1 & (3, 3) & (0, 4) \\\\\nD_1 & (4, 0) & (1,1)\n\\end{matrix}\n\\]\n\nPayoffs are given as (player picking between rows, player picking between columns)\n\nLet us look at the row player’s decision. First, let us hold constant their opponent (the column player) and assume they choose cooperate.\n\\[\n\\begin{matrix}\n& C_2  \\\\\nC_1 & (3, 3) \\\\\nD_1 & (4, 0)\n\\end{matrix}\n\\]\nWe can see \\(4 &gt; 3\\), so the row player should choose defect.\nNow, let us hold constant their opponent (the column player) and assume they choose defect.\n\\[\n\\begin{matrix}\n& D_2 \\\\\nC_1 & (0, 4) \\\\\nD_1 & (1,1)\n\\end{matrix}\n\\]\nWe can see \\(1&gt;0\\), so the row player should choose defect.\nThus, the row player, no matter what their opponent does, should choose defect. Thus, defect is a dominant strategy.\n\nSince this game is symmetrical, the column player also has a dominant strategy of defect.\nThus, the dominant strategy equilibrium of this game is (defect, defect).\n\n\n\n\n\n\nBest Responses\nThe strategy \\(s_i\\) for player \\(i\\), is a best response to a specific opponent’s strategy \\(s_{-i}\\), if it yields maximum payoff to \\(i\\) holding the opponents specific strategy \\(s_{-i}\\) constant.\n\\[\n\\underbrace{u_i(s_i, s_{-i})}_{\\text{utility of } s_i} ≥ \\underbrace{u_i(s_i', s_{-i})}_{\\text{utility for others}} \\quad \\forall \\ s_i'\n\\]\nIn other words, that means, holding the opponents strategy constant, player \\(i\\) has no profitable deviation to any other strategy to obtain a better payoff.\nBest responses of player \\(i\\) are denoted as the following:\n\\[\nBr_i(s_{-i}) = s_i\n\\]\n\n\n\n\n\n\nExample: Stag-Hunt Game\n\n\n\n\n\nTake the classic game: the stag-hunt game. The matrix representation is as follows:\n\\[\n\\begin{matrix}\n& S_2 & H_2 \\\\\nS_1 & (2, 2) & (0, 1) \\\\\nH_1 & (1, 0) & (1,1)\n\\end{matrix}\n\\]\nLet us compute the best responses of the player choosing between rows, to each of the opponents possible strategies.\nLet us say the opponent plays stag. What is the row player’s best response?\n\\[\n\\begin{matrix}\n& S_2 \\\\\nS_1 & (2, 2) \\\\\nH_1 & (1, 0)\n\\end{matrix}\n\\]\nWe can see \\(2&gt;1\\), thus the row player’s best response in this case is to play stag.\nLet us say the opponent plays hunt. What is the row player’s best response?\n\\[\n\\begin{matrix}\n& H_2 \\\\\nS_1  & (0, 1) \\\\\nS_2 & (1,1)\n\\end{matrix}\n\\]\nWe can see \\(1&gt;0\\), thus the row player’s best response in this case is to play hunt.\nWe can notate the best responses of the row player (player 1) as follows:\n\\[\n\\begin{cases}\nBr_1(S_2) = S_1 \\\\\nBr_1(H_2) = H_1\n\\end{cases}\n\\]\n\n\n\n\n\n\nPure Strategy Nash Equilibrium\nA strategy profile \\(s^*\\) is a Nash Equilibrium, when all players are playing best responses (and no player has any profitable deviations, holding the opponents strategy consistent):\n\\[\n\\underbrace{u_i(s_i^*, s_{-i}^*)}_{s_i^* \\text{ utility}} ≥ \\underbrace{u_i(s_i', s_{-i}^*)}_{\\text{utility of others}} \\quad \\forall \\ s_i', \\ \\forall \\ i\n\\]\nThere are a few characteristics of Nash Equilibrium:\n\nThere can be more than one Nash Equilibrium in a game, thus, they are not unique.\nAll dominant strategy equilibrium are Nash Equilibrium as well, but not all Nash Equilibrium are dominant strategy equilibrium.\n\nNash Equilibrium can be computed in 2 ways:\n\nCalculate all the best responses of each player. The strategy profiles where all players are playing best responses is a Nash Equilibrium.\nTest different cases of strategy profiles. If no profitable deviation exists while holding other player’s strategies constant, then that is a Nash Equilibrium.\n\n\n\n\n\n\n\nExample: Assurance Game\n\n\n\n\n\nTake this famous game - the assurance game. The matrix form is as follows:\n\\[\n\\begin{matrix}\n& B_2 & R_2 \\\\\nB_1 & (1, 1) & (3, 0) \\\\\nR_1 & (0, 3) & (4,4)\n\\end{matrix}\n\\]\nTo find the Nash Equilibrium, let us compute best responses for both players. Start with the row player (best responses underlined and bolded):\n\\[\n\\begin{matrix}\n& B_2 & R_2 \\\\\nB_1 & (\\underline{\\mathbf 1}, 1) & (3, 0) \\\\\nR_1 & (0, 3) & (\\underline{\\mathbf 4},4)\n\\end{matrix}\n\\]\nNow, let us find the best responses of the column player:\n\\[\n\\begin{matrix}\n& B_2 & R_2 \\\\\nB_1 & (\\underline{\\mathbf 1}, \\underline{\\mathbf 1}) & (3, 0) \\\\\nR_1 & (0, 3) & (\\underline{\\mathbf 4},\\underline{\\mathbf 4})\n\\end{matrix}\n\\]\nWe can see that strategy profiles (nukes, nukes) and (no nukes, no nukes) are both strategy profiles where both players are playing best responses. Thus, these two strategy profiles are the Nash Equilibrium.\n\n\n\n\n\n\n\n\n\nTrembling Hand Perfect Equilibrium\n\n\n\n\n\nOne assumption of Game Theory is that players are always rational - they will always aim to maximise their payoff. However, in the real world, this isn’t always the case. Sometimes, players will play an unoptimal action.\nIf our goal of our models is to make predictions about outcomes, we want our models to be robust.\nThe Trembling Hand Perfect Equilibrium assumes that one of the players plays an unoptimal (non-Nash Equilibrium) action \\(\\epsilon\\) proportion of the time.\nIf the Nash Equilibrium remains the Nash Equilibrium, even with one player acting unoptimally \\(\\epsilon\\) of the time, then the equilibrium is robust.\nTake this following game:\n\\[\n\\begin{matrix}\n& L & R \\\\\nU & (1, 1) & (2, 0) \\\\\nD & (0, 2) & (2,2)\n\\end{matrix}\n\\]\nThis game has two Nash Equilibriua: (Up, Left) and (Down, Right).\nLet us test the robustness of the (Up, Left) Equilibrium. Let us assume player 1 (row player) is irrational at probability \\(\\epsilon\\), and selects Down. The remainder of the time \\(1 - \\epsilon\\) they still play the optimal Up. Let us call this strategy \\(s_1^\\epsilon\\)\nWill player 2 still prefer to play Left and mantain the current equilibrium?\nWe can solve this by finding the expected utility of player 2 playing Left and Right, and seeing if the utility of Left is still higher. Remember, player 1 is playing strategy \\(s_1^\\epsilon\\).\n\\[\n\\begin{cases}\nu_2(L, s_i^\\epsilon) = P(U) \\times 1 + P(D) \\times 2 = (1-\\epsilon)(1) + \\epsilon(2) = 1 + \\epsilon \\\\\nu_2(R, s_i^\\epsilon) = P(U) \\times 0 + P(D) \\times 2 = (1-\\epsilon)(0) + \\epsilon(2) = 2 \\epsilon\n\\end{cases}\n\\]\nIs the utility for player 2 of playing Left greater or equal than playing Right?\n\\[\n\\begin{split}\nu_2(L, s_i^\\epsilon) & ≥ u_2(R, s_i^\\epsilon) \\\\\n1 + \\epsilon & ≥ 2 \\epsilon \\\\\n\\epsilon & ≤ 1\n\\end{split}\n\\]\nThis tells us that Left is the preferred strategy of player 2, as long as \\(\\epsilon ≤ 1\\). Remember, \\(\\epsilon\\) is a probability between 0 and 1. That means, all values of \\(\\epsilon\\) ensure Left is the preferred strategy.\nThus, (Up, Left) is still the equilibrium, no matter how irrational player 1 might be.\n\n\n\n\n\n\nMixed Strategies\nLet us say player \\(i\\) has possible strategies \\(\\{s_1, \\dots, s_m \\}\\). A mixed strategy for player \\(i\\) is a probability distribution over their set of strategies:\n\\[\n\\sigma_i = (\\sigma_i(s_1), \\dots, \\sigma_i(s_m))\n\\]\nWhere \\(\\sigma_i(s_k)\\) is the probability of player \\(i\\) playing their strategy \\(s_k\\).\n\n\n\n\n\n\nExample of a Mixed Strategy\n\n\n\n\n\nLet us say player \\(i\\) has some mixed strategy:\n\\[\n\\sigma_i = (0.25, 0.75)\n\\]\nWhat this means is that player 1 will play their first strategy \\(s_1\\) 25% of the time, and play their second strategy \\(s_2\\) 75% of the time.\n\n\n\nMixed Strategies have a few properties:\n\n\\(\\sigma_i(s_k) \\in [0, 1], \\quad \\forall \\ s_k \\in \\{s_1, \\dots, s_m \\}\\). This is because of the laws of probability - a probability must be between 0 and 1.\n\\(\\sum\\limits_k \\sigma_i(s_k) = 1\\), In other words - all probabilities in the mixed strategy should sum to 1.\n\nPure strategies are technically mixed-strategies where one strategy has probability 1, and all other strategies have probability 0.\n\n\n\nMixed Strategy Nash Equilibrium\nThe mixed-strategy profile \\(\\sigma^* = (\\sigma_1^*, \\dots, \\sigma_N^*)\\) is a Nash Equilibrium if \\(\\sigma_i^*\\) is a best response to \\(\\sigma_{-i}^*\\) for all players \\(i\\).\n\\[\n\\underbrace{u_i(\\sigma_i^*, \\sigma_{-i}^*)}_{\\sigma_i^* \\text{utility}} ≥ \\underbrace{u_i(\\sigma_i', \\sigma_{-i}^*)}_{\\text{utility of others}} \\quad \\forall \\ \\sigma_i', \\ \\forall \\ i\n\\]\nA few notes on Mixed Strategy Nash Equilibrium.\n\nIf a mixed strategy for player \\(i\\), \\(\\sigma_i^*\\) is in a Nash Equilibrium, player \\(i\\) must be indifferent between the strategies they are mixing. This is because if player \\(i\\) preferred one strategy over all others, they would simply play that strategy, not mix.\nAll pure-strategy Nash equilibrium are also mixed-strategy nash equilibrium, just that the mixed strategy in question has a probability of playing one strategy 100% of the time.\nAll simultaneous games where all players have a finite set of strategies, has a mixed strategy Nash Equilibrium (this proof got John Nash as Nobel Prize).\n\nFinding Mixed Strategy Nash Equilibrium should take this procedure:\n\nFind all pure-strategy Nash Equilibrium.\nIf there are any strategies that are strictly dominated by one other strategy, eliminate them, as they are never a part of mixed-strategy Nash.\nFind the mixed strategy nash equilibrium (shown below).\n\n\n\n\n\n\n\nExample: Matching Pennies Game\n\n\n\n\n\nTake the following Matching Pennies game:\n\\[\n\\begin{matrix}\n& H_2 & T_2 \\\\\nH_1 & (1, -1) & (-1, 1) \\\\\nT_1 & (-1, 1) & (1,-1)\n\\end{matrix}\n\\]\nFirst, let us find the pure-strategy Nash Equilibrium.\n\\[\n\\begin{matrix}\n& H_2 & T_2 \\\\\nH_1 & (\\underline{\\mathbf 1}, -1) & (-1, \\underline{\\mathbf 1}) \\\\\nT_1 & (-1, \\underline{\\mathbf 1}) & (\\underline{\\mathbf 1},-1)\n\\end{matrix}\n\\]\nWe can see there are no pure-strategy Nash Equilibrium. We also do not have any strictly dominated strategies.\nWe now need to find the probabilities each player plays each strategy at their disposal.\nLet us define \\(p\\) as player 1’s (row player) probability of playing Heads, which means they will play Tails with probability \\(1-p\\). Similarly, let us define \\(q\\) as player 2’s probability of playing Heads, which means they will play tails at probability \\(1-q\\):\n\\[\n\\begin{matrix}\n& H_2 & T_2 \\\\\nH_1 & (1, -1) & (-1, 1) & p \\\\\nT_1 & (-1, 1) & (1,-1) & 1-p \\\\\n& q & 1-q\n\\end{matrix}\n\\]\nLet us look at player 1. What is their expected utility when playing Heads or Tails (which depends on player 2’s probability \\(q\\)):\n\\[\n\\begin{cases}\nu_1(H_1) = 1q -1(1-q) = 2q - 1 \\\\\nu_1(T_1) = -1q + 1(1-q) = -2q + 1\n\\end{cases}\n\\]\nWe know that for player 1 to mix between these two strategies, they must be indifferent. Thus:\n\\[\n\\begin{split}\nu_1(H_1) & = u_1(T_1) \\\\\n2q - 1 & = -2q + 1 \\\\\n4q & = 2 \\\\\nq & = \\frac{1}{2}\n\\end{split}\n\\]\nThus, player 1 will play Heads 50% of the time.\nWe can do the same exercise with player 2, and we will also get \\(p = \\frac{1}{2}\\).\nThus, the Mixed Strategy Nash Equilibrium is: \\(\\left( (\\frac{1}{2}, \\frac{1}{2}), (\\frac{1}{2},\\frac{1}{2}) \\right)\\).\n\nThis is in the form \\(((\\sigma_1(H_1), \\sigma_1(T_1), (\\sigma_2(H_2), \\sigma_2(T_2))\\)\n\n\n\n\n\n\n\n\n\n\nDynamic Games of Complete Information\n\nStrategies and Decision Nodes\nIn dynamic games, players move sequentially. This means that if player 1 moves first, player 2 may have different “states” of the game in which to make a decision.\nEach potential location in the game in which a player has to choose an action is called a decision node.\nA strategy, by definition, is a complete plan of action. This implies that a strategy of a player must include what a player would do at every possible decision node in the game. Even if the decision node is never reached, the strategy profile of the player must specify an action to do at that decision node.\nThe total number of different strategies is the product of the number of actions possible at every decision node.\n\n\n\n\n\n\nExample of Strategies: Budget Game\n\n\n\n\n\nTake a typical dynamic game (where players move sequentially):\n\n\n\n\n\nIn this game, Congress has one decision node, and the President has 2 decision nodes.\nThus, congress has 2 strategies: Small Budget, or Large Budget.\nPresident has 4 strategies, since he has 2 in the first decision node, and 2 in the second, and the total number of strategies is the product: \\(2 \\times 2\\).\n\nThe strategies are: (Approve, Approve), (Approve, Reject), (Reject, Approve), (Reject, Reject)\nThe first part of each represents what they would do in the top decision node (after P1 plays small budget), the second part of each represents the bottom decision node.\n\n\n\n\n\n\n\nNash Equilibrium in Dynamic Games\nNash Equilibrium have the same definition in Dynamic Games - a strategy profile where all players are playing best responses, and no player has a profitable deviation.\nHowever, finding Nash Equilibrium for Dynamic Games requires you to convert the tree-form into normal form.\n\n\n\n\n\n\nExample of Normal Form: Budget Game\n\n\n\n\n\nTake a typical dynamic game (where players move sequentially):\n\n\n\n\n\nWe discussed the potential strategies of each player above (let us simplify the strategies to the first letter of their name):\n\nCongress: S or L\nPresident: (AA), (AR), (RA), (RR) - the first letter represents what they would do in the top decision node (after P1 plays small budget).\n\nWe put this into normal form as follows:\n\\[\n\\begin{matrix}\n& AA & AR & RA & RR \\\\\nS & (10, 2) & (10, 2) & (2, 1) & (2, 1)\\\\\nL & (5, 10) & (0, 0) & (5, 10) & (0, 0)\n\\end{matrix}\n\\]\nWe can quickly find the Nash Equilibrium in all the same ways as before:\n\\[\n\\begin{matrix}\n& AA & AR & RA & RR \\\\\nS & (\\underline{\\mathbf{10}}, \\underline{\\mathbf 2}) & (\\underline{\\mathbf{10}}, \\underline{\\mathbf 2}) & (2, 1) & (\\underline{\\mathbf 2}, 1)\\\\\nL & (5, \\underline{\\mathbf{10}}) & (0, 0) & (\\underline{\\mathbf 5}, \\underline{\\mathbf{10}}) & (0, 0)\n\\end{matrix}\n\\]\nWe have three Nash Equilibria: \\((S, AA), \\ (S, AR), \\ (L, RA)\\).\n\n\n\nHowever, Nash Equilibrium have a problem in dynamic games: actions at nodes not reached on the path of equilibrium, do not affect payoffs.\n\nThis means that Nash Equilibrium include solutions where players do not play optimally off the path (what we call non-credible threats).\nThis is a big issue, since off-the-path behaviour determines how previous players anticipate what to do.\n\n\n\n\n\n\n\nExample of Non-Credible Threats: Budget Game\n\n\n\n\n\nTake a typical dynamic game (where players move sequentially):\n\n\n\n\n\nFrom above, we found three Nash Equilibrium: \\((S, AA), \\ (S, AR), \\ (L, RA)\\).\nLet us take a look at the third Nash Equilibrium, \\(L, RA\\):\nThis equilibrium says that the President will choose to Reject in the top node (after congress passes a small budget). But, this makes no sense - we can see President can increase their payoff to 2 if they choose approve.\nIf president does play optimally and moves to Approve in the top node, now Congress has a profitable deviation from \\(L\\) to \\(S\\), as they can go from a payoff of 5 to 10.\nThus, this \\(L, RA\\) equilibrium does not make much sense - it completely depends on the assumption that off-the-path on the top-node, the President will choose an unoptimal action of \\(R\\).\nThis shows the issue with Nash Equilibrium in Dynamic Games.\n\n\n\n\n\n\nSubgame Perfect Nash Equilibrium\nA Subgame Perfect Nash Equilibrium (SPNE) is a subset of Nash Equilibria, where all players act optimally, even in nodes that are not reached in equilibrium.\nSubgame Perfect Nash Equilibrium can also be defined as a strategy profile that is a Nash Equilibrium in all Subgames of the Game.\n\nA subgame of an extensive game is a game consisting of any node and its subsequent actions and nodes.\n\nTo find SPNE, we use backwards induction:\n\nStart with the last player to move. Determine their optimal action, and eliminate their non-optimal actions.\nNow move forward to the second-to-last player to move. Determine their optimal action - assuming that the outcomes eliminated in step 1 are not feasible to obtain.\nNow keep moving forward one player, and find their optimal action, until you reach the front.\n\n\n\n\n\n\n\nExample of Backwards Induction\n\n\n\n\n\nTake this following dynamic game:\n\n\n\n\n\nLet us start with the last player to move, player 3.\n\nOn the left decision node, \\(9&gt;4\\), so player 3 chooses \\(r\\).\nOn the middle decision node, \\(8&gt;7\\), so player 3 chooses \\(l\\).\nOn the right decision node, \\(3 &gt; 0\\), so player 3 chooses \\(r\\).\n\nNow, go one step backward to player 2:\n\nPlayer 2 can choose between \\(a\\) and \\(b\\) - however, they should only consider outcomes that player 3 will choose (they are anticipating what player 3 will do).\nUnder this assumption, if player 2 chooses \\(a\\), they know player 3 will choose \\(l\\), and player 2 will get a payoff of 2.\nIf player 2 chooses \\(b\\), they know player 3 will choose \\(r\\), and player 2 will get a pyyoff of 5.\nSince \\(5&gt;2\\), player 2 will choose \\(b\\).\n\nNow, go one step backward to player 1:\n\nPlayer 1 can choose between \\(L\\) and \\(R\\)- however, they should only consider outcomes that player 3 and player 2 will choose (they are anticipating what later players will do).\nUnder this assumption, if player 1 chooses \\(L\\), they know player 3 will choose \\(r\\), and that will get player 1 a payoff of 5.\nIf player 1 chooses \\(R\\), they know player 2 will choose \\(b\\), and player 3 will choose \\(r\\), and that will get player 1 a payoff of 1.\nSince \\(5&gt;1\\), that means player 1 will play \\(L\\).\n\nThus, the SPNE of this game is: \\((L, b, rlr)\\).\n\nRemember, the strategy profile consists of what a player will do at each node in the game. Player 3 has 3 nodes, so we have to indicate that in our SPNE.\n\n\n\n\n\n\n\n\n\n\nIncomplete Information Games\n\nInformation Sets and Beliefs\nIn incomplete information games, sometimes, a player is unsure which decision node they are on. An information set is a set of decision nodes in which a player is unsure which one they are at.\n\nFor example, if player \\(i\\) is unsure they are at decision node \\(A\\) or \\(B\\), those two decision nodes will be a part of the same information set.\n\n\n\n\n\n\n\nExample of Information Sets\n\n\n\n\n\nTake this following game: Alex moves first, then Chris - however, Chris cannot observe the action Alex took:\n\n\n\n\n\nThus, Chris is unsure if they are at the left side of the game, or the right side of the game, since Chris did not observe what Alex did.\nThus, these two nodes are on the same information set for Chris - he knows he is in this information set, but he is unsure which node he is at within this information set.\n\n\n\nPlayers have to choose the same action to do for all nodes in an information set. This is because they are unsure which node within the information set they are at, so they have to do the same action for all of the nodes.\nPlayers have a set of beliefs regarding which node they are at within the information set. This is often described as a probability distribution over each node within the information set.\n\nFor example, player \\(i\\) might believe they have an 80% chance of being at node \\(A\\), and 20% chance at being in node \\(B\\). Beliefs within an information set should sum to 1.\n\n\n\n\n\n\n\nExample of Beliefs\n\n\n\n\n\nTake this following game: Alex moves first, then Chris - however, Chris cannot observe the action Alex took:\n\n\n\n\n\nChris is unsure if he is at the left or right node. However, Chris can put a belief that he is at each node - we can label these beliefs \\(\\mu\\) and \\(1-\\mu\\).\n\n\\(\\mu\\) is Chris’s belief of the chance he is at the left node.\n\\(1-\\mu\\) is Chris’s belief of the chance he is not at the left node, so at the right node.\n\nChris will use these beliefs to help him decide if he should play \\(o\\) or \\(f\\).\n\n\n\nPlayers will use these beliefs to determine what strategy they should play at the information set. Players can also “update” their beliefs based on new information from prior events in the game.\n\n\n\nBayes’ Rule\nPlayers can also “update” their beliefs based on new information from prior events in the game. They do this updating via Bayes’ Rule.\nLet us assume \\(P(A)\\) is the probability of us being at a specific node. Let us assume \\(P(B)\\) is the probability of us being in the information set that contains the specific node.\nThus, \\(P(A|B)\\) is the probability of us being at the specific node, assuming we have entered the information set. Bayes’ Rule says the following about \\(P(A|B)\\):\n\\[\n\\underbrace{P(A|B)}_{\\text{posterior}} = \\frac{\\overbrace{P(B|A)}^{\\text{likelihood}} \\overbrace{P(A)}^{\\text{prior}}}{\\underbrace{P(B|A)P(A) + P(B|A^C)P(A^C)}_{\\text{evidence}}}\n\\]\nWe often call \\(P(A)\\) the prior probability, and \\(P(A|B)\\) the posterior probability. Bayes’ Rule allows us to use new information to update our original prior probability \\(P(A)\\) to get our posterior probability \\(P(B|A)\\).\n\n\n\nPerfect Bayesian Equilibrium\nA Perfect Bayesian Equilibrium (PBE) consists of:\n\nA strategy profile: specifying the action to take for each player, at every information set in the game.\nA belief profile: a probability distribution over each node in each information set.\n\nFor every player, players should act sequentially rational. This means that:\n\nPlayers act optimally at every information set (maximise their payoffs), given their belief profiles.\nPlayer’s beliefs are consistent with the strategy profiles, if on-the-equilibrium-path beliefs are consistent with Bayes Rule, and if off-the-equilibrium-path beliefs are sufficiently restricted to ensure players are acting optimally with regard to their beliefs.\n\nTo find a Perfect Bayesian Equilibrium, you should generally start with backwards induction to find strategies, then check if the beliefs are consistent with these strategies.\n\n\n\n\n\n\nExample: Selten’s Horse\n\n\n\n\n\nTake this game:\n\n\n\n\n\nLet us find the Perfect Bayesian Equilibria of this game.\nFirst, let us start off with the strategy profiles by doing backwards induction.\nWhat should player 3 play in their node?\n\nWe can see that no matter what their belief \\(p\\) is, \\(L\\) always has a better payoff than \\(R\\). Thus, 3 will play \\(L\\).\n\nNow, what should player 2 do?\n\nIf player 2 chooses \\(A\\), they get a payoff of 3.\nIf player \\(A\\) chooses \\(D\\), they know that player 3 will choose \\(L\\), so player 2 will get a payoff of 5.\nSince \\(5&gt;2\\), player 2 chooses \\(D\\).\n\nNow, what should player 1 do?\n\nIf player 1 plays \\(D\\), they know player 3 will play \\(L\\), thus getting player 1 a payoff of 4.\nIf player 1 plays \\(A\\), they know player 2 will play \\(D\\) and player 3 will play \\(L\\), getting them a payoff of 5.\nSince \\(5&gt;4\\), player 1 will play \\(A\\).\n\nSo now, we have found the strategy profile \\((A, D, L)\\) is optimal. But what about the beliefs?\nWell, what beliefs are consistent with this strategy? We can see that under this equilibrium path, player 3’s information set is on-the-equilibrium path (it will be reached).\n\nWe see that on the equilibrium path, we will always end up on the \\((1-p)\\) node (since we know player 1 does not want to play \\(D\\)).\nThus, the belief that makes sense should be \\(p=0\\), since player 1 never plays \\(D\\), so if we end up in the information set, we must be on \\(1-p\\),\n\nThus, the PBE is \\((A, D, L, p = 0)\\).\n\n\n\n\n\n\nSignalling Games\nSignalling games are a very common form of incomplete-information game in Game Theory. They take the following form:\n\nNature chooses with probability \\(p\\) the type \\(\\theta\\) of the world. Player 1 observes \\(\\theta\\). Player 2 does not observe \\(\\theta\\).\nPlayer 1 chooses one action \\(m\\) in set \\(M\\), which act as signals for \\(\\theta\\).\nPlayer 2 receives \\(m\\) (not \\(\\theta\\)), and chooses some action \\(a \\in A\\).\n\nThis game basically says that player 1 has some private information \\(\\theta\\) about the world, and they can either decide to reveal or conceal this information to player 2 through their action \\(m\\).\n\n\n\n\n\n\nExample: Value of Education\n\n\n\n\n\nThe value of education model, developed by Spence (1973), is an example of a signalling game.\nThere are two players: the Job-Seeker, and the Boss\n\nNature chooses the Job-Seeker’s ability: High ability (H), or Low ability (L), with some probability \\(p\\).\nJob-Seeker decides whether or not to get a Degree (D). The Degree acts as a signal as whether the Job-Seeker is type high ability or low ability.\nThe Boss observes whether or not the Job-Seeker gets a Manager (M) or Blue-Collar (B) job. The Boss makes this decision based on their beliefs on if the job-seeker is High ability or Low ability, according to their signal and updating beliefs by Bayes’ Rule.\n\nThe idea is that getting a degree is more costly for a low ability job-seeker, but it is a positive signal that you are type high ability. Bosses want to put High-ability people in manager roles, and low-ability people in blue-collar roles.\n\n\n\n\n\n\n\n\nSignalling Games have 3 types of equilibrium:\n\nSeparating Equilibrium (see below)\nPooling Equilibrium (see below)\nSemi-separating equilibrium (we are not too concerned with this).\n\n\n\n\nSeparating Equilibrium\nA separating equilibrium is a PBE of a signalling game, where each type \\(\\theta\\) of sender, sends a different message \\(m\\).\nBy sending a different message \\(m\\) for each type \\(\\theta\\), the sender is essentially “revealing” their type \\(\\theta\\) to the second player who does not know \\(\\theta\\). With this information, the second player can update their beliefs on what type \\(\\theta\\) the sender is.\nTo solve for a separating equilibrium, you should do the following:\n\nChoose one potential separating equilibrium.\nUpdate beliefs of player 2 using Bayes’ Rule.\nCalculate best responses for Player 2 given these updated beliefs.\nNow, holding Player 2’s strategies constant, check if player 1 has any profitable deviations.\nIf player 1 has no profitable deviations, you have a Seperating Equilibrium.\nNow, check all other potential separating equilibria.\n\n\n\n\n\n\n\nFinding Seperating Equilibrium: Value of Education\n\n\n\n\n\nTake the value of education game from earlier:\n\n\n\n\n\nThere are 2 potential seperating equilibrium:\n\nIn type \\(H\\), player 1 plays \\(U\\), and in type \\(L\\), player 1 plays \\(D\\) (we will call this \\(UD\\))\nIn type \\(H\\), player 1 plays \\(D\\), and in type \\(L\\), player 1 plays \\(U\\) (we will call this \\(DU\\))\n\nLet us check \\(UD\\) first.\n\nGiven this set of strategies, we know Player 2’s beliefs \\(\\mu_U = 1\\) and \\(\\mu_D = 0\\).\nWith these beliefs, we know on the left side, player 2 should play \\(M\\), and on the right side, player 2 should play \\(B\\).\n\nNow does player 1 have any profitable deviation?\n\nIn type \\(H\\), Player 1 is playing \\(U\\). He knows that after \\(U\\), player 2 plays \\(M\\), getting player 1 a payoff of 10. If player 1 deviates to \\(D\\), he knows player 2 plays \\(B\\) on the right side, so player 1 gets a payoff of 4, which is less than 10. So this is not a profitable deviation.\nIn type \\(L\\), player 1 is playing \\(D\\). He knows that after \\(D\\), player 2 plays \\(B\\), getting player 1 a payoff of 1. If player 1 deviates to \\(U\\), he knows player 2 plays \\(M\\) on the right side, so player 1 gets a payoff of 10. That is a profitable deviation.\n\nThus, Player 1 has a profitable deviation in type \\(L\\), so this is not a separating equilibrium.\n\nNow, let us check \\(DU\\):\n\nGiven this set of strategies, we know player 2’s beliefs \\(\\mu_U = 0\\) and \\(\\mu_D = 1\\).\nWith these beliefs, we know on the left side, player 2 should play \\(B\\), and on the right side, player 2 should play \\(M\\).\n\nNow does player 1 have any profitable deviation?\n\nIn type \\(H\\), player 1 is playing \\(D\\). He knows that after \\(D\\), player 2 plays \\(M\\), getting player 1 a payoff of 8. If player 1 deviates to \\(U\\), he knows player 2 plays \\(B\\), getting him a payoff of 6. That is not a profitable deviation.\nIn type \\(L\\), player 1 is playing \\(U\\). He knows that after \\(U\\), player 2 plays \\(B\\), getting player 1 a payoff of 6. If player 1 deviates to \\(D\\), he knows player 2 plays \\(M\\), getting him a payoff of 5. This is not a profitable deviation.\n\nThus, there is a seperating equilibrium \\((DU, BM, \\mu_U = 0, \\mu_D = 1)\\).\n\nWhere \\(DU\\) is in the form of type \\(H\\), type \\(L\\); and \\(BM\\) is in the form of left information set, right information set.\n\n\n\n\n\n\n\nPooling Equilibrium\nA pooling equilibrium is a PBE of a signalling game, where no matter what type \\(\\theta\\) the sender is, the sender always sends the same message \\(m\\).\nBy sending the same message \\(m\\) for all types \\(\\theta\\), the sender refuses to reveal any information about their \\(\\theta\\). Thus, player 2’s posterior and prior beliefs about sender’s type \\(\\theta\\) remain the same.\nTo solve for a separating equilibrium, you should do the following:\n\nChoose one potential pooling equilibrium.\nUpdate beliefs of player 2 on-the-equilibrium-path using Bayes’ Rule (actually, this will stay the same as the prior beliefs). The off-the-equilibrium path information set will be dealt with later.\nCalculate best responses for Player 2 given these beliefs.\nNow, holding Player 2’s strategies constant, check if player 1 has any profitable deviations.\nIf player 1 has no profitable deviations, you have a Seperating Equilibrium. If player 1 might have a profitable deviation depending on what player 2 chooses off-the-equilibrium path, you will need to find the beliefs off-the-equilibrium path that eliminate this profitable deviation.\nNow, check all other potential pooling equilibria.\n\n\n\n\n\n\n\nFinding Pooling Equilibrium: Value of Education\n\n\n\n\n\nTake the value of education game from earlier (assume \\(p = 0.25\\)):\n\n\n\n\n\nThere are 2 potential pooling equilibrium:\n\nPlayer 1 plays \\(UU\\) (playing \\(U\\) no matter the type).\nPlayer 1 plays \\(DD\\) (playing \\(D\\) no matter the type).\n\nLet us test \\(UU\\) (I will not test \\(DD\\), but the same process applies).\nFirst, we know that information set \\(\\mu_U\\) is on the equilibrium path. Since we cannot update our beliefs, our beliefs remain the same as the prior (\\(p = 0.25\\) as given above):\n\nThus, \\(\\mu_U = 0.25\\), and \\(1-\\mu_U = 0.75\\).\n\nNow, let us find the optimal action of player 2 in this \\(\\mu_U\\) information set, based on the beliefs above:\n\\[\n\\begin{cases}\nu_2(M) = 10\\mu_U + 0(1-\\mu_U) = 10(0.25) = 2.5 \\\\\nu_2(B) = 5\\mu_u + 3(1-\\mu_u) = 5(0.25) +3(0.75) = 3.5\n\\end{cases}\n\\]\nWe see that \\(3.5&gt;2.5\\), so we know player 2 will play \\(B\\) on the left information set.\nNow, does player 1 have a profitable deviation?\n\nIn type \\(H\\), player 1 is playing \\(U\\). As calculated above, we know player 2 plays \\(B\\) here, so player 1 gets a payoff of 6. If player 1 deviates to \\(D\\), he could potentially get either 8 or 4 (depending on what player 2 plays on the right side).\nIn type \\(L\\), player 1 is playing \\(U\\). As calculated above, we know player 2 plays \\(B\\) here, so player 1 gets a payoff of 6. If player 1 deviates to \\(D\\), he could potentially get either 5 or 1, which or both less than 6. Thus, there is no profitable deviation.\n\nThus, player 1 only has a potential profitable deviation in type \\(H\\), if player 2 plays \\(M\\) on the right side information set (which will give player 1 a deviation payoff of 8, higher than his original 6 sticking with \\(U\\)).\nThus, for this to be a equilbrium, we must eliminate this profitable deviation, by making sure player 2 does not play \\(M\\) on the right-side, but instead plays \\(B\\).\n\nIf player 2 plays \\(B\\) on the right side, the player 1 deviation in type \\(H\\) would only yield payoff 4, so there would be no profitable deviation for player 1.\n\nThis means that the expected utility of \\(B\\) for player 2 must be higher than the expected utility of \\(M\\) on the right-side information set:\n\\[\n\\begin{split}\nu_2(B) & ≥ u_2 (M) \\\\\n5\\mu_D + 3(1-\\mu_D) & ≥ 10\\mu_D + 0(1-\\mu_D) \\\\\n5\\mu_D + 3 - 3\\mu_D &≥ 10\\mu_D \\\\\n2 \\mu_D + 3&≥ 10\\mu_D \\\\\n8\\mu_D&≤3 \\\\\n\\mu_D&≤\\frac{3}{8}\n\\end{split}\n\\]\nThus, this set of strategies is only a pooling PBE if \\(\\mu_D ≤ \\frac{3}{8}\\), since only this off-the-equilibrium-path belief condition ensures that there is no profitable deviation for player 1.\nThus, the pooling PBE is: \\((UU, BB, \\mu_U = \\frac{1}{4}, \\mu_D ≤ \\frac{3}{8})\\).\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Guide to Game Theory"
    ]
  },
  {
    "objectID": "linear.html",
    "href": "linear.html",
    "title": "Linear Algebra Reference",
    "section": "",
    "text": "This chapter is a reference for the essentials of linear algebra for political science (for mostly statistics/quantitative methods).\nUse the right sidebar for easy navigation.\n\n\nVectors\n\nScalars and Vectors\nA scalar is any single element or component, like a real number (ex. \\(x_1 \\in \\mathbb{R}\\)).\nA vector is a collection of scalars: \\((x_1 \\ x_2 \\ x_3 \\ x_4)\\). Each scalar is considered a element/component. Vectors can be both row vectors (horiztonal), and column vectors (vertical).\nThe dimension of a vector is the number of components/scalars/elements in the vector. Vectors will be denoted with a bold lowercase letter: \\(\\mathbf x\\), to distinguish them from scalars \\(x\\).\nVectors can be visualised graphically. Each element corresponds to a distance in a direction. For example, take the vector \\((3 \\ 2)\\). Graphically:\n\n\n\n\n\nIf a vector has 3 dimensions, the graphical representation will be in 3 dimensions, and so on.\n\n\n\nVector Addition/Subtraction\nVector addition and subtraction is done just by adding the respective elements to each other:\n\\[\n\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} + \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 1+3 \\\\ 2+4 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 6 \\end{pmatrix}\n\\]\n\nVector addition only works if the two vectors are of the same dimensionality.\n\nVector addition can be viewed graphically. Take two 2-dimensional vectors \\(\\mathbf A\\) and \\(\\mathbf B\\):\n\n\n\n\n\n\n\n\nScalar Multiplication and Normalisation\nVector scalar multiplication is done by multiplying all elements of the vector by the scalar:\n\\[\n4 \\times \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 4 \\times 1 \\\\ 4 \\times 2 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 8 \\end{pmatrix}\n\\]\nGraphically, this just multiplies the length of the vector by the scalar (and if the scalar is negative, the direction switches 180 degrees).\nThe norm of a vector is its length (when thinking graphically):\n\\[\n|| \\mathbf{x}|| = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2}\n\\]\nNormalizing a vector is scalar multiplying the vector by \\(1/||\\mathbf{x} ||\\).\n\nThis results in the norm of the vector equaling 1. This can be useful if you want to standardise and compare vectors.\n\n\n\n\nScalar Product\nScalar product, also known as dot product, takes two vectors and creates a scalar.\n\\[\n\\mathbf a \\cdot \\mathbf b = \\sum_i a_ib_i = a_1 b_1 + a_2 b_2+ \\dots a_n b_n\n\\]\n\nEssentially, multiply each respective element with each other. Then sum all of the products.\n\n\n\n\n\n\n\nExample of Scalar Product\n\n\n\n\n\nLet us do an example of scalar product:\n\\[\n\\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix} \\cdot \\begin{pmatrix} 4 \\\\ 5 \\end{pmatrix}\n\\]\nWe multiply each respective element with each other, then sum all of the products:\n\\[\n2 \\times 4 + 3 \\times 5 = 8 + 15 = 23\n\\]\n\n\n\nDot product calculates the projection/shadow of vector \\(\\mathbf a\\) on vector \\(\\mathbf b\\). In the figure below, the dot product calcualtes the length of the blue-highlighted line segment:\n\n\n\n\n\nFrom here, we can tell if the two vectors are perpendicular, then the dot product would be 0. This is useful for measures of similarity/correlation.\n\n\n\n\n\n\nMatrices\n\nTypes of Matrices\nA matrix is a collection of scalars, that are put in a \\(n \\times m\\) order with \\(n\\) number of rows, and \\(m\\) number of columns.\n\\[\n\\mathbf A_{2 \\times 3} = \\begin{pmatrix}\n2 & 3 & 2 \\\\\n1 & 4 & 1\n\\end{pmatrix}\n\\]\nMatrices can be viewed as a set of row vectors combined, or a set of column vectors combined (this is how datasets are arranged). Each element by the matrix can be denoted \\(a_{ij}\\), which is the element in the \\(i\\)th row and \\(j\\)th column:\n\\[\n\\mathbf A_{2 \\times 3} = \\begin{pmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23}\n\\end{pmatrix}\n\\]\nThere are several very common types of matrices that you need to know.\n\n\n\n\n\n\nSquare and Zero Matrix\n\n\n\n\n\nSquare Matrix is a matrix that have an equal number of rows and columns.\n\\[\n\\mathbf A_{2 \\times 2} = \\begin{pmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{pmatrix}\n\\]\nThese are useful because many matrix manipulations, like inversions and determinants.\nZero Matrix is a square matrix with all 0’s.\n\n\n\n\n\n\n\n\n\nDiagonal, Identity, and Lower/Upper Triangular Matrix\n\n\n\n\n\nDiagonal matrices only have elements along the top-left bottom-right diagonal.\n\\[\n\\mathbf A_{3 \\times 3} = \\begin{pmatrix}\na_{11} & 0 & 0 \\\\\n0 & a_{22} & 0 \\\\\n0 & 0 & a_{33} \\\\\n\\end{pmatrix}\n\\]\nAn identity matrix (notated \\(\\mathbf I\\))is a diagonal matrix, but all the diagonal elements equal 1:\n\\[\n\\mathbf I_{2 \\times 2} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\\]\n\nAny matrix times \\(\\mathbf I\\) equals itself (like a 1 in normal multiplication).\n\nA Lower/Upper Triangular Matrix is a matrix where only has values above/below the diagonal. For example, the following is a lower triangular matrix:\n\\[\n\\mathbf A_{3 \\times 3} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n3 & 4 & 0 \\\\\n3 & 3 & 4\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nSubmatrix\n\n\n\n\n\nA submatrix is a matrix if you were to remove a row and a column (that is specified by an element).\nFor example, take this 3 by 3 matrix:\n\\[\n\\mathbf A_{3 \\times 3} = \\begin{pmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{pmatrix}\n\\]\nLet us find the submatrix of \\(a_{21}\\). This means we will eliminate the 2nd row, and 1st column:\n\\[\n\\mathbf A_{2 \\times 2} = \\begin{pmatrix}\na_{12} & a_{13} \\\\\na_{32} & a_{33}\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nPermutation Matrix\n\n\n\n\n\nA permutation matrix is a matrix that only has one non-zero element in each row and column.\n\\[\n\\mathbf A_{3 \\times 3} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{pmatrix}\n\\]\nThe identity matrix is a permutation matrix.\n\n\n\n\n\n\n\n\n\nSingular/Non-Singular Matrix\n\n\n\n\n\nA singular matrix is one who’s determinant is zero. These cannot be inverted.\nA non-singular matrix is one who’s determinant is not zero. These can be inverted. For non-singular matrices:\n\\[\nAA^{-1} = I\n\\]\n\n\n\n\n\n\n\n\n\nBlock/Partitioned/Block Diagonal Matrix\n\n\n\n\n\nA block or partitioned matrix is a matrix which contains matrices within.\n\\[\n\\mathbf A_{4 \\times 4} = \\begin{pmatrix}\n\\mathbf A_{2 \\times 2} & \\mathbf B_{2 \\times 2} \\\\\n\\mathbf C_{2 \\times 2} & \\mathbf D_{2 \\times 2}\n\\end{pmatrix}\n\\]\n\nNote how the block matrix is 4 by 4, since if we expand out each matrix within, we would get a 4 by 4 matrix.\n\nA block diagonal matrix is a block/partitioned matrix with only matrices on its diagonal:\n\\[\n\\mathbf A_{4 \\times 4} = \\begin{pmatrix}\n\\mathbf A_{2 \\times 2} & 0 \\\\\n0 & \\mathbf D_{2 \\times 2}\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nOrthogonal/Orthonormal Matrix\n\n\n\n\n\nAn orthogonal matrix is one with columns perpendicular to each other (when treating each column as a vector). In other words, the dot product of any two columns is zero.\n\\[\n\\mathbf A_{3 \\times 3} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 5 \\\\\n0 & 3 & 0\n\\end{pmatrix}\n\\]\n\nThe identity matrix is also orthogonal.\nAny matrix with one element in each row and column will be orthogonal.\n\nAn orthonormal matrix is an orthogonal matrix but the lengths/norms of all the columns is 1:\n\\[\n\\mathbf A_{3 \\times 3} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\nMatrix Transpose\nThe matrix transpose is a matrix flipped along its diagonal. It is denoted either \\(\\mathbf A^\\mathsf{T}\\) or \\(\\mathbf A '\\).\nIn other words, the rows and column locations of each element are inverted (essentially elements \\(a^\\mathsf{T}_{ij} = a_{ji}\\)):\n\\[\n\\begin{pmatrix}\n2 & 3 & 5 \\\\\n1 & 4 & 6\n\\end{pmatrix}^\\mathsf{T} = \\begin{pmatrix}\n2 & 1 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{pmatrix}\n\\]\n\nNotice how the first column became the first row, the second column became the second row.\nYou can also get the transpose of a vector.\n\n\n\n\n\n\n\nProperties of Transposes\n\n\n\n\n\nThe Vector Property says that the dot product of vectors can be written with transposes:\n\\[\n\\mathbf a \\cdot \\mathbf b = \\mathbf a^\\mathsf{T}\\mathbf b\n\\]\nThe Inverse Property says that the transpose of a transpose is the original matrix:\n\\[\n\\left(\\mathbf A^\\mathsf{T} \\right)^\\mathsf{T} = \\mathbf A\n\\]\nThe Addition Property states that the transpose of a sum of two matrices, is equal to the individual transposes of both matrices added together:\n\\[\n(\\mathbf A + \\mathbf B)^\\mathsf{T} = \\mathbf A^\\mathsf{T} + \\mathbf B^\\mathsf{T} = \\mathbf B^\\mathsf{T} + \\mathbf A^\\mathsf{T}\n\\]\nThe Multiplication Property says the following (note the order of multiplication):\n\\[\n( \\mathbf{AB})^\\mathsf{T} = \\mathbf B^\\mathsf{T} \\mathbf A^\\mathsf{T}\n\\]\nThe Symmetrical Property says that a matrix that is symmetrical does not change when inversed:\n\\[\n\\mathbf A^\\mathsf{T} = \\mathbf A \\quad \\text{s.t.} \\quad \\mathbf A \\text{ is symmetrical}\n\\]\nThe Inverse Transpose Property says the following about inverses and transposes:\n\\[\n\\left( \\mathbf A^{-1} \\right)^\\mathsf{T} = \\left( \\mathbf A^\\mathsf{T} \\right)^{-1}\n\\]\n\n\n\n\n\n\nMatrix Algebra\nMatrix addition/subtraction is the same as vector addition - add the respective elements together:\n\\[\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{pmatrix} + \\begin{pmatrix}\n5 & 6 \\\\\n7 & 8\n\\end{pmatrix} = \\begin{pmatrix}\n1 + 5 & 2 +6 \\\\\n3+7 & 4 + 8\n\\end{pmatrix} = \\begin{pmatrix}\n6 & 8 \\\\\n10 & 12\n\\end{pmatrix}\n\\]\nMatrix scalar multiplication is the same as vector scalar multiplication - multiply each element by the scalar:\n\\[\n3 \\times \\begin{pmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{pmatrix} = \\begin{pmatrix}\n3 \\times 1 & 3 \\times 2 \\\\\n3 \\times 3 & 3 \\times 4\n\\end{pmatrix} = \\begin{pmatrix}\n3 & 6 \\\\\n9 & 12\n\\end{pmatrix}\n\\]\nMatrix Plain Multiplication is a little more complicated. Let us say you want to multiply \\(\\mathbf A\\) and \\(\\mathbf B\\) to get a new matrix \\(\\mathbf C\\). The elements of \\(\\mathbf C\\) are calculated as follows:\n\\[\nc_{ij} = \\sum_ka_{ik}b_{kj} = a_{i1}b_{1j} + a_{i2}b_{2j} + a_{i3}b_{3j}\\dots\n\\]\nIn other words, \\(c_{ij}\\) is the dot product of the \\(i\\)th row of \\(\\mathbf A\\), and the \\(j\\)th column of \\(\\mathbf B\\).\n\n\n\n\n\n\nExample of Matrix Multiplication\n\n\n\n\n\nLet us solve the following problem:\n\\[\n\\begin{pmatrix}\n2 & 1 \\\\\n3 & 5 \\end{pmatrix} \\begin{pmatrix}\n6 & 1 \\\\\n2 & 3 \\end{pmatrix} = \\mathbf C\n\\]\nLet us do each dot product for each element of \\(\\mathbf C\\):\n\n\\(c_{11}\\) is the dot product of the 1st row of the 1st matrix, and the 1st column of the 2nd matrix: \\((2 \\ 1) \\cdot (6 \\ 2)\\). That means \\(c_{11} = 2 \\times 6 + 1 \\times 2 = 12+2 = 14\\).\n\\(c_{12}\\) is the dot product of the 1st row of the 1st matrix, and the 2nd column of the 2nd matrix: \\((2 \\ 1) \\cdot (1 \\ 3)\\). That means \\(c_{12} = 2 \\times 1 + 1 \\times 3 = 2 + 3 = 5\\).\n\\(c_{21}\\) is the dot product of the 2nd row of the 1st matrix, and the 1st column of the 2nd matrix: \\((3 \\ 5) \\cdot (6 \\ 2)\\). That means \\(c_{21} = 3 \\times 6 + 5 \\times 2 = 18 + 10 = 28\\).\n\\(c_{22}\\) is the dot product of the 2nd row of the 1st matrix, and the 2nd column of the 2nd matrix: \\((3 \\ 5) \\cdot (1 \\ 3)\\). That means \\(c_{22} = 3 \\times 1 + 5 \\times 3 = 3 + 15 = 18\\).\n\nThus, we now have our answer:\n\\[\n\\begin{pmatrix}\n2 & 1 \\\\\n3 & 5 \\end{pmatrix} \\begin{pmatrix}\n6 & 1 \\\\\n2 & 3 \\end{pmatrix} = \\begin{pmatrix}\n14 & 5 \\\\\n28 & 18 \\end{pmatrix}\n\\]\n\n\n\nMatrix multiplication is only possible when the number of columns in \\(\\mathbf A\\) is equal to the number of rows in \\(\\mathbf B\\). So for example, we can multiply \\(\\mathbf A_{2 \\times 3}\\) and \\(\\mathbf B_{3 \\times 4}\\). We cannot multiply \\(\\mathbf A_{2 \\times 3}\\) and \\(\\mathbf B_{2 \\times 3}\\).\nThe dimensions of product \\(\\mathbf C\\) is the number of rows in \\(\\mathbf A\\) and the number of columns in \\(\\mathbf B\\). So for example, if we multiply \\(\\mathbf A_{2 \\times 3}\\) and \\(\\mathbf B_{3 \\times 4}\\), we will get \\(\\mathbf C_{2 \\times 4}\\).\n\n\n\n\n\n\nProperties of Matrix Algebra\n\n\n\n\n\nThe Associative property applies to addition/subtraction and multiplication:\n\\[\n\\begin{split}\n& (\\mathbf A + \\mathbf B) + \\mathbf C = \\mathbf A + (\\mathbf B + \\mathbf C) \\\\\n& (\\mathbf A \\mathbf B)\\mathbf C = \\mathbf A(\\mathbf B \\mathbf C)\n\\end{split}\n\\]\nThe Distributive Property states the following is true:\n\\[\n(\\mathbf A + \\mathbf B) \\mathbf C = \\mathbf A \\mathbf C + \\mathbf B \\mathbf C\n\\]\nThe Commutative Property applies only to addition/subtraction, not multiplication. Commutative property also applies to dot products.\n\\[\n\\begin{split}\n& \\mathbf A + \\mathbf B = \\mathbf B + \\mathbf A \\\\\n& \\mathbf a \\cdot \\mathbf b = \\mathbf b \\cdot \\mathbf a\n\\end{split}\n\\]\nMatrix Multiplication does not have the commutative property: \\(\\mathbf A \\mathbf B ≠ \\mathbf B \\mathbf A\\). Although there are two exceptions: \\(\\mathbf A \\mathbf I = \\mathbf I \\mathbf A\\), and \\(\\mathbf A \\mathbf A^{-1} = \\mathbf A^{-1}  \\mathbf A\\).\n\n\n\n\n\n\nKronecker Product\nTake the Kronecker Product of \\(\\mathbf A\\) and \\(\\mathbf B\\):\n\\[\n\\mathbf A \\otimes \\mathbf B = \\mathbf C\n\\]\nLet us define \\(\\mathbf A\\) and \\(\\mathbf B\\) as the following:\n\\[\n\\mathbf A_{2 \\times 2} = \\begin{pmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{pmatrix}, \\ \\mathbf B_{2 \\times 2} = \\begin{pmatrix}\nb_{11} & b_{12} \\\\\nb_{21} & b_{22}\n\\end{pmatrix}\n\\]\nThe resulting Kronecker Product \\(\\mathbf C\\) would be defined as a block matrix:\n\\[\n\\mathbf C_{4 \\times 4} = \\begin{pmatrix}\na_{11}\\mathbf B & a_{12} \\mathbf B \\\\\na_{21} \\mathbf B & a_{22} \\mathbf B\n\\end{pmatrix}\n\\]\nEssentially, we treat \\(\\mathbf A\\) as a collection of scalars. We scalar multiply each scalar element of \\(\\mathbf A\\) by the matrix of \\(\\mathbf B\\).\nIf \\(\\mathbf A\\) has dimensions \\(n \\times m\\), and \\(\\mathbf B\\) has dimensions \\(p \\times q\\), then \\(\\mathbf C\\) will have dimensions \\(np \\times mq\\).\n\n\n\nTraces and Determinants\nThe trace of \\(\\mathbf A\\) is a sum of all diagonal elements. Traces are used in Eigenvalues.\n\\[\nTr(\\mathbf A) = \\sum_i a_{ii} = a_{11} + a_{22} + \\dots\n\\]\n\n\n\n\n\n\nProperty of Traces\n\n\n\n\n\nThe Addition Property of traces states that the trace of the sum of two matrices is equivalent to the sum of the traces of each individual matrix:\n\\[\nTr(\\mathbf A + \\mathbf B) =Tr(\\mathbf A) + Tr(\\mathbf B) = Tr(\\mathbf B) + Tr(\\mathbf A)\n\\]\nThe Transpose Property says the trace of the transpose is equal to the trace of the original (since the diagonal remains the same):\n\\[\nTr(\\mathbf A^\\mathsf{T}) = Tr(\\mathbf A)\n\\]\nThe Multiplication Property says that the trace of multiplication has the commutative property (only for two matrices):\n\\[\nTr(\\mathbf{AB}) = Tr(\\mathbf{BA})\n\\]\n\n\n\nDeterminants tell us if a matrix is singular (and thus has no inverse). If the determinant is 0, then the matrix is singular. The determinant is only computable for square matrices. For a 2 by 2 matrix:\n\\[\n|\\mathbf A_{2 \\times 2}| = \\left| \\begin{pmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{pmatrix} \\right | = a_{11} a_{22} - a_{12}a_{21}\n\\]\n\n\n\n\n\n\nExample of a Determinant\n\n\n\n\n\nLet us find the determinant of the following matrix:\n\\[\n\\mathbf B_{2 \\times 2} = \\begin{pmatrix}\n2 & 3 \\\\\n1 & 4\n\\end{pmatrix}\n\\]\nUsing the formula above:\n\\[\n|\\mathbf B| = 2 \\times 4 - 3 \\times 1 = 8 - 3 = 5\n\\]\nThus, this matrix is non-singular.\n\n\n\nFor 3 by 3, there is a method called the butterfly method to find the determinant. Take this matrix.\n\\[\n\\mathbf A_{3 \\times 3} = \\begin{pmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{pmatrix}\n\\]\nThe determinant is defined as following:\n\\[\n\\begin{split}\n| \\mathbf A| = & a_{11}a_{22}a_{33} + a_{12} a_{23} a_{31} + a_{13}a_{21}a_{32} \\\\\n& -a_{31}a_{22}a_{13} - a_{11}a_{23}a_{32} - a{12}a_{21}a_{33}\n\\end{split}\n\\]\n\n\n\n\n\n\nProperties of Determinants\n\n\n\n\n\nThe Transpose Property states that the determinant of a transpose is equal to the determinant of the original:\n\\[\n\\det(\\mathbf A^\\mathsf{T}) = \\det(\\mathbf A)\n\\]\nThe Identity Property states that the determinant of an identity matrix is 1:\n\\[\n\\det (\\mathbf I)=1\n\\]\nThe Multiplication Property states that the determinant of a product is equal to the individual determinants multiplied:\n\\[\n\\det (\\mathbf {AB}) = \\det (\\mathbf A) \\det (\\mathbf B)\n\\]\nThe Inverse Property says that the determinant of an inverse is the inverse of the determinant of the original matrix:\n\\[\n\\det(\\mathbf A^{-1}) = \\frac{1}{\\det(\\mathbf A)}\n\\]\nThe Triangular/Diagonal Property is the product of all diagonal elements:\n\\[\n\\det (\\mathbf A) = \\prod_i a_{ii}\n\\]\n\n\n\n\n\n\nLaplace Expansion and Cofactors\nFor anything larger than a 3 by 3 matrix, we should us a Laplace expansion to find the determinant.\nFirst, you choose a row or column of the matrix.\n\nFor every element in that row or column, find the submatrix of that element.\nCalculate the determinant of each of the submatrices. This is called the minor.\nNow, convert the minors to cofactors. The cofactor is the minor times \\((-1)^{i+j}\\).\nThen, take each element, multiply by its cofactor. Sum all of these products together.\n\nThe final sum is the determinant of the matrix.\n\n\n\n\n\n\nExample of Laplace Expansion\n\n\n\n\n\nFor example, take this matrix:\n\\[\n\\mathbf A = \\begin{pmatrix}\n1 & 2 & 1 \\\\\n0 & 1 & 1 \\\\\n5 & 3 & 0\n\\end{pmatrix}\n\\]\nLet us expand over the 1st row \\((1 \\ 2 \\ 1 )\\). We expand over the submatrices of each element in that row.\n\nFor \\(a_{11} = 1\\), the submatrix is \\(\\begin{pmatrix} 1 & 1 \\\\ 3 & 0 \\end{pmatrix}\\), and the determinant/minor of that is \\(1 \\times 0 - 1 \\times 3 = -3\\).\nFor \\(a_{12} = 2\\), the submatrix is \\(\\begin{pmatrix} 0 & 1 \\\\ 5 & 0 \\end{pmatrix}\\). The determinant/minor of that is \\(0 - 5 = -5\\).\nFor \\(a_{13} = 1\\), the submatrix is \\(\\begin{pmatrix} 0 & 1 \\\\ 5 & 3 \\end{pmatrix}\\). The determinant/minor of that is \\(0 - 5 = -5\\).\n\nNow, let us find the cofactors \\((-1)^{i + j} \\times \\text{minor}\\):\n\nFor \\(a_{11}\\), the cofactor is \\((-1)^2 \\times -3 = 1 \\times -3 = -3\\).\nFor \\(a_{12}\\), the cofactor is \\((-1)^3 \\times -5 = -1 \\times -5 = 5\\).\nFor \\(a_{13}\\), the cofactor is \\((-1)^4 \\times -5 = 1 \\times -5 = -5\\).\n\nNow, take each element, multiply by its cofactor. Sum all of these products together.\n\\[\n1(-3) + 2(5) + 1(-5) = -3 + 10 - 5 = 2\n\\]\nThus, the determinant of the matrix is \\(2\\).\n\n\n\nThis works for any matrix of any size, for any row or any column.\n\nSo, you should choose rows/columns with more 0’s, since the final sum involves multiplying the element with the cofactor, and if the element is 0, then you don’t have to consider it.\n\n\n\n\nMatrix Inverse\nIf you take a matrix \\(\\mathbf A\\), and multiply by the inverse \\(\\mathbf A^{-1}\\), the result will be the identity matrix \\(\\mathbf{I}\\).\nYou can invert any square matrix that does not have a determinant of a 0. This is because the inverse is defined as the following:\n\\[\n\\mathbf A^{-1} = \\frac{1}{|\\mathbf A|} \\mathbf C^\\mathsf{T}\n\\]\n\nWhere \\(| \\mathbf A|\\) is the determinant of the matrix \\(\\mathbf A\\), and \\(\\mathbf C^\\mathsf{T}\\) is the transpose of the cofactor matrix (consisting of the cofactor of every element of \\(\\mathbf A\\).\n\n\n\n\n\n\n\nExample of a 2 by 2 Matrix Inverse\n\n\n\n\n\nLet us solve for the matrix inverse of a 2 by 2 matrix.\n\\[\n\\mathbf A_{2 \\times 2} = \\begin{pmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{pmatrix}\n\\]\nWe know the determinant of \\(\\mathbf A\\) with the formula for 2 by 2 matrix determinants:\n\\[\n| \\mathbf A | = a_{11}a_{22} - a_{12} a_{21}\n\\]\nNow, let us find the cofactors (note, the determinant of a scalar is just the scalar):\n\n\\(c_{11} = (-1)^{1+1}a_{22} = a_{22}\\)\n\\(c_{12} = (-1)^{1+2}a_{21} = -a_{21}\\)\n\\(c_{21} = (-1)^{2+1} a_{12} = -a_{12}\\)\n\\(c_{22} = (-1)^{2+2} a_{11} = a_{11}\\)\n\nThus, our cofactor matrix is:\n\\[\n\\mathbf C_{2 \\times 2} = \\begin{pmatrix}\na_{22} & -a_{21} \\\\\n-a_{12} & a_{11}\n\\end{pmatrix}\n\\]\nThe transpose of the cofactor matrix is thus (flipping rows to columns):\n\\[\n\\mathbf C^\\mathsf{T} = \\begin{pmatrix}\na_{22} & -a_{12} \\\\\n-a_{21} & a_{11}\n\\end{pmatrix}\n\\]\nThus, the inverse is:\n\\[\n\\mathbf A^{-1} = \\frac{1}{|\\mathbf A|} \\mathbf C^\\mathsf{T} = \\frac{1}{a_{11}a_{22} - a_{12} a_{21}} \\begin{pmatrix}\na_{22} & -a_{12} \\\\\n-a_{21} & a_{11}\n\\end{pmatrix}\n\\]\nThus, for example, the following is true:\n\\[\n\\mathbf A = \\begin{pmatrix}\n3 & 1 \\\\\n5 & 2\n\\end{pmatrix}, \\ \\mathbf A^{-1}  = \\begin{pmatrix}\n2 & -1 \\\\\n-5 & 3\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nProperties of Inverses\n\n\n\n\n\nThe Inverse Property states that the inverse of an inverse is the original matrix:\n\\[\n\\left( \\mathbf A^{-1} \\right)^{-1} = \\mathbf A\n\\]\nThe Multiplication Property states the inverse of a product is the following (note the order of the multiplication):\n\\[\n(\\mathbf{AB})^{-1}=\\mathbf B^{-1} \\mathbf A^{-1}\n\\]\nThe Scalar Multiplication Property states that the scalar product is the following:\n\\[\n(c \\mathbf A)^{-1} = \\frac{1}{c} \\mathbf A^{-1}, \\quad \\text{s.t.} \\quad  c ≠ 0\n\\]\n\n\n\n\n\n\n\n\n\nVector Spaces\n\nLinear Mapping\nA mapping is any rule that maps elements from one set to another. A function \\(f\\) is a mapping \\(f: A \\rightarrow B\\).\nA linear mapping is a mapping that is linear, which must meet the following properties:\n\n\\(f(a+b) = f(a) + f(b)\\)\n\\(f(ca) = c f(a)\\)\n\nWe can represent linear mappings for finite sets by matrices. Let us say \\(\\mathbf X_{n \\times m}\\) is a matrix, and \\(\\mathbf y_{m}\\) is a vector. Let us find their product:\n\\[\n\\mathbf {Xy} = \\mathbf z_m\n\\]\nWhat this is saying is take the vector \\(\\mathbf y\\), and operate on it using the matrix \\(\\mathbf X\\), to produce a new vector \\(\\mathbf z\\). Here, the matrix \\(\\mathbf X\\) is an operator that maps \\(\\mathbf y \\rightarrow \\mathbf z\\).\n\n\n\nLinear Combinations and Independence\nA linear combination is a combination of vectors that is linear (i.e. vectors can be added, and scalar multiplied). For example, this is a linear combination:\n\\[\nt \\mathbf x + (1-t) \\mathbf y\n\\]\nLinear combinations either represents lines (in \\(\\mathbb R^2\\)), planes (in \\(\\mathbb R^3\\)), and hyperplanes (a plane of one less dimensions than the space) in higher dimensions.\nNow, let us take some linear combination:\n\\[\na_1 \\mathbf x_1 + a_2 \\mathbf x_2+\\dots + a_n \\mathbf x_n\n\\]\nThis set of vectors \\((\\mathbf x_1, \\dots, \\mathbf x_n)\\) is linearlly independent if you cannot go from one vector \\(\\mathbf x_j \\in \\{ \\mathbf x_1, \\dots, \\mathbf x_n \\}\\), and linearly transform it (by adding/subtracting/multiplying a constant) into another vector.\n\n\n\n\n\n\nExample of Linear Independence\n\n\n\n\n\nLet us say we have these two vectors:\n\\[\n\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n\\]\nAre these linearly independent? That means I cannot use a linear transformation to go from one to another.\nNo, there is no constant you can multiply to get from vector 1 to vector 2, and there are no other vectors to add/subtract to to go from one to another.\nNow consider these two vectors:\n\\[\n\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix}\n\\]\nThese are not independent - you can multiply the first vector by a scalar of 2 to get the second vector.\n\n\n\nThis can be complicated to see in higher dimensions (since it is hard to consider how multiple vectors can be combined to match another). There, we use the matrix rank (see below).\n\n\n\nSpanning Vectors and Dimension\nA collection of spanning vectors spans some space, if you can write any vector in that space, as a linear combination of the spanning vectors.\nFor example, take vector \\(\\mathbf e_1 = (1 \\ 0)\\) and \\(\\mathbf e_2 = (0 \\ 1)\\). These vectors span some space including \\(\\mathbf z\\), if vector \\(\\mathbf z\\) can be written as:\n\\[\n\\mathbf z = a \\mathbf e_1 + b \\mathbf e_2, \\quad \\text{e.x.} \\quad \\mathbf z = (a \\ b)\n\\]\n\nIn fact, \\(\\mathbf e_1\\) and \\(\\mathbf e_2\\) span the set of all 2-dimensional vectors.\n\nThis allows us to write vectors in terms of the core vectors, and to understand the dimension of the space. The dimension of the space is the smallest number of linearly independent vectors that span the space.\n\n\n\n\n\n\nExample of Dimensionality\n\n\n\n\n\nTake these two vectors:\n\\[\n\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\]\nThese are linearly independent - no factor multipled can get the other vector. Thus, this is 2 dimensional space\nNow, let us add a third vector.\n\\[\n\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}\n\\]\nIs the third vector linearly independent? No. We can write the third vector with a combination of the other two:\n\\[\n\\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix} = 3 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + 2 \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\]\nThus, the third vector is in the space spanned by the first 2 vectors. The first two vectors spans this space, and thus, it is 2 dimensional space.\n\n\n\nGenerally, the dimensionality of the space matches the number of vectors that span the space (so a 2 dimensional space is often spanned by 2 vectors, 3 spanned by 3, etc.).\nNote: Dimensionality of a vector space is not always the same as the dimensionality of the vectors.\n\n\n\nMatrix Rank\nAs we discussed before, it can be difficult to find if vectors are linearly independent in higher dimensions.\nWe can stack the row vectors into a matrix (we can also do them in columns):\n\\[\n\\begin{pmatrix} \\mathbf x_1 \\\\ \\mathbf x_2 \\\\ \\mathbf x_3 \\end{pmatrix} =\n\\begin{pmatrix} x_{11} & x_{12} & x_{13} \\\\\nx_{21} & x_{22} & x_{23} \\\\\nx_{31} & x_{32} & x_{33} \\end{pmatrix}\n\\]\nThe Rank of a matrix is the number of linearly independent rows/columns in a matrix.\nIf the Rank is equal to the total number of rows/columns (all rows/columns are linearly independent), the matrix has full rank. A matrix with full rank is non-singular, and thus, can be inverted, and has a non-zero determinant.\nThus, if we find the determinant of the matrix, if it is 0, the vectors are not linearly independent, and if it is not 0 , they are linearly independent.\nWe can also know a matrix is not full rank, if the space of the dimension is less than the number of vectors (as explained above).\n\n\n\nQuadratic Forms\nA quadratic form takes the following form:\n\\[\n\\mathbf x^\\mathsf{T} \\mathbf A \\mathbf x\n\\]\nWhere matrix \\(\\mathbf A\\) is a square matrix.\nIf \\(\\mathbf x^\\mathsf{T} \\mathbf A \\mathbf x &gt; 0\\) for all values, then the matrix is positive definite. If \\(\\mathbf x^\\mathsf{T} \\mathbf A \\mathbf x &lt; 0\\), then negative definite. If \\(\\mathbf x^\\mathsf{T} \\mathbf A \\mathbf x ≤ 0\\) it is positive semi-definite.\nThis will be useful for optimisation and multivariable calculus\n\n\n\n\n\n\nSolving Systems of Equations\n\nMatrices and Systems of Equations\nYou can write any system of linear equations as a matrix times a vector. Let us take this set of equations:\n\\[\n\\begin{cases}\na_{11}x_1 + a_{12} x_2 + a_{13} x_3 = y_1 \\\\\na_{21}x_1 + a_{22}x_2 + a_{23}x_3 = y_2 \\\\\na_{31}x_1 + a_{32}x_2 + a_{33}x_3 = y_3\n\\end{cases}\n\\]\nWe can write this system of equations as follows:\n\\[\n\\begin{pmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}\n= \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix}\n\\]\nOr even simpler, we can represent it as:\n\\[\n\\mathbf A\\mathbf x = \\mathbf y\n\\]\nA unique solution exists when you have the same amount of equations as unknowns, and the equations are non-contradictory.\nOverdetermined systems are when there are more equations than unknowns, which might contradict each other. Underdetermined systems are when there are not enough equations compared to unknowns, so we cannot solve it.\n\n\n\n\n\n\nNon-Linearly Independent and Identification\n\n\n\n\n\nTake this system of linear equations:\n\\[\n\\begin{cases}\nx+y=1 \\\\\n2x + 2y = 2\n\\end{cases}\n\\]\nWe can see that the two equations are a common factor of each other. Or in other words, these two vectors are non-linearly independent.\nWe know when solving for these equations, we cannot actually solve for a unique solution.\nWe know if a matrix is full rank, then all rows/columns are linearly independent.\n\n\n\n\n\n\nSolving Systems of Equations\nMatrix inversion is a way to solve a system of equations. Take this system of equations:\n\\[\n\\mathbf{Ax} = \\mathbf y\n\\]\nYou can solve for \\(\\mathbf x\\) by inverting matrix \\(\\mathbf A\\) (assuming matrix a is full rank):\n\\[\n\\begin{align}\n\\mathbf{Ax} & = \\mathbf y \\\\\n\\color{blue}{\\mathbf{A}^{-1}}\\color{black}{\\mathbf{Ax}} & = \\color{blue}{\\mathbf A^{-1}}\\color{black}{\\mathbf y} && (\\text{multiply both sides by } \\color{blue}{\\mathbf A^{-1}}\\color{black}) \\\\\n\\mathbf x &= \\mathbf A^{-1}\\mathbf y && (\\mathbf A^{-1} \\mathbf A\\text{ inverses cancel})\n\\end{align}\n\\]\n\n\n\n\n\n\nExample of Matrix Inversion\n\n\n\n\n\nTake this system of equations:\n\\[\n\\begin{cases}\n3x - 7y = -11 \\\\\n5x + 10y = 25\n\\end{cases}\n\\]\nWe can write this in linear algebra:\n\\[\n\\begin{pmatrix}\n3 & -7 \\\\\n5 & 10\n\\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} =\n\\begin{pmatrix} -11 \\\\ 25 \\end{pmatrix}\n\\]\nNow, let us find the inverse of the first matrix:\n\\[\n\\mathbf A^{-1} = \\frac{1}{|\\mathbf A|}\\mathbf C^\\mathsf{T}\n\\]\nWe know the determinant \\(|\\mathbf A| = 3(10) - (-7)(5) = 65\\).\nNow, let us find the cofactor matrix:\n\n\\(c_{11} = (-1)^{1+1}\\times 10 = 10\\)\n\\(c_{12} = (-1)^{1+2} \\times 5 = -5\\)\n\\(c_{21} = (-1)^{2+1} \\times -7 = 7\\)\n\\(c_{22} = (-1)^{2+2} \\times 3 = 3\\)\n\nThus, the cofactor matrix transposed should be:\n\\[\n\\mathbf C^\\mathsf{T} = \\begin{pmatrix} 10 & -5 \\\\ 7 & 3 \\end{pmatrix}^\\mathsf{T} = \\begin{pmatrix} 10 & 7 \\\\ -5 & 3 \\end{pmatrix}\n\\]\nThus, the inverse of matrix \\(\\mathbf{A}\\) should be:\n\\[\n\\mathbf A^{-1} = \\frac{1}{65} \\begin{pmatrix} 10 & 7 \\\\ -5 & 3 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{13} & \\frac{7}{65} \\\\ -\\frac{1}{13} & \\frac{3}{65} \\end{pmatrix}\n\\]\nWe know the solution should be:\n\\[\n\\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{13} & \\frac{7}{65} \\\\ -\\frac{1}{13} & \\frac{3}{65} \\end{pmatrix} \\begin{pmatrix} -11 \\\\ 25 \\end{pmatrix}\n\\]\nThus, doing matrix multiplication to obtain \\(x\\) and \\(y\\):\n\n\\(x =\\frac{2}{13}(-11) + \\frac{7}{65}(25) = -\\frac{22}{13} + \\frac{35}{13} = \\frac{13}{13}=1\\)\n\\(y=-\\frac{1}{13}(-11) + \\frac{3}{65}(25) = \\frac{11}{13}+ \\frac{15}{13} = \\frac{26}{13} = 2\\)\n\nThe solution to this system of equations is: \\((1, 2)\\).\n\n\n\nCramer’s rule is another rule to solve equations, only for square matrices. It is not super commonly used. Given the system of equations:\n\\[\n\\mathbf{Ax} = \\mathbf y\n\\]\nThe element \\(x_i\\) is defined as:\n\\[\nx_i = \\frac{|\\mathbf B_i|}{\\mathbf A}\n\\]\nWhere \\(\\mathbf B_i\\) is a matrix obtained by taking the matrix \\(\\mathbf A\\), and replacing the \\(i\\)th column with the column vector \\(\\mathbf y\\).\n\n\n\n\n\n\nEigenvalues and Eigenvectors\n\nDefinitions\nA matrix, as we discussed, can act as an operator that maps some vector to another \\(\\mathbf A : \\mathbf x \\rightarrow \\mathbf y\\).\nGeometrically, vector \\(\\mathbf x\\) points in some dimensional space. Then the matrix \\(\\mathbf A\\) comes along, and transforms it into a different vector \\(\\mathbf x\\), which might flip the direction, or rotate, or change its norm, etc. Sometimes, \\(\\mathbf y\\) will stay in the same direction or opposite direction (along the same line) as the original, even after the operator.\nBasically, an eigenvector of matrix \\(\\mathbf A\\) is a vector \\(\\mathbf x\\), that does not change its direction (stays on the same line) when you apply the operator \\(\\mathbf A\\) to get vector \\(\\lambda \\mathbf x\\). Mathematically:\n\\[\n\\mathbf{Ax} = \\lambda \\mathbf x\n\\]\nThe \\(\\lambda\\) is an eigenvalue that corresponds to the eigenvector.\n\n\n\nComputing Eigenvalues\nLet us start with the eigenvector formula:\n\\[\n\\mathbf{Ax} = \\lambda \\mathbf x\n\\]\nWe can multiply the right side by the identity matrix (which does not change the result):\n\\[\n\\mathbf{Ax} = \\lambda \\mathbf {Ix}\n\\]\nNow, let us move everything to one side, and simplify:\n\\[\n\\begin{split}\n\\mathbf{Ax} - \\lambda \\mathbf {Ix} & = 0 \\\\\n(\\mathbf A - \\lambda \\mathbf I) \\mathbf x & = 0\n\\end{split}\n\\]\n\\((\\mathbf A - \\lambda \\mathbf I)\\) is an singular matrix, meaning determinant \\(|(\\mathbf A - \\lambda \\mathbf I)| = 0\\). All values of \\(\\lambda\\) that solve this determinant equation will be eigenvalues of the matrix.\n\n\n\n\n\n\nExample of Solving for Eigenvalues\n\n\n\n\n\nWe know \\(|(\\mathbf A - \\lambda \\mathbf I)| = 0\\). Let us say:\n\\[\n\\mathbf A = \\begin{pmatrix} 2 & 1 \\\\ 3 & 4 \\end{pmatrix}, \\quad \\lambda \\mathbf I \\begin{pmatrix} \\lambda & 0 \\\\ 0 & \\lambda \\end{pmatrix}\n\\]\nThus:\n\\[\n\\mathbf A - \\lambda \\mathbf I = \\begin{pmatrix} 2 - \\lambda & 1 \\\\ 3 & 4 - \\lambda \\end{pmatrix}\n\\]\nWe know the determinant should equal 0. We can solve for \\(\\lambda\\):\n\\[\n\\begin{split}\n0 & = |(\\mathbf A - \\lambda \\mathbf I)| \\\\\n0 & = (2-\\lambda)(4-\\lambda) - 1(3) \\\\\n0 & = 8-2\\lambda - 4 \\lambda +\\lambda^2 - 4 \\\\\n0 & = \\lambda^2 - 6\\lambda +4 \\\\\n\\end{split}\n\\]\nLet us use the quadratic formula:\n\\[\n\\begin{split}\n\\lambda & = \\frac{-b ± \\sqrt{b^2 - 4ac}}{2a} \\\\\n\\lambda & = \\frac{6 ± \\sqrt{36 - 4(1)(4)}}{2(1)} \\\\\n\\lambda & = \\frac{6 ± \\sqrt{20}}{2} \\\\\n\\lambda & = 3 ± \\frac{\\sqrt{20}}{2} \\\\\n\\lambda & = 3 ± \\frac{2 \\sqrt{5}}{2} \\\\\n\\lambda & = 3 ± \\sqrt{5}\n\\end{split}\n\\]\nThus, we have found our eigenvalues.\n\n\n\n\n\n\nCalculating Eigenvectors\nLet us say we have the matrix \\(\\mathbf{A}\\):\n\\[\n\\mathbf A = \\begin{pmatrix} 2 & 3 \\\\ 2 & 1 \\end{pmatrix}\n\\]\nFirst, you need to calculate the eigenvalues \\(\\lambda\\) (as shown in the previous section).\n\n\n\n\n\n\nExample of Calculating Eigenvalues\n\n\n\n\n\nWe know \\(|(\\mathbf A - \\lambda \\mathbf I)| = 0\\).\n\\[\n\\mathbf A - \\lambda \\mathbf I = \\begin{pmatrix} 2 - \\lambda & 3 \\\\ 2 & 1 - \\lambda \\end{pmatrix}\n\\]\nNow solve \\(|(\\mathbf A - \\lambda \\mathbf I)| = 0\\).\n\\[\n\\begin{split}\n0 & = |(\\mathbf A - \\lambda \\mathbf I)| \\\\\n0 & = (2 - \\lambda)(1- \\lambda) - 2(3) \\\\\n0 & = 2 -2 \\lambda - \\lambda + \\lambda^2 - 6 \\\\\n0 & = \\lambda ^2 - 3\\lambda - 4 \\\\\n0 & = (\\lambda - 4)(\\lambda + 1) \\\\\n\\lambda & = 4, -1\n\\end{split}\n\\]\n\n\n\nFor every eigenvalue, you will have an eigenvector that makes the eigenvector equation \\(\\mathbf{Ax} = \\lambda \\mathbf x\\) true. From above:\n\\[\n\\begin{split}\n& \\mathbf{Ax} = 4 \\mathbf x \\\\\n& \\mathbf{Ax} = -1 \\mathbf x\n\\end{split}\n\\]\nRemember, \\(\\mathbf x\\) is a vector here of 2 elements, \\(x_1, x_2\\) - but we only have one equation for each eigenvalue pair. This means there will be one solution without a unique solution.\n\n\n\n\n\n\nEigenvalues and Non-Unique Solutions\n\n\n\n\n\nIt might seem simple to just solve for our \\(\\mathbf x\\) with two unknowns \\(x_1, x_2\\).\nBut the issue is - our equations are not linearly independent, so we do not have enough information to solve for a unique solution for one.\nThis is actually okay - why? Well, take a look at these potential eigenvectors:\n\\[\n\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}\n\\]\nThese are different vectors - but they are all eigenvectors. This is because of the equation \\(\\mathbf{Ax} = \\lambda \\mathbf x\\) - if both sides have vector \\((1 \\ 0)\\) multiplied by a constant (like the 2nd and 3rd vectors), the equation is still equal.\nThus, eigenvectors are not defined uniquely - only up to a singular multiplicative constant.\n\n\n\nWhat we typically do is define \\(x_1 = 1\\) (there are some cases where this does not work). Using this, we can solve for the answer.\n\n\n\n\n\n\nExample of Calculating Eigenvectors\n\n\n\n\n\nLet us continue the same example from before.\n\\[\n\\mathbf A = \\begin{pmatrix} 2 & 3 \\\\ 2 & 1 \\end{pmatrix}, \\quad \\lambda = 4, -1, \\quad \\mathbf x = \\begin{pmatrix} 1 \\\\ c \\end{pmatrix}\n\\]\nLet us solve for the eigenvalue of \\(\\lambda = 4\\).\n\\[\n\\begin{split}\n\\mathbf{Ax} & = 4 \\mathbf x \\\\\n\\begin{pmatrix} 2 & 3 \\\\ 2 & 1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ c \\end{pmatrix} & = 4\\begin{pmatrix} 1 \\\\ c \\end{pmatrix} \\\\\n\\begin{pmatrix} 2+3c \\\\ 2+c \\end{pmatrix} & = \\begin{pmatrix} 4 \\\\ 4c \\end{pmatrix}\n\\end{split}\n\\]\nThat gives us two equations:\n\\[\n\\begin{split}\n& 2 + 3c = 4 \\\\\n& 2 + c = 4c\n\\end{split}\n\\]\nThe answer is \\(c = \\frac{2}{3}\\) (both answers give us the equation).\nThus, the eigenvector with \\(\\lambda = 4\\) is:\n\\[\n\\mathbf x = \\begin{pmatrix} 1 \\\\ \\frac{2}{3} \\end{pmatrix}\n\\]\nWe can do the same for \\(\\lambda = -1\\), and we will get eigenvector:\n\\[\n\\mathbf x = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n\\]\n\n\n\n\n\n\nEigenvector Decomposition\nMatrix decomposition is to take a matrix \\(\\mathbf A\\), and decompose it into other matrices, that when multiplied together, get the original matrix.\nIf matrix \\(\\mathbf A\\) has unique eigenvalues, you can write:\n\\[\n\\mathbf A = \\mathbf {QDQ}^{-1}\n\\]\nWhere \\(\\mathbf Q\\) is made up of eigenvectors of matrix \\(\\mathbf A\\), and \\(\\mathbf D\\) is a diagonal matrix with the eigenvalues \\(\\lambda\\) on the diagonal:\n\\[\n\\mathbf D = \\begin{pmatrix} \\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{pmatrix}, \\quad\n\\mathbf Q = \\begin{pmatrix} \\mathbf x_1 & \\mathbf x_2\\end{pmatrix}\n\\]\nEigenvectors decomposition has a few uses:\n\n\n\n\n\n\nTaking the Power of Matrices\n\n\n\n\n\nLet us say you want to find \\(\\mathbf A^z\\). We can use matrix decomposition for this:\n\\[\n\\mathbf A^z = {QDQ}^{-1}{QDQ}^{-1}{QDQ}^{-1}\\dots\n\\]\nNotice how the \\(\\mathbf{Q}^{-1} \\mathbf Q\\) occurs quite frequently, and we know by properties of inverses, that \\(\\mathbf{Q}^{-1} \\mathbf Q = \\mathbf I\\), and multiplying by \\(\\mathbf I\\) does nothing.\nThus, we can rewrite the formula above as:\n\\[\n\\mathbf A^z = \\mathbf{QD}^z\\mathbf Q^{-1}\n\\]\nThis is much simpler than calculating \\(\\mathbf A^z\\), since taking diagonal matrix to a power is defined as:\n\\[\n\\mathbf D^z = \\begin{pmatrix} \\lambda_1^z & 0 \\\\ 0 & \\lambda_2^z\\end{pmatrix}\n\\]\nWhich is much easier to do.\n\n\n\n\n\n\n\n\n\nFinding Determinants\n\n\n\n\n\nLet us say you want to find the determinant of \\(\\mathbf A\\). The following is true:\n\\[\n\\begin{split}\n\\det (\\mathbf A) & = \\det(\\mathbf{QD}^z\\mathbf Q^{-1}) \\\\\n& = \\det (\\mathbf Q) \\det (\\mathbf D) \\det (\\mathbf Q^{-1}) \\\\\n& = \\det (\\mathbf Q) \\det (\\mathbf D) \\frac{1}{\\det (\\mathbf Q)} \\\\\n& = \\det (\\mathbf D)\n\\end{split}\n\\]\nSince \\(\\mathbf D\\) is diagonal, the determinant of a diagonal matrix is just the product of the diagonal elements. Thus:\n\\[\n\\det (\\mathbf A) = \\prod_i \\lambda_i\n\\]\n\n\n\n\n\n\n\n\n\nPrinciple Components Analysis\n\n\n\n\n\nPrinciple Components Analysis (PCA) is used when matrix \\(\\mathbf A\\) is symmetric, positive semi-definite.\n\nThis means the eigenvectors are orthogonal (perpendicular) to each other.\nThis means eigenvalues will always be real and non-negative.\n\nPCA is done on the covariance matrix, which is symmetric and positive semi-definite.\nIt is a data reduction technique commonly used in statistics and data science.\n\nThe eigenvectors of the covariance matrix form the principle components of thee system.\nThe eigenvalues tell us how much of the variance each component explains (more important principle components have higher eigenvalues).\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Linear Algebra Reference"
    ]
  },
  {
    "objectID": "2.html",
    "href": "2.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "In the last chapter, we discussed the basics of statistics, and briefly introduced regression as a way to find correlations.\nThis chapter dives deep into multiple linear regression, the foundational model for all of statistics. We cover the specification of the model, estimation and statistical inference, as well as extensions.\nUse the right sidebar for quick navigation. R-code is provided at the bottom.\n\n\nBasics of the Model\n\nModel Specification\nLet us say we have some outcome variable \\(y\\), and several explanatory variables \\(x_1, x_2, \\dots, x_k\\). We have data on \\(n\\) number of observations \\(i = 1, \\dots n\\).\nThe linear regression model can be written as a conditional expectation \\(E(y|x)\\) function:\n\\[\nE(y_i |x_i) = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_kx_{ki}\n\\]\nThe linear model can also be specified for any specific outcome value \\(y_i\\) for unit \\(i\\):\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_kx_{ki} + u_i\n\\]\nWe can also specify the linear model in terms of linear algebra:\n\\[\n\\begin{pmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\\end{pmatrix} =\n\\begin{pmatrix}1 & x_{11} & \\dots & x_{k1} \\\\1 & x_{12} & \\dots & x_{k2} \\\\\\vdots & \\vdots & \\vdots & \\vdots \\\\1 & x_{1n} & \\dots & x_{kn}\\end{pmatrix}\n\\begin{pmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_k\\end{pmatrix}\n+ \\begin{pmatrix}u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n\\end{pmatrix}\n\\]\n\\[\n\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u\n\\]\n\n\n\n\n\n\nMore Info on Conditional Expectations\n\n\n\n\n\nImagine \\(y\\) is income, and \\(x\\) is age.\nAt age \\(x=20\\), not every 20 year old makes the same amount of income. There is some distribution, with some making more, and some making less. This is the distribution \\(y|x=20\\).\nWe can find the expected value of this distribution, \\(E(y|x=20)\\). This is a conditional expectation, and indicates the expected income of a 20 year old if we randomly chose one from the distribution.\nAt \\(x=30\\), the \\(E(y|x)\\) probably is different (30 year olds make more money). Thus, the linear model is essentially stating that the expected \\(y\\) depends on \\(x\\). Or in terms of this example, the expected income depends on the individual’s age.\n\n\n\n\n\n\n\n\n\nMore Info on the Error Term \\(u_i\\)\n\n\n\n\n\nThe \\(u_i\\) is called the error term. This indicates that not every value of \\(y_i\\) in our data will be exactly on the linear best-fit line.\nGraphically, it is the highlighted part:\n\n\n\n\n\nIn social science terms, the \\(u_i\\) is the effect of any other variable not included in our model on \\(y\\).\nFor example, if \\(x\\) is age, and \\(y\\) is income, we will have the following relationship:\n\\[\n\\text{income}_i = \\beta_0 + \\beta_1 \\text{age}_i + u_i\n\\]\nHowever, not every individual lies perfectly on this linear line. This is because there are other factors outside of age that affect \\(y\\) (income), and these other factors are bundled into the error term.\n\n\n\n\n\n\nEstimation Process\nTo estimate the population parameters \\(\\beta_0, \\dots, \\beta_k\\), we use our sample data, and try to find the values \\(\\widehat{\\beta_0}, \\dots, \\widehat{\\beta_k}\\) that minimise the square sum of residuals (SSR):\n\\[\n\\begin{align}\nSSR & = \\sum\\limits_{i=1}^n(y_i - \\hat y_i)^2 \\\\\n& = \\sum\\limits_{i=1}^n(y_i - \\color{blue}{(\\widehat{\\beta_0} + \\widehat{\\beta_1}x_{1i} + \\dots + \\widehat{\\beta_k}x_{ki})}\\color{black})^2 && (\\text{plug in } \\color{blue}{\\hat y = \\widehat{\\beta_0} + \\widehat{\\beta_1}x_{1i} + \\dots }\\color{black}) \\\\\n& = \\sum\\limits_{i=1}^n(y_i - \\widehat{\\beta_0} - \\widehat{\\beta_1}x_{1i} - \\dots - \\widehat{\\beta_k}x_{ki})^2 &&(\\text{distribute negative sign})\n\\end{align}\n\\]\nIn the linear algebra representation (where \\(\\mathbf b\\) is the vector of estimated parameters \\(\\widehat{\\beta_0}, \\dots, \\widehat{\\beta_k}\\)):\n\\[\n\\begin{align}\nS(\\hat{\\boldsymbol\\beta}) & = (\\mathbf y - \\hat{\\mathbf y})^\\mathsf{T} (\\mathbf y - \\hat{\\mathbf y})\\\\\n& = (\\mathbf y - \\color{blue}{\\mathbf X \\hat{\\boldsymbol\\beta}}\\color{black})^\\mathsf{T} (\\mathbf y - \\color{blue}{\\mathbf{X} \\hat{\\boldsymbol\\beta}}\\color{black}) && (\\text{plug in } \\color{blue}{\\hat{\\mathbf y}  = \\mathbf X \\hat{\\boldsymbol\\beta}}\\color{black}) \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\nIntuitive Visualisation of SSR\n\n\n\n\n\nThe residuals are the difference from our predicted best-fit line result \\(\\widehat{y_i}\\), and the actual value of \\(y_i\\) in the data. Below highlighted in red are the residuals.\n\n\n\n\n\nAfter we have the residual values, we simply square each of them, then sum all of them together. That is the sum of squared residuals.\n\n\n\nThis estimation is called the ordinary least squares (OLS) estimator. The solutions to the OLS estimator can be derived mathematically.\n\n\n\nDeriving OLS Estimates\nOLS wants to minimise the sum of squared residuals \\(S(\\hat{\\boldsymbol\\beta})\\) - the differences between the actual \\(\\mathbf y\\) and our predicted \\(\\hat{\\mathbf y}\\):\n\\[\n\\begin{align}\nS(\\hat{\\boldsymbol\\beta}) & = (\\mathbf y - \\hat{\\mathbf y})^\\mathsf{T} (\\mathbf y - \\hat{\\mathbf y})\\\\\n& = (\\mathbf y - \\color{blue}{\\mathbf X \\hat{\\boldsymbol\\beta}}\\color{black})^\\mathsf{T} (\\mathbf y - \\color{blue}{\\mathbf{X} \\hat{\\boldsymbol\\beta}}\\color{black}) && (\\text{plug in } \\color{blue}{\\hat{\\mathbf y}  = \\mathbf X \\hat{\\boldsymbol\\beta}}\\color{black}) \\\\\n& = \\mathbf y^\\mathsf{T} \\mathbf y - \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf y - \\mathbf y^\\mathsf{T} \\mathbf{X}\\hat{\\boldsymbol\\beta} + \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf{Xb} && (\\text{distribute out)} \\\\\n& = \\mathbf y^\\mathsf{T} \\mathbf y - \\color{blue}{2\\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf y}\\color{black} + \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf{X} \\hat{\\boldsymbol\\beta} &&(\\text{combine } \\color{blue}{- \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf y - \\mathbf y^\\mathsf{T} \\mathbf{X}\\hat{\\boldsymbol\\beta}}\\color{black})\n\\end{align}\n\\]\nNow, let us find the first order condition:\n\\[\n\\frac{\\partial S(\\hat{\\boldsymbol\\beta})}{\\partial \\hat{\\boldsymbol\\beta}} = -2\\mathbf X^\\mathsf{T} \\mathbf y + 2 \\mathbf X^\\mathsf{T} \\mathbf{X} \\hat{\\boldsymbol\\beta} = 0\n\\]\nWhen assuming \\(\\mathbf X^\\mathsf{T} \\mathbf X\\) is invertable (which is true if \\(\\mathbf X\\) is full rank), we can isolate \\(\\hat{\\beta}\\) to find the solution to OLS:\n\\[\n\\begin{align}\n-2\\mathbf X^T\\mathbf y + 2 \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol{\\hat{\\beta}} & = 0 \\\\\n2 \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol{\\hat\\beta} & = 2\\mathbf X^\\mathsf{T} \\mathbf y && (+ 2\\mathbf X^\\mathsf{T} \\mathbf y \\text{ to both sides}) \\\\\n\\boldsymbol{\\hat\\beta} & = (2\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} 2 \\mathbf X^\\mathsf{T} \\mathbf y && (\\times (2\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\text{ to both sides})\\\\\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y &&(\\text{cancel out } 2^{-1}\\times 2)\n\\end{align}\n\\]\nThose are our coefficient solutions to OLS. With the estimated parameters \\(\\widehat{\\beta_0}, \\dots, \\widehat{\\beta_k}\\), we now have a best-fit line, called the fitted values.\nFor more detailed analysis of the OLS estimator, see the causal inference section.\n\n\n\n\n\n\nInterpretation\n\nInterpretation of Parameters\nI define \\(\\widehat{\\beta_j} \\in \\{\\widehat{\\beta_1}, \\dots, \\widehat{\\beta_k}\\}\\), multiplied to \\(x_j \\in \\{x_1, \\dots, x_k\\}\\). \\(\\widehat{\\beta_0}\\) is the intercept.\n\n\n\n\n\n\n\n\n\nContinuous \\(x_j\\)\nBinary \\(x_j\\)\n\n\nContinuous \\(y\\)\nFor every one unit increase in \\(x_j\\), there is an expected \\(\\widehat{\\beta_j}\\) unit change in \\(y\\).\nWhen all explanatory variables equal 0, the expected value of \\(y\\) is \\(\\widehat{\\beta_0}\\).\nThere is a \\(\\widehat{\\beta_j}\\) unit difference in \\(y\\) between category \\(x_j = 1\\) and category \\(x_j = 0\\).\nFor category \\(x_j = 0\\), the expected value of \\(y\\) is \\(\\widehat{\\beta_0}\\) (when all other explanatory variables equal 0).\n\n\nBinary \\(y\\)\nFor every one unit increase in \\(x_j\\), there is an expected \\(\\widehat{\\beta_j} \\times 100\\) percentage point change in the probability of a unit being in category \\(y=1\\).\nWhen all explanatory variables equal 0, the expected probability of a unit being in category \\(y=1\\) is \\(\\widehat{\\beta_0} \\times 100\\)\nThere is a \\(\\widehat{\\beta_j}\\times 100\\) percentage point difference in the probability of a unit being in category \\(y=1\\) between category \\(x_j = 1\\) and category \\(x_j = 0\\).\nFor category \\(x_j = 0\\), the expected probability of a unit being in category \\(y=1\\) is \\(\\widehat{\\beta_0} \\times 100\\) (when all other explanatory variables equal 0).\n\n\n\nIf you have multiple explanatory variables, always add: while holding all other explanatory variables not \\(x_j\\) constant.\n\n\n\nStandardised Interpretations\nSometimes, a \\(\\beta_j\\) increase in \\(y\\) for every one unit increase in \\(x\\) is not particularly useful for us to interpret. For example, if \\(y\\) is democracy, what does a 5 unit increase in democracy actually mean?\nWe can add more relevant detail by expressing the change of \\(y\\) and \\(x\\) in terms of their standard deviations. Or in other words, we want to find the change in \\(\\frac{\\hat y_i}{\\sigma_y}\\) for every one standard deviation \\(\\sigma_x\\) increase in \\(x\\). For simplicity, let us use a simple linear regression \\(E(y_i|x_i) = \\beta_0 + \\beta_1 x_i\\):\n\\[\n\\begin{align}\n& E \\left(\\frac{y_i}{\\sigma_y} | x_i = x + \\sigma_x \\right ) - E \\left(\\frac{y_i}{\\sigma_y} | x_i = x \\right ) \\\\\n& = \\frac{E(y_i|x_i = x+ \\sigma_x)}{\\sigma_y} - \\frac{E(y_i|x_i = x)}{\\sigma_y} &&(\\text{property of expectation}) \\\\\n& = \\frac{E(y_i|x_i = x+ \\sigma_x) - E(y_i|x_i = x)}{\\sigma_y} && (\\text{combine into 1 fraction})\\\\\n& = \\frac{\\beta_0 + \\beta_1(x+\\sigma_x) - [\\beta_0 + \\beta_1(x)]}{\\sigma_y} && (\\text{plug in regression models})\\\\\n& = \\frac{\\beta_0 + \\beta_1x + \\beta_1\\sigma_x - \\beta_0 -\\beta_1x}{\\sigma_y} && (\\text{distribute out})\\\\\n& = \\frac{\\beta_1\\sigma_x}{\\sigma_y} && (\\text{cancel and simplify})\n\\end{align}\n\\]\nThus, for a one standard deviation \\(\\sigma_x\\) increase in \\(x_j\\), there is an expected \\(\\frac{\\beta_j\\sigma_x}{\\sigma_y}\\)-standard deviation change in \\(y\\).\n\n\n\nResidual Standard Deviation\nResiduals are the distance of the actual value \\(y_i\\) of observation \\(i\\), compared to the predicted \\(\\widehat{y_i}\\) from our fitted values/best-fit line. They can be obtained after we fit our model:\n\\[\n\\begin{align}\n\\mathbf{\\hat u} & = \\mathbf y - \\mathbf{\\hat y} \\\\\n& = \\mathbf y - \\color{blue}{\\mathbf{X} \\hat{\\boldsymbol\\beta}} && \\color{black}(\\text{plug in } \\color{blue}{\\hat{\\mathbf y} = \\mathbf{X} \\hat{\\boldsymbol\\beta}} \\color{black}) \\\\\n& = \\mathbf y - \\mathbf X \\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y} && \\color{black}(\\text{plug in } \\color{blue}{\\boldsymbol{\\hat\\beta}  = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y}\\color{black})\n\\end{align}\n\\]\nThe residual standard deviation \\(\\hat\\sigma\\) measures the spread/variance of our residuals - so, how far away the actual values \\(y_i\\) are from our predicted values \\(\\widehat{y_i}\\) in general for all observations.\nThe residual variance is estimated with the formula below (with the residual standard deviation being the square root):\n\\[\n\\hat\\sigma^2 = \\frac{\\sum_{i=1}^n \\hat u_i^2}{n-k-1} = \\frac{\\mathbf{\\hat u}^\\mathsf{T} \\mathbf{\\hat u}}{n-k-1}\n\\]\n\n\n\n\n\n\nVisualisation of Residual Standard Deviation\n\n\n\n\n\nBelow is a figure illustrating different residual standard deviations, with the same best-fit line.\n\n\n\n\n\n\n\n\nSmaller \\(\\hat\\sigma\\) mean the actual values are, on average, close to our predicted values, and larger \\(\\hat\\sigma\\) mean the actual values are, on average, further away from our predicted values.\n\n\n\nR-Squared\nOur fitted values equation takes the following form:\n\\[\n\\begin{align}\n\\hat{\\mathbf y} & = \\mathbf X \\hat{\\boldsymbol\\beta}  \\\\\n\\hat{\\mathbf y} & = \\mathbf X\\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y} && \\color{black}(\\text{plug in } \\color{blue}{\\boldsymbol{\\hat\\beta} = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y}\\color{black}) \\\\\n\\hat{\\mathbf y} &= \\color{blue}{\\mathbf P}\\color{black}{\\mathbf y}  && (\\text{plug in } \\color{blue}{\\mathbf P : = \\mathbf X(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}} \\color{black})\n\\end{align}\n\\]\nWe can see that \\(\\mathbf P\\) is a matrix that turns \\(\\mathbf y \\rightarrow \\hat{\\mathbf y}\\). Matrix \\(\\mathbf P\\) is our linear model that projects the true values \\(\\mathbf y\\) into a space of our regressors \\(\\mathbf X\\).\nOne thing we might be interested in is how well our model \\(\\mathbf{Py}\\) explains the actual \\(\\mathbf y\\). One way we can do this is the scalar product of two vectors. We know that the scalar/dot product calculates the project/shadow of one vector on another. Thus, the scalar product \\(\\mathbf y^\\mathsf{T}\\mathbf{Py}\\) describes the shadow the actual \\(y\\) casts on our projected model.\nHowever, this value will change based on the scale of our \\(y\\) variable. Thus, we will divide it by \\(\\mathbf y^\\mathsf{T}\\mathbf y\\), which is the “maximum” shadow possible (perfect shadow). This ratio is called \\(R^2\\).\n\\[\nR^2 = \\frac{\\mathbf y^\\mathsf{T}\\mathbf{Py}}{\\mathbf y^\\mathsf{T}\\mathbf y}\n\\]\nR-Squared (\\(R^2\\)) measures the proportion of variation in \\(y\\) that is explained by our explanatory variables. R-Squared is always between 0 and 1 (or 0-100 as a percentage). Higher values indicate our model better explains the variation in \\(y\\).\nInterpreting R-squared: The Model explains \\(R^2 \\times 100\\) percent of the variation in \\(y\\).\n\n\n\n\n\n\nStatistical Inference\n\nHomoscedasticity and Heteroscedasticity\nHomoscedasticity is defined as:\n\\[\nVar(\\mathbf u | \\mathbf X) = \\sigma^2 \\mathbf I_n = \\begin{pmatrix}\n\\sigma^2 & 0 & \\dots & 0 \\\\\n0 & \\sigma^2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & 0 \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{pmatrix}\n\\]\nOr in other words, no matter the values of any explanatory variable, the error term variance is constant.\nIf this is false, then we have heteroscedasticity.\n\n\n\n\n\n\nIntuitive Visualisation of Homoscedasticity\n\n\n\n\n\nAn easy way to identify homoscedasticity is to look at a residual plot (just the plot of all \\(\\widehat{u_i}\\)):\n\nNotice how the homoscedasticity residuals seem to have the same up-down variance, no matter the value of \\(x\\).\nThe heteroscedasticity residuals have a clear pattern - the up-down variance is smaller when \\(x\\) is smaller, and the up-down variance is larger when \\(x\\) is larger.\nEssentially, if you see a pattern in the residual plot, it is likely heteroscedasticity.\n\n\n\nIf you have homoscedasticity, you should use normal OLS standard errors.\nIf you have heteroscedasticity, you should use robust OLS standard errors. You should also use robust standard errors if you are not sure which errors to use.\n\n\n\nDeriving Standard Errors\nWe will only derive homoscedastic (normal) standard errors. The robust standard error derivation is beyond the scope of this lesson (just trust the computer that it will calculate it properly).\nLet us assume homoscedasticity. We want to find the variance of our estimator, \\(Var(\\boldsymbol{\\hat\\beta} | \\mathbf X)\\). Let us start off with our OLS solution. We can simplify as follows:\n\\[\n\\begin{align}\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}(\\mathbf X \\boldsymbol\\beta + \\mathbf u) && (\\text{plug in } \\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u) \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&(\\text{multiply out})\\\\\n& = \\color{blue}{\\mathbf I}\\color{black}{\\boldsymbol\\beta} + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&( \\ \\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X = \\mathbf I}\\color{black})\\\\\n& = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{identity property of } \\mathbf I)\n\\end{align}\n\\]\n\\[\nVar(\\boldsymbol{\\hat\\beta} | \\mathbf X) = Var(\\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u \\ | \\ \\mathbf X)\n\\]\n\\(\\boldsymbol\\beta\\) is a vector of fixed constants. \\((\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u\\) can be imagined as a matrix of fixed constants, since we are conditioning the above variance on \\(\\mathbf X\\) (so for each \\(\\mathbf X\\), the statement is fixed).\n\n\n\n\n\n\nMathematical Lemma\n\n\n\n\n\nIf \\(\\mathbf u\\) is an \\(n\\) dimensional vector of random variables, \\(\\mathbf c\\) is an \\(m\\) dimensional vector, and \\(\\mathbf B\\) is an \\(n \\times m\\) dimensional matrix with fixed constants, then the following is true:\n\\[\nVar(\\mathbf c + \\mathbf{Bu}) = \\mathbf B Var(\\mathbf u)\\mathbf B^T\n\\]\nI will not prove this lemma here, but it is provable.\n\n\n\nWith the Lemma above, and with the definition of homoscedasticity, we can simplify:\n\\[\n\\begin{align}\nVar(\\boldsymbol{\\hat\\beta} | \\mathbf X) & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} Var(\\mathbf u | \\mathbf X) [(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}]^{-1} && (\\text{lemma})\\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} Var(\\mathbf u | \\mathbf X) \\color{blue}{\\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1}} && \\color{black}( \\ \\color{blue}{[(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}]^{-1} = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1}}\\color{black})\\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\color{blue}{\\sigma^2 \\mathbf I_n}\\color{black}{ \\mathbf X} (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\color{blue}{Var(\\mathbf u | \\mathbf X) = \\sigma^2 \\mathbf I_n}\\color{black}) \\\\\n& =  \\color{red}{\\sigma^2} \\color{black} (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf I_n \\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\text{move scalar } \\color{red}{\\sigma^2}\\color{black})\\\\\n& =  \\sigma^2 (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}  \\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\text{identity property of } \\mathbf I_n)\\\\\n& =  \\sigma^2 (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\text{inverses } \\mathbf X^\\mathsf{T}  \\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\text{ cancel})\n\\end{align}\n\\]\nHowever, we do not actually know what \\(\\sigma^2\\) is. We can estimate it with \\(\\hat\\sigma^2\\) (discussed here).\nThe standard errors are the square root of the variance. Thus, our standard errors for any coefficient estimate \\(\\hat\\beta_j\\) are:\n\\[\nse(\\hat\\beta_j) = \\hat\\sigma \\sqrt{(\\mathbf X^T \\mathbf X)^{-1}_{jj}}\n\\]\n\n\n\nT-Tests\nIn regression, our typical hypotheses are:\n\n\\(H_0 : \\beta_j = 0\\) (i.e. there is no relationship between \\(x_j\\) and \\(y\\)).\n\\(H_1:\\beta_j ≠ 0\\) (i.e. there is a relationship between \\(x_j\\) and \\(y\\)).\n\nUsing the standard error (see above), we calculate the \\(t\\)-statistic, and using the \\(t\\)-statistic, we calculate a p-value.\n\n\n\n\n\n\nDetails of Running a Hypothesis Test\n\n\n\n\n\nFirst, we calculate the t-test statistic:\n\\[\nt = \\frac{\\widehat{\\beta_1} - H_0}{\\widehat{se}(\\widehat{\\beta_1})}\n\\]\n\nWhere \\(H_0\\) is typically 0, but if you do decide to alter the null hypothesis, you would plug it in.\n\nNow, we consult a t-distribution of \\(n-k-1\\) degrees of freedom. We use a t-distribution because the standard error calculation used in OLS is slightly imprecise.\n\nNote: we can only do this step if we believe the central limit theorem is met (that our errors are asymptotically normal). We need a large enough sample size.\n\nWe start from the middle of the t-distribution, and move t-test-statstic number of standard deviations from both sides of the middle.\nThen, we find the probability of getting a t-test statistic even further from the middle than the one we got. The area highlighted in the figure below showcases this. In the figure, the t-test statistic is 2.228.\n\n\n\n\n\nThe area highlighted, divided by the entire area under the curve, is the p-value.\n\n\n\nThe p-value we get is the probability of getting a test statistic equally or more extreme than the one we got, given the null hypothesis is true.\n\nIf \\(p&lt;0.05\\), we believe the probability of a null hypothesis is low enough, such that we reject the null hypothesis (that there is no relationship between \\(x\\) and \\(y\\)), and conclude our alternate hypothesis (that there is a relationship between \\(x\\) and \\(y\\)).\nIf \\(p &gt; 0.05\\), we cannot reject the null hypothesis, and cannot reject that there is no relationship between \\(x\\) and \\(y\\).\n\nNOTE: this is not causality - we are only looking at the relationship. Causality needs to be established with an adequate research design.\n\n\n\nConfidence Intervals\nThe 95% confidence intervals of coefficients have the following bounds:\n\\[\n\\widehat{\\beta_j} - 1.96 \\widehat{se}(\\widehat{\\beta_j}), \\ \\ \\widehat{\\beta_j} + 1.96 \\widehat{se}(\\widehat{\\beta_j})\n\\]\n\nThe 1.96 is an approximation assuming a normal distribution. The actual confidence intervals (calculated by computers) will use a t-distribution of \\(n-k-1\\), which will result in a slightly different multiplicative factor.\n\nThe confidence interval means that under repeated sampling and estimating \\(\\widehat{\\beta_j}\\), 95% of the confidence intervals that we construct will include the true \\(\\beta_j\\) value in the population.\nIf the confidence interval contains 0, we cannot conclude a relationship between \\(x_j\\) and \\(y\\), as 0 is a plausible value of \\(\\beta_j\\). These results will always match those of the t-test.\n\n\n\nF-Tests\nF-tests are used to test more than one coefficient at a time. The hypotheses will be:\n\n\\(M_0 : y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_g x_g + u_i\\) (the smaller null model).\n\\(M_a : y = \\beta_0 + \\beta_1x_1 + \\dots + \\beta_g x_g + \\dots + \\beta_kx_k + u_i\\) (the bigger model with additional variables).\n\n\n\n\n\n\n\nDetails of the F-test\n\n\n\n\n\nF-tests compare the \\(R^2\\) of the two models through the F-statistic:\n\\[\nF = \\frac{(SSR_0 - SSR_a) / (k_a - k_0)}{SSR_a /(n - k_a - 1)}\n\\]\nWe then consult a F-distribution with \\(k_a - k_0\\) and \\(n-k_a - 1\\) degrees of freedom, obtaining a p-value (in the same way as the t-test).\n\n\n\nThe p-value we get is the probability of getting a test statistic equally or more extreme than the one we got, given the null hypothesis is true.\n\nIf \\(p&lt;0.05\\), the we believe the probability of the null hypothesis is low enough, such that we reject the null hypothesis (that \\(M_0\\) is the better model), and conclude our alternate hypothesis (that \\(M_a\\) is a better model). This also means the extra coefficients in \\(M_a\\) are jointly statistically significant.\nIf \\(p &gt; 0.05\\), we cannot reject the null hypothesis, and cannot reject that \\(M_0\\) is a better model. Thus, the extra coefficients in \\(M_a\\) are jointly not statistically significant.\n\n\n\n\nPredictive Inference\nWe can predict using the linear regression by plugging in explanatory variable values, and finding the predicted \\(\\widehat{y_i}\\).\n\\[\n\\begin{align}\n\\hat{\\mathbf y} & = \\mathbf X \\hat{\\boldsymbol\\beta}  \\\\\n\\hat{\\mathbf y} & = \\mathbf X\\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y} && \\color{black}(\\text{plug in } \\color{blue}{\\boldsymbol{\\hat\\beta} = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y}\\color{black}) \\\\\n\\end{align}\n\\]\nWe also have confidence intervals for every predicted \\(\\widehat{y_i}\\). These intervals are calculated with the residual standard deviation (covered previously):\n\\[\n\\widehat{y_i} - 1.96 \\hat\\sigma, \\ \\widehat{y_i} + 1.96 \\hat\\sigma\n\\]\n\n\n\n\n\n\nExtensions\n\nCategorical Explanatory Variables\nTake an explanatory variable \\(x\\), which has \\(n\\) number of categories \\(1, \\dots, n\\). To include \\(x\\) in our regression, we would create \\(n-1\\) dummy variables, to create the following regression model:\n\\[\nE(y_i|x_i) = \\beta_0 + \\beta_1x_{1i} + \\dots + \\beta_k x_{n-1 \\ i}\n\\]\n\nCategories \\(1, \\dots, n-1\\) get there own binary variable \\(x_1, \\dots, x_{n-1}\\).\nCategory \\(n\\) (the reference category) does not get its own variable. We can change which category we wish to be the reference.\n\nInterpretation is as follows (category \\(j\\) is any one of category \\(1, \\dots, n-1\\)).\n\n\\(\\beta_j\\) is the difference in expected \\(y\\) between category \\(j\\) and the reference category.\n\\(\\beta_0\\) is the expected \\(y\\) of the reference category.\nThus, category \\(j\\) has an expected \\(y\\) of \\(\\beta_0 + \\beta_j\\).\n\n\n\n\n\n\n\nExample of a Categorical Explanatory Variable\n\n\n\n\n\nLet us say that \\(x\\) is the variable development level of a country, with 3 categories: low (L), medium (M), and high (H). \\(y\\) will be the crime rate of the country.\nLet us set low development (L) as our reference category. Our regression will be:\n\\[\nE(y|x) = \\beta_0 + \\beta_1x_M + \\beta_2 x_H\n\\]\nNow let us interpret the coefficients:\n\n\\(\\beta_0\\) is the expected crime rate for a country of low (L) development.\n\\(\\beta_1\\) is the difference in expected crime rate between a medium (M) developed country and a low (L) developed country (since low is the reference category).\n\\(\\beta_2\\) is the difference in expected crime rate between a high (H) developed country and a low (L) developed country (since low is the reference category).\n\nThe expected/predicted \\(y\\) (crime rate) for each category is:\n\nLow (L): \\(\\beta_0\\)\nMedium (M): \\(\\beta_0 + \\beta_1\\)\nHigh (H): \\(\\beta_0 + \\beta_2\\).\n\n\n\n\nEach coefficient \\(\\beta_j\\)’s statistical significance is a difference-in-means significance test, not the significance of the categorical variable as a whole. To find if the entire categorical variable is significant, you should use a F-test.\n\n\n\nFixed Effects\nWhen we have hierarchical or panel data, we need to control for differences between clusters. We essentially include the cluster variable as a categorical variable in our regression.\n\n\n\n\n\n\nHierarchical/Clustered Data\n\n\n\n\n\nHierarchical data is data where the basic units of analysis \\(i\\) are clustered, grouped, or nested into clusters.\nFor example, let us say we are measuring how income affects voter turnout in european countries. We have observations from France, Switzerland, Germany, and many other countries. However, these observations can be grouped by the country they came from.\nWhy is this grouping important? This is because there may be something in common between observations within the same cluster. For example, Switzerland might just have higher voter turnout in general due to something about Swiss institutions or culture.\nThis means that observations aren’t random - i.e. we know that if we select from switzerland, it is likely to have higher turnout - observations from the same country are correlated. Thus, we need some way to account for this clustering of observations. We will explore this below.\n\n\n\n\n\n\n\n\n\nPanel Data\n\n\n\n\n\nPanel data is data that can be clustered in two ways - by unit, and by time. For example, let us say we have a dataset on all countries and their GDP between 1960-2020.\n\nWe will have clusters based on country: Germany will have an observation in 1960, in 1961, …, to 2020. Same for every other country. These observations are grouped by the unit (country in this case).\nWe will also have clusters based on time: We will have all GDP observations for all countries in 1960, in 1961, etc. These observations are grouped by the time (year in this case).\n\n\n\n\nLet us say we have \\(m\\) number of clusters \\(i = 1, \\dots, m\\). Within each cluster, we will have units \\(t = 1, \\dots, n\\). Our cluster fixed effects model will take the form:\n\\[\n\\begin{split}\ny_{it} & = \\alpha_i + \\beta_1x_1 + \\dots + \\beta_kx_k + u_{it} \\\\\n& \\text{where } \\alpha_i = \\beta_{00} + \\underbrace{\\beta_{02}D_{i2} + \\beta_{03}D_{i3} + \\dots + \\beta_{0m}D_{im}}_{\\text{unique intercepts for each cluster}}\n\\end{split}\n\\]\n\nWhere \\(D_{i2}, D_{i3}, \\dots, D_{im}\\) are dummy variables for clusters \\(2, \\dots, m\\). Cluster 1 is the reference category.\n\\(y_{it}\\) indicates the \\(y\\) value of the \\(t\\)th individual in the \\(i\\)th cluster.\n\nFor panel data, we use two-way fixed effects, which is basically just two fixed effects for different clustering. Let us say we have \\(i = 1, \\dots, m\\) units with \\(t = 1, \\dots, n\\) different numbers of time periods. Our two way fixed effects model takes the form:\n\\[\n\\begin{split}\ny_{it} & = \\alpha_i + \\gamma_t + \\beta_1x_1 + \\dots + \\beta_kx_k + u_{it} \\\\\n& \\text{where } \\alpha_i =  \\alpha_{00} + \\underbrace{\\alpha_{02}D_{i2} + \\alpha_{03}D_{i3} + \\dots + \\alpha_{0m}D_{im}}_{\\text{unique intercepts for each unit}} \\\\\n& \\text{where } \\gamma_t =  \\gamma_{00} + \\underbrace{\\gamma_{02}T_{i2} + \\gamma_{03}D_{t3} + \\dots + \\gamma_{0n}T_{in}}_{\\text{unique intercepts for each time}} \\\\\n\\end{split}\n\\]\n\nWhere \\(D_{i2}, D_{i3}, \\dots, D_{im}\\) are dummy variables for units \\(2, \\dots, m\\)., and \\(T_{i2}, T_{i3}, \\dots, T_{in}\\) are dummy variables for time periods \\(2, \\dots, n\\).\n\\(y_{it}\\) indicates the observation of unit \\(i\\) in time period \\(t\\).\n\n\n\n\n\n\n\nIntuitive Explanation of Fixed Effects\n\n\n\n\n\nFor one-way fixed effects, we essentially add a unique intercept term for every cluster, accounting for the average differences in \\(y\\) between each category.\n\n\\(\\beta_{00}\\) is the intercept for the reference category 1.\n\\(\\beta_{00} + \\beta_{0i}\\) is the intercept for the \\(i\\)th category.\n\nFor two-way fixed effects, we add a unique intercept term for every year and country, accounting for the average differences in \\(y\\) between each country, and the average differences in \\(y\\) between each year.\n\n\n\n\n\n\nInteraction Effects\nAn interaction between two variables means they are multiplied in the regression equation:\n\\[\ny_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} + \\beta_3 x_{1i} x_{2i}\n\\]\nInterpretation of the relationship between \\(x_1\\) and \\(y\\) is as follows:\n\n\n\n\n\n\n\n\n\nBinary \\(x_2\\)\nContinuous \\(x_2\\)\n\n\nBinary \\(x_1\\)\nWhen \\(x_2 = 0\\), the effect of \\(x_1\\) (going from 0 to 1) on \\(y\\) is \\(\\widehat{\\beta_1}\\).\nWhen \\(x_2 = 1\\), the effect of \\(x_1\\) (going from 0 to 1) on \\(y\\) is \\(\\widehat{\\beta_1} + \\widehat{ \\beta_3}\\).\nThe effect of \\(x_1\\) (going from 0 to 1) on \\(y\\) is \\(\\widehat{\\beta_1} + \\widehat{\\beta_3} x_2\\).\n\n\nContinuous \\(x_1\\)\nWhen \\(x_2 = 0\\), for every increase in one unit of \\(x_1\\), there is an expected \\(\\widehat{\\beta_1}\\) unit change in \\(y\\).\nWhen \\(x_2 = 1\\), for every increase in one unit of \\(x_1\\), there is an expected \\(\\widehat{\\beta_1}+ \\widehat{\\beta_3}\\) change in \\(y\\).\nFor every increase of one unit in \\(x_1\\), there is an expected \\(\\widehat{\\beta_1} + \\widehat{\\beta_3} x_2\\) change in \\(y\\).\n\n\n\n\n\n\n\n\n\nProof of Interpretations of Interactions\n\n\n\n\n\nWe can solve for the change of \\(x_1\\) on \\(y\\) using a partial derivative of \\(y\\) in respect to \\(x_1\\):\n\\[\n\\begin{split}\n\\frac{\\partial \\widehat{y_i}}{\\partial x_{1i}} & = \\frac{\\partial}{\\partial x_{1i}} \\left[ \\widehat{\\beta_0} + \\widehat{\\beta_1}x_{1i} + \\widehat{\\beta_2}x_{2i} + \\widehat{\\beta_3}x_{1i}x_{2i}\\right] \\\\\n\\frac{\\partial \\widehat{y_i}}{\\partial x_{1i}} & = \\widehat{\\beta_1} + \\widehat{\\beta_3}x_2\n\\end{split}\n\\]\nThis gives us the effect of \\(x_1\\) on \\(y\\).\n\n\n\n\\(\\widehat{\\beta_0}\\) is still the expected \\(y\\) when all explanatory variables equal 0.\nThe coefficient of the interaction \\(\\widehat{\\beta_3}\\), when statistically significant, indicates a statistically significant interaction effect. If it is not statistically significant, then the interaction effect is not statistically significant (and can be dropped).\n\n\n\nPolynomial Transformations\nSometimes the relationship between two variables is not a straight line - we can add more flexibility with polynomials. The most common form of polynomial transformation is the quadratic transformation:\n\\[\ny_i = \\beta_0 + \\beta_1x_{i} + \\beta_2 x_{i}^2 + u_i\n\\]\nOur estimated \\(\\widehat{\\beta_0}\\) remains the expected value of \\(y\\) when all explanatory variables equal 0.\nUnfortunately, the \\(\\widehat{\\beta_1}\\) and \\(\\widehat{\\beta_2}\\) coefficients are not directly interpretable.\n\n\\(\\widehat{\\beta_2}\\)’s sign can tell us if the best-fit parabola opens upward or downward.\nThe significance of \\(\\widehat{\\beta_2}\\) also indicates if the quadratic term is statistically significant. If it is not, we can remove the transformation.\n\nWe can interpret two things about the quadratic transformation:\n\nFor every one unit increase in \\(x\\), there is an expected \\(\\widehat{\\beta_1} + 2 \\widehat{\\beta_2}x\\) unit increase in \\(y\\).\nThe minimum/maximum point in the best-fit parabola occurs at \\(x_i = - \\widehat{\\beta_1}/2 \\widehat{\\beta_2}\\)\n\n\n\n\n\n\n\nProof of Polynomial Interpretations\n\n\n\n\n\nWe can derive the change in \\(y\\) given a one unit increase in \\(x\\) by finding the partial derivative of \\(y\\) in respect to \\(x\\):\n\\[\n\\begin{split}\n\\frac{\\partial \\widehat{y_i}}{\\partial x} & = \\frac{\\partial}{\\partial x} \\left[ \\widehat{\\beta_0} + \\widehat{\\beta_1}x_i + \\widehat{\\beta_2}x_i^2 \\right] \\\\\n\\frac{\\partial \\widehat{y_i}}{\\partial x} & = \\widehat{\\beta_1} + 2 \\widehat{\\beta_2}x_i\n\\end{split}\n\\]\nWe can also solve for the \\(x_i\\) that results in the minimum/maximum of the best-fit parabola by setting the partial derivative equal to 0:\n\\[\n\\begin{split}\n0 & = \\widehat{\\beta_1} + 2 \\widehat{\\beta_2}x_i \\\\\nx_i & = -\\widehat{\\beta_1}/2 \\widehat{\\beta_2}\n\\end{split}\n\\]\n\n\n\nWe can go beyond quadratic - as long as we always include lower degree terms in our model:\n\nCubic: \\(y_i = \\beta_0 + \\beta_1x_{i} + \\beta_2 x_{i}^2 + \\beta_3 x_i^3 + u_i\\)\nQuartic: \\(y_i = \\beta_0 + \\beta_1x_{i} + \\beta_2 x_{i}^2 + \\beta_3 x_i^3 + \\beta_4 x_i^4 + u_i\\)\n\n\n\n\nLogarithmic Transformations\nLogarithmic transformations are often used to change skewed variables into normally distributed variables.\n\n\n\n\n\n\nLogging a Skewed Variable\n\n\n\n\n\nMany monetary variables are heavily skewed. Natural logging these variables can turn them into normal distributions. This is useful, since skewed variables tend to have heteroscedasticity, and by making them normal, we can use the smaller normal standard errors.\nFor example, take this variable called expenses with a significant right skew:\n\n\n\n\n\nIf we take the log of this variable, we get the following distribution that is almost normal:\n\n\n\n\n\n\n\n\nWe have 3 types of logarithmic transformations:\n\n\n\n\n\n\n\n\n\n\\(x\\)\n\\(\\log (x)\\)\n\n\n\\(y\\)\nLinear Model:\n\\(y = \\beta_0 + \\beta_1 x + u\\)\nLinear-Log Model:\n\\(y = \\beta_0 + \\beta_1 \\log x + u\\)\n\n\n\\(\\log (y)\\)\nLog-Linear Model:\n\\(\\log(y) = \\beta_0 + \\beta_1 x + u\\)\nLog-Log Model:\n\\(\\log y = \\beta_0 + \\beta_1 \\log x + u\\)\n\n\n\n\nInterpreting the models:\n\n\n\n\n\n\n\n\n\n\\(x\\)\n\\(\\log (x)\\)\n\n\n\\(y\\)\nLinear Model:\nWhen \\(x\\) increases by one unit, there is an expected \\(\\widehat{\\beta_1}\\) unit change in \\(y\\).\nLinear-Log Model:\nWhen \\(x\\) increases by 10%, there is an expected \\(0.096 \\widehat{\\beta_1}\\) unit change in \\(y\\).\n\n\n\\(\\log (y)\\)\nLog-Linear Model:\nFor every one unit increase in \\(x\\), \\(y\\) is multiplied by \\(e^{\\widehat{\\beta_1}}\\).\nLog-Log Model:\nMultiplying \\(x\\) by \\(e\\) will multiply the expected value of \\(y\\) by \\(e^{\\widehat{\\beta_1}}\\).\n\n\n\n\n\n\n\n\n\nProof of Interpretations for Log Transformations\n\n\n\n\n\nProof of Linear-Log Model:\n\\[\n\\begin{split}\n& E(y_i|x_i = x) = \\beta_0 + \\beta_1 \\log x \\\\\n& E(y_i | x_i = e^A x) = \\beta_0 + \\beta_1 \\log(e^A x) \\\\\n& = \\beta_0 + \\beta_1 (\\log(e^A) + \\log x) \\\\\n& = \\beta_0 + \\beta_1 (A + \\log x) \\\\\n& = \\beta_0 + \\beta_1A + \\beta_1 \\log x\n\\end{split}\n\\]\n\\[\n\\begin{split}\nE(y_i|x_i = \\alpha x) - E(y_i|x_i = x) & = \\beta_0 + \\beta_1 A + \\beta_1 \\log (x) - (\\beta_0 + \\beta_1 \\log x) \\\\\n& = \\beta_1 A\n\\end{split}\n\\]\n\nWhen \\(A = 0.095\\), then \\(e^A = 1.1\\). Thus, a 1.1 times increase of \\(x\\) results in a \\(0.095 \\widehat{\\beta_1}\\) change in \\(y\\).\n\n\nProof of Log-Linear Model:\n\\[\n\\begin{split}\nE(\\log y_i | x_i = x) =  \\log y_i & = \\beta_0 + \\beta_1 x \\\\\ny_i & = e^{\\beta_0 + \\beta_1 x} \\\\\ny_i & = e^{\\beta_0}e^{\\beta_1 x} \\\\\nE(\\log y_i|x_i = x+1) = \\log y_i & = \\beta_0 + \\beta_1(x+1) \\\\\ny_i & = e^{\\beta_0 + \\beta_1 + \\beta_1 x} \\\\\ny_i & = e^{\\beta_0}e^{\\beta_1}e^{\\beta_1x}\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\frac{E(\\log y_i|x_i = x+1)}{E(\\log y_i | x_i = x)} & = \\frac{e^{\\beta_0}e^{\\beta_1}e^{\\beta_1x}}{e^{\\beta_0}e^{\\beta_1x}} \\\\\n& = e^{\\beta_1}\n\\end{split}\n\\]\n\nThus, when \\(x\\) increases by one, there is a multiplicative increase of \\(e^{\\beta_1}\\).\n\n\nProof of Log-Log model:\n\\[\n\\begin{split}\nE(\\log y_i | x_i = x) =  \\log y_i & = \\beta_0 + \\beta_1 \\log x \\\\\ny_i & = e^{\\beta_0 + \\beta_1 \\log x} \\\\\ny_i & = e^{\\beta_0}e^{\\beta_1 \\log x} \\\\\nE(\\log y_i|x_i = ex) = \\log y_i & = \\beta_0 + \\beta_1 \\log (ex) \\\\\ny_i & = e^{\\beta_0 + \\beta_1 \\log e + \\beta_1 \\log x} \\\\\ny_i & = e^{\\beta_0}e^{\\beta_1}e^{\\beta_1 \\log x}\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\frac{E(\\log y_i|x_i = ex)}{E(\\log y_i | x_i = x)} & = \\frac{e^{\\beta_0}e^{\\beta_1}e^{\\beta_1 \\log x}}{e^{\\beta_0}e^{\\beta_1 \\log x}} \\\\\n& = e^{\\beta_1}\n\\end{split}\n\\]\n\nThus, when \\(x\\) is multiplied by \\(e\\), there is a multiplicative increase of \\(e^{\\beta_1}\\).\n\n\n\n\n\n\n\n\n\n\nImplementation in R\nYou will need package fixest.\n\nlibrary(fixest)\n\nRegression with normal standard errors can be done with the lm() function:\n\nmodel &lt;- lm(y ~ x1 + x2 + x3, data = mydata)\nsummary(model)\n\nRegression with robust standard errors can be done with the feols() function:\n\nmodel &lt;- feols(y ~ x1 + x2 + x3, data = mydata, se = \"hetero\")\nsummary(model)\n\nOutput will include coefficients, standard errors, p-values, and more.\n\n\n\n\n\n\nBinary and Categorical Variables\n\n\n\n\n\nYou can include binary and categorical variables by using the as.factor() function:\n\nfeols(y ~ x1 + as.factor(x2) + x3, data = mydata, se = \"hetero\")\n\nYou can do the same for \\(y\\) or \\(x\\). Just remember, \\(y\\) cannot be a categorical variable (use multinomial logsitic regression instead).\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n\n\n\n\nYou can include one-way fixed effects by adding a | after your regression formula in feols():\n\nmodel &lt;- feols(y ~ x1 + x2 + x3 | cluster,\n               data = mydata, se = \"hetero\")\nsummary(model)\n\nYou can add two-way fixed effects as follows:\n\nmodel &lt;- feols(y ~ x1 + x2 + x3 | unit + year,\n               data = mydata, se = \"hetero\")\nsummary(model)\n\n\n\n\n\n\n\n\n\n\nInteraction Effects\n\n\n\n\n\nTwo interact two variables, use * between them. This will automatically include both the interaction term, and the two variables by themselves.\n\nfeols(y ~ x1 + x2*x3, data = mydata, se = \"hetero\")\n\nIf for some reason, you only want the interaction term, but not the variables by themselves, you can use a colon : between the two variables:\n\nfeols(y ~ x1 + x2:x3, data = mydata, se = \"hetero\")\n\n\n\n\n\n\n\n\n\n\nPolynomial Transformations\n\n\n\n\n\nTo conduct a polynomial transformation, you can use the I() function. The second argument is the degree of the polynomial:\n\nfeols(y ~ x1 + I(x2, 3), data = mydata, se = \"hetero\") #cubic for x2\n\n\n\n\n\n\n\n\n\n\nLogarithmic Transformations\n\n\n\n\n\nThe best way to do a logarithmic transformation is to create a new variable that is the log of the variable you want to transform using the log() function, before you even start the regression:\n\nmydata$x1_log &lt;- log(mydata$x1)\n\n\n\n\n\n\n\n\n\n\nConfidence Intervals\n\n\n\n\n\nTo find the confidence intervals for coefficients, first estimate the model with lm() or feols() as shown previously, then use the confint() command:\n\nconfint(model)\n\n\n\n\n\n\n\n\n\n\nF-Tests\n\n\n\n\n\nTo run a f-test, use the anova() command, and input your two different models, with the null model going first.\n\nanova(model1, model2)\n\nNote: F-tests only work with models that are run with homoscedastic standard errors. Robust standard errors will not work.\n\n\n\n\n\n\n\n\n\nLaTeX Regression Tables\n\n\n\n\n\nYou can use the texreg package to make nice regression tables automatically.\n\nlibrary(texreg)\n\nThe syntax for texreg() is as follows:\n\ntexreg(l = list(model1, model2, model3),\n       custom.model.names = c(\"model 1\", \"model 2\", \"model 3\"),\n       custom.coef.names = c(\"intercept\", \"x1\", \"x2\"),\n       digits = 3)\n\nYou can replace texreg() with screenreg() if you want a nicer regression table in the R-console.\nNote: you must have the same amount of model names as total models in your texreg, and you must have the same amount of coeficient names as the total amount of coefficients in all of your models.\n\n\n\n\n\n\n\n\n\nPrediction\n\n\n\n\n\nWe can use the predict() function to generate fitted value predictions in R:\n\nmy_predictions &lt;- predict(model, newdata = my_new_data)\n\nmy_new_data is a dataframe with a bunch of explanatory variable values (for every explanatory variable) for a collection of observations, that you wish to predict \\(\\hat y\\) for.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "2 Multiple Linear Regression"
    ]
  },
  {
    "objectID": "3.html",
    "href": "3.html",
    "title": "Classic Least Squares Theory",
    "section": "",
    "text": "Last chapter, we discussed the multiple linear regression model, and how it can help us measure relationships between explanatory and outcome variables.\nThis chapter introduces some key theory regarding the ordinary least squares estimator behind linear regression. Topics covered includes properties of estimators, the OLS estimator, and the Method of Moments estimator.\nUse the right sidebar for quick navigation.\n\n\nEstimators\n\nEstimands and Estimators\nAn estimand is the true value of some true parameter \\(\\theta\\) in the population we are trying to measure.\nWe often do not have data on the population. We typically have a sample from the population, and use an estimator (procedure) to produce a sample estimate \\(\\hat\\theta\\).\nHowever, because of sampling variability (not all random samples will be identical), each sample \\(n\\) will have a different estimate \\(\\hat\\theta_n\\).\nIf we keep taking \\(N\\) number of samples, we will have \\(N\\) number of estimates \\(\\hat\\theta_1, \\hat\\theta_2, \\dots, \\hat\\theta_N\\). Thus, any specific estimate \\(\\theta_n\\) from sample \\(n\\) can be thought of as a random draw from the sampling distribution \\(\\hat\\theta_1, \\hat\\theta_2, \\dots, \\hat\\theta_N\\).\n\n\n\n\n\n\nExample of a Sampling Distribution\n\n\n\n\n\nLet us say we want to find the mean salary of all individuals in the UK. The true value of the mean salary for every individual is \\(\\theta\\).\nHowever, asking all 60 million people is nearly impossible. So, we take a randomly sample of 1000 individuals, and then find the sample mean. Our estimator is thus the sample mean estimator.\nOur first sample of 1000 individuals yields an estimate \\(\\hat\\theta_1\\). If we take another sample, we will get slightly different people in this sample, and get another estimate \\(\\hat\\theta_2\\). We keep taking samples, and get more and more estimates \\(\\hat\\theta_3, \\hat\\theta_4, \\dots, \\hat\\theta_n\\).\nWe plot all of these samples into a distribution as follows:\n\n\n\n\n\nThis indicates the potential estimates we can get. If we were to conduct only one sample, we would essentially be selecting a random \\(\\hat\\theta_i\\) value from this distribution.\n\n\n\nThe sampling distribution of an estimator is the key property of estimators. The two parameters of interest from this sampling distribution are its expectation and variance.\n\n\n\nUnbiased Estimators\nAn estimator of a parameter is unbiased, if its estimates \\(\\hat\\theta_n\\) have an expectation equal to the true population value of the parameter:\n\\[\nE(\\hat\\theta_n) = \\theta\n\\]\nOr in other words, if we repeatedly sample and use the estimator, on average, the estimates will be equal to the true population value.\nWe want an unbiased estimator, because if \\(E(\\hat\\theta_n) = \\theta\\), that means our “best guess” of the estimator value is the true parameter value \\(\\theta\\). That means any one estimate \\(\\hat\\theta_n\\) is on average, correct.\nIf our estimator is biased, we can quantify bias with the following formula:\n\\[\nBias(\\hat\\theta_n) = E(\\hat\\theta_n)-\\theta\n\\]\n\n\n\nVariance and Efficiency\nUnbiasedness is not the only desirable property of estimators - we also care about the variance. After all, if we have two unbiased estimators, the one with less variance will be on average, closer to the true population value, for any one estimate \\(\\hat\\theta\\).\n\n\n\n\n\n\nExample of the Importance of Variance\n\n\n\n\n\nFor example, let us say the true population parameter is \\(\\theta = 0\\). We will have two estimators: estimator \\(A\\) and estimator \\(B\\):\n\nEstimator \\(A\\), after two samples (for simplicity), produces estimates -1 and 1.\nEstimator \\(B\\), after two samples, produces estimates -100 and 100.\n\nBoth estimators are unbiased \\(E(\\hat\\theta_n) = 0\\). However, clearly, estimator \\(A\\) is, on average, closer to \\(\\theta =0\\) than estimator \\(B\\). This is because while both estimators are unbiased, estimator \\(A\\) has a smaller variance than estimator \\(B\\) - that is on average, estimator \\(A\\)’s estimators are more closely “packed around” the expectation of the estimator.\n\n\n\nThe variance of an estimator can be quantified as:\n\\[\nVar(\\hat\\theta_n) = E[(\\hat\\theta_n - E(\\hat\\theta_n))^2]\n\\]\nAn efficient estimator is one that, on average, has the closest estimated value \\(\\hat\\theta_n\\) to the true population parameter. If two estimators are both unbiased, the one with lower variance is more efficient. Efficiency can be quantified as the estimator with the lowest mean squared error:\n\\[\nMSE(\\hat\\theta_n) = E[(\\hat\\theta_n - \\theta)^2] =Var(\\hat\\theta_n) + Bias(\\hat\\theta_n)^2\n\\]\nWe generally want an efficient estimator, since we know it will be giving us the closest guess to the true population parameter \\(\\theta\\).\n\n\n\n\n\n\nEfficient but Biased\n\n\n\n\n\nInterestingly, it is possible for a biased estimator to be more efficient than an unbiased estimator.\nThis is particularly the case when the biased estimator has a slight bias but small variance, while the unbiased estimator has a giant variance. In this case, the biased estimator is producing estimates \\(\\hat\\theta\\) that on average, are closer to the true population parameter \\(\\theta\\).\n\n\n\n\n\n\nAsymptotically Consistent Estimators\nAsymptotic properties are properties of estimators as the sample size \\(n\\) approaches infinity.\nAn estimator is consistent, if as we increase sample size towards infinity, the estimate will become more and more concentrated around the true population value \\(\\theta\\). At \\(n = ∞\\), our sampling distribution collapses to just one value, the true population value \\(\\theta\\). Mathematically:\n\\[\nPr(|\\hat\\theta_n - \\theta|&gt; \\epsilon) \\rightarrow 0, \\text { as } n \\rightarrow ∞\n\\]\nOr in other words, the probability that the distance between an estimate \\(\\hat\\theta_n\\) and the true population value \\(\\theta\\) will be higher than a small close-to-zero value \\(\\epsilon\\) will be 0, since our estimates \\(\\hat\\theta_n\\) will converge at the \\(\\theta\\).\nThis is a useful property, since even if our estimator is biased, if it is asymptotically consistent, we know that with large enough sample sizes, that bias becomes infinitely small and negligible.\n\n\n\n\n\n\nBiased but Consistent\n\n\n\n\n\nAn estimator can be both biased, but consistent. In smaller sample sizes, the estimator might not be on average correct, but over a large enough sample size, it will become “unbiased”.\nFor example, in the figure below, we can see that this estimator is biased at small values of \\(n\\), but as \\(n\\) increases, it becomes more consistent, collapsing its distribution around the true \\(\\theta\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaw of Large Numbers and Consistency\n\n\n\n\n\nThe law of large numbers states that the sample average of a random sample, is a consistent estimator of the population mean.\nFor example, let us say we have a random variable \\(x\\). We take a random sample of \\(n\\) units, so our sample is \\((x_1, \\dots, x_n)\\).\n\nLet us define \\(\\bar x_n\\) as our sample average.\nLet us define \\(\\mu\\) as the true population mean of variable \\(x\\).\n\nThe law of large numbers states that:\n\\[\nplim( \\bar x_n) = \\mu\n\\]\n\nWhere \\(plim\\) states that as \\(n\\) approaches infinity, the probability distribution of \\(\\bar x_n\\) collapses around \\(\\mu\\).\n\n\nWhy is this the case? This sample mean estimator is calculated simply through the formula for mean:\n\\[\n\\bar x_n = \\frac{1}{n}\\sum\\limits_{i=1}^n x_i\n\\]\nLet us define the variance of our sample of \\(x_1, \\dots, x_n\\) as \\(Var(x_i) = \\sigma^2\\). We can now find the variance of our sampling distribution of estimator \\(\\bar x_n\\):\n\\[\n\\begin{split}\nVar(\\bar x_n) & = Var\\left( \\frac{1}{n}\\sum\\limits_{i=1}^n x_i \\right) \\\\\n& = \\frac{1}{n^2} Var \\left(\\sum\\limits_{i=1}^n x_i\\right) \\\\\n& = \\frac{1}{n^2} \\sum\\limits_{i=1}^n Var(x_i) \\\\\n& = \\frac{1}{n^2} \\sigma^2 \\\\\n& = \\frac{\\sigma^2}{n}\n\\end{split}\n\\]\nAnd as sample size \\(n\\) increases to infinity, we get:\n\\[\n\\lim\\limits_{n \\rightarrow ∞} Var(\\bar x_n) = \\lim\\limits_{n \\rightarrow ∞} \\frac{\\sigma^2}{n} = 0\n\\]\nThus, the variance of our estimator \\(\\bar x_n\\) shrinks to zero, so as sample size increases to infinity \\(n\\), the sampling distribution of estimator \\(\\bar x_n\\) collapses around the true population mean.\n\n\n\n\n\n\nAsymptotic Normality\nAnother asymptotic property of estimators, as sample size \\(n\\) approaches infinity, is that the sampling distribution approaches a normal distribution.\nThe central limit theorem establishes asymptotic normality of estimators. Let us say we have \\(N\\) number of random variables \\(\\hat\\theta_1, \\dots, \\hat\\theta_N\\) (estimates are realisations of random variables). The central limit theorem states that:\n\\[\nPr(w_n &lt; w) \\rightarrow \\Phi(w) \\quad \\text{as } n \\rightarrow ∞\n\\]\n\nWhere \\(w_n\\) is a transformed version of the random variable \\(\\hat\\theta_n\\), defined as \\(w_n = \\frac{\\bar\\theta_n - \\mu}{\\sigma / \\sqrt{n}}\\).\nWhere \\(Pr(w_n &lt; w)\\) is the cumulative density function of the random variable \\(w_n\\).\nWhere \\(\\Phi(w)\\) is the cumulative density function (cdf) of the standard normal distribution \\(\\mathcal N(0, 1)\\).\n\nThe importance of CLM comes from the fact that as we increase sample size, our sampling distribution becomes more and more normally distributed. This property is essentially for carrying out statistical inference and significance tests, as they generally assume that our estimator is normally distributed.\n\n\n\nNonparametric Bootstrap\nMost traditional statistical tests rely on asymptotic normality. However, asymptotic normality can only be satisfied if we have a large enough sample size. When we are dealing with small samples, we cannot invoke central limit theorem.\nNonparametric Bootstrap, instead of assuming some sampling distribution, is a method to simulate the sampling distribution. This is done by re-sampling from the sample with replacement. The procedure is as follows:\n\nYou take the sample you observe (with sample size \\(n\\)), and randomly re-sample \\(n\\) observations from that sample with replacement (so allowing observations to repeat in our re-sample).\nContinue to do this over and over again to get \\(B\\) number of re-samples.\nFor each re-sample \\(b\\), you should calculate the \\(\\widehat{\\theta_b}\\). Plot all of the sample \\(\\widehat{\\theta_b}\\) in a distribution.\n\nYou can also estimate the standard error of \\(\\hat\\theta\\) using the standard deviation of the distribution. However, do not use these standard errors for confidence intervals or tests unless you are confident the sampling distribution is approximately normal.\nNonparametric Bootstrap is also used in some more complex estimators where it is very difficult to calculate or estimate the standard errors.\n\n\n\n\n\n\nOrdinary Least Squares Estimator\n\nDeriving the Estimator\nOur linear regression model, and the fitted values \\(\\hat{\\mathbf{y}}\\), take the following form:\n\\[\n\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u, \\qquad \\hat{\\mathbf y} = \\mathbf X \\hat{\\boldsymbol\\beta}\n\\]\nOLS wants to minimise the sum of squared residuals \\(S(\\hat{\\boldsymbol\\beta})\\) - the differences between the actual \\(\\mathbf y\\) and our predicted \\(\\hat{\\mathbf y}\\):\n\\[\n\\begin{align}\nS(\\hat{\\boldsymbol\\beta}) & = (\\mathbf y - \\hat{\\mathbf y})^\\mathsf{T} (\\mathbf y - \\hat{\\mathbf y})\\\\\n& = (\\mathbf y - \\color{blue}{\\mathbf X \\hat{\\boldsymbol\\beta}}\\color{black})^\\mathsf{T} (\\mathbf y - \\color{blue}{\\mathbf{X} \\hat{\\boldsymbol\\beta}}\\color{black}) && (\\text{plug in } \\color{blue}{\\hat{\\mathbf y}  = \\mathbf X \\hat{\\boldsymbol\\beta}}\\color{black}) \\\\\n& = \\mathbf y^\\mathsf{T} \\mathbf y - \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf y - \\mathbf y^\\mathsf{T} \\mathbf{X}\\hat{\\boldsymbol\\beta} + \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf{Xb} && (\\text{distribute out)} \\\\\n& = \\mathbf y^\\mathsf{T} \\mathbf y - \\color{blue}{2\\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf y}\\color{black} + \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf{X} \\hat{\\boldsymbol\\beta} &&(\\text{combine } \\color{blue}{- \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf y - \\mathbf y^\\mathsf{T} \\mathbf{X}\\hat{\\boldsymbol\\beta}}\\color{black})\n\\end{align}\n\\]\nNow, let us find the first order condition:\n\\[\n\\frac{\\partial S(\\hat{\\boldsymbol\\beta})}{\\partial \\hat{\\boldsymbol\\beta}} = -2\\mathbf X^\\mathsf{T} \\mathbf y + 2 \\mathbf X^\\mathsf{T} \\mathbf{X} \\hat{\\boldsymbol\\beta} = 0\n\\]\nWhen assuming \\(\\mathbf X^\\mathsf{T} \\mathbf X\\) is invertable (which is true if \\(\\mathbf X\\) is full rank), we can isolate \\(\\hat{\\beta}\\) to find the solution to OLS:\n\\[\n\\begin{align}\n-2\\mathbf X^T\\mathbf y + 2 \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol{\\hat{\\beta}} & = 0 \\\\\n2 \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol{\\hat\\beta} & = 2\\mathbf X^\\mathsf{T} \\mathbf y && (+ 2\\mathbf X^\\mathsf{T} \\mathbf y \\text{ to both sides}) \\\\\n\\boldsymbol{\\hat\\beta} & = (2\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} 2 \\mathbf X^\\mathsf{T} \\mathbf y && (\\times (2\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\text{ to both sides})\\\\\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y &&(\\text{cancel out } 2^{-1}\\times 2)\n\\end{align}\n\\]\nThose are our coefficient solutions to OLS.\n\n\n\nRegression Anatomy Theorem\nTake our multiple linear regression: \\(y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + ... + \\beta_k x_{ki} + u_i\\).\nLet us say we are interested in \\(x_1\\). Let us make \\(x_1\\) the outcome variable of a regression with explanatory variables \\(x_2, ..., x_k\\):\n\\[\nx_{1i} = \\gamma_0 + \\gamma_1 x_{2i} + ... + \\gamma_{k-1}x_{ki} + \\widetilde{r_{1i}}\n\\]\nThe error term \\(\\widetilde{r_{1i}}\\) is the part of \\(x_1\\) that cannot be explained by \\(x_2, ..., x_k\\).\nNow, take the regression of with outcome variable \\(y\\), with all explanatory variables except \\(x_1\\):\n\\[\ny_i = \\delta_0 + \\delta_1 x_{2i} + ... + \\delta_{k-1} x_{ki} + \\widetilde{y_i}\n\\]\nThe error term \\(\\widetilde{y_i}\\) is the part of \\(y_i\\) that cannot be explained by \\(x_2, ..., x_k\\). That implies \\(x_1\\) must be the one explaining \\(\\widetilde{y_i}\\). But, \\(x_1\\) may also correlated with \\(x_2, ..., x_k\\), and those correlated parts are already picked up in the regression coefficients of \\(x_2, ..., x_k\\). Thus, \\(\\widetilde{y_i}\\) must be explained by the part of \\(x_1\\) that is uncorrelated with \\(x_2, ..., x_k\\), which we derived earlier as \\(\\widetilde{r_{1i}}\\).\nThus, we can create another regression with explanatory variable \\(\\widetilde{x_{1i}}\\) and outcome variable \\(\\widetilde{y_i}\\):\n\\[\n\\widetilde{y_i} = \\alpha_0 + \\alpha_1 \\widetilde{r_{1i}} + u_i\n\\]\nWe plug \\(\\widetilde{y_i}\\) back into our regression of \\(y_i\\) with explanatory variables \\(x_2 ..., x_k\\):\n\\[\n\\begin{align}\ny_i & = \\delta_0 + \\delta_1 x_{2i} + ... + \\delta_{k-1} x_{ki} + \\widetilde{y_i} \\\\\ny_i & = \\delta_0 + \\delta_1 x_{2i} + ... + \\delta_{k-1} x_{ki} + \\alpha_0 + \\alpha_1 \\widetilde{r_{1i}} + u_i && (\\text{plug in } \\widetilde{y_i} = \\alpha_0 + \\alpha_1 \\widetilde{r_{1i}} + u_i)\\\\\ny_i  & = \\underbrace{(\\delta_0 + \\alpha_0)}_{\\beta_0} + \\underbrace{\\alpha_1 \\widetilde{r_{1i}}}_{\\beta_1 x_{1i}} + \\underbrace{\\delta_1x_{2i}}_{\\beta_2 x_{2i}} + ... + \\underbrace{\\delta_{k-1} x_{ki}}_{\\beta_kx_{ki}} + \\underbrace{u_i}_{u_i} && (\\text{rearrange})\n\\end{align}\n\\]\nThis new regression mirrors the original multiple linear regression. Importantly, we see the estimate of \\(\\alpha_1\\) will be the same as \\(\\beta_1\\) in the original regression. This coefficient explains the expected change in \\(y\\), given an increase in the part of \\(x_1\\) uncorrelated with \\(x_2, ..., x_k\\).\nSo essentially, we have partialed out the effect of the other explanatory variables, and only focus on the effect on \\(y\\) of the uncorrelated part of \\(x_1\\) (which is \\(\\widetilde{r_{1i}}\\)). This is what controlling for confounders is.\n\n\n\nOLS as an Unbiased Estimator\nOLS is an unbiased estimator of the relationship between any \\(x_j\\) and \\(y\\) under 4 conditions:\n\nLinearity in parameters: the model of the population (data generating process) can be modelled as \\(\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u\\).\nRandom Sampling: the observations in our sample are randomly sampled.\nNo Perfect Multicolinearity: There is no exact linear relationships between the regressors. This ensures that \\(\\mathbf X^\\mathsf{T} \\mathbf X\\) is invertible, which is required for the derivation of OLS.\nZero Conditional Mean: \\(E(\\mathbf u|\\mathbf X) = 0\\). This implies that no \\(x_j\\) is correlated with \\(\\mathbf u\\) (exogeneity), and no function of multiple regressors is correlated with \\(\\mathbf u\\).\n\nLet us prove OLS is unbiased - i.e. \\(E(\\hat{\\boldsymbol\\beta}) = \\boldsymbol\\beta\\). Let us manipulate our OLS solution:\n\\[\n\\begin{align}\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}\\color{blue}{(\\mathbf X \\boldsymbol\\beta + \\mathbf u)} && \\color{black}(\\text{plug in } \\color{blue}{\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u}\\color{black}) \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&(\\text{multiply out})\\\\\n& = \\color{blue}{\\mathbf I}\\color{black}{\\boldsymbol\\beta} + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&( \\ \\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X = \\mathbf I}\\color{black})\\\\\n& = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{identity property of } \\mathbf I)\n\\end{align}\n\\]\nNow, let us take the expectation of \\(\\boldsymbol{\\hat\\beta}\\) conditional on \\(\\mathbf X\\). Remember condition 4, \\(E(\\mathbf u | \\mathbf X) = 0\\):\n\\[\n\\begin{align}\nE(\\boldsymbol{\\hat\\beta}|\\mathbf X) & = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} E(\\mathbf u | \\mathbf X) &&(\\mathbf u \\text{ conditional on value of } \\mathbf X) \\\\\nE(\\boldsymbol{\\hat\\beta}|\\mathbf X) & = \\boldsymbol\\beta &&(E(\\mathbf u | \\mathbf X) = 0)\n\\end{align}\n\\]\nNow, we can use the law of iterated expectations (LIE) to conclude this proof:\n\\[\n\\begin{align}\nE(\\boldsymbol{\\hat\\beta}) & = E(E(\\boldsymbol{\\hat\\beta}|\\mathbf X)) && (\\text{LIE: E(X) = E(E(X|Y))})\\\\\n& = E(\\boldsymbol\\beta) && (\\text{LIE: E(X) = E(E(X|Y))})\\\\\n& = \\boldsymbol\\beta && (\\text{expecation of a constant})\n\\end{align}\n\\]\nThus, OLS is unbiased under the 4 conditions above.\n\n\n\nGauss-Markov Theorem\nThe Gauss-Markov Theorem states that the OLS estimator is the best linear unbiased estimator (BLUE) - the unbiased linear estimator with the lowest variance, under 5 conditions:\n\nLinearity (see unbiasedness conditions)\nRandom Sampling (…)\nNo Perfect Multicollinearity (…)\nZero-Conditional Mean (…)\nHomoscedasticity (the new condition).\n\nHomoscedasticity is when no matter the values of any explanatory variable, the error term variance is constant at \\(\\sigma^2\\). The error term variance does not change based on the values of the explanatory variables:\n\\[\nVar(\\mathbf u | \\mathbf X) = \\sigma^2 \\mathbf I_n = \\begin{pmatrix}\n\\sigma^2 & 0 & \\dots & 0 \\\\\n0 & \\sigma^2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & 0 \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\nVisualisation of Homoscedasticity\n\n\n\n\n\nAn easy way to identify homoscedasticity is to look at a residual plot (just the plot of all \\(\\widehat{u_i}\\)):\n\n\n\n\n\nNotice how the homoscedasticity residuals seem to have the same up-down variance, no matter the value of \\(x\\).\nThe heteroscedasticity residuals have a clear pattern - the up-down variance is smaller when \\(x\\) is smaller, and the up-down variance is larger when \\(x\\) is larger.\nEssentially, if you see a pattern in the residual plot, it is likely heteroscedasticity.\n\n\n\nThe Gauss-Markov Theorem is one of the main reasons we focus so heavily on the OLS estimator. If we believe our data-generating structure to be linear, then OLS is the best unbiased estimator we can use, since it has the lowest variance.\n\n\n\nDeriving Variance\nLet us assume homoscedasticity. We want to find the variance of our estimator, \\(Var(\\boldsymbol{\\hat\\beta} | \\mathbf X)\\). Let us start off with our OLS solution. We can simplify as follows:\n\\[\n\\begin{align}\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}\\color{blue}{(\\mathbf X \\boldsymbol\\beta + \\mathbf u)} && \\color{black}(\\text{plug in } \\color{blue}{\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u}\\color{black}) \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&(\\text{multiply out})\\\\\n& = \\color{blue}{\\mathbf I}\\color{black}{\\boldsymbol\\beta} + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&( \\ \\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X = \\mathbf I}\\color{black})\\\\\n& = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{identity property of } \\mathbf I)\n\\end{align}\n\\]\n\\[\nVar(\\boldsymbol{\\hat\\beta} | \\mathbf X) = Var(\\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u \\ | \\ \\mathbf X)\n\\]\n\\(\\boldsymbol\\beta\\) is a vector of fixed constants. \\((\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u\\) can be imagined as a matrix of fixed constants, since we are conditioning the above variance on \\(\\mathbf X\\) (so for each \\(\\mathbf X\\), the statement is fixed).\n\n\n\n\n\n\nMathematical Lemma\n\n\n\n\n\nIf \\(\\mathbf u\\) is an \\(n\\) dimensional vector of random variables, \\(\\mathbf c\\) is an \\(m\\) dimensional vector, and \\(\\mathbf B\\) is an \\(n \\times m\\) dimensional matrix with fixed constants, then the following is true:\n\\[\nVar(\\mathbf c + \\mathbf{Bu}) = \\mathbf B Var(\\mathbf u)\\mathbf B^\\mathsf{T}\n\\]\nI will not prove this lemma here, but it is provable.\n\n\n\nWith the Lemma above, and with the definition of homoscedasticity, we can simplify:\n\\[\n\\begin{align}\nVar(\\boldsymbol{\\hat\\beta} | \\mathbf X) & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} Var(\\mathbf u | \\mathbf X) [(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}]^{-1} && (\\text{lemma})\\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} Var(\\mathbf u | \\mathbf X) \\color{blue}{\\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1}} && \\color{black}( \\ \\color{blue}{[(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}]^{-1} = \\mathbf X(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1}}\\color{black})\\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\color{blue}{\\sigma^2 \\mathbf I_n}\\color{black}{ \\mathbf X} (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\color{blue}{Var(\\mathbf u | \\mathbf X) = \\sigma^2 \\mathbf I_n}\\color{black}) \\\\\n& =  \\color{red}{\\sigma^2} \\color{black} (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf I_n \\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\text{move scalar } \\color{red}{\\sigma^2}\\color{black})\\\\\n& =  \\sigma^2 (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}  \\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\text{identity property of } \\mathbf I_n)\\\\\n& =  \\sigma^2 (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\text{inverses } \\mathbf X^\\mathsf{T}  \\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\text{ cancel})\n\\end{align}\n\\]\nHowever, we do not actually know what \\(\\sigma^2\\) is. We can estimate it with \\(\\hat\\sigma^2\\) (discussed here).\nWe can use these standard errors (square root of variance) for hypothesis testing, if we believe homoscedasticity is met. If not, we will need to use robust standard errors, which we will not derive here. In modern econometrics, it has become more common to use robust standard errors by default, unless we can definitively prove homoscedasticity is met.\n\n\n\nAsymptotic Consistency of OLS\nOLS is an asymptotically consistent estimator of the relationship between any \\(x_j\\) and \\(y\\) under 4 conditions. These conditions are identical to the unbiasedness conditions EXCEPT condition 4, which is weakened from the original unbiasedness condition.\n\nLinearity (see unbiasedness)\nRandom Sampling (…)\nNo Perfect Multicolinearity (…)\nZero Mean and Exogeneity: \\(E(u_i) = 0\\), and \\(Cov(x_i, u_i) = 0\\), which implies \\(E(\\mathbf x_i u_i) = 0\\). This means that no regressor should be correlated with \\(\\mathbf u\\). This is weaker than Zero-Conditional mean, since it means a function of regressors can be correlated with \\(\\mathbf u\\).\n\nWe need condition 3 to ensure \\(\\mathbf X^\\mathsf{T} \\mathbf X\\) is invertible, in order to have OLS estimates. Once we have OLS estimates (derivation above), we can manipulate it as following:\n\\[\n\\begin{align}\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}\\color{blue}{(\\mathbf X \\boldsymbol\\beta + \\mathbf u)} && \\color{black}(\\text{plug in } \\color{blue}{\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u}\\color{black}) \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&(\\text{multiply out})\\\\\n& = \\color{blue}{\\mathbf I}\\color{black}{\\boldsymbol\\beta} + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&( \\ \\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X = \\mathbf I}\\color{black})\\\\\n& = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{identity property of } \\mathbf I)\n\\end{align}\n\\]\n\n\n\n\n\n\nVector Notation\n\n\n\n\n\nThe following statements are true:\n\\[\n\\begin{split}\n& \\mathbf X^\\mathsf{T} \\mathbf X = \\sum\\limits_{i=1}^n \\mathbf x_i \\mathbf x_i^\\mathsf{T} \\\\\n& \\mathbf X^\\mathsf{T} \\mathbf  u = \\sum\\limits_{i=1}^n \\mathbf x_i u_i\n\\end{split}\n\\]\n\n\n\nUsing vector notation, law of large numbers, and zero-mean and exogeneity condition, we can simplify the above to:\n\\[\n\\begin{align}\n\\boldsymbol{\\hat\\beta} & = \\boldsymbol\\beta + \\left( \\sum\\limits_{i=1}^n \\mathbf x_i \\mathbf x_i^\\mathsf{T} \\right)^{-1} \\left( \\sum\\limits_{i=1}^n \\mathbf x_i \\mathbf u \\right) && (\\text{vector notation})\\\\\n\\boldsymbol{\\hat\\beta} & = \\boldsymbol\\beta + \\left( \\frac{1}{n}\\sum\\limits_{i=1}^n \\mathbf x_i \\mathbf x_i^\\mathsf{T} \\right)^{-1} \\left( \\frac{1}{n}\\sum\\limits_{i=1}^n \\mathbf x_i \\mathbf u \\right) && ( \\ \\left(\\frac{1}{n} \\right)^{-1} \\text{and } \\frac{1}{n} \\text{ cancel out}) \\\\\n\\text{plim} \\boldsymbol{\\hat\\beta} & = \\boldsymbol\\beta + \\left( \\text{plim} \\frac{1}{n}\\sum\\limits_{i=1}^n \\mathbf x_i \\mathbf x_i^\\mathsf{T} \\right)^{-1} \\left( \\text{plim} \\frac{1}{n}\\sum\\limits_{i=1}^n \\mathbf x_i u_i \\right) && (\\text{apply plim}) \\\\\n\\text{plim} \\boldsymbol{\\hat\\beta} & = \\boldsymbol\\beta + (E(\\mathbf x_i \\mathbf x_i^\\mathsf{T}))^{-1}E(\\mathbf x_i  u_i) && (\\text{law of large numbers})\\\\\n\\text{plim} \\boldsymbol{\\hat\\beta} & = \\boldsymbol\\beta && (E(\\mathbf x_i u_i) = 0)\n\\end{align}\n\\]\nThus, OLS is asymptotically consistent under the 4 conditions above.\n\n\n\nOLS as a Conditional Expectation Function\n\n\n\n\n\n\nConditional Expectation Functions\n\n\n\n\n\nA conditional expectation function (CEF) says that the value of \\(E(y)\\) depends on the value of \\(x\\). We notate a conditional expectation function as \\(E(y|x)\\). As we noted earlier, the linear regression model can be a conditional expectation function of \\(E(y|x)\\).\nA best linear approximation of a conditional expectation function can take the following form:\n\\[\nE(y_i|x_i) = b_0 + b_1x_i\n\\]\nWith parameters \\(b_0, b_1\\) that minimise the mean squared errors (MSE).\n\\[\n\\begin{split}\nMSE & = E(y_i - E(y_i|x_i))^2 \\\\\n& = \\frac{1}{n}\\sum\\limits_{i=1}^n( y_i - E(y_i|x_i))^2\n\\end{split}\n\\]\n\n\n\nOLS is a best-linear approximation of the conditional expectation function. Suppose we have the conditional expectation function, and its mean squared errors:\n\\[\n\\begin{align}\nE(y_i|x_i) & = b_0 + b_1x_i \\\\\nMSE & = E(y_i - E(y_i|x_i))^2 \\\\\n& =  E(y_i - \\beta_0 - \\beta_1x_i)^2\n\\end{align}\n\\]\nThe first order conditions are (using chain rule and partial derivatives):\n\\[\n\\begin{split}\n& E(y_i - b_0 - b_1x_i) = 0 \\\\\n& E(x_i(y_i - b_0 - b_1x_i) = 0\n\\end{split}\n\\]\nNow, recall our OLS minimisation conditions (simple linear regression):\n\\[\n\\begin{split}\n& \\sum\\limits_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\\n& \\sum\\limits_{i=1}^n x_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0\n\\end{split}\n\\]\nSince by definition, average/expectation is \\(E(x) = \\frac{1}{n} \\sum x_i\\), we can rewrite as:\n\\[\n\\begin{split}\n& n \\times E(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i) = 0 \\\\\n& n \\times E(x_i(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i)) = 0\n\\end{split}\n\\]\nAnd since anything multiplied to a zero turns into zero, we can ignore the \\(n\\) in the first order condition. Thus, our conditions are:\n\\[\n\\begin{split}\n& E(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i) = 0 \\\\\n& E(x_i(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i)) = 0\n\\end{split}\n\\]\nWhich as we can see, are the exact same minimisation conditions as the conditional expectation function. Thus, OLS is the best approximation of the conditional expectation function.\nThis property is very useful for causal inference, as it means OLS calculates the expected \\(y\\), which allows us to find causal effects by comparing the expected \\(y\\) of the treatment and control groups (assuming the OLS estimator is unbiased).\n\n\n\n\n\n\nMethod of Moments Estimator\n\nMethod of Moments\nThe Method of Moments Estimator is another estimator of the true value of populations in the parameter. The estimator defines key population moments of interest - which are the population parameters written in terms of expected value functions set equal to 0.\nThen, the Method of Moments uses the sample equivalents of the population moments to estimate the population parameter. For example, to estimate the population mean, the Method of Moments uses the sample mean.\nIn order to define a method of moments for a set of parameters \\(\\theta_1, \\dots, \\theta_k\\), we need to specify at least one population moment per parameter. Or in other words, we must have more than \\(k\\) population moments.\nOur population moments can be defined as the expected value of some function \\(m(\\theta; y)\\) that consists of both the variable \\(y\\) and our unknown parameter \\(\\theta\\). The expectation of the function \\(m(\\theta; y)\\) should equal 0.\n\\[\nE(m(\\theta; y)) = 0\n\\]\nOur sample moments will be the sample analogues of \\(\\theta\\) and \\(y\\), which are \\(\\hat\\theta\\) and \\(y_i\\):\n\\[\n\\frac{1}{n}\\sum\\limits_{i=1}^n m(\\hat\\theta; y_i) = 0\n\\]\nMethod of moments estimators are asymptotically consistent, because of the law of large numbers.\n\n\n\nPopulation Mean Estimator\nLet us say that we have some random variable \\(y\\), with a true population mean \\(\\mu\\). We want to estimate \\(\\mu\\), but we only have a sample of the population.\nHow can we define \\(\\mu\\) in a moment of the form: \\(E(m(\\mu, y)) = 0\\)? Well, we know \\(\\mu\\) is the expectation of \\(y\\), so \\(\\mu = E(y)\\). Since they are equal, \\(\\mu - E(y) = 0\\). Thus, we can define the mean as a moment of the following condition:\n\\[\nE(y - \\mu) = 0\n\\]\nThe method of moments estimator uses the sample equivalent of the population moment. The sample equivalent of \\(\\mu\\), is the sample mean \\(\\bar y\\):\n\\[\nE(y_i - \\hat\\mu) = \\frac{1}{n}\\sum\\limits_{i=1}^n (y_i - \\hat\\mu) = 0\n\\]\nWith this equation, we can then solve for \\(\\hat\\mu\\):\n\\[\n\\begin{align}\n0 & = \\frac{1}{n}\\sum\\limits_{i=1}^n (y_i - \\hat\\mu) \\\\\n0 & = \\frac{1}{n}\\sum\\limits_{i=1}^ny_i - \\frac{1}{n}\\sum\\limits_{i=1}^n \\hat\\mu  && (\\text{multiply out})\\\\\n0 & = \\frac{1}{n}\\sum\\limits_{i=1}^ny_i - \\frac{1}{n} n \\hat\\mu &&(\\text{summation property of constant } \\hat\\mu)\\\\\n0 & = \\bar y - \\hat \\mu && (\\text{definition of mean }\\frac{1}{n}\\sum\\limits_{i=1}^ny_i = \\bar y)\\\\\n\\hat\\mu & = \\bar y && (+\\hat\\mu\\text{ to both sides})\n\\end{align}\n\\]\nSo, we see the method of moments estimates our true population mean \\(\\mu\\), with the sample mean \\(\\bar y\\). As a method of moments estimator, it is also asymptotically consistent.\n\n\n\nOLS as a Method of Moments Estimator\nOLS is a special case of the Method of Moments Estimator. Consider the bivariate regression model. The OLS estimator can be derived as a method of moments estimator, with 2 moments (expectation functions set equal to 0), one for each parameter (\\(\\beta_0, \\beta_1\\)):\n\\[\n\\begin{split}\n& E(y-\\beta_0 -\\beta_1x) = 0 \\\\\n& E(x(y - \\beta_0 - \\beta_1 x)) = 0\n\\end{split}\n\\]\nThe estimates of these moments would use the sample equivalents: \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\).\n\\[\n\\begin{split}\n& E(y-\\hat\\beta_0 -\\hat\\beta_1x) = 0 \\\\\n& E(x(y - \\hat\\beta_0 - \\hat\\beta_1 x)) = 0\n\\end{split}\n\\]\nRemember our OLS minimisation conditions:\n\\[\n\\begin{split}\n& \\sum\\limits_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\\n& \\sum\\limits_{i=1}^n x_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0\n\\end{split}\n\\]\nSince by definition, average/expectation is \\(E(x) = \\frac{1}{n} \\sum x_i\\), we can rewrite the OLS minimisation conditions as:\n\\[\n\\begin{split}\n& n \\times E(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i) = 0 \\\\\n& n \\times E(x_i(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i)) = 0\n\\end{split}\n\\]\nAnd since anything multiplied to a zero turns into zero, we can ignore the \\(n\\) in the first order condition, and only focus on the expected value part. Thus, our conditions are:\n\\[\n\\begin{split}\n& E(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i) = 0 \\\\\n& E(x_i(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i)) = 0\n\\end{split}\n\\]\nWhich as we can see, are the exact same minimisation conditions as the method of moments estimator. Thus, the OLS estimator is a special case of the Method of Moments estimator, and they produce the same coefficients. This is an important property for the instrumental variables method that will be covered later.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "3 Classic Least Squares Theory"
    ]
  },
  {
    "objectID": "4.html",
    "href": "4.html",
    "title": "Causal Frameworks",
    "section": "",
    "text": "In the past few chapters, we have focused on correlations.\nThis chapter introduces the main causal frameworks (potential outcomes, causal graphs), the main causal estimands used in causal inference, and the idea of selection bias and confounders. This chapter is the foundation in which the next set of methods for causal inference will be built on.\nUse the right sidebar for quick navigation. R-code for causal diagrams is provided at the bottom.\n\n\nPotential Outcomes Framework\n\nTreatment and Potential Outcomes\nIn causal inference, we are interested in how treatment \\(D\\) causes outcome variable \\(Y\\).\n\\(D\\) is our treatment variable. The indicator of treatment for each unit \\(i\\) is \\(D_i\\).\n\\[\nD_i = \\begin{cases}\n1 \\quad \\text{if unit } i \\text{ recieved the treatment} \\\\\n0 \\quad \\text{if unit } i \\text{ did not recieve the treatment}\n\\end{cases}\n\\]\n\n\n\n\n\n\nFurther information on Treatment Variables\n\n\n\n\n\nCausal variables/treatments must occur before the outcome. A variable cannot cause something to occur in the past.\nCausal variables/treatments must be able to be manipulated (in order to imagine a world where the treatment did not occur).\n\nFor example, \\(D\\) cannot be sex assigned at birth, ethnicity, etc.\nFor example, major global events (how did 9/11 cause the Arab spring?)\n\n\n\n\nImagine there are two hypothetical parallel worlds - one where unit \\(i\\) receives the treatment \\(D\\), and one where unit \\(i\\) does not receive the treatment \\(D\\). Everything else in these worlds is identical.\nPotential Outcomes for unit \\(i\\) are denoted:\n\\[\nY_{di}, Y_i(d) =\\begin{cases}\n& Y_{1i}, \\ Y_i(1) \\quad \\text{Outcome for unit } i \\text{ when } D_i = 1 \\\\\n& Y_{0i}, \\ Y_i(0) \\quad \\text{Outcome for unit } i \\text{ when } D_i = 0 \\\\\n\\end{cases}\n\\]\n\n\n\n\n\n\nExample of Potential Outcomes\n\n\n\n\n\nFor example, imagine we are interested in finding the effect of democracy \\(D\\) on GDP growth \\(Y\\). Potential outcome \\(Y_{1i}\\) is the potential GDP growth of country \\(i\\) if they were a democracy, and outcome \\(Y_{0i}\\) is the potential GDP growth of a country \\(i\\) if they were not a democracy.\n\n\n\n\n\n\nObserved Outcomes and “Missing Data”\nOf course, there is not two parallel worlds with 2 potential outcomes. In the real world, each unit \\(i\\) either receives treatment \\(D\\), or does not. We do not observe the other potential outcome.\n\\(Y_i\\) is the observed outcome for unit \\(i\\). This is given by formula:\n\\[\nY_i = D_i \\cdot Y_{1i} + (1-D_i) \\cdot Y_{0i}\n\\]\nIf we plug in \\(D_i = 0, 1\\) to the equation above, we get the observed outcomes:\n\\[\nY_i = \\begin{cases}\nY_{1i} \\quad \\text{if } D_i = 1 \\\\\nY_{0i} \\quad \\text{if } D_i = 0 \\\\\n\\end{cases}\n\\]\nBefore the treatment (A priori), both potential outcomes could be observed. After the treatment, one is observed, and the other is counterfactual. For any given experiment, only one will ever be seen, and the counterfactual will never be seen (missing data problem).\n\n\n\n\n\n\nNeyman Urn Model\n\n\n\n\n\nPotential Outcomes can be visualised with the Neyman Urn Model.\nBefore the treatment, we have a box (we cannot see) with both potential outcomes.\n\n\n\n\n\nThen, when we apply treatment, we stick our hand into the box that we cannot see, and pull out one observed outcome.\n\n\n\n\n\nWe are essentially sampling from potential outcomes to get observed outcomes.\n\n\n\nThis missing data problem is called the fundamental problem of causal inference.\n\n\n\nStable Unit Treatment Value Assumption\nThe above given observed and potential outcome frameworks depends on the Stable Unit Treatment Value Assumption (SUTVA).\n\\[\n\\begin{align}\nY_{(D_1, D_2, \\dots, D_N)i} & = Y_{(D_1', D_2', \\dots, D_N')i} \\\\\nY_{di} \\text{ under current randomisation} & = Y_{di} \\text{ under all other randomisations}\n\\end{align}\n\\]\nOr more intuitively, the potential outcomes of unit \\(i\\) only depends on their own treatment status, and no other unit’s treatment status. Thus, changing everyone else’s treatment status has no effects on unit \\(i\\)’s potential outcomes \\(Y_{di}\\). The treatment is also the same for everyone (treatment is stable and consistent)\n\n\n\n\n\n\nExamples of SUTVA Violations\n\n\n\n\n\n\nSpill-over effects: If we are testing a new curriculum, one student \\(j\\) getting the new curriculum may teach their friend \\(i\\) the new curriculum, thus affecting the potential outcomes of \\(i\\).\nContagion: If we are studying a disease, diseases can spread, so another unit \\(j\\) getting a disease affects the potential outcomes of unit \\(i\\).\nDilution: If we are studying vaccines - there is herd immunity - other people getting the vaccine also reduces our chances of getting the disease.\nVariable levels of treatment: If we are doing a drug trial, if some people got two doses, while others only got one dose. This is not a consistent treatment.\nTechnical errors: If someone who is supposed to be treated accidentally is not treated. This is not a consistent treatment.\n\n\n\n\nWhen SUTVA is violated, potential outcomes become very messy, and we no longer have the neat framework as before.\n\n\n\n\n\n\nCausal Estimands\n\nIndividual Treatment Effect\nRemember the potential outcomes from parallel worlds \\(Y_{1i}\\) and \\(Y_{0i}\\).\nSince these two parallel worlds are identical except for the fact one receives the treatment \\(D\\) and the other does not, the causal effect of \\(D\\) should be the difference between the potential outcomes of these two worlds. Thus, the individual treatment effect of a unit \\(i\\) is:\n\\[\n\\tau_i = Y_{1i} - Y_{0i}\n\\]\nThis is the specific treatment effect for a specific unit \\(i\\). This cannot be observed, because we do not see both potential outcomes for the same unit \\(i\\).\nThis is also very hard to estimate, as we cannot reliably fill in the missing potential outcome for any one unit \\(i\\). Thus, we almost never use individual treatment effects, and use group treatment effects.\n\n\n\nAverage Treatment Effect (ATE)\nATE is a group-level causal estimand.\n\n\n\n\n\n\nGroup-Level Causal Estimands\n\n\n\n\n\nConsider a population of units \\(i = 1, \\dots, N\\).\nThe population has potential outcomes represented in two (only partially observed) vectors:\n\\[\n\\begin{split}\n& Y_1 = (Y_{11}, Y_{12}, \\dots, Y_{1N}) \\\\\n& Y_0 = (Y_{01}, Y_{02}, \\dots, Y_{0N})\n\\end{split}\n\\]\nWe compare these two vectors of potential outcomes. The most common way to do this is to use their expected values.\n\n\n\nThe Average Treatment Effect is defined as:\n\\[\n\\begin{split}\n\\tau_{ATE} & = E(Y_{1i} - Y_{0i}) \\\\\n& = \\underbrace{\\frac{1}{N} \\sum\\limits_{i=1}^N (Y_{1i} - Y_{0i})}_{\\text{a formula for average}}\n\\end{split}\n\\]\nWe cannot calculate this with observed data - since we need all potential outcomes to do this. We can estimate this (covered throughout this course).\n\n\n\nAverage Treatment Effect on the Treated (ATT)\nAn alternative estimand to the ATE is the Average Treatment Effect on the Treated (ATT):\n\\[\n\\begin{split}\n\\tau_{ATT} & = E(Y_{1i} - Y_{0i} \\ | \\ D_i = 1) \\\\\n& = \\underbrace{\\frac{1}{N_1} \\sum\\limits_{i=1}^N D_i (Y_{1i} - Y_{0i}) \\quad  \\text{where } N_1 = \\sum\\limits_{i=1}^ND_i}_{\\text{a formula for the average only for treated units}}\n\\end{split}\n\\]\nThis is the causal effect of only units who have received the treatment. Note that frequently the ATT is not equal to the ATE, so be aware of which estimand you are trying to estimate/identify.\n\n\n\n\n\n\nATT vs. ATE\n\n\n\n\n\nWhen does \\(\\tau_{ATT} = \\tau_{ATE}\\)?\n\nWhen the expectation of the potential outcomes of both the treated and control are the same, then the two equal each other.\n\nThe opposite is also true: if the expectation of the potential outcomes of both the treated and control are different, then the two are not equal.\n\n\n\nThe opposite estimand is the Average Treatment effect on the Untreated (ATU), which only measures the causal effect of units who did not recieve the treatment.\nThis is not used very often, since it is kind of uninituive to think about treatment effects on individuals who did not recieve treatment. However, it can be useful in understanding identification assumptions.\n\n\n\nConditional Average Treatment Effect (CATE)\nThe conditional average treatment effect is any treatment effect where there is a condition on a characteristic/covariate:\n\\[\n\\tau_{CATE}(x) = E(Y_{1i} - Y_{0i} \\ | \\ \\underbrace{X_i = x)}_{\\text{condition}}\n\\]\nThis is the causal effect of only variables who meet the condition of the covariate specified. For example, you could find the conditional average treatment effect of only women (so the covariate which we are conditioning on is gender). You can also condition on multiple covariates.\nThis is often used for tailoring products/medicine/advertising to certain groups of people. It is also frequently used in identification strategies.\nThis estimand will go by other names, including the Local Average Treatment Effect (LATE).\n\n\n\n\n\n\nSelection Bias and Confounders\n\nNaive Estimator and Selection Bias\nA natural way to estimate the ATE is to use a naive estimator: find the average difference of observed outcomes. This is called the naive estimator:\n\\[\n\\hat\\tau_{naive} = \\underbrace{E(Y_i|D_i = 1)}_{\\text{for treated}} - \\underbrace{E(Y_i|D_i = 0)}_{\\text{for control}}\n\\]\nHowever, there is an issue - we can show this with algebra:\n\\[\n\\begin{align}\n\\hat\\tau_{naive} & = E(Y_i|D_i = 1) - E(Y_i|D_i = 0) \\\\\n& = \\underbrace{E(Y_{1i}|D_i = 1) - E(Y_{0i} | D_i = 0)}_{\\because \\text{ observed potential outcomes}} \\\\\n& = E(Y_{1i}|D_i = 1) - E(Y_{0i} | D_i = 0) + \\underbrace{E(Y_{0i}|D_i = 1) \\color{red}{- E(Y_{0i}|D_i = 1)}}_{\\because \\text{ this equals 0, so we can add it}} \\\\\n& = \\underbrace{E(Y_{1i}|D_i = 1) \\color{red}{- E(Y_{0i}|D_i = 1)}}_{\\tau_{ATT}} + \\underbrace{E(Y_{0i}|D_i = 1) - E(Y_{0i} | D_i = 0)}_{\\text{Selection Bias}} \\\\\n\\end{align}\n\\]\nWe can see that our naive estimator produces the \\(\\tau_{ATT}\\) plus an extra bit (called the selection bias). Thus, our naive estimator is biased, so we should be careful about using this naive estimator (correlation does not equal causation).\n\n\n\n\n\n\nNaive Estimator Biased for ATU\n\n\n\n\n\nThe proof above shows how the naive estimator is a biased estimator for the \\(\\tau_{ATT}\\). We can also prove it is a biased estimator of the ATU:\n\\[\n\\begin{split}\n\\hat\\tau_{naive} & = E(Y_i|D_i = 1) - E(Y_i|D_i = 0) \\\\\n& = \\underbrace{E(Y_{1i}|D_i = 1) - E(Y_{0i} | D_i = 0)}_{\\because \\text{ observed potential outcomes}} \\\\\n& = E(Y_{1i}|D_i = 1) - E(Y_{0i} | D_i = 0) + \\underbrace{E(Y_{1i}|D_i = 0) - E(Y_{1i}|D_i = 0)}_{\\because \\text{ this equals 0, so we can add it}} \\\\\n& = \\underbrace{E(Y_{1i}|D_i = 0)- E(Y_{0i}|D_i = 0)}_{\\tau_{ATU}} + \\underbrace{E(Y_{1i}|D_i = 1) - E(Y_{1i} | D_i = 0)}_{\\text{Selection Bias}} \\\\\n\\end{split}\n\\]\n\n\n\n\n\n\n\n\n\nNaive Estimator Biased for ATE\n\n\n\n\n\nThe proofs above shows how the naive estimator is a biased estimator for the \\(\\tau_{ATT}\\) and \\(\\tau_{ATU}\\). We can also prove it is a biased estimator of the ATE.\nLet us first start with the ATE. Let us call \\(Y_{1i} - Y_{0i} := \\tau_i\\) for notation simplicity:\n\\[\n\\begin{align}\n\\tau_{ATE} & = E(Y_{1i} - Y_{0i})  = E(\\tau_i)\\\\\n& = \\underbrace{E(\\tau_i|D_i = 1)Pr(D_i = 1) + E(\\tau_i|D_i = 0)Pr(D_i = 0)}_{\\because \\text{ weighted average of ATE and ATU by proportion}} \\\\\n& = E(\\tau_i|D_i = 1) \\underbrace{(1 -Pr(D_i = 0))}_{\\because \\text{ complement prob.}} + E(\\tau_i|D_i = 0)Pr(D_i = 0) \\\\\n\\end{align}\n\\]\nLet us call \\(Pr(D_i = 0) := \\pi\\) for notation simplicity. Now, continue:\n\\[\n\\begin{split}\n& = \\underbrace{E(\\Delta|D_i = 1) - \\pi E(\\Delta|D_i = 1)}_{\\because \\text{ distribute out}} + E(\\tau_i|D_i = 0)\\pi \\\\\n& = E(\\tau_i|D_i = 1) + \\underbrace{\\pi[E(\\tau_i|D_i = 0) - E(\\tau_i|D_i = 1)]}_{\\because \\ \\pi \\text{ factored out }} \\\\\n& = E(Y_{1i} |D_i = 1) - E(Y_{0i}|D_i = 1) + \\pi[E(\\tau_i|D_i = 0) - E(\\tau_i|D_i = 1)] \\\\\n\\end{split}\n\\]\nLet us call the part \\(\\pi[E(\\tau_i|D_i = 0) - E(\\tau_i|D_i = 1)] := \\Pi(\\tau_i)\\). Now, continue to simplify:\n\\[\n\\begin{split}\n& = E(Y_{1i} |D_i = 1) - E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) + \\underbrace{E(Y_{1i} |D_i = 0) - E(Y_{0i}|D_i = 0)}_{\\because \\text{ these two cancel out so we add 0}}  \\\\\n& = E(Y_{1i} |D_i = 1) - E(Y_{0i}|D_i = 0) + E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& = \\underbrace{E(Y_i|D_i = 1)}_{\\because \\text{ observed outcome}} - \\underbrace{E(Y_i|D_i = 0)}_{\\because \\text{ observed outcome}} + E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& = \\underbrace{E(Y_{i} |D_i = 1) - E(Y_{i}|D_i = 0)}_{\\hat\\tau_{naive}} + E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& = \\hat\\tau_{naive}+ E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i)\n\\end{split}\n\\]\nThus, we can see that \\(\\tau_{ATE}\\) is not equivalent to \\(\\hat\\tau_{naive}\\). Let us isolate \\(\\hat\\tau_{naive}\\) to identify the selection bias.\n\\[\n\\begin{split}\n& \\tau_{ATE} = \\hat\\tau_{naive}+ E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& -\\hat\\tau_{naive} = -\\tau{ATE} + E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& \\hat\\tau_{naive} = \\tau_{ATE} - E(Y_{1i} |D_i = 0) + E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& \\hat\\tau_{naive} = \\tau_{ATE} + \\underbrace{E(Y_{0i}|D_i = 1)- E(Y_{1i} |D_i = 0) + \\Pi(\\tau_i)}_{\\text{selection bias}}\n\\end{split}\n\\]\n\n\n\n\n\n\nConfounders\nTake the selection bias formula from above:\n\\[\n\\underbrace{E(Y_{0i}|D_i = 1)}_{Y_{0i}\\text{ (treated)}} - \\underbrace{E(Y_{0i} | D_i = 0)}_{Y_{0i}\\text{ (control)}}\n\\]\nIf selection bias is non-zero, this essentially states that the expected potential outcome before treatment \\(Y_{0i}\\) between the treatment and control groups is not equal.\nOr in other words, the treatment and control groups have some other variable causing differences even before treatment has begun. This implies that the differences between the treatment and control group may not be due to treatment \\(D\\), but due to the underlying differences before treatment even occurred.\nConfounders are variables that cause the differences between treatment and control groups before the treatment has started. Confounders correlate with both the treatment variable and the outcome. If a variable only correlates with \\(D\\) or \\(Y\\), then it is not a confounder. If must correlate with both \\(D\\) and \\(Y\\).\nThis is why correlation does not equal causation - if the treatment and control group are different before we start the experiment, we cannot say the difference between the two is purely a result of treatment \\(D\\).\n\n\n\nOmitted Variable Bias in Regression\nWe can demonstrate how confounders cause bias in regression. Suppose there is some confounding variable \\(z\\) that we have not included in a “short” regression. The actual, “true” regression of the population, would include this confounder \\(z\\)\n\\[\n\\underbrace{\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u}_{\\text{short regression}}\n\\qquad \\underbrace{\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf z\\boldsymbol\\delta + \\mathbf u}_{\\text{true regression with z} }\n\\]\nThe OLS estimate of the “short regression” excluding confounder \\(z\\) is:\n\\[\n\\boldsymbol{\\hat\\beta} = (\\mathbf X^T \\mathbf X)^{-1} \\mathbf X^T \\mathbf y\n\\]\nLet us now plug in the “true” model into where \\(\\mathbf y\\) is:\n\\[\n\\begin{align}\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} (\\color{blue}{\\mathbf X \\boldsymbol\\beta + \\mathbf z\\boldsymbol\\delta + \\mathbf u}\\color{black}) && (\\text{plug in } \\color{blue}{\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf z\\boldsymbol\\delta + \\mathbf u}\\color{black} )\\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf z\\boldsymbol\\delta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{multiply out})\\\\\n& = \\color{blue}{\\mathbf I}\\color{black}{\\boldsymbol\\beta} + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf z\\boldsymbol\\delta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{inverses } (\\color{blue}{\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X = \\mathbf I}\\color{black})\\\\\n& = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf z\\boldsymbol\\delta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{identity property of } \\mathbf I)\n\\end{align}\n\\]\nNow, let us find the expected value of \\(\\boldsymbol{\\hat\\beta}\\), which is conditional on \\(\\mathbf X, \\mathbf z\\), and simplify (using zero conditional mean):\n\\[\n\\begin{align}\nE(\\boldsymbol{\\hat\\beta}|\\mathbf X, \\mathbf z) & = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf z\\boldsymbol\\delta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} E(\\mathbf u | \\mathbf X, \\mathbf z) \\\\\n& = \\boldsymbol\\beta + (\\mathbf X^T \\mathbf X)^{-1} \\mathbf X^T \\mathbf z\\boldsymbol\\delta &&(\\because E(\\mathbf u | \\mathbf X, \\mathbf z) = 0)\n\\end{align}\n\\]\nNow, what if we had a regression of outcome variable being the confounder \\(z\\), on the explanatory variables \\(\\mathbf X\\), such that \\(\\mathbf z = \\mathbf X \\boldsymbol\\pi + \\mathbf v\\). Our OLS estimate would have the solution:\n\\[\n\\boldsymbol{\\hat\\pi} = (\\mathbf X^T\\mathbf X)^{-1}\\mathbf X^T \\mathbf z\n\\]\nNow, we can plug \\(\\boldsymbol{\\hat\\pi}\\) into our expected value of \\(\\boldsymbol{\\hat\\beta}\\):\n\\[\nE(\\boldsymbol{\\hat\\beta}|\\mathbf X, \\mathbf z)  = \\boldsymbol\\beta + \\boldsymbol{\\hat\\pi \\delta}\n\\]\nNow, using the law of iterated expectations, we get (assuming we are using an unbiased estimator for \\(\\boldsymbol{\\hat\\pi}\\) such that \\(E(\\boldsymbol{\\hat\\pi}) = \\boldsymbol\\pi\\)):\n\\[\n\\begin{align}\nE(\\boldsymbol{\\hat\\beta}) & = E(E(\\boldsymbol{\\hat\\beta}|\\mathbf X, \\mathbf z)) && (\\text{LIE: } E(X) = E(E(X|Y))\\\\\n& = E(\\boldsymbol\\beta + \\boldsymbol{\\hat\\pi \\delta})  && (\\text{LIE: } E(X) = E(E(X|Y)) \\\\\n& = \\boldsymbol\\beta + E(\\boldsymbol{\\hat\\pi}) \\boldsymbol \\delta && (\\boldsymbol\\beta, \\boldsymbol\\delta\\text{ are constants})\\\\\n& = \\boldsymbol\\beta + \\boldsymbol{\\pi \\delta} && (\\text{unbiased estimator } E(\\boldsymbol{\\hat\\pi}) = \\boldsymbol\\pi)\n\\end{align}\n\\]\nThus, we can see by not including confounder \\(z\\) in our “short regression”, the estimator is now biased by \\(\\boldsymbol{\\hat\\pi \\delta}\\).\n\n\n\nAssignment Mechanism\nThe Assignment Mechanism is the procedure that determines the treatment status of each unit. In causal inference, we want to restrict the assignment mechanism, in order to remove the effect of selection bias.\nThere are two types of studies that use different assignment mechanisms:\n\nRandomisation Experiments: The assignment mechanism is both known, and controlled by the researcher. Generally, the researcher chooses some type of randomisation.\nObservational Studies: The assignment mechanism is not known to, or not under the control of the researcher. This means that confounders may be driving selection into treatment and control, inducing bias.\n\nGenerally, the most credible studies are randomisation studies, since we can control interventions to parse out the effect of confounders. Observational studies generally rely on more assumptions that need to be met, and need to be defended for the study to be credible.\n\n\n\n\n\n\nDirected Acyclic Graphs\n\nComponents of the Graphs\nCausal Diagrams are a visual way to represent causal theories and frameworks, which allows us to visualise how variables interact with each other.\nEach Directed Acyclic Graph (DAG) has the following components:\n\nNodes: representing variables (which are also called vertices).\nDirected Edges: Arrows that encode one-way causal theories between variables. For example, we might believe \\(Z\\) causes \\(X\\), so we will draw an arrow from \\(Z\\) to \\(X\\). These connections are observable (solid) or unobservable (dashed).\n\n\n\n\n\n\n\nExample of Directed Acyclic Graphs\n\n\n\n\n\nBelow is an example of a directed acyclic graph:\n\n\n\n\n\nWhat does this diagram show?\n\nWe have two unobserved variables: \\(Q\\) and \\(Y\\)\nWe have three observed variables: \\(Z\\), \\(D\\), and \\(Y\\).\nWe can see the causal theories represented by arrows.\n\nWhat can we learn from this diagram?\n\n\\(Z \\rightarrow Y\\) is confounded by \\(W\\): \\(W\\) is affecting who gets treatment \\(Z\\), and causing \\(Y\\). Thus, \\(W\\) is affecting who gets selected into treatment \\(Z\\), and selecting your potential outcome \\(Y\\). Thus, this is an example of selection bias.\n\\(D \\rightarrow Y\\) is confounded by \\(Q\\).\n\\(Z \\rightarrow D\\) is not confounded, so we can estimate this causal effect.\n\nNote: All these conclusions are only true if our causal theory is correct (we have specified all the possible variables, and we have specified the correct causal relationships).\n\n\n\nFeatures of a Directed Acyclic Graph:\n\nThey must be acyclic: This means that they are not circular - \\(A\\) does not terminate back at \\(A\\).\nNon-Connections: The absence of relationships between variables.\n\n\n\n\nRepresenting Interventions\nTreatments (interventions by the researcher, for example) are when we determine one variable exogenously (such as by randomisation).\nOr in other words, one variable is determined randomly externally, not caused by any variables within the directed acyclic graphs.\nTreatments are represented by the do() operator. When the treatment is exogenous, we can break all the connections into that variable’s node.\nThis is because we are determining the value of the variable, not any other variables.\n\n\n\n\n\n\nExample of Interventions\n\n\n\n\n\n\n\n\n\n\nAn intervention here is on variable \\(D\\). That means the value of \\(D\\) is being chosen outside of this graph (by randomisation, or the researcher).\nThis allows us to delete the arrow between \\(Q \\rightarrow D\\) and \\(Z \\rightarrow D\\). This is because we are exogenously determining \\(D\\), so \\(Q\\) and \\(Z\\) are not determining the value of \\(D\\).\n\n\n\nWith exogenously determined variables, we can find the causal effect that variable is causing on another.\n\n\n\nBlocked Paths\nA set of nodes \\(\\{ \\mathbf S \\}\\) blocks a path \\(p\\) if either:\n\nIf the path \\(p\\) contains at least one arrow-emitting node included in the set of nodes \\(S\\), or\nThe path \\(p\\) contains at least one collision node (multiple arrows point into it) that is outside the set of nodes \\(S\\), and the collision node has no descendant within the set of nodes \\(S\\) (no arrows go out of it to another node).\n\nTake this directed acyclic graph:\n\n\n\n\n\nWe can see the following:\n\nThe path \\(D \\rightarrow P \\rightarrow Y\\) is blocked by set \\(\\{P\\}\\), because the node \\(P\\) is one arrow-emitting node that is in the path \\(D \\rightarrow P \\rightarrow Y\\).\nThe path \\(D \\leftarrow M \\rightarrow Y\\) is blocked by set \\(\\{M\\}\\), because the node \\(M\\) is one arrow-emitting node in the path \\(D \\leftarrow M \\rightarrow Y\\).\n\\(D \\leftarrow Z \\rightarrow M \\rightarrow Y\\) is blocked by \\(\\{M\\}\\), \\(\\{Z\\}\\), or \\(\\{M, Z\\}\\) - note \\(D\\) and \\(Y\\) do not emit arrows so they cannot block.\n\nBlocking paths is important, since in order to estimate \\(D \\rightarrow Y\\), we need to block any other path between \\(D\\) and \\(Y\\) that is not directly \\(D \\rightarrow Y\\).\n\n\n\n\n\n\nImplementation in R\nThis section will show how you can create DAGs in R. We will need the ggdag and dagitty packages.\n\nlibrary(ggdag)\nlibrary(dagitty)\n\n\n\n\n\n\n\nSimple DAGs with Dagify\n\n\n\n\n\nYou can create a very simple DAG with dagify as follows:\n\ndag_object &lt;- dagify(\n  Y ~ X + D, #Y is caused by X and D\n  D ~ X #D is caused by X\n)\n\nggdag(dag_object) + theme_dag()\n\nThis dag is not very customisable. This can be an issue if you want nodes to be in a specific location. See below for a more customisable DAG.\n\n\n\n\n\n\n\n\n\nCustom DAGs with Dagitty\n\n\n\n\n\nYou can create more complex DAGs with Dagitty. Dagitty allows us to position nodes in a coordinate system, which is useful in some purposes.\n\ndag_object &lt;- dagitty('dag {\n      D [pos = \"0, 1\"]\n      Y [pos = \"2, 1\"]\n      X [pos = \"1, 2\"]\n      \n      D -&gt; Y\n      D &lt;- X -&gt; Y\n  }')\n\nggdag(dag_object) + theme_dag()\n\nThe pos arguments have the coordinates of where you want to put each node.\nBelow are the path connections, where you can use -&gt; and &lt;- to indicate relationships.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "4 Causal Frameworks"
    ]
  },
  {
    "objectID": "5.html",
    "href": "5.html",
    "title": "Randomised Controlled Trials",
    "section": "",
    "text": "Up until now, we have focused on theory of statistics and causal inference. But now, we are ready to dive into methodology - different designs to measure and identify causal effects.\nThis chapter introduces the “gold standard” of causal inference: randomised controlled trials. This chapter also covers extensions, such as stratified experiments and survey experiments.\nUse the right sidebar for quick navigation.\n\n\nRandomisation\n\nRandomised Experiments\nExperiments are a research design where the assignment mechanism is controlled by the researcher.\nRandomised Experiments use randomisation as the assignment mechanism. Treatment values are assigned to \\(N\\) units at random, with both known and positive probabilities of being assigned to treatment and control groups.\nQuick notation for randomised experiments:\n\nWe have \\(N\\) total number of units in our experiment.\nA randomly subset of \\(N_1\\) units are assigned to treatment \\(D = 1\\).\nThe remaining \\(N_0 = N - N_1\\) are assigned to control.\n\n\n\n\n\n\n\nMore on Randomisation\n\n\n\n\n\n\\(N_1\\), the number of individuals assigned to treatment, does not necessarily need to be 50% (although this is quite a common number).\nAlso note that when you fix the number of units to be treated at \\(N_1\\), technically, not all units have an independent probability of being selected. This is because once you have assigned \\(N_1\\) individuals to treatment, we know the remaining individuals must be assigned to control. This usually is not a huge issue.\nYou can use bernoulli randomisation (simple randomisation) to avoid this issue. Bernoulli gives every individual a certain chance of being selected. This does mean that with different randomisation trials, we will get different numbers of treated individuals for each trial.\n\n\n\n\n\n\nIdentification Assumptions\nRandomisation implies that assignment probabilities do not depend on the potential outcomes. The potential outcome values do not affect our chances of being selected for treatment.\n\\[\nPr(D=1|Y_0, Y_1) = Pr(D=1)\n\\]\nOr in other words, treatment is independent of potential outcomes (also unconfounded or ignorability):\n\\[\n(Y_1, Y_0)  \\perp\\!\\!\\!\\!\\perp D\n\\]\nThis implies that \\(E(Y_{0i})\\) is the same between treatment and control groups, and \\(E(Y_{1i})\\) is also the same between treatment and control:\n\\[\n\\begin{split}\n& E(Y_{0i} | D_i = 1) = E(Y_{0i} | D_i = 0) = E(Y_{0i})\\\\\n& E(Y_{1i} | D_i = 1) = E(Y_{1i} | D_i = 0) = E(Y_{1i})\n\\end{split}\n\\tag{1}\\]\n\n\n\nProof of Identification\nLet us return to our naive estimator, and our problem of selection bias. Using the above property in Equation 1, we can simplify:\n\\[\n\\begin{align}\n& \\underbrace{E(Y_{1i}|D_i = 1)- E(Y_{0i}|D_i = 1)}_{\\tau_{ATT}} + \\underbrace{E(Y_{0i}|D_i = 1) - E(Y_{0i} | D_i = 0)}_{\\text{Selection Bias}} \\\\\n& = \\underbrace{E(Y_{1i}|D_i = 1)- E(Y_{0i}|D_i = 1)}_{\\tau_{ATT}} + \\underbrace{E(Y_{0i}) - E(Y_{0i})}_{\\text{Selection Bias}} && (\\because \\text{eq. (1)} \\ )\\\\\n& = \\underbrace{E(Y_{1i}|D_i = 1)- E(Y_{0i}|D_i = 1)}_{\\tau_{ATT}} + 0\n\\end{align}\n\\]\nThus, under randomisation, selection bias is equal to 0, and thus our comparison of observed outcomes is now an unbiased estimator of \\(\\tau_{ATT}\\). Now look at the formula for the ATT estimand. We can simplify as follows using Equation 1:\n\\[\n\\begin{align}\n\\tau_{ATT} & = E(Y_{1i} -Y_{0i}|D_i = 1)\\\\\n& = E(Y_{1i} |D_i = 1) - E(Y_{0i} | D_i = 1) \\\\\n& = E(Y_{1i} ) - E(Y_{0i}) && (\\because \\text{equation (1)} \\ )\\\\\n& = \\underbrace{E(Y_{1i} - Y_{0i})}_{\\tau_{ATE}}\n\\end{align}\n\\]\nAnd now we see that \\(\\tau_{ATT}\\) and \\(\\tau_{ATE}\\) are equivalent under randomisation, and we can identify the \\(\\tau_{ATE}\\) with our observed data.\n\n\n\n\n\n\nGraphical Identification\n\n\n\n\n\nLet us look at a direct acyclic graph:\n\n\n\n\n\nBecause we are randomly assigning treatment \\(D\\), we are exogenously determining \\(D\\). Thus, values of \\(D\\) are not being caused by \\(U\\), they are being caused by randomisation.\nThus, we can eliminate the arrow between \\(U \\rightarrow D\\). This allows us to estimate \\(D \\rightarrow Y\\) without any confounders.\n\n\n\n\n\n\nThe Balancing Property\nRandomisation balances all observed and unobserved pre-treatment characteristics between units between the treatment and control.\nThis is because not only is \\((Y_1, Y_0) \\perp\\!\\!\\!\\!\\perp D\\), but also any covariate \\(X\\) is also independent of treatment: \\(X \\perp\\!\\!\\!\\!\\perp D\\).\nThis means that if randomisation is successful, we should expect minimal differences between control and treatment groups for all pre-treatment characteristics values.\n\n\n\n\n\n\nDetails on the Balancing Property\n\n\n\n\n\nIn any one sample, we actually are likely to have some imbalances in \\(X\\) between control and treatment simply due to chance.\n\nYou could control for imbalanced covariates, but you do not have to (we will discuss this later).\n\nYou can adopt other randomisation procedures, such as stratified randomisation, to guarantee balance on \\(X\\).\n\n\n\nWe can test this assumption by finding the average \\(X\\) values for both control and treatment groups, and see if there are any statistical significant differences in \\(X\\) between control and treatment. This is typically done with a t-test or a regression:\n\\[\nX_i = \\alpha + \\gamma D_i + u_i \\quad \\text{test if } \\gamma \\text{ is significant}\n\\]\n\n\n\nComplications and Limitations\nRandomisation can be complicated by a few factors:\n\nMissing data (often due to individuals dropping out). We are concerned that there is some covariate that is causing some people to drop out, which re-introduces selection bias.\nMeasurement Problems: Hawthorne Effect - subjects know what you are studying, and will change their behaviour as a result.\nNon-Compliance: Some units assigned to treatment might not take the treatment, and some units assigned to control may take the treatment (this can often be dealt with by using an instrumental variable design).\n\nRandomisation does not help with external validity - the ability to extrapolate our results to external situations.\n\n\n\n\n\n\nMore on External Validity\n\n\n\n\n\nExternal validity asks if we can generalise our conclusions from our subjects, to other subjects outside our experiment. Can we extrapolate our estimates to to other populations?\nFor example, if we measured the effect of migration on tolerance for our subjects in India, can we say the same effect is true of someone in Japan, the US, or Europe?\nThis is important - if we cannot extrapolate, some results may be very niche.\nTo extrapolate to a greater population, our actual sample of observations in our experiment, should be representative of the greater population. This is often violated, as random sample for experiments is very very difficult.\nThis is called 𝑋-Validity: we can study this with data - to see how representative our population is compared to the population.\nNon-representative programme of treatment is another threat: Sometimes, treatments will differ between areas.\nFor example, if we are encouraging people to migrate to test how that changes their tolerance, how are the governmental/ngo/private agencies working with you affecting the process. Would less capable agencies create different effects?\nThis is called \\(C\\)-validity, and we cannot measure this with data, unless you redo your experiment in another context.\n\n\n\n\n\n\n\n\n\nCausal Estimation\n\nDifference in Means Estimator\nOur causal estimand is the Average Treatment Effect (ATE):\n\\[\n\\tau_{ATE} = E(Y_{1i}) - E(Y_{0i})\n\\]\nWe can estimate this using the difference-in-means estimator, by taking the sample mean \\(Y\\) of the treatment group, minus the sample mean \\(Y\\) of the control group:\n\\[\n\\hat\\tau_{ATE} = \\bar Y_1 - \\bar Y_0\n\\]\nThis is an unbiased estimator because selection bias is eliminated with randomisation. This is also an asymptotically consistent estimator due to the law of large numbers.\n\n\n\nOrdinary Least Squares Estimator\nWe can also estimate the \\(\\tau_{ATE}\\) with a bivariate regression:\n\\[\nY_i = \\hat\\gamma + \\hat\\tau D_i + \\hat\\epsilon_i\n\\]\nHere, \\(\\hat\\tau\\) is our estimator of the ATE. This gives the same estimate as the difference-in-means estimator.\n\n\n\n\n\n\nProof OLS is Equivalent to Difference-in-Means\n\n\n\n\n\nRemember that OLS is the best approximation of the conditional expectation function \\(E(y|x)\\).\nThus, we can write the regression as:\n\\[\nE(Y_i|D_i) = \\hat\\gamma + \\hat\\tau D_i + \\hat\\epsilon_i\n\\]\nNow, let us find the difference between treatment \\(E(Y_i|D_i =1)\\) and control \\(E(Y_i|D_i = 0)\\):\n\\[\n\\begin{split}\n& E(Y_i|D_i = 1) - E(Y_i|D_i = 0) \\\\\n= & \\ \\hat\\gamma + \\hat\\tau(1) - (\\hat\\gamma + \\hat\\tau(0)) \\\\\n= & \\ \\hat\\gamma + \\hat\\tau - \\hat\\gamma \\\\\n= & \\ \\hat\\tau\n\\end{split}\n\\]\nThus, the difference in means is equivalent to \\(\\hat\\tau\\) regression coefficient.\n\n\n\nFurthermore, \\(\\hat\\gamma\\) is equivalent to the average \\(Y\\) in the control group \\(\\bar Y_0\\).\nWe do not need to include covariates. This is because randomisation allows us to meet the asymptotic consistency condition of both randomisation and exogeneity.\nHowever, sometimes pre-treatment covariates are included. We should not include post-treatment covariates.\n\n\n\n\n\n\nIncluding Pre-Treatment Covariates\n\n\n\n\n\nThere are several reasons one might want to include pre-treatment covariates:\n\nCan increase precision (reduce standard error), by getting better predictions of \\(Y\\).\nCan control for observable imbalance that was observed in the balance tables. Many researchers will compare a model without and with an imbalanced covariate, to show that the covariate does not matter significantly.\nCan allow for estimation of heterogenous treatment effects by including interactions in the model.\n\nThere is one risk: it may introduce small-sample bias. This will be discussed later in the discussion of the fully-interacted estimator.\nWe should not include post-treatment covariates. Anything that is measured post-treatment could be measuring a treatment effect (something that results from the treatment). This may “model away” your treatment effect.\n\n\n\n\n\n\n\n\n\nStatistical Inference\n\nStandard Inference\nWe can use a t-test for statistical inference.\n\nEstimate the \\(\\hat\\tau_{ATE}\\) and robust standard error \\(\\widehat{rse}(\\hat\\tau_{ATE})\\).\nState hypotheses, normally \\(H_0 : \\tau_{ATE} = 0\\) and \\(H_1 \\tau_{ATE} ≠ 0\\).\nCalculate the t-test statistic \\(\\hat\\tau /\\widehat{rse}(\\hat\\tau)\\).\nRefer to the relevant t-distribution, and calculate the p-value.\n\nGenerally, we use a statistical significance level of \\(\\alpha = 0.05\\), so we reject the null if \\(|t|&gt;1.96\\).\nFor more complex randomisation schemes, you will need different standard errors. For example, if you use a cluster randomisation scheme, you might need clustered standard errors.\nWe can also use Nonparametric Bootstrap to create our sampling distribution for statistical inference, instead of relying on asymptotic normality of the standard t-test.\n\n\n\n\n\n\nBlock Bootstrap\n\n\n\n\n\nFor blocked experiments, you should randomly sample blocks, not units, to create your bootstrap re-samples.\nFor example, if your data is clustered in cities, you should re-sample by cities.\n\n\n\n\n\n\nRandomisation Inference\nConsider a new sharp null hypothesis, that all individual causal effects are zero (not just the average causal effect):\n\\[\nH_0^s : Y_1 = Y_0, \\quad H_A^s : Y_1 ≠ Y_0\n\\]\nAssuming \\(H_0\\) is true, we can actually fully construct the potential outcomes \\(Y_{0i}\\) and \\(Y_{1i}\\), since we know every unit has 0 individual treatment effect.\nOnce constructed, we can imagine what different treatments we would observe under different randomization schemes (given \\(H_0\\) is true). Thus, we can construct the sampling distribution, so we do not need to “imagine” a hypothetical sampling distribution.\nProcedure for conducting randomisation inference (also called permutation test or Fisher’s exact test) is as follows:\n\nCalculate the total number of randomisations possible. This is calculated as a permutation of \\(_NP_{N_1}\\) (\\(N\\) choose \\(N_1\\)).\nCalculate and store the value of \\(\\widehat{\\tau_j}\\) of each permuted dataset \\(j\\). Thus, we will have a distribution of \\(\\widehat{\\tau_j}\\).\nCalculate \\(p\\)-value as the proportion of \\(\\widehat{\\tau_j}\\) that are as or more extreme than the actually observed \\(\\hat\\tau\\).\n\n\n\n\n\n\n\nDetails on Randomisation Inference\n\n\n\n\n\nIf we have \\(N\\) total units, and \\(N_1\\) in the treatment group and \\(N_0\\) in the control group, we can calculate all possible randomisation permutations as follows:\n\\[\n\\begin{pmatrix} N \\\\ N_1 \\end{pmatrix} = \\frac{N!}{N_1 ! N_0 !}\n\\]\nThis is the total number of assignments possible given \\(N\\), \\(N_1\\), and \\(N_0\\).\nThen, we can calculate the \\(\\widehat{\\tau_j}\\) of every possible randomisation assignment. The figure below shows this:\n\n\n\n\n\nNow, plot all \\(\\widehat{\\tau_j}\\) in a distribution:\n\n\n\n\n\nLet us say our sample \\(\\hat\\tau = 6\\). We would simply find the area under the curve that is above \\(\\hat\\tau = 6\\), and below \\(-\\hat\\tau = -6\\).\n\n\n\n\n\n\n\n\n\nPros/Cons of Randomisation Inference\n\n\n\n\n\nRandomisation Inference is assumption free - we do not need asymptotic properties or hypothetical sampling distributions.\nSince we also do not need asymptotic property inferences, we can do inference with very small samples as well.\nDownsides: the randomisation inference only tests if the sharp null hypothesis is true, but sometimes, that might not be something we want to test.\n\n\n\n\n\n\n\n\n\nOther Randomisation Procedures\n\nStratified Randomisation\nStratified (also called blocked or conditional) randomisation are when randomisation occurs separately within levels of some covariates(s) \\(X\\). Generally, you separate your sample of \\(N\\) units into \\(J\\) subgroups.\nFor example, you could split people up into male or female, and random sample within each group, rather than everyone together.\n\n\n\n\n\n\nExample of Stratification\n\n\n\n\n\nLet us say you have 4 subjects, with pre-treatment potential outcomes of \\(Y_{0i} = \\{2, 2, 8, 8 \\}\\).\nIf you just randomly assign, then there is a 33% chance that you end up with the random assignment where \\(\\{8, 8\\}\\) are placed in one group, and \\(\\{2, 2\\}\\) are placed in another group.\nThis is a concern: our treatment and control groups would be very imbalanced in this situation, which violates our independence assumption.\nWith blocking, we could divide our sample into \\(J = 2\\) subgroups, having group 1 being \\(\\{2, 2\\}\\), and group 2 being \\(\\{8, 8\\}\\). Then, we randomly sample one from each group into the treatment. This way, we are guaranteed better balance.\n\n\n\nThis can prevent imbalances as normal randomisation can have a high probability (in certain situations) of creating imbalances.\n\n\n\n\n\n\nEstimation with Stratification\n\n\n\n\n\nTo estimate the ATE, you will need a weighted average of the ATE for each subgroup \\(j\\), with the weights being the proportion of units each group \\(j\\) accounts for:\n\\[\n\\tau_{ATE} = \\sum\\limits_{j=1}^J \\frac{N_j}{N} \\tau_j\n\\]\n\n\n\n\n\n\nCluster Randomisation\nCluster randomisation is when we randomly assign units (or have individuals naturally) in groups. Every unit within a group (called a cluster) will get the same treatment. We randomly sample the groups to get the treatment or control.\nFor example, we could randomise development treatment at the village level, or randomise treatment of a cirriculum at the school level.\nThe main reason for this is to prevent SUTVA violations.\nFor example, imagine you are testing the effects of a new curriculum. If you randomise by each student, students will talk to their friends, and treated individuals may teach control individuals about the new curriculum. But by randomising by school (either an entire school gets or does not get the new curriculum), this concern is not a huge issue.\n\n\n\n\n\n\nSurvey Experiments\n\nFraming/Endorsement Experiments\n\n\nPriming Experiments\n\n\nList Experiments\n\n\n\n\n\n Back to top",
    "crumbs": [
      "5 Randomised Controlled Trials"
    ]
  },
  {
    "objectID": "6.html",
    "href": "6.html",
    "title": "Selection on Observables",
    "section": "",
    "text": "In the last chapter, we discussed randomisation. Randomisation is great, but, it requires specific circumstances of the research having control over the assignment mechanism. However, in the social sciences, this rarely occurs.\nThis chapter introduces the selection on observables framework, which allows us to identify causal effects in an observational setting by controlling for observable pre-treatment covariates. We discuss the main estimators, including regression, matching, and weighting.\nUse the right sidebar for quick navigation. R-code provided at the bottom.\n\n\nIdentification\n\nBlocking Backdoor Paths\nWithout randomisation, we need some other way to account for pre-treatment covariates that may be confounding and causing selection bias. Controlling for a set of nodes/confounders \\(X\\) can identify the causal effect of \\(D \\rightarrow Y\\), if:\n\nNo node within set \\(X\\) is a descendant of \\(D\\) (no element within \\(X\\) results from \\(D\\)).\nThe nodes within set \\(X\\) block all back-door paths from \\(D \\rightarrow Y\\).\n\n\n\n\n\n\nIn the figure above, let us block the backdoor paths between \\(D \\rightarrow Y\\):\n\nBackdoor path \\(D \\rightarrow X \\rightarrow Y\\). To block this path, we must control for \\(X\\).\nBackdoor path \\(D \\rightarrow V \\rightarrow Y\\). We do not need to control for \\(V\\), since it is post-treatment (a descendant of \\(D\\)). In fact, \\(V\\) is a bad control (see below).\n\nThus, to identify \\(D \\rightarrow Y\\) here, we only need to control for \\(X\\), and no other variable.\n\n\n\n\n\n\nGood and Bad Controls\n\n\n\n\n\nGood controls block backdoor paths, which facilitate identification of the causal effect.\nBad controls are when we control for post-treatment variables. For example, \\(P\\) below is a bad control, since it is caused by \\(D\\), so it is post-treatment.\n\n\n\n\n\nYou also never want to control variables that only predict \\(D\\). These are bad because controlling for these removes variation in \\(D\\) that could be useful.\nNeutral controls are ones that don’t identify the causal effect, but improve efficiency. For example, \\(Q\\) below affects \\(Y\\), but there is no backdoor path. Controlling \\(Q\\) will not help identification, but can control noise in \\(Y\\) which may increase efficiency.\n\n\n\n\n\n\n\n\n\n\n\nIdentification Assumptions\nOnce we have determined the set of confounders \\(X\\) that we need to control to block all backdoor paths, the assumptions needed for identification of causal effects are:\n\nConditional Ignorability (also known as exogeneity or independence): Among units with identical confounder values \\(X_i\\), treatment \\(D_i\\) is as-if randomly assigned. Or in other words, potential outcomes are independent from treatment within each specific confounder value \\(X_i = x\\).\n\n\\[\n(Y_{0i}, Y_{1i}) \\perp\\!\\!\\!\\!\\perp D_i  \\ | \\ X_i = x, \\quad \\forall \\ x \\in \\mathcal X\n\\]\nThis implies that for any given value of all confounders \\(X_i = x\\), we know that potential outcomes \\(Y_{di}\\) are equivalent between treatment and control:\n\\[\n\\begin{split}\nE(Y_{1i}|X_i = x) = E(Y_{1i}|D_i = 1, X_i = x) = E(Y_{1i}|D_i = 0, X_i = x) \\\\\nE(Y_{0i}|X_i = x) = E(Y_{0i}|D_i = 1, X_i = x) = E(Y_{0i}|D_i = 0, X_i = x)\n\\end{split}\n\\tag{1}\\]\n\nCommon Support: for any unit \\(i\\) with value of \\(X_i\\), there is a non-zero probability that they could be assigned to both control \\(D_i = 0\\) or treatment \\(D_i = 1\\).\n\n\\[\n0 &lt; Pr(D_i = 1 \\ | X_i = x) &lt; 1 \\quad \\forall \\ x \\in \\mathcal X\n\\]\n\n\n\n\n\n\nExample of Identification Assumptions\n\n\n\n\n\nImagine we have a theory that being abducted \\(D\\) causes turning out to vote.\nBlattman (2009) finds that age is the primary way violent groups chose to abduct individuals: abduction parties released young children and older adults, but kept all adolescent and young males.\nThat means our theory is that age \\(X\\) affects selection into treatment \\(D\\). Young children and older adults are less likely to get abducted \\(D\\), while adolescent and young males are more likely \\(D\\).\n\n\n\n\n\n\nIdentification of the ATE\nWith our assumptions above, we can identify the ATE. We start with the conditional average treatment effect, conditional on some value of confounders \\(X_i = x\\). Note the properties shown in Equation 1 .\n\\[\n\\begin{align}\n\\tau_{CATE}(x) & = E(Y_{1i} - Y_{0i} \\ | \\ X_i = x) \\\\\n& = E(Y_{1i}|X_i = x) - E(Y_{0i}|X_i = x) && (\\text{property of expectation}) \\\\\n& = E(Y_{1i}|D_i = 1, X_i = x) - E(Y_{0i}|D_i = 0X_i = x) &&( \\because \\text{equation (1)} \\ ) \\\\\n& = \\underbrace{E(Y_i|D_i = 1, X_i = x)}_{\\because \\text{ observable}} - \\underbrace{E(Y_i|D_i = 0, X_i = x)}_{\\because \\text{ observable}}\n\\end{align}\n\\tag{2}\\]\nNow, let us discuss the ATE, and plug in the CATE from Equation 2 to identify it:\n\\[\n\\begin{align}\n\\tau_{ATE} & = E(Y_{1i} - Y_{0i}) \\\\\n& = \\int \\underbrace{E(Y_{1i} - Y_{0i} \\ | \\ X_i = x)}_{\\tau_{CATE}(x)} d \\ \\underbrace{Pr(X_i = x)}_{\\text{weight}} && (\\text{weighted average})\\\\\n& = \\int(\\underbrace{E(Y_i|D_i = 1, X_i) - E(Y_i|D_i = 0, X_i)}_{\\because \\text{ equation (2)}})d \\ Pr(X_i = x)\n\\end{align}\n\\tag{3}\\]\nThus \\(\\tau_{ATE}\\) is identified as the weighted average of all the CATEs, who themselves are difference-in-means of the observed \\(Y_i\\) at every possible value of \\(X_i = x\\).\nWe assumed that the pre-treatment covariate \\(X\\) is continuous. This is why we need an integral. However, we can simplify this if \\(X\\) is discrete:\n\\[\n\\tau_{ATE} = \\sum\\limits_{x \\in \\mathcal X} ( E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)) Pr(X_i = x)\n\\tag{4}\\]\n\n\n\nIdentification of the ATT\nWe can weaken conditional ignorability, and still identify the ATT. Only \\(Y_{0i}\\) needs to be independent of \\(D_i\\) for units with the same covariates \\(X_i\\). Or in other words, \\((Y_{0i}) \\perp\\!\\!\\!\\perp D_i | X_i = x\\). This implies:\n\\[\nE(Y_{0i}|X_i = x) = E(Y_{0i}|D_i = 0, X_i = x) = E(Y_{0i}|D_i = 1, X_i = x)\n\\tag{5}\\]\nStart with the conditional ATT, using weakened conditional ignorability from Equation 5 :\n\\[\n\\begin{split}\n\\tau_{CATT}(x) & = E(Y_{1i}-Y_{0i}|D_i = 1, X_i = x) \\\\\n& = E(Y_{1i}|D_i = 1, X_i = x) - E(Y_{0i}|D_i = 1, X_i = x) \\\\\n& = E(Y_{1i}|D_i = 1, X_i = x) - \\underbrace{E(Y_{0i}|D_i = 0, X_i = x)}_{\\because \\text{ equation (5)}} \\\\\n& = \\underbrace{E(Y_i|D_i=1, X_i = x)}_{\\because \\text{ observable}} - \\underbrace{E(Y_1|D_i = 0, X_i x)}_{\\because \\text{ observable}}\n\\end{split}\n\\tag{6}\\]\nNow, look at the ATT, and plug in CATT from Equation 6 to identify it.\n\\[\n\\begin{align}\n\\tau_{ATT} & = E(Y_{1i} - Y_{0i}|D_i = 1) \\\\\n& = \\int \\underbrace{E(Y_{1i} - Y_{0i}|D_i = 1, X_i = x)}_{\\tau_{CATT}(x)}d \\ \\underbrace{Pr(X_i = x|D_i = 1)}_{Pr(X_i = x) \\text{ within treated}} \\\\\n& = \\int (\\underbrace{E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)}_{\\because \\text{ equation (6)}})d \\ Pr(X_i = x|D_i = 1)\n\\end{align}\n\\]\nWe can simplify this if \\(X\\) is discrete:\n\\[\n\\tau_{ATT} = \\sum\\limits_{x \\in \\mathcal X} ( E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)) Pr(X_i = x | D_i = 1)\n\\]\nEven when all assumptions are met for identification of the ATE, the \\(\\tau_{ATE}\\) can be different than the \\(\\tau_{ATT}\\). This is because the weights \\(Pr(X_i = x|D_i = 1)\\) for the ATT are different than the ATE \\(Pr(X_i = x)\\).\n\n\n\n\n\n\nParametric Estimators\n\nOrdinary Least Squares Estimator\nOLS is a natural approach for controlling for confounders \\(X\\), since \\(\\hat\\beta_{OLS}\\) estimates partial out the effects of covariates. OLS is a good estimator of \\(\\tau_{ATE}\\) under 2 conditions:\n\nConstant treatment effect: \\(\\tau_i = Y_{1i} - Y_{0i}\\) for all units \\(i\\).\nLinearity: Potential outcomes are linear, and can be written as:\n\n\\[\nY_i(d) = \\beta_0 + d\\beta_1 + \\mathbf X_i \\gamma + \\epsilon_i \\quad \\text{for} \\quad d = 0, 1\n\\]\nWhy these conditions? Suppose we have the above linear potential outcomes. We can show:\n\\[\n\\begin{align}\n\\tau_i & = Y_{1i} - Y_{0i} && (\\text{definition of } \\tau_i) \\\\\n& = (\\beta_0 + (1)\\beta_1 + \\mathbf X_i \\gamma + \\epsilon_i) - (\\beta_0 + (0)\\beta_1 + \\mathbf X_i \\gamma + \\epsilon_i) && (\\text{plug in } Y_i(1), Y_i(0) \\ )\\\\\n& = (\\beta_0 + \\beta_1 + \\mathbf X_i \\gamma + \\epsilon_i) - (\\beta_0 + \\mathbf X_i \\gamma + \\epsilon_i) && (\\text{multiply}) \\\\\n& = \\beta_0 + \\beta_1 + \\mathbf X_i \\gamma + \\epsilon_i - \\beta_0 - \\mathbf X_i\\gamma - \\epsilon_i && (\\text{distribute negative sign})\\\\\n& = \\beta_1 && (\\text{cancel out terms})\n\\end{align}\n\\]\nWe also know that conditional ignorability implies zero-conditional mean. Thus \\(\\beta_1\\) is an unbiased and asymptotically consistent estimator of the ATE.\nYou should be cautious using OLS when assumption 2, linearity, is violated. OLS is the best linear estimator, but how far your data is from linearity will determine if the estimator is useful.\n\n\n\n\n\n\nNon-Linearity\n\n\n\n\n\nWhat if potential outcomes \\(Y_i(d)\\) is an unknown and non-linear function of \\(d\\) and \\(X_i\\).\nWe know the OLS is the best linear predictor of the conditional expectation function in terms of mean squared error. Thus, \\(\\beta_1\\) will provide the best linear approximation to the population regression function.\nThis does not mean it is good - just the best linear approximation.\n\n\n\nYou should not use OLS if you believe assumption 1, heterogeneity, is violated. The reasoning is explained below.\n\n\n\nOLS Bias under Heterogeneity\nWhat if there are heterogenous treatment effects (where \\(\\tau_i\\) is different between units)? Standard OLS in this case is no longer an unbiased estimator of the ATE.\nRecall the discrete identification of the ATE (in equation Equation 4 ) is a weighted average of CATEs:\n\\[\n\\hat\\tau_{ATE} = \\sum\\limits_{x \\in \\mathcal X} ( \\hat\\tau_{CATE}(x)) \\underbrace{Pr(X_i = x)}_{\\text{weight}} \\\\\n\\]\nOLS, when there are non constant treatment effects, can also be rewritten as a weighted average of CATEs:\n\\[\n\\hat\\beta_{OLS} = \\sum\\limits_{x \\in \\mathcal X} ( \\hat\\tau_{CATE}(x)) \\underbrace{ \\frac{Var(D_i|X_i = X)Pr(X_i = x)}{\\sum Var(D_i | X_i = x')Pr(X_i = x')} }_{\\text{weight}} \\\\\n\\]\nNotice how the weights are different. The weights in the OLS are the conditional variances of \\(D_i\\). This means that OLS is not an unbiased estimator of the ATE or ATT, but rather, a weighted average of the ATT and ATU.\nOLS, under heterogeneity, actually provides an unbiased estimator of the conditional variance weighted average treatment effect. This is not the same as the ATE or the ATT.\n\n\n\n\n\n\nConditional Variance Weighted Average Treatment Effect (CVW-ATE)\n\n\n\n\n\nThis estimand can also be described as a weighted average of the ATT (average treatment effect on the treated) and the ATU (average treatment effect on the untreated):\n\\[\n\\tau_{OLS} = w_1 \\cdot \\tau_{ATT} + w_0 \\cdot \\tau_{ATU}\n\\]\nWhere:\n\\[\n\\begin{split}\nw_1 & = \\frac{(1 - Pr(D=1)) Var(\\pi(X)|D = 0)}{Pr(D=1)Var(\\pi(X)|D=1) + (1-Pr(D=1)Var(\\pi(X)|D=0)} \\\\\nw_0 & = 1 - w_1\n\\end{split}\n\\]\nThe reason for this is because regression is prone to extrapolation beyond common support - i.e. it can “estimate” potential outcomes for units that are not observed. This can lead to bias.\nThis is in contrast to the subclassification estimator, which cannot be computed if there are missing observable outcomes for a substratum/category of \\(X\\).\nThe weights of \\(D_i(X_i = x)\\) can also be seen as propensity scores of \\(\\pi(x)(1 - \\pi(x)\\). Therefore:\n\nWeights are higher for groups with propensity scores close to 0.5.\nWeights are low for groups with propensity scores close to 0 or 1.\nOLS minimises estimation uncertainty by downweighting groups of \\(X_i\\) where group-specific ATEs are less precisely estimated.\n\n\n\n\n\n\n\nFully Interacted Estimator\nThe Fully-Interacted Estimator, a newly developed large-sample regression estimator (Lin 2013), solves the heterogeneity bias in the OLS estimator. The fully-interacted estimator takes the form:\n\\[\n\\widehat{Y_i} = \\hat\\alpha + D_i \\widehat{\\tau}_{int} + (\\mathbf X_i - \\mathbf {\\bar X}) \\hat\\beta +D_i (\\mathbf X_i - \\mathbf{\\bar X}) \\hat\\gamma\n\\]\n\nWhere \\(X_i\\) are covariate values sufficient to satisfy conditional independence.\nWhere \\(\\bar X\\) contains the sample means of all \\(X_i\\) covariates.\n\nThis estimator \\(\\hat\\tau_{int}\\) is technically biased when estimating \\(\\tau_{ATE}\\). However, the bias is arbitrarily small in large samples under conditional ignorability.\nThis estimator thus allows us to accurately estimate the ATE even under heterogenous treatment effects, assuming our sample size is sufficiently large.\n\n\n\n\n\n\nOther Solutions to the OLS Bias under Heterogeneity\n\n\n\n\n\nThere are a few other solutions to this issue of OLS bias under heterogeneity:\n\nDoubly-robust estimation uses a weighted average of regression and IPW estimators, which will be asymptotically consistent as long as the regression model is correctly specified.\nMatching as pre-processing uses matching to make treatment and control groups similar, then runs regression models to estimate causal effects.\n\n\n\n\n\n\n\n\n\n\nNonparametric Estimators\n\nSubclassification Estimator\nUsing the discrete identification of the ATE shown in Equation 4 , we can instead use the sample equivalents to get the subclassification estimator:\n\\[\n\\hat\\tau_{ATE} = \\sum\\limits_{j=1}^M \\underbrace{(\\bar Y_{1j} - \\bar Y_{0j})}_{\\tau_{CATE}(j)} \\underbrace{\\frac{n_j}{n}}_{\\text{weight}}\n\\]\n\nWhere \\(M\\) is the number of levels/categories of \\(X\\), and \\(j\\) is one specific level/category of \\(X\\).\nWhere \\(n_j\\) is the number of units in a level/category \\(j\\) of \\(X\\).\nWhere \\(\\bar Y_{dj}\\) is the mean outcome for units with \\(D_i = d\\) in level/category \\(j\\) of \\(X\\).\n\nFor subclassificaion to be possible, within each level \\(j\\) of covariate \\(X\\), there must be at least one unit in control \\(D=0\\) and treatment \\(D=1\\).\n\n\n\n\n\n\nIntuitive Procedure of Subclassification\n\n\n\n\n\nMore intuitively, the procedure is as follows:\n\nChoose one specific value for all covaraites \\(X\\). Find the average treatment effect within this specific value of \\(X\\).\nMultiply that average treatment effect by the number of observations that meet this specific value of \\(X\\) divided by the total number of units.\nDo this for every possible values of all covaraites \\(X\\), then sum up all the weighted average treatment effects to get the overall ATE.\n\n\n\n\n\n\n\n\n\n\nSubclassification with Multiple Confounders\n\n\n\n\n\nLet us say we have 2 confounders, \\(X_1\\) and \\(X_2\\). Both confounders are categorical with 3 categories.\nWe would need to create \\(M=9\\) levels of strata, for every possible combination of values of \\(X_1\\) and \\(X_2\\). Then, we would estimate the within-strata average treatment effect, and weight them.\nThis illustrates how with large amounts of confounders, you will need a huge number of stratum. This makes subclassification infeasible in many cases.\n\n\n\n\n\n\n\n\n\nSubclassification for the ATT\n\n\n\n\n\nWhen pre-treatment covariate \\(X\\) is discrete, the identification result of the ATT is:\n\\[\n\\tau_{ATT} = \\sum\\limits_{x \\in \\mathcal X} ( E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)) Pr(X_i = x | D_i = 1)\n\\]\nWe can calculate this within our give sample to get the subclassificaiton estimator:\n\\[\n\\hat\\tau_{ATT} = \\sum\\limits_{j=1}^M(\\bar Y_{1j} - \\bar Y_{0j}) \\frac{n_{1j}}{n_1}\n\\]\n\nWhere \\(M\\) is the number of strata (levels/categories of \\(X\\)).\nWhere \\(n_j\\) is the number of units in a level/category \\(j\\) of \\(X\\).\nWhere \\(n_{1j}\\) is the number of treated cells \\(D = 1\\) in a level/category \\(j\\) of \\(X\\).\nWhere \\(\\bar Y_{dj}\\) is the mean outcome for units with \\(D_i = d\\) in level/category \\(j\\) of \\(X\\).\n\n\n\n\n\n\n\nMatching Estimator\nWe have a missing data problem in causal inference: we do not know all the potential outcomes. Matching “estimates” missing potential outcomes of a unit.\nFor each observation in the treated group, matching finds an observation in the untreated group that have the most similar values of a set of pre-treatment covariates \\(X\\). Thus, we have pairs of treatment-control observations that act as counterfactuals. We can estimate the ATT as the average difference in observed outcomes within the pairs:\n\\[\n\\hat\\tau_{ATT} = \\frac{1}{n_1} \\sum\\limits_{i:D_i = 1}(Y_i - \\widetilde{Y_i})\n\\]\n\nWhere \\(n_1\\) is the number of units in the treatment group.\nWhere \\(Y_i\\) is the unit’s observed \\(Y\\) in the treatment group.\nWhere \\(\\tilde Y_i\\) is unit \\(i\\)’s closest neighbour in the untreated group.\n\n\n\n\n\n\n\nChoices during Matching\n\n\n\n\n\nWe have to make several choices when conducting matching.\n\nWhat covariates to match on. We generally want to select a set of pre-treatment covariates \\(X\\) such that these covariates ensure the conditional ignorability assumption is met.\nMatch with or without replacement. Matching with replacement means that once you have used one control unit to match to a treatment unit, you can still use that same control unit to match to another treatment unit. This has advantages since you can ensure better and closer matches. However, matching without replacement is also possible.\nHow many to match. You can decide to match multiple control units to one treatment unit, and use the average of the treatment units to approximate a true control unit. This may result in more accurate matches for treatment units that may not have a good single control unit to match to.\n\nWe can also choose to use more advanced matching methods, such as Mahalanobis Distance matching or Propensity Score matching, which are shown below. These are good for matching on more \\(X\\).\n\n\n\n\n\n\n\n\n\nMatching on Multiple Neighbours\n\n\n\n\n\nSometimes, a treatment unit may not have one close control unit to match to. Instead, we could use a combination of control units to match to the treatment unit, and use the average \\(Y\\) of those combination of control units to approximate a more accurate match.\nSuppose we use \\(M_i\\) number of close control units to match to a treatment unit \\(i\\). Then, the matching estimator would be defined as follows:\n\\[\n\\hat\\tau_{ATT} = \\frac{1}{n_1} \\sum\\limits_{i:D_i = 1}(Y_i - \\left(\\frac{1}{M_i} \\sum\\limits_{m=1}^{M_i} \\widetilde{Y_{i_m}}\\right))\n\\]\nWhere \\(\\widetilde{Y_{i_m}}\\) is the obsered outcome for the \\(m\\)th match of unit \\(i\\).\n\n\n\n\n\n\n\n\n\nWeaknesses of Matching\n\n\n\n\n\nMatching does not always create “perfect” matches. This means that the pairs matched together may not be identical to each other in terms of covariates \\(X\\) or potential outcomes.\nThe inability to find exact matches can cause bias, especially for the more covariates we match on (see below).\n\n\n\n\n\n\nMatching with Multiple Covariates\nConsider that we \\(k&gt;1\\) number of confounders \\(X\\). Now, we have to match observations in \\(k\\) variables, which implies we are in a multidimensional \\(\\mathbb R^k\\) space.\nThe most commonly used distance metric is Mahalanobis Distance - which measures the distance in \\(X_i\\) between units \\(i\\) and \\(j\\):\n\\[\nD_M (\\mathbf X_i, \\mathbf X_j) = \\sqrt{(\\mathbf X_i - \\mathbf X_j)^T \\boldsymbol\\Sigma_X^{-1} (\\mathbf X_i - \\mathbf X_j)}\n\\]\n\nWhere \\(\\boldsymbol \\Sigma_X\\) is the sample variance-covariance matrix of \\(\\mathbf X_i\\).\n\n\n\n\n\n\n\nEuclidean Distance\n\n\n\n\n\nEuclidean distance is another common distance metric:\n\\[\nD_E ( \\mathbf X_i, \\mathbf X_j) = \\sqrt{(x_{1i} - x_{1j})^2 + (x_{2i}-x_{2j})^2 + \\dots + (x_{ki}-x_{kj})^2}\n\\]\nEuclidean distance, while very simple, is not recommended. This is because Euclidean distance with non-normalised variables can get you very bizarre results that depend on the scale of the variables.\nThere are other distance metrics, but these are exceedingly rare in selection on observables.\n\n\n\nThere is one issue with matching in multi-dimensional spaces. It becomes very difficult to match every unit \\(i\\) on every covariate \\(X\\), even if we have a large number of observations.\n\n\n\n\n\n\nCurse of Dimensionality\n\n\n\n\n\nWhen we try to match on more than one \\(X\\) variable, we go from matching on a number line \\(\\mathbb R^1\\) to a \\(n\\)-dimensional space, \\(\\mathbb R^n\\).\nThe search space increases exponentially as you increase the number of dimensions.\n\nTake a look at the figure on the left. If we only match on a one dimensional plane (lets say the horizontal line between 0 and 1), we can see our red line covers approximately 30% of the horizontal line. But in 3 dimensions, our red box covers a significantly less proportion of the entire cube.\nThe figure on the right illustrates this. \\(d\\) represents the dimensions. We can see as the dimensions increase, the fraction of volume increases significantly slower relative to distance.\nThus, with a bigger space, the distance between two units increases, so you get worse matches.\n\n\n\nThis curse of dimensionality creates a bias problem - since we get non-exact matches. The more dimensions you add, the worse it becomes.\n\n\n\n\n\n\nMore on Bias\n\n\n\n\n\nThe poor matches caused by increased dimensionality inject error into our estimates of missing potential outcomes.\nThe bias term as you increase the number of dimensions \\(k\\), changes by \\(N^{(-1/k)}\\). This implies no \\(\\sqrt{n}\\) consistency for \\(k&gt;2\\).\nIf \\(N_0\\) (number of untreated units) is much larger than \\(N_1\\) (number of treated units), bias will typically be smaller.\nThere are ways to correct this bias, including Abadie and Imbens (2011) Bias Correction method.\nThere is a new method: Bias-corrected matching, which estimates bias ineherent to mathching estimators via regression, then subtracts it from the matching estimate to correct for it.\n\n\n\n\n\n\nPropensity Scores Matching\nPropensity Score matching is an alternative way to match over many dimensions. The propensity score is an unobserved property, defined as the probability of a unit \\(i\\) of receiving treatment:\n\\[\n\\pi(X_i) \\equiv Pr(D_i = 1|X_i)\n\\]\nWhen supposing the conditional ignorability and common support assumptions, the propensity score \\(\\pi(X_i)\\) has the balancing property: \\(D_i \\perp X_i \\ | \\ \\pi(X_i)\\). This implies that conditional ignorability holds on the propensity scores alone:\n\\[\n(Y_{1i}, Y_{0i}) \\perp\\!\\!\\!\\!\\perp D_i \\ | \\ \\pi(X_i)\n\\]\nThus, instead of conditioning on \\(X_i\\) as we did in selection on observables, we can instead condition on \\(\\pi (X_i)\\), and still identify the causal estimand.\nHowever, we do not actually observe \\(\\pi (X_i)\\). We estimate \\(\\pi (X_i)\\) with a binary response model, with outcome variable \\(D_i\\), and explanatory variables \\(X_i\\). This will get us a fitted probability \\(Pr(D_i = 1) = \\hat\\pi(X_i)\\).\nThen, once we have the propensity score estimates \\(\\hat\\pi(X_i)\\), we can do nearest neighbour matching with the propensity scores (in \\(\\mathbb R^1\\)). This will allow us to identify the \\(\\tau_{ATT}\\).\n\n\n\n\n\n\nBalance Checks\n\n\n\n\n\nThe accurate estimation of \\(\\tau_{ATT}\\) implies an accurate prediction of the propensity scores \\(\\pi(X_i)\\). We can test our matched treatment and control groups to see if the balancing property holds for covariates \\(X_i\\).\n\n\n\n\n\n\nGenetic Matching\n\n\n\n\n\n\nWeighting Estimator\n\nIdentification with Weighting\nWe know that the ATE can be written as a weighted average, as shown in Equation 4 . We can rewrite the \\(\\tau_{ATE}\\) as follows using observed potential outcomes outcomes and conditional ignorability ( Equation 1 ).\n\\[\n\\begin{split}\n& = \\sum\\limits_{x \\in \\mathcal X} \\underbrace{(E(Y_{1i}|D_i = 1, X_i = x)}_{\\because \\text{ observed}} - \\underbrace{E(Y_{0i}|D_i = 0, X_i = x)}_{\\because \\text{ observed}})Pr(X_i = x) \\\\\n& = \\sum\\limits_{x \\in \\mathcal X}  (\\underbrace{E(Y_{1i}|X_i = x)}_{\\because \\text{ eq. (1)}} - \\underbrace{E(Y_{0i}|X_i = x)}_{\\because \\text{ eq. (1)}})Pr(X_i = x) \\\\\n& = \\underbrace{E[E(Y_{1i}|X_i = x) - E(Y_{0i}|X_i = x)]}_{\\text{definition of weighted average}}\n\\end{split}\n\\]\nLet us do an algebra trick - multiply both terms within the CATE by 1 (in blue):\n\\[\n\\begin{split}\n& = E \\left [E(Y_{1i}|X_i=x) \\color{blue}{\\frac{\\pi(X_i)}{\\pi(X_i)}}\\color{black} - (E(Y_{0i}|X_i=x) \\color{blue}{\\frac{1-\\pi(X_i)}{1-\\pi(X_i)}} \\right] \\\\\n& \\color{black} = E \\left[ \\frac{E(Y_{1i}|X_i = x) \\pi(X_i)}{\\pi(X_i)} -  \\frac{E(Y_{0i}|X_i = x) (1-\\pi(X_i))}{1-\\pi(X_i)} \\right]\n\\end{split}\n\\]\nWe know that propensity score \\(\\pi(X_i) := E(D_i|X_i = x)\\). Thus, we can convert the above to:\n\\[\n\\begin{split}\n& = E \\left[ \\frac{E(Y_{1i}|X_i = x)E(D_i|X_i = x)}{\\pi(X_i)} - \\frac{E(Y_{0i}|X_i = x)(1-E(D_i|X_i = x))}{1-\\pi(X_i)}\\right] \\\\\n& = E \\left[ \\frac{E(Y_{1i}|X_i = x)E(D_i|X_i = x)}{\\pi(X_i)} - \\frac{E(Y_{0i}|X_i = x)E(1-D_i|X_i = x)}{1-\\pi(X_i)}\\right]\n\\end{split}\n\\]\n\\[\n\\begin{align}\n& = E \\left[ \\frac{E(Y_{1i}D_i|X_i = x)}{\\pi(X_i)} - \\frac{E(Y_{0i}(1-D_i)|X_i = x)}{1 - \\pi(X_i)}\\right] && (\\text{property of expectation})\\\\\n& = E \\left[ E \\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} |X_i = x\\right) - E \\left( \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)} | X_i = x \\right) \\right] && (\\text{property of expectation})\\\\\n& = E\\left[ E\\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} - \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)} |X_i = x \\right) \\right]  && (\\text{property of expectation})\n\\end{align}\n\\]\n\\[\n\\begin{align}\n& = E\\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} - \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)}\\right) && (\\text{LIE: } E(X) = E[E(X|Y)] \\ ) \\\\\n& = E\\left( \\frac{Y_{i}D_i}{\\pi(X_i)} - \\frac{Y_{i}(1-D_i)}{1-\\pi(X_i)}\\right) && (\\text{observered outcome}) \\\\\n& = E \\left( \\frac{\\color{blue}{Y_i} \\color{black}D_i(1-\\pi(X_i))-\\color{blue}{Y_i}\\color{black}(1-D_i)\\pi(X_i)}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{getting common denom.}) \\\\\n& = E\\left( Y_i \\frac{D_i(1-\\pi(X_i))-(1-D_i)\\pi(X_i)}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{factor out }Y_i) \\\\\n& = E\\left( Y_i \\frac{D_i - D_i\\pi(X_i)-(\\pi(X_i) -D_i\\pi(X_i))}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{distribute out}) \\\\\n&  = E\\left( Y_i \\frac{D_i \\color{blue}{- D_i\\pi(X_i)} \\color{black}-\\pi(X_i) \\color{blue}{+ D_i\\pi(X_i)}}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{distribute out negative})\\\\\n& = E\\left( Y_i \\frac{D_i -\\pi(X_i) }{\\pi(X_i)(1-\\pi(X_i))}\\right) && (\\text{cancel out})\\\\\n\\end{align}\n\\tag{7}\\]\nAnd thus, we have identified the ATE.\n\n\n\nInverse Probability Weighting Estimator\nAn alternative use of propensity scores is weighting. As shown above, under conditional ignorability and common support, we can identify the ATE as:\n\\[\n\\tau_{ATE} = E\\left[ Y_i \\times \\underbrace{\\frac{D_i - \\pi(X_i)}{\\pi(X_i) (1 - \\pi(X_i))}}_{\\text{weight}}\\right]\n\\]\nThe inverse probability weighting (IPW) estimator is the sample estimator:\n\\[\n\\begin{split}\n\\hat\\tau_{ATE} & = \\frac{1}{N} \\sum\\limits_{i=1}^N \\left(Y_i \\frac{D_i - \\hat\\pi(X_i)}{\\hat\\pi(X_i) (1 - \\hat\\pi(X_i))} \\right) \\\\\n& = \\frac{1}{N} \\sum\\limits_{i=1}^N \\left(\\frac{D_i Y_i}{\\hat\\pi(X_i)} - \\frac{(1-D_i) Y_i}{1 - \\hat\\pi(X_i)} \\right)\n\\end{split}\n\\]\n\nThe second equation is equivalent to the first, shown by Equation 7 .\n\nEssentially, those who are unlikely to be treated but do get treated get weighted more, and individuals who are likely to be treated but do not get treated get weighted more.\n\n\n\n\n\n\nWeighting Estimator for ATT\n\n\n\n\n\nThe identification of the ATT under both conditional ignorability and common support are:\n\\[\n\\tau_{ATT} = \\frac{1}{Pr(D = 1)} \\times E\\left[ Y_i \\times \\underbrace{\\frac{D_i - \\pi(X_i)}{(1 - \\pi(X_i))}}_{\\text{weight}}\\right]\n\\]\nThe sample IPW estimator would be:\n\\[\n\\begin{split}\n\\hat\\tau_{ATT} & = \\frac{1}{N_1}\\sum\\limits_{i=1}^N \\left( Y_i \\frac{D_i - \\hat\\pi(X_i)}{1 - \\hat\\pi(X_i)} \\right) \\\\\n& = \\frac{1}{N_1} \\sum\\limits_{i=1}^N \\left( D_iY_i - (1-D_i)Y_i \\frac{\\hat\\pi(X_i)}{1 - \\hat\\pi(X_i)} \\right)\n\\end{split}\n\\]\n\n\n\nThe IPW estimator is asymptotically consistent, but has very poor small sample properties. They are highly sensitive to extreme values of \\(\\hat\\pi(X_i)\\). This generates high variance (inefficiency), and can produce significant bias under model mispecification.\n\n\n\n\n\n\nFalsification Tests\n\nTesting Assumptions with Falsification\nThe stronger (bolder) our assumptions for identification, the less credible our results are. Selection on Observables involves a very strong and hard to verify assumption: conditional ignorability. Can we really be sure that we have controlled for all confounders \\(X_i\\) needed to satisfy conditional ignorability?\nPlacebo tests are a type of falsification test to show evidence against our assumptions. Suppose that we make the assumption of conditional ignorability \\((Y_{0i}, Y_{1i}) \\perp D_i | X_i\\). Suppose we are concerned about the presence of another confounder \\(U\\) that is not included in \\(X_i\\).\n\n\n\n\n\nThe presence of \\(U\\) will falsify our conditional ignorability assumption, and means we cannot identify the causal effect of \\(D \\rightarrow Y\\).\n\n\n\n\n\n\nFalsification vs. Validation\n\n\n\n\n\nFalsification is a principle of trying to criticise our own research, rather than defend it. Falsification is about testing if our assumptions are not met. Failing a test provides evidence that our assumption is not met.\n\nEx. Covariates are balanced - thus there is no evidence that our assumptions are not met. We are not saying that our assumption is correct, just that there is no evidence against it.\n\nValidation is the opposite - we test to see if there is evidence in favour of our assumptions.\n\nEx. Covariates are balanced - thus our assumptions are met.\n\n\n\n\nFor falsification tests, we should not just pay attention to statistical significance - we must also pay attention to the magnitude of the point estimation.\n\n\n\nPlacebo Outcome Test\nA placebo outcome test utilities another alternative outcome variable \\(Y'\\) that is caused by our hypothesised unobserved confounder \\(U\\):\n\n\n\n\n\nWe can see that if \\(U\\) does not exist, \\(D\\) should have zero effect on the new outcome \\(Y'\\). Thus, if \\(U\\) is present, we should find a relationship between \\(D\\) and \\(Y'\\).\n\\[\nY'_i = \\gamma + \\delta D_i + u_i\n\\]\n\nIf we find that there is an effect of \\(D\\) on the new outcome \\(Y'\\) (non-zero \\(\\delta\\)), that is evidence that \\(U\\) exists, and is evidence to reject our conditional ignorability assumption (falsifies our design - red flag!).\nIf you do not find an effect of \\(D\\) on new outcome \\(Y'\\) (\\(\\delta = 0\\)) you find no evidence of \\(U\\), and no evidence to reject our conditional ignorability assumption (fails to falsify our design).\n\nWe must be sure that \\(Y\\) is not related to \\(Y'\\) except through \\(D\\) and \\(U\\). If this is true, we just run our original research design but replace \\(Y\\) with \\(Y'\\).\n\n\n\nPlacebo Treatment Test\nA placebo treatment test involves some other treatment \\(D'\\), that was assigned at the same time\n\n\n\n\n\nWe can see that if \\(U\\) does not exist, the effect of \\(D'\\) should have no effect on \\(Y\\). If \\(U\\) does exist, there should be some effect of \\(D'\\) on \\(Y\\).\n\\[\nY_i = \\gamma + \\delta D'_i + u_i\n\\]\n\nIf we find that there is an effect of \\(D'\\) on \\(Y\\) (non-zero \\(\\delta\\)), that is evidence that \\(U\\) exists, and is evidence to reject our conditional ignorability assumption (falsifies our design - red flag!).\nIf you do not find an effect of \\(D'\\) on \\(Y\\) (\\(\\delta = 0\\)) you find no evidence of \\(U\\), and no evidence to reject our conditional ignorability assumption (fails to falsify our design).\n\nWe must be sure that \\(Y\\) is not related to \\(D'\\) except through \\(D\\) and \\(U\\). If this is true, we just run our original research design but replace \\(D\\) with \\(D'\\).\n\n\n\n\n\n\nPartial Identification\n\nDecomposing the ATE\nWith falsification, we were concerned with what assumptions we needed to be not-false in order to identify the ATE. However, we can take a different approach - what can we learn about the ATE without any assumptions?\nLet us decompose the ATE into parts:\n\\[\n\\begin{align}\n\\tau_{ATE}  = & E(Y_{1i} - Y_{0i}) \\\\\n& \\\\\n= & E(Y_{1i} - Y_{0i} | D_i = 1) Pr(D_i = 1) \\\\\n& \\quad - E(Y_{1i} - Y_{0i}|D_i = 0)Pr(D_i = 0)  && (\\text{def. of weighted avg.})\\\\\n& \\\\\n= & [ \\color{blue}{E(Y_i |D_i = 1)} \\color{black}- \\color{red}{E(Y_{0i}|D_i = 1)} \\color{black}] Pr(D_i = 1) \\\\\n& \\quad + [ \\color{red}{E(Y_{1i}|D_i = 0)} \\color{black} - \\color{blue}{E(Y_i|D_i = 0)} \\color{black}]Pr(D_i = 0) && (\\text{observed + unobserved})\n\\end{align}\n\\]\nSome of the quantities are observed (in blue), and some of the quantities are unobserved (in red). Previously, we made assumptions (conditional ignorability, common support) to fill the unobserved quantities. But, we can make actually any assumption as possible.\n\n\n\nNonparametric Bounds\nOne way to fill in our unobserved outcomes through the “best” and “worst” possible outcomes. This allows us to construct a plausible range of the ATE.\nFirst, let us construct the worst-case scenario - the lowest possible \\(\\tau\\).\n\n\\(E(Y_{0i}|D_i = 1) = Y_H\\). Units in the treated \\(D_i=1\\), their potential outcome \\(Y_{0i}\\) will be the highest \\(Y\\) possible, \\(Y_H\\).\n\\(E(Y_{1i}|D_i = 0) = Y_L\\). Units in the control \\(D_i=0\\), their unobserved potential outcome \\(Y_{1i}\\) will be the lowest \\(Y\\) possible, \\(Y_L\\).\n\nThus, the lowest possible \\(\\tau\\) (sharp lower bound) is:\n\\[\n\\begin{split}\n\\tau_L = & [ \\color{blue}{E(Y_i |D_i = 1)} \\color{black}  - \\color{red}{Y_H} \\color{black}] Pr(D_i = 1) \\\\\n& \\quad + [ \\color{red}{Y_L} \\color{black} - \\color{blue}{E(Y_i|D_i = 0)} \\color{black}]Pr(D_i = 0)\n\\end{split}\n\\]\nNow, let us construct the best-case scenario - the highest possible \\(\\tau\\).\n\n\\(E(Y_{0i}|D_i = 1) = Y_L\\). Units in the treated \\(D_i=1\\), their potential outcome \\(Y_{0i}\\) will be the lowest \\(Y\\) possible, \\(Y_L\\).\n\\(E(Y_{1i}|D_i = 0) = Y_H\\). Units in the control \\(D_i=0\\), their unobserved potential outcome \\(Y_{1i}\\) will be the highest \\(Y\\) possible, \\(Y_H\\).\n\nThus, the highest possible \\(\\tau\\) (sharp upper bound) is:\n\\[\n\\begin{split}\n\\tau_H = & [ \\color{blue}{E(Y_i |D_i = 1)} \\color{black}  - \\color{red}{Y_L} \\color{black}] Pr(D_i = 1) \\\\\n& \\quad + [ \\color{red}{Y_H} \\color{black} - \\color{blue}{E(Y_i|D_i = 0)} \\color{black}]Pr(D_i = 0)\n\\end{split}\n\\]\nWe know that the true \\(\\tau_{ATE} \\in [\\tau_L, \\tau_H]\\).\n\n\n\nMonotone Treatment Selection Assumption\nOur extreme case from above is not very useful. However, we can layer on assumptions to lower the possible \\(\\tau\\) values.\nOne assumption is the Monotone Treatment Selection (MTS) assumption. This assumption basically says that potential outcomes for units in treatment, are always higher than for those in the control.\n\\[\n\\begin{split}\n& E(Y_{0i}|D_i = 0) ≤ \\overbrace{E(Y_{0i}|D_i = 1)}^{\\text{unobserved}} \\\\\n& \\underbrace{E(Y_{1i} |D_i = 0)}_{\\text{unobserved}} ≤ E(Y_{1i} |D_i = 1)\n\\end{split}\n\\]\nThis is basically saying that selection bias is one-direction.\nThis implies a tighter sharp upper bound on \\(\\tau\\).\n\\[\n\\begin{align}\n\\tau_H = & [ \\underbrace{E(Y_i |D_i = 1)}_{\\text{observed}}  - \\color{red}{E(Y_{0i}|D_i = 0)} \\color{black}] Pr(D_i = 1) \\\\\n& \\quad + [ \\color{red}{E(Y_{1i}|D_i =1)} \\color{black} - \\underbrace{E(Y_i|D_i = 0)}_{\\text{observed}}]Pr(D_i = 0) \\\\\n= & E(Y_i|D_i = 1) - E(Y_i|D_i = 0) && (\\text{def. of weighted avg.})\\\\\n\\end{align}\n\\]\nThis indicates that the upper bound of plausible \\(\\tau_{ATE}\\) values is the naive estimator of differences in observed outcomes.\nWe can also make the reverse assumption, where selection bias is in the opposite direction. This means a tighter sharp lower bound \\(\\underline\\tau\\). These assumptions help narrow our possible \\(\\tau_{ATE}\\) values, and can allow us to test if our estimated \\(\\hat\\tau\\) is reasonable (within the plausible bounds).\n\n\n\n\n\n\nImplementation in R\nFor all methods, you will need the tidyverse package:\n\nlibrary(tidyverse)\nlibrary(MatchIt)\nlibrary(estimatr)\n\nSee how to perform each estimator in R:\n\n\n\n\n\n\nDistance Matching\n\n\n\n\n\nFirst, let us conduct nearest neighbour matching with Mahalanobis distance by using the matchit() function.\n\nmatch_object = MatchIt::matchit(D ~ X1 + X2 + X3,\n                                data = my_data,\n                                method = \"nearest\", #distance matching\n                                distance = \"mahalanobis\")\n\n# for output summary\nsummary(match_object)\n\nSecond, let us save the matched data with the match.data() function.\n\nmatch_data &lt;- MatchIt::match.data(match_object,\n                                  weights = 'nn_weights')\n\nThird, we can test if matching worked by using a balance table and a love plot:\n\n# balance table\ncobalt::bal.tab(D ~ X1 + X2 + X3, \n                data = match_data, # from the 2nd step\n                weights = \"nn_weights\",\n                disp = c(\"means\", \"sds\"))\n\n#love plot\ncobalt::love.plot(match_object,\n                  data = my_data, #original dataset\n                  stars = 'raw')\n\nFinally, we can estimate the treatment effect. There are two options - either using a weighted regression, or using the matching algorithm:\n\n# using weighted regression\nestimate &lt;- lm_robust(Y ~ D,\n                      data = match_data, #data from step 2\n                      weights = nn_weights)\nsummary(estimate)\n\n## using the Matching package:\nestimate = Matching::Match(Y = my_data$Y, #outcome\n                           Tr = my_data$D, #treatment\n                           X = my_data[,c(\"X1\", \"X2\", \"X3\")], #covariates\n                           M=1, #number of neighbours\n                           BiasAdjust = TRUE, #for biased adjustment\n                           Weight = 2)\nsummary(estimate)\n\nYou will have the estimates that you can use.\n\n\n\n\n\n\n\n\n\nPropensity Score Matching\n\n\n\n\n\nFirst, we want to estimate propensity scores with a logistic regression (or a random forest):\n\n#logistic model\npscore_model = glm(D ~ X1 + X2,\n                   data = my_data,\n                   family = \"binomial\")\n\n# estimate propensity scores\nmy_data$pscore_estimate = predict(pscore_model,\n                                  my_data,\n                                  type = \"response\")\n\nNow, let us match with propensity scores:\n\n# match\nmatch_object = MatchIt::matchit(D ~ pscore_estimate,\n                                data = my_data,\n                                method = \"nearest\",\n                                distance = \"Mahalanobis\")\n\n# save matched data\nmatch_data &lt;- MatchIt::match.data(match_object,\n                                  weights = 'pscore_weights')\n\nThird, we can test if matching worked with a balance table and a love plot:\n\n#balance table\ncobalt::bal.tab(D ~ X1 + X2 + X3,\n                data = match_data, #matched data from step 2\n                weights = \"pscore_weights\",\n                disp = c(\"means\", \"sds\"))\n\n#love plot\ncobalt::love.plot(match_object,\n                  data = my_data, #original dataset\n                  addl = ~ X1 + X2 + X3,\n                  stars = 'raw')\n\nFinally, let us do the estimation:\n\nestimate &lt;- lm_robust(Y ~ D,\n                      data = match_data, #from step 2\n                      weights = pscore_weights)\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nInverse Probability Weighting\n\n\n\n\n\nFirst, we want to estimate propoensity scores with a logistic regression (or a random forest):\n\n#logistic model\npscore_model = glm(D ~ X1 + X2,\n                   data = my_data,\n                   family = \"binomial\")\n\n# estimate propensity scores\nmy_data$pscore_estimate = predict(pscore_model,\n                                  type = \"response\")\n\nSecond, we calculate the inverse probability weights based on the formula from earlier:\n\nmy_data$ipweight = ifelse(my_data$D == 1, # condition\n                       1/my_data$pscore_estimate,\n                       1/(1-my_data$pscore_estimate))\n\nFinally, we can estimate the ATE, or ATT, or use a weighted regression for the ATE:\n\n# ATE estimator\nmean((my_data$D * my_data$Y) * my_data$ipweight - ((1 - my_data$D) * my_data$Y) * my_data$ipweight)\n\n# ATT estimator\nsum(my_data$D * my_data$Y - (1 - my_data$D) * my_data$Y * (my_data$pscore_estimate/(1 - my_data$pscore_estimate)))/sum(my_data$D)\n\n# ATE with weighted regression\nestimate &lt;- lm_robust(Y ~ D, \n                      data = my_data,\n                      weights = ipweight)\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nOLS Estimator\n\n\n\n\n\nFor the OLS estimator, we can use the lm_robust() function:\n\nestimate &lt;- lm_robust(Y ~ D + X1 + X2 + X3,\n                      data = my_data)\nsummary(estimate)\n\nWe can also use the fixest package and the feols() function:\n\nlibrary(fixest)\n\nestimate &lt;- feols(Y ~ D + X1 + X2 + X3,\n                  data = my_data,\n                  se = \"hetero\")\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nFully Interacted Estimator\n\n\n\n\n\nFor the fully interacted estimator, we can use the lm_lin() function.\n\nestimate &lt;- lm_lin(Y ~ D,\n                   covariates = ~ X1 + X2 + X3,\n                   data = my_data)\nsummary(estimate)\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "6 Selection on Observables"
    ]
  },
  {
    "objectID": "quant2.html",
    "href": "quant2.html",
    "title": "Causal Frameworks",
    "section": "",
    "text": "In the last chapter, we focused on the classical linear regression model, and how it can explain relationships (correlations) between variables.\nThis chapter introduces the main causal frameworks (potential outcomes, causal graphs), the main causal estimands used in causal inference, and the idea of selection bias and confounders. This chapter is the foundation in which the next set of methods for causal inference will be built on.\nUse the right sidebar for quick navigation. R-code for causal diagrams is provided at the bottom.\n\n\nPotential Outcomes Framework\n\nTreatment Variable\nIn causal inference, we are interested in how treatment \\(D\\) causes outcome variable \\(Y\\). How does a voting system affect voter turnout? How does a minimum wage law affect unemployment?\n\\(D\\) is our treatment variable. The indicator of treatment for each unit \\(i\\) is \\(D_i\\).\n\\[\nD_i = \\begin{cases}\n1 \\quad \\text{if unit } i \\text{ recieved the treatment} \\\\\n0 \\quad \\text{if unit } i \\text{ did not recieve the treatment}\n\\end{cases}\n\\]\nCausal variables/treatments \\(D\\) must occur before the outcome \\(Y\\). A variable cannot cause something to occur in the past.\nCausal variables/treatments must be able to be manipulated (in order to imagine a world where the treatment did not occur). For example, \\(D\\) cannot be sex assigned at birth, ethnicity, etc. For example, major global events (how did 9/11 cause the Arab spring?)\n\n\n\nPotential Outcomes\nImagine there are two hypothetical parallel worlds - one where unit \\(i\\) receives the treatment \\(D\\), and one where unit \\(i\\) does not receive the treatment \\(D\\). Everything else in these worlds is identical. Unit \\(i\\) has \\(Y\\) values in both of these parallel worlds.\nPotential Outcomes for unit \\(i\\) are denoted:\n\\[\nY_{di}, Y_i(d) =\\begin{cases}\n& Y_{1i}, \\ Y_i(1) \\quad \\text{Outcome for unit } i \\text{ when } D_i = 1 \\\\\n& Y_{0i}, \\ Y_i(0) \\quad \\text{Outcome for unit } i \\text{ when } D_i = 0 \\\\\n\\end{cases}\n\\]\nFor example, imagine we are interested in finding the effect of democracy \\(D\\) on GDP growth \\(Y\\). Potential outcome \\(Y_{1i}\\) is the potential GDP growth of country \\(i\\) if they were a democracy, and outcome \\(Y_{0i}\\) is the potential GDP growth of a country \\(i\\) if they were not a democracy.\nThe reason we want hypothetical worlds is that since these two parallel worlds are identical except for treatment \\(D_i\\), then the differences between the two parallel worlds must be caused by the treatment \\(D_i\\).\nNote that these are hypothetical worlds - we will obviously not observe both of them at the same time. The next section discusses this.\n\n\n\nObserved Outcomes and “Missing Data”\nOf course, there is not two parallel worlds with 2 potential outcomes. In the real world, each unit \\(i\\) either receives treatment \\(D\\), or does not. We do not observe the other potential outcome.\n\\(Y_i\\) is the observed outcome for unit \\(i\\). This is given by formula:\n\\[\nY_i = D_i \\cdot Y_{1i} + (1-D_i) \\cdot Y_{0i}\n\\]\nIf we plug in \\(D_i = 0, 1\\) to the equation above, we get the observed outcomes:\n\\[\nY_i = \\begin{cases}\nY_{1i} \\quad \\text{if } D_i = 1 \\\\\nY_{0i} \\quad \\text{if } D_i = 0 \\\\\n\\end{cases}\n\\]\nBefore the treatment (A priori), both potential outcomes could be observed. After the treatment, one is observed, and the other is counterfactual. For any given experiment, only one will ever be seen, and the counterfactual will never be seen. This is a “missing data” problem.\n\n\n\n\n\n\nNeyman Urn Model\n\n\n\n\n\nPotential Outcomes can be visualised with the Neyman Urn Model.\nBefore the treatment, we have a box (we cannot see) with both potential outcomes.\n\n\n\n\n\nThen, when we apply treatment, we stick our hand into the box that we cannot see, and pull out one observed outcome.\n\n\n\n\n\nWe are essentially sampling from potential outcomes to get observed outcomes.\n\n\n\nThis missing data problem is called the fundamental problem of causal inference.\n\n\n\nStable Unit Treatment Value Assumption\nThe above given observed and potential outcome frameworks depends on the Stable Unit Treatment Value Assumption (SUTVA).\n\\[\n\\begin{align}\nY_{(D_1, D_2, \\dots, D_N)i} & = Y_{(D_1', D_2', \\dots, D_N')i} \\\\\nY_{di} \\text{ under current randomisation} & = Y_{di} \\text{ under all other randomisations}\n\\end{align}\n\\]\nOr more intuitively, the potential outcomes of unit \\(i\\) only depends on their own treatment status, and no other unit’s treatment status. Thus, changing everyone else’s treatment status has no effects on unit \\(i\\)’s potential outcomes \\(Y_{di}\\). The treatment is also the same for everyone (treatment is stable and consistent)\nSUTVA can be violated in many different ways:\n\nSpill-over effects: If we are testing a new curriculum, one student \\(j\\) getting the new curriculum may teach their friend \\(i\\) the new curriculum, thus affecting the potential outcomes of \\(i\\).\nContagion: If we are studying a disease, diseases can spread, so another unit \\(j\\) getting a disease affects the potential outcomes of unit \\(i\\).\nDilution: If we are studying vaccines - there is herd immunity - other people getting the vaccine also reduces our chances of getting the disease.\nVariable levels of treatment: If we are doing a drug trial, if some people got two doses, while others only got one dose. This is not a consistent treatment.\nTechnical errors: If someone who is supposed to be treated accidentally is not treated. This is not a consistent treatment.\n\nWhen SUTVA is violated, potential outcomes become very messy, and we no longer have the neat framework as before.\n\n\n\n\n\n\nCausal Estimands\n\nIndividual Treatment Effect\nRemember the potential outcomes from parallel worlds \\(Y_{1i}\\) and \\(Y_{0i}\\).\nSince these two parallel worlds are identical except for the fact one receives the treatment \\(D\\) and the other does not, the causal effect of \\(D\\) should be the difference between the potential outcomes of these two worlds. Thus, the individual treatment effect \\(\\tau\\) of a unit \\(i\\) is:\n\\[\n\\tau_i = Y_{1i} - Y_{0i}\n\\]\nThis is the specific treatment effect for a specific unit \\(i\\). This cannot be observed, because we do not see both potential outcomes for the same unit \\(i\\).\nThis is also very hard to estimate, as we cannot reliably fill in the missing potential outcome for any one unit \\(i\\). Thus, we almost never use individual treatment effects, and use group treatment effects.\n\n\n\nAverage Treatment Effect (ATE)\nWe have a hard time estimating individual treatment effects. So let us consider another form of causal estimand: group level estimands. Consider a population of units \\(i = 1, \\dots, N\\). The population has potential outcomes represented in two (only partially observed) vectors:\n\\[\n\\begin{split}\n& Y_1 = (Y_{11}, Y_{12}, \\dots, Y_{1N}) \\\\\n& Y_0 = (Y_{01}, Y_{02}, \\dots, Y_{0N})\n\\end{split}\n\\]\nWe compare these two vectors of potential outcomes. The most common way to do this is to use their expected values.\nThe Average Treatment Effect is defined as:\n\\[\n\\begin{split}\n\\tau_{ATE} & = \\E(Y_{1i} - Y_{0i}) \\\\\n& = \\underbrace{\\frac{1}{N} \\sum\\limits_{i=1}^N (Y_{1i} - Y_{0i})}_{\\text{a formula for average}}\n\\end{split}\n\\]\nWe cannot calculate this with observed data - since we need all potential outcomes to do this. However, we can creatively estimate this quantity through a number of research designs and assumptions, which we will explore throughout causal inference.\n\n\n\nAverage Treatment Effect on the Treated (ATT)\nAn alternative estimand to the ATE is the Average Treatment Effect on the Treated (ATT):\n\\[\n\\begin{split}\n\\tau_{ATT} & = \\E(Y_{1i} - Y_{0i} \\ | \\ D_i = 1) \\\\\n& = \\underbrace{\\frac{1}{N_1} \\sum\\limits_{i=1}^N D_i (Y_{1i} - Y_{0i}) \\quad  \\text{where } N_1 = \\sum\\limits_{i=1}^ND_i}_{\\text{a formula for the average only for treated units}}\n\\end{split}\n\\]\nThis is the causal effect of only units who have received the treatment. Note that frequently the ATT is not equal to the ATE, so be aware of which estimand you are trying to estimate/identify.\nSometimes, \\(\\tau_{ATT} = \\tau_{ATE}\\). This occurs when the expectation of the potential outcomes of both the treated and control are the same, then the two equal each other.\nThe opposite is also true: if the expectation of the potential outcomes of both the treated and control are different, then the two are not equal.\nThe opposite estimand is the Average Treatment effect on the Untreated (ATU), which only measures the causal effect of units who did not recieve the treatment.\nThis is not used very often, since it is kind of uninituive to think about treatment effects on individuals who did not recieve treatment. However, it can be useful in understanding identification assumptions.\n\n\n\nConditional Average Treatment Effect (CATE)\nThe conditional average treatment effect is any treatment effect where there is a condition on a characteristic/covariate:\n\\[\n\\tau_{CATE}(x) = \\E(Y_{1i} - Y_{0i} \\ | \\ \\underbrace{X_i = x)}_{\\text{condition}}\n\\]\nThis is the causal effect of only variables who meet the condition of the covariate specified. For example, you could find the conditional average treatment effect of only women (so the covariate which we are conditioning on is gender). You can also condition on multiple covariates.\nThis is often used for tailoring products/medicine/advertising to certain groups of people. It is also frequently used in identification strategies.\nThis estimand will go by other names, including the Local Average Treatment Effect (LATE).\n\n\n\n\n\n\nSelection Bias and Confounders\n\nNaive Estimator and Selection Bias\nA natural way to estimate the ATE is to use a naive estimator: find the average difference of observed outcomes. This is called the naive estimator:\n\\[\n\\hat\\tau_{naive} = \\underbrace{\\E(Y_i|D_i = 1)}_{\\text{for treated}} - \\underbrace{\\E(Y_i|D_i = 0)}_{\\text{for control}}\n\\]\nHowever, there is an issue - we can show this with algebra:\n\\[\n\\begin{align}\n\\hat\\tau_{naive} & = \\E(Y_i|D_i = 1) - \\E(Y_i|D_i = 0) \\\\\n& = \\underbrace{\\E(Y_{1i}|D_i = 1) - \\E(Y_{0i} | D_i = 0)}_{\\because \\text{ observed potential outcomes}} \\\\\n& = \\E(Y_{1i}|D_i = 1) - \\E(Y_{0i} | D_i = 0) + \\underbrace{\\E(Y_{0i}|D_i = 1) \\color{red}{- \\E(Y_{0i}|D_i = 1)}}_{\\because \\text{ this equals 0, so we can add it}} \\\\\n& = \\underbrace{\\E(Y_{1i}|D_i = 1) \\color{red}{- \\E(Y_{0i}|D_i = 1)}}_{\\tau_{ATT}} + \\underbrace{\\E(Y_{0i}|D_i = 1) - \\E(Y_{0i} | D_i = 0)}_{\\text{Selection Bias}} \\\\\n\\end{align}\n\\]\nWe can see that our naive estimator produces the \\(\\tau_{ATT}\\) plus an extra bit (called the selection bias). Thus, our naive estimator is biased, so we should be careful about using this naive estimator (correlation does not equal causation).\n\n\n\n\n\n\nNaive Estimator Biased for ATU\n\n\n\n\n\nThe proof above shows how the naive estimator is a biased estimator for the \\(\\tau_{ATT}\\). We can also prove it is a biased estimator of the ATU:\n\\[\n\\begin{split}\n\\hat\\tau_{naive} & = \\E(Y_i|D_i = 1) - \\E(Y_i|D_i = 0) \\\\\n& = \\underbrace{\\E(Y_{1i}|D_i = 1) - \\E(Y_{0i} | D_i = 0)}_{\\because \\text{ observed potential outcomes}} \\\\\n& = \\E(Y_{1i}|D_i = 1) - \\E(Y_{0i} | D_i = 0) + \\underbrace{\\E(Y_{1i}|D_i = 0) - \\E(Y_{1i}|D_i = 0)}_{\\because \\text{ this equals 0, so we can add it}} \\\\\n& = \\underbrace{\\E(Y_{1i}|D_i = 0)- \\E(Y_{0i}|D_i = 0)}_{\\tau_{ATU}} + \\underbrace{\\E(Y_{1i}|D_i = 1) - \\E(Y_{1i} | D_i = 0)}_{\\text{Selection Bias}} \\\\\n\\end{split}\n\\]\n\n\n\n\n\n\n\n\n\nNaive Estimator Biased for ATE\n\n\n\n\n\nThe proofs above shows how the naive estimator is a biased estimator for the \\(\\tau_{ATT}\\) and \\(\\tau_{ATU}\\). We can also prove it is a biased estimator of the ATE.\nLet us first start with the ATE. Let us call \\(Y_{1i} - Y_{0i} := \\tau_i\\) for notation simplicity:\n\\[\n\\begin{align}\n\\tau_{ATE} & = \\E(Y_{1i} - Y_{0i})  = \\E(\\tau_i)\\\\\n& = \\underbrace{\\E(\\tau_i|D_i = 1)\\P(D_i = 1) + \\E(\\tau_i|D_i = 0)\\P(D_i = 0)}_{\\because \\text{ weighted average of ATE and ATU by proportion}} \\\\\n& = \\E(\\tau_i|D_i = 1) \\underbrace{(1 -\\P(D_i = 0))}_{\\because \\text{ complement prob.}} + \\E(\\tau_i|D_i = 0)\\P(D_i = 0) \\\\\n\\end{align}\n\\]\nLet us call \\(\\P(D_i = 0) := \\pi\\) for notation simplicity. Now, continue:\n\\[\n\\begin{split}\n& = \\underbrace{\\E(\\Delta|D_i = 1) - \\pi \\E(\\Delta|D_i = 1)}_{\\because \\text{ distribute out}} + \\E(\\tau_i|D_i = 0)\\pi \\\\\n& = \\E(\\tau_i|D_i = 1) + \\underbrace{\\pi[\\E(\\tau_i|D_i = 0) - \\E(\\tau_i|D_i = 1)]}_{\\because \\ \\pi \\text{ factored out }} \\\\\n& = \\E(Y_{1i} |D_i = 1) - \\E(Y_{0i}|D_i = 1) + \\pi[\\E(\\tau_i|D_i = 0) - \\E(\\tau_i|D_i = 1)] \\\\\n\\end{split}\n\\]\nLet us call the part \\(\\pi[\\E(\\tau_i|D_i = 0) - \\E(\\tau_i|D_i = 1)] := \\Pi(\\tau_i)\\). Now, continue to simplify:\n\\[\n\\begin{split}\n& = \\E(Y_{1i} |D_i = 1) - \\E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) + \\underbrace{\\E(Y_{1i} |D_i = 0) - \\E(Y_{0i}|D_i = 0)}_{\\because \\text{ these two cancel out so we add 0}}  \\\\\n& = \\E(Y_{1i} |D_i = 1) - \\E(Y_{0i}|D_i = 0) + \\E(Y_{1i} |D_i = 0)- \\E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& = \\underbrace{\\E(Y_i|D_i = 1)}_{\\because \\text{ observed outcome}} - \\underbrace{\\E(Y_i|D_i = 0)}_{\\because \\text{ observed outcome}} + \\E(Y_{1i} |D_i = 0)- \\E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& = \\underbrace{\\E(Y_{i} |D_i = 1) - \\E(Y_{i}|D_i = 0)}_{\\hat\\tau_{naive}} + \\E(Y_{1i} |D_i = 0)- \\E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& = \\hat\\tau_{naive}+ \\E(Y_{1i} |D_i = 0)- \\E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i)\n\\end{split}\n\\]\nThus, we can see that \\(\\tau_{ATE}\\) is not equivalent to \\(\\hat\\tau_{naive}\\). Let us isolate \\(\\hat\\tau_{naive}\\) to identify the selection bias.\n\\[\n\\begin{split}\n& \\tau_{ATE} = \\hat\\tau_{naive}+ \\E(Y_{1i} |D_i = 0)- \\E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& -\\hat\\tau_{naive} = -\\tau{ATE} + \\E(Y_{1i} |D_i = 0)- \\E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& \\hat\\tau_{naive} = \\tau_{ATE} - \\E(Y_{1i} |D_i = 0) + \\E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& \\hat\\tau_{naive} = \\tau_{ATE} + \\underbrace{\\E(Y_{0i}|D_i = 1)- \\E(Y_{1i} |D_i = 0) + \\Pi(\\tau_i)}_{\\text{selection bias}}\n\\end{split}\n\\]\n\n\n\n\n\n\nConfounders\nTake the selection bias formula from above:\n\\[\n\\underbrace{\\E(Y_{0i}|D_i = 1)}_{Y_{0i}\\text{ (treated)}} - \\underbrace{\\E(Y_{0i} | D_i = 0)}_{Y_{0i}\\text{ (control)}}\n\\]\nIf selection bias is non-zero, this essentially states that the expected potential outcome before treatment \\(Y_{0i}\\) between the treatment and control groups is not equal.\nOr in other words, the treatment and control groups have some other variable causing differences even before treatment has begun. This implies that the differences between the treatment and control group may not be due to treatment \\(D\\), but due to the underlying differences before treatment even occurred.\nConfounders are variables that cause the differences between treatment and control groups before the treatment has started. Confounders correlate with both the treatment variable and the outcome. If a variable only correlates with \\(D\\) or \\(Y\\), then it is not a confounder. If must correlate with both \\(D\\) and \\(Y\\).\nThis is why correlation does not equal causation - if the treatment and control group are different before we start the experiment, we cannot say the difference between the two is purely a result of treatment \\(D\\).\nWe can also think about confounders in terms of omitted variable bias. The formula for omitted variable bias is:\n\\[\n\\E\\hat{\\beta} = \\beta + \\eta \\delta\n\\]\nWhere \\(\\beta\\) are the true population parameters, \\(\\eta\\) is the relationship between the confounder and treatment, and \\(\\delta\\) is the relationship between confounder and outcome.\nWe can see that if either \\(\\eta = 0\\) or \\(\\delta = 0\\), then there is 0 omitted variable bias - which implies that a confounder that causes bias must be correlated with both the treatment and outcome.\n\n\n\nAssignment Mechanism\nThe Assignment Mechanism is the procedure that determines the treatment status of each unit. In causal inference, we want to restrict the assignment mechanism, in order to remove the effect of selection bias.\nThere are two types of studies that use different assignment mechanisms:\n\nRandomised Experiments: The assignment mechanism is both known, and controlled by the researcher. Generally, the researcher chooses some type of randomisation.\nObservational Studies: The assignment mechanism is not known to, or not under the control of the researcher. This means that confounders may be driving selection into treatment and control, inducing bias.\n\nGenerally, the most credible studies are randomisation studies, since we can control interventions to parse out the effect of confounders. Observational studies generally rely on more assumptions that need to be met, and need to be defended for the study to be credible.\nIn the next chapter, we will start off with Randomised Controlled Trials, which are a form of randomisation experiment. After that, we will start discusses observational studies, and what assumptions we need in these studies to identify the causal effect without selection bias.\n\n\n\n\n\n\nDirected Acyclic Graphs\n\nCausal Diagrams\nCausal Diagrams are a visual way to represent causal theories and frameworks, which allows us to visualise how variables interact with each other. Each Directed Acyclic Graph (DAG) has the following components:\n\nNodes: representing variables (which are also called vertices).\nDirected Edges: Arrows that encode one-way causal theories between variables. For example, we might believe \\(Z\\) causes \\(X\\), so we will draw an arrow from \\(Z\\) to \\(X\\). These connections are observable (solid) or unobservable (dashed).\n\n\n\n\n\n\nIn the figure above, we have two unobserved variables: \\(Q\\) and \\(Y\\) We have three observed variables: \\(Z\\), \\(D\\), and \\(Y\\). We can see the causal theories represented by arrows.\nWe can see the causal effec \\(Z \\rightarrow Y\\) is confounded by \\(W\\): \\(W\\) is affecting who gets treatment \\(Z\\), and causing \\(Y\\). Thus, \\(W\\) is affecting who gets selected into treatment \\(Z\\), and selecting your potential outcome \\(Y\\). Thus, this is an example of selection bias. \\(D \\rightarrow Y\\) is confounded by \\(Q\\). \\(Z \\rightarrow D\\) is not confounded, so we can estimate this causal effect.\n\n\n\nRepresenting Interventions\nTreatments (interventions by the researcher, for example) are when we determine one variable exogenously (such as by randomisation). Or in other words, one variable is determined randomly externally, not caused by any variables within the directed acyclic graphs.\nTreatments are represented by the do() operator. When the treatment is exogenous, we can break all the connections into that variable’s node. This is because we are determining the value of the variable, not any other variables.\n\n\n\n\n\nAn intervention here is on variable \\(D\\). That means the value of \\(D\\) is being chosen outside of this graph (by randomisation, or the researcher). This allows us to delete the arrow between \\(Q \\rightarrow D\\) and \\(Z \\rightarrow D\\). This is because we are exogenously determining \\(D\\), so \\(Q\\) and \\(Z\\) are not determining the value of \\(D\\).\nWith exogenously determined variables, we can find the causal effect that variable is causing on another.\n\n\n\nBlocked Paths\nA set of nodes \\(\\{ \\mathbf S \\}\\) blocks a path \\(p\\) if either:\n\nIf the path \\(p\\) contains at least one arrow-emitting node included in the set of nodes \\(S\\), or\nThe path \\(p\\) contains at least one collision node (multiple arrows point into it) that is outside the set of nodes \\(S\\), and the collision node has no descendant within the set of nodes \\(S\\) (no arrows go out of it to another node).\n\n\n\n\n\n\nTake this above causal diagram. We can see the following:\n\nThe path \\(D \\rightarrow P \\rightarrow Y\\) is blocked by set \\(\\{P\\}\\), because node \\(P\\) is one arrow-emitting node in the path \\(D \\rightarrow P \\rightarrow Y\\).\nThe path \\(D \\leftarrow M \\rightarrow Y\\) is blocked by set \\(\\{M\\}\\), because node \\(M\\) is one arrow-emitting node in the path \\(D \\leftarrow M \\rightarrow Y\\).\n\\(D \\leftarrow Z \\rightarrow M \\rightarrow Y\\) is blocked by \\(\\{M\\}\\), \\(\\{Z\\}\\), or \\(\\{M, Z\\}\\).\n\nBlocking paths is important, since in order to estimate \\(D \\rightarrow Y\\), we need to block any other path between \\(D\\) and \\(Y\\) that is not directly \\(D \\rightarrow Y\\).\n\n\n\n\n\n\nImplementation in R\nThis section will show how you can create DAGs in R. We will need the ggdag and dagitty packages.\n\nlibrary(ggdag)\nlibrary(dagitty)\n\n\n\n\n\n\n\nSimple DAGs with Dagify\n\n\n\n\n\nYou can create a very simple DAG with dagify as follows:\n\ndag_object &lt;- dagify(\n  Y ~ X + D, #Y is caused by X and D\n  D ~ X #D is caused by X\n)\n\nggdag(dag_object) + theme_dag()\n\nThis dag is not very customisable. This can be an issue if you want nodes to be in a specific location. See below for a more customisable DAG.\n\n\n\n\n\n\n\n\n\nCustom DAGs with Dagitty\n\n\n\n\n\nYou can create more complex DAGs with Dagitty. Dagitty allows us to position nodes in a coordinate system, which is useful in some purposes.\n\ndag_object &lt;- dagitty('dag {\n      D [pos = \"0, 1\"]\n      Y [pos = \"2, 1\"]\n      X [pos = \"1, 2\"]\n      \n      D -&gt; Y\n      D &lt;- X -&gt; Y\n  }')\n\nggdag(dag_object) + theme_dag()\n\nThe pos arguments have the coordinates of where you want to put each node.\nBelow are the path connections, where you can use -&gt; and &lt;- to indicate relationships.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "2 Causal Frameworks"
    ]
  },
  {
    "objectID": "quant6.html",
    "href": "quant6.html",
    "title": "Selection on Observables",
    "section": "",
    "text": "In the last chapter, we discussed randomisation. Randomisation is great, but, it requires specific circumstances of the research having control over the assignment mechanism. However, in the social sciences, this rarely occurs.\nThis chapter introduces the selection on observables framework, which allows us to identify causal effects in an observational setting by controlling for observable pre-treatment covariates. We discuss the main estimators, including regression, matching, and weighting.\nUse the right sidebar for quick navigation. R-code provided at the bottom.\n\n\nIdentification\n\nBlocking Backdoor Paths\nWithout randomisation, we need some other way to account for pre-treatment covariates that may be confounding and causing selection bias. Controlling for a set of nodes/confounders \\(X\\) can identify the causal effect of \\(D \\rightarrow Y\\), if:\n\nNo node within set \\(X\\) is a descendant of \\(D\\) (no element within \\(X\\) results from \\(D\\)).\nThe nodes within set \\(X\\) block all back-door paths from \\(D \\rightarrow Y\\).\n\n\n\n\n\n\nIn the figure above, let us block the backdoor paths between \\(D \\rightarrow Y\\):\n\nBackdoor path \\(D \\rightarrow X \\rightarrow Y\\). To block this path, we must control for \\(X\\).\nBackdoor path \\(D \\rightarrow V \\rightarrow Y\\). We do not need to control for \\(V\\), since it is post-treatment (a descendant of \\(D\\)). In fact, \\(V\\) is a bad control (see below).\n\nThus, to identify \\(D \\rightarrow Y\\) here, we only need to control for \\(X\\), and no other variable.\n\n\n\n\n\n\nGood and Bad Controls\n\n\n\n\n\nGood controls block backdoor paths, which facilitate identification of the causal effect.\nBad controls are when we control for post-treatment variables. For example, \\(P\\) below is a bad control, since it is caused by \\(D\\), so it is post-treatment.\n\n\n\n\n\nYou also never want to control variables that only predict \\(D\\). These are bad because controlling for these removes variation in \\(D\\) that could be useful.\nNeutral controls are ones that don’t identify the causal effect, but improve efficiency. For example, \\(Q\\) below affects \\(Y\\), but there is no backdoor path. Controlling \\(Q\\) will not help identification, but can control noise in \\(Y\\) which may increase efficiency.\n\n\n\n\n\n\n\n\n\n\n\nIdentification Assumptions\nOnce we have determined the set of confounders \\(X\\) that we need to control to block all backdoor paths, the assumptions needed for identification of causal effects are:\n\nConditional Ignorability (also known as exogeneity or independence): Among units with identical confounder values \\(X_i\\), treatment \\(D_i\\) is as-if randomly assigned. Or in other words, potential outcomes are independent from treatment within each specific confounder value \\(X_i = x\\).\n\n\\[\n(Y_{0i}, Y_{1i}) \\perp\\!\\!\\!\\!\\perp D_i  \\ | \\ X_i = x, \\quad \\forall \\ x \\in \\mathcal X\n\\]\nThis implies that for any given value of all confounders \\(X_i = x\\), we know that potential outcomes \\(Y_{di}\\) are equivalent between treatment and control:\n\\[\n\\begin{split}\nE(Y_{1i}|X_i = x) = E(Y_{1i}|D_i = 1, X_i = x) = E(Y_{1i}|D_i = 0, X_i = x) \\\\\nE(Y_{0i}|X_i = x) = E(Y_{0i}|D_i = 1, X_i = x) = E(Y_{0i}|D_i = 0, X_i = x)\n\\end{split}\n\\tag{1}\\]\n\nCommon Support: for any unit \\(i\\) with value of \\(X_i\\), there is a non-zero probability that they could be assigned to both control \\(D_i = 0\\) or treatment \\(D_i = 1\\).\n\n\\[\n0 &lt; P(D_i = 1 \\ | X_i = x) &lt; 1 \\quad \\forall \\ x \\in \\mathcal X\n\\]\n\n\n\n\n\n\nExample of Identification Assumptions\n\n\n\n\n\nImagine we have a theory that being abducted \\(D\\) causes turning out to vote.\nBlattman (2009) finds that age is the primary way violent groups chose to abduct individuals: abduction parties released young children and older adults, but kept all adolescent and young males.\nThat means our theory is that age \\(X\\) affects selection into treatment \\(D\\). Young children and older adults are less likely to get abducted \\(D\\), while adolescent and young males are more likely \\(D\\).\n\n\n\n\n\n\nIdentification of the ATE\nWith our assumptions above, we can identify the ATE. We start with the conditional average treatment effect, conditional on some value of confounders \\(X_i = x\\). Note the properties shown in Equation 1 .\n\\[\n\\begin{align}\n\\tau_{CATE}(x) & = E(Y_{1i} - Y_{0i} \\ | \\ X_i = x) \\\\\n& = E(Y_{1i}|X_i = x) - E(Y_{0i}|X_i = x) && (\\text{property of expectation}) \\\\\n& = E(Y_{1i}|D_i = 1, X_i = x) - E(Y_{0i}|D_i = 0X_i = x) &&( \\because \\text{equation (1)} \\ ) \\\\\n& = \\underbrace{E(Y_i|D_i = 1, X_i = x)}_{\\because \\text{ observable}} - \\underbrace{E(Y_i|D_i = 0, X_i = x)}_{\\because \\text{ observable}}\n\\end{align}\n\\tag{2}\\]\nNow, let us discuss the ATE, and plug in the CATE from Equation 2 to identify it:\n\\[\n\\begin{align}\n\\tau_{ATE} & = E(Y_{1i} - Y_{0i}) \\\\\n& = \\int \\underbrace{E(Y_{1i} - Y_{0i} \\ | \\ X_i = x)}_{\\tau_{CATE}(x)} d \\ \\underbrace{P(X_i = x)}_{\\text{weight}} && (\\text{weighted average})\\\\\n& = \\int(\\underbrace{E(Y_i|D_i = 1, X_i) - E(Y_i|D_i = 0, X_i)}_{\\because \\text{ equation (2)}})d \\ P(X_i = x)\n\\end{align}\n\\tag{3}\\]\nThus \\(\\tau_{ATE}\\) is identified as the weighted average of all the CATEs, who themselves are difference-in-means of the observed \\(Y_i\\) at every possible value of \\(X_i = x\\).\nWe assumed that the pre-treatment covariate \\(X\\) is continuous. This is why we need an integral. However, we can simplify this if \\(X\\) is discrete:\n\\[\n\\tau_{ATE} = \\sum\\limits_{x \\in \\mathcal X} ( E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)) P(X_i = x)\n\\tag{4}\\]\n\n\n\nIdentification of the ATT\nWe can weaken conditional ignorability, and still identify the ATT. Only \\(Y_{0i}\\) needs to be independent of \\(D_i\\) for units with the same covariates \\(X_i\\). Or in other words, \\((Y_{0i}) \\perp\\!\\!\\!\\perp D_i | X_i = x\\). This implies:\n\\[\nE(Y_{0i}|X_i = x) = E(Y_{0i}|D_i = 0, X_i = x) = E(Y_{0i}|D_i = 1, X_i = x)\n\\tag{5}\\]\nStart with the conditional ATT, using weakened conditional ignorability from Equation 5 :\n\\[\n\\begin{split}\n\\tau_{CATT}(x) & = E(Y_{1i}-Y_{0i}|D_i = 1, X_i = x) \\\\\n& = E(Y_{1i}|D_i = 1, X_i = x) - E(Y_{0i}|D_i = 1, X_i = x) \\\\\n& = E(Y_{1i}|D_i = 1, X_i = x) - \\underbrace{E(Y_{0i}|D_i = 0, X_i = x)}_{\\because \\text{ equation (5)}} \\\\\n& = \\underbrace{E(Y_i|D_i=1, X_i = x)}_{\\because \\text{ observable}} - \\underbrace{E(Y_1|D_i = 0, X_i x)}_{\\because \\text{ observable}}\n\\end{split}\n\\tag{6}\\]\nNow, look at the ATT, and plug in CATT from Equation 6 to identify it.\n\\[\n\\begin{align}\n\\tau_{ATT} & = E(Y_{1i} - Y_{0i}|D_i = 1) \\\\\n& = \\int \\underbrace{E(Y_{1i} - Y_{0i}|D_i = 1, X_i = x)}_{\\tau_{CATT}(x)}d \\ \\underbrace{P(X_i = x|D_i = 1)}_{P(X_i = x) \\text{ within treated}} \\\\\n& = \\int (\\underbrace{E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)}_{\\because \\text{ equation (6)}})d \\ P(X_i = x|D_i = 1)\n\\end{align}\n\\]\nWe can simplify this if \\(X\\) is discrete:\n\\[\n\\tau_{ATT} = \\sum\\limits_{x \\in \\mathcal X} ( E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)) Pr(X_i = x | D_i = 1)\n\\]\nEven when all assumptions are met for identification of the ATE, the \\(\\tau_{ATE}\\) can be different than the \\(\\tau_{ATT}\\). This is because the weights \\(Pr(X_i = x|D_i = 1)\\) for the ATT are different than the ATE \\(Pr(X_i = x)\\).\n\n\n\n\n\n\nParametric Estimators\n\nOrdinary Least Squares Estimator\nOLS is a natural approach for controlling for confounders \\(X\\), since \\(\\hat\\beta_{OLS}\\) estimates partial out the effects of covariates. OLS is a good estimator of \\(\\tau_{ATE}\\) under 2 conditions:\n\nConstant treatment effect: \\(\\tau_i = Y_{1i} - Y_{0i}\\) for all units \\(i\\).\nLinearity: Potential outcomes are linear, and can be written as:\n\n\\[\nY_i(d) = \\beta_0 + d\\beta_1 + \\mathbf X_i \\gamma + \\varepsilon_i \\quad \\text{for} \\quad d = 0, 1\n\\]\nWhy these conditions? Suppose we have the above linear potential outcomes. We can show:\n\\[\n\\begin{align}\n\\tau_i & = Y_{1i} - Y_{0i} && (\\text{definition of } \\tau_i) \\\\\n& = (\\beta_0 + (1)\\beta_1 + \\mathbf X_i \\gamma + \\varepsilon_i) - (\\beta_0 + (0)\\beta_1 + \\mathbf X_i \\gamma + \\varepsilon_i) && (\\text{plug in } Y_i(1), Y_i(0) \\ )\\\\\n& = (\\beta_0 + \\beta_1 + \\mathbf X_i \\gamma + \\varepsilon_i) - (\\beta_0 + \\mathbf X_i \\gamma + \\varepsilon_i) && (\\text{multiply}) \\\\\n& = \\beta_0 + \\beta_1 + \\mathbf X_i \\gamma + \\varepsilon_i - \\beta_0 - \\mathbf X_i\\gamma - \\varepsilon_i && (\\text{distribute negative sign})\\\\\n& = \\beta_1 && (\\text{cancel out terms})\n\\end{align}\n\\]\nWe also know that conditional ignorability implies zero-conditional mean. Thus \\(\\beta_1\\) is an unbiased and asymptotically consistent estimator of the ATE.\nYou should be cautious using OLS when assumption 2, linearity, is violated. OLS is the best linear estimator, but how far your data is from linearity will determine if the estimator is useful.\n\n\n\n\n\n\nNon-Linearity\n\n\n\n\n\nWhat if potential outcomes \\(Y_i(d)\\) is an unknown and non-linear function of \\(d\\) and \\(X_i\\).\nWe know the OLS is the best linear predictor of the conditional expectation function in terms of mean squared error. Thus, \\(\\beta_1\\) will provide the best linear approximation to the population regression function.\nThis does not mean it is good - just the best linear approximation.\n\n\n\nYou should not use OLS if you believe assumption 1, heterogeneity, is violated. The reasoning is explained below.\n\n\n\nOLS Bias under Heterogeneity\nWhat if there are heterogenous treatment effects (where \\(\\tau_i\\) is different between units)? Standard OLS in this case is no longer an unbiased estimator of the ATE.\nRecall the discrete identification of the ATE (in equation Equation 4 ) is a weighted average of CATEs:\n\\[\n\\hat\\tau_{ATE} = \\sum\\limits_{x \\in \\mathcal X} ( \\hat\\tau_{CATE}(x)) \\underbrace{P(X_i = x)}_{\\text{weight}} \\\\\n\\]\nOLS, when there are non constant treatment effects, can also be rewritten as a weighted average of CATEs:\n\\[\n\\hat\\beta_{OLS} = \\sum\\limits_{x \\in \\mathcal X} ( \\hat\\tau_{CATE}(x)) \\underbrace{ \\frac{Var(D_i|X_i = X)P(X_i = x)}{\\sum Var(D_i | X_i = x')P(X_i = x')} }_{\\text{weight}} \\\\\n\\]\nNotice how the weights are different. The weights in the OLS are the conditional variances of \\(D_i\\). This means that OLS is not an unbiased estimator of the ATE or ATT, but rather, a weighted average of the ATT and ATU.\nOLS, under heterogeneity, actually provides an unbiased estimator of the conditional variance weighted average treatment effect. This is not the same as the ATE or the ATT.\n\n\n\n\n\n\nConditional Variance Weighted Average Treatment Effect (CVW-ATE)\n\n\n\n\n\nThis estimand can also be described as a weighted average of the ATT (average treatment effect on the treated) and the ATU (average treatment effect on the untreated):\n\\[\n\\tau_{OLS} = w_1 \\cdot \\tau_{ATT} + w_0 \\cdot \\tau_{ATU}\n\\]\nWhere:\n\\[\n\\begin{split}\nw_1 & = \\frac{(1 - P(D=1)) V(\\pi(X)|D = 0)}{P(D=1)V(\\pi(X)|D=1) + (1-P(D=1)V(\\pi(X)|D=0)} \\\\\nw_0 & = 1 - w_1\n\\end{split}\n\\]\nThe reason for this is because regression is prone to extrapolation beyond common support - i.e. it can “estimate” potential outcomes for units that are not observed. This can lead to bias.\nThis is in contrast to the subclassification estimator, which cannot be computed if there are missing observable outcomes for a substratum/category of \\(X\\).\nThe weights of \\(D_i(X_i = x)\\) can also be seen as propensity scores of \\(\\pi(x)(1 - \\pi(x)\\). Therefore:\n\nWeights are higher for groups with propensity scores close to 0.5.\nWeights are low for groups with propensity scores close to 0 or 1.\nOLS minimises estimation uncertainty by downweighting groups of \\(X_i\\) where group-specific ATEs are less precisely estimated.\n\n\n\n\n\n\n\nFully Interacted Estimator\nThe Fully-Interacted Estimator, a newly developed large-sample regression estimator (Lin 2013), solves the heterogeneity bias in the OLS estimator. The fully-interacted estimator takes the form:\n\\[\nY_i = \\hat\\alpha + D_i \\widehat{\\tau}_{int} + (\\mathbf X_i - \\mathbf {\\bar X}) \\hat\\beta +D_i (\\mathbf X_i - \\mathbf{\\bar X}) \\hat\\gamma + \\hat\\varepsilon_i\n\\]\n\nWhere \\(X_i\\) are covariate values sufficient to satisfy conditional independence.\nWhere \\(\\bar X\\) contains the sample means of all \\(X_i\\) covariates.\n\nThis estimator \\(\\hat\\tau_{int}\\) is technically biased when estimating \\(\\tau_{ATE}\\). However, the bias is arbitrarily small in large samples under conditional ignorability.\nThis estimator thus allows us to accurately estimate the ATE even under heterogenous treatment effects, assuming our sample size is sufficiently large.\n\n\n\n\n\n\nOther Solutions to the OLS Bias under Heterogeneity\n\n\n\n\n\nThere are a few other solutions to this issue of OLS bias under heterogeneity:\n\nDoubly-robust estimation uses a weighted average of regression and IPW estimators, which will be asymptotically consistent as long as the regression model is correctly specified.\nMatching as pre-processing uses matching to make treatment and control groups similar, then runs regression models to estimate causal effects.\n\n\n\n\n\n\n\n\n\n\nNonparametric Estimators\n\nSubclassification Estimator\nUsing the discrete identification of the ATE shown in Equation 4 , we can instead use the sample equivalents to get the subclassification estimator:\n\\[\n\\hat\\tau_{ATE} = \\sum\\limits_{j=1}^M \\underbrace{(\\bar Y_{1j} - \\bar Y_{0j})}_{\\tau_{CATE}(j)} \\underbrace{\\frac{n_j}{n}}_{\\text{weight}}\n\\]\n\nWhere \\(M\\) is the number of levels/categories of \\(X\\), and \\(j\\) is one specific level/category of \\(X\\).\nWhere \\(n_j\\) is the number of units in a level/category \\(j\\) of \\(X\\).\nWhere \\(\\bar Y_{dj}\\) is the mean outcome for units with \\(D_i = d\\) in level/category \\(j\\) of \\(X\\).\n\nFor subclassificaion to be possible, within each level \\(j\\) of covariate \\(X\\), there must be at least one unit in control \\(D=0\\) and treatment \\(D=1\\).\nSubclassification is not a particularly popular estimator, because it only works if all covariates are discrete, and only if you have a manageable amount of categories and covariates.\n\n\n\n\n\n\nIntuitive Procedure of Subclassification\n\n\n\n\n\nMore intuitively, the procedure is as follows:\n\nChoose one specific value for all covaraites \\(X\\). Find the average treatment effect within this specific value of \\(X\\).\nMultiply that average treatment effect by the number of observations that meet this specific value of \\(X\\) divided by the total number of units.\nDo this for every possible values of all covaraites \\(X\\), then sum up all the weighted average treatment effects to get the overall ATE.\n\n\n\n\n\n\n\n\n\n\nSubclassification with Multiple Confounders\n\n\n\n\n\nLet us say we have 2 confounders, \\(X_1\\) and \\(X_2\\). Both confounders are categorical with 3 categories.\nWe would need to create \\(M=9\\) levels of strata, for every possible combination of values of \\(X_1\\) and \\(X_2\\). Then, we would estimate the within-strata average treatment effect, and weight them.\nThis illustrates how with large amounts of confounders, you will need a huge number of stratum. This makes subclassification infeasible in many cases.\n\n\n\n\n\n\n\n\n\nSubclassification for the ATT\n\n\n\n\n\nWhen pre-treatment covariate \\(X\\) is discrete, the identification result of the ATT is:\n\\[\n\\tau_{ATT} = \\sum\\limits_{x \\in \\mathcal X} ( E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)) P(X_i = x | D_i = 1)\n\\]\nWe can calculate this within our give sample to get the subclassificaiton estimator:\n\\[\n\\hat\\tau_{ATT} = \\sum\\limits_{j=1}^M(\\bar Y_{1j} - \\bar Y_{0j}) \\frac{n_{1j}}{n_1}\n\\]\n\nWhere \\(M\\) is the number of strata (levels/categories of \\(X\\)).\nWhere \\(n_j\\) is the number of units in a level/category \\(j\\) of \\(X\\).\nWhere \\(n_{1j}\\) is the number of treated cells \\(D = 1\\) in a level/category \\(j\\) of \\(X\\).\nWhere \\(\\bar Y_{dj}\\) is the mean outcome for units with \\(D_i = d\\) in level/category \\(j\\) of \\(X\\).\n\n\n\n\n\n\n\nMatching Estimator\nWe have a missing data problem in causal inference: we do not know all the potential outcomes. Matching “estimates” missing potential outcomes of a unit.\nFor each observation in the treated group, matching finds an observation in the untreated group that have the most similar values of a set of pre-treatment covariates \\(X\\). Thus, we have pairs of treatment-control observations that act as counterfactuals. We can estimate the ATT as the average difference in observed outcomes within the pairs:\n\\[\n\\hat\\tau_{ATT} = \\frac{1}{n_1} \\sum\\limits_{i:D_i = 1}(Y_i - \\widetilde{Y_i})\n\\]\n\nWhere \\(n_1\\) is the number of units in the treatment group.\nWhere \\(Y_i\\) is the unit’s observed \\(Y\\) in the treatment group.\nWhere \\(\\tilde Y_i\\) is unit \\(i\\)’s closest neighbour in the untreated group.\n\nSometimes, a treatment unit may not have one close control unit to match to. Instead, we could use a combination of control units to match to the treatment unit, and use the average \\(Y\\) of those combination of control units to approximate a more accurate match.\nSuppose we use \\(M_i\\) number of close control units to match to a treatment unit \\(i\\). Then, the matching estimator would be defined as follows:\n\\[\n\\hat\\tau_{ATT} = \\frac{1}{n_1} \\sum\\limits_{i:D_i = 1}(Y_i - \\left(\\frac{1}{M_i} \\sum\\limits_{m=1}^{M_i} \\widetilde{Y_{i_m}}\\right))\n\\]\nWhere \\(\\widetilde{Y_{i_m}}\\) is the obsered outcome for the \\(m\\)th match of unit \\(i\\).\n\n\n\n\n\n\nChoices during Matching\n\n\n\n\n\nWe have to make several choices when conducting matching.\n\nWhat covariates to match on. We generally want to select a set of pre-treatment covariates \\(X\\) such that these covariates ensure the conditional ignorability assumption is met.\nMatch with or without replacement. Matching with replacement means that once you have used one control unit to match to a treatment unit, you can still use that same control unit to match to another treatment unit. This has advantages since you can ensure better and closer matches. However, matching without replacement is also possible.\nHow many to match. You can decide to match multiple control units to one treatment unit, and use the average of the treatment units to approximate a true control unit. This may result in more accurate matches for treatment units that may not have a good single control unit to match to.\n\nWe can also choose to use more advanced matching methods, such as Mahalanobis Distance matching or Propensity Score matching, which are shown below. These are good for matching on more \\(X\\).\n\n\n\n\n\n\n\n\n\nWeaknesses of Matching\n\n\n\n\n\nMatching does not always create “perfect” matches. This means that the pairs matched together may not be identical to each other in terms of covariates \\(X\\) or potential outcomes.\nThe inability to find exact matches can cause bias, especially for the more covariates we match on (see below).\n\n\n\n\n\n\nMatching with Multiple Covariates\nConsider that we \\(k&gt;1\\) number of confounders \\(X\\). Now, we have to match observations in \\(k\\) variables, which implies we are in a multidimensional \\(\\mathbb R^k\\) space.\nThe most commonly used distance metric is Mahalanobis Distance - which measures the distance in \\(X_i\\) between units \\(i\\) and \\(j\\):\n\\[\nD_M (\\mathbf X_i, \\mathbf X_j) = \\sqrt{(\\mathbf X_i - \\mathbf X_j)^T \\boldsymbol\\Sigma_X^{-1} (\\mathbf X_i - \\mathbf X_j)}\n\\]\n\nWhere \\(\\boldsymbol \\Sigma_X\\) is the sample variance-covariance matrix of \\(\\mathbf X_i\\).\n\n\n\n\n\n\n\nEuclidean Distance\n\n\n\n\n\nEuclidean distance is another common distance metric:\n\\[\nD_E ( \\mathbf X_i, \\mathbf X_j) = \\sqrt{(x_{1i} - x_{1j})^2 + (x_{2i}-x_{2j})^2 + \\dots + (x_{ki}-x_{kj})^2}\n\\]\nEuclidean distance, while very simple, is not recommended. This is because Euclidean distance with non-normalised variables can get you very bizarre results that depend on the scale of the variables.\nThere are other distance metrics, but these are exceedingly rare in selection on observables.\n\n\n\nThere is one issue with matching in multi-dimensional spaces. It becomes very difficult to match every unit \\(i\\) on every covariate \\(X\\), even if we have a large number of observations.\n\n\n\n\n\n\nCurse of Dimensionality\n\n\n\n\n\nWhen we try to match on more than one \\(X\\) variable, we go from matching on a number line \\(\\mathbb R^1\\) to a \\(n\\)-dimensional space, \\(\\mathbb R^n\\).\nThe search space increases exponentially as you increase the number of dimensions.\n\nTake a look at the figure on the left. If we only match on a one dimensional plane (lets say the horizontal line between 0 and 1), we can see our red line covers approximately 30% of the horizontal line. But in 3 dimensions, our red box covers a significantly less proportion of the entire cube.\nThe figure on the right illustrates this. \\(d\\) represents the dimensions. We can see as the dimensions increase, the fraction of volume increases significantly slower relative to distance.\nThus, with a bigger space, the distance between two units increases, so you get worse matches.\n\n\n\nThis curse of dimensionality creates a bias problem - since we get non-exact matches. The more dimensions you add, the worse it becomes.\n\n\n\n\n\n\nMore on Bias\n\n\n\n\n\nThe poor matches caused by increased dimensionality inject error into our estimates of missing potential outcomes.\nThe bias term as you increase the number of dimensions \\(k\\), changes by \\(N^{(-1/k)}\\). This implies no \\(\\sqrt{n}\\) consistency for \\(k&gt;2\\).\nIf \\(N_0\\) (number of untreated units) is much larger than \\(N_1\\) (number of treated units), bias will typically be smaller.\nThere are ways to correct this bias, including Abadie and Imbens (2011) Bias Correction method.\nThere is a new method: Bias-corrected matching, which estimates bias ineherent to mathching estimators via regression, then subtracts it from the matching estimate to correct for it.\n\n\n\n\n\n\nPropensity Scores Matching\nPropensity Score matching is an alternative way to match over many dimensions. The propensity score is an unobserved property, defined as the probability of a unit \\(i\\) of receiving treatment:\n\\[\n\\pi(X_i) \\equiv P(D_i = 1|X_i)\n\\]\nWhen supposing the conditional ignorability and common support assumptions, the propensity score \\(\\pi(X_i)\\) has the balancing property: \\(D_i \\perp X_i \\ | \\ \\pi(X_i)\\). This implies that conditional ignorability holds on the propensity scores alone:\n\\[\n(Y_{1i}, Y_{0i}) \\perp\\!\\!\\!\\!\\perp D_i \\ | \\ \\pi(X_i)\n\\]\nThus, instead of conditioning on \\(X_i\\) as we did in selection on observables, we can instead condition on \\(\\pi (X_i)\\), and still identify the causal estimand.\nHowever, we do not actually observe \\(\\pi (X_i)\\). We estimate \\(\\pi (X_i)\\) with a binary response model, with outcome variable \\(D_i\\), and explanatory variables \\(X_i\\). This will get us a fitted probability \\(P(D_i = 1) = \\hat\\pi(X_i)\\).\nThen, once we have the propensity score estimates \\(\\hat\\pi(X_i)\\), we can do nearest neighbour matching with the propensity scores (in \\(\\mathbb R^1\\)). This will allow us to identify the \\(\\tau_{ATT}\\).\n\n\n\n\n\n\nBalance Checks\n\n\n\n\n\nThe accurate estimation of \\(\\tau_{ATT}\\) implies an accurate prediction of the propensity scores \\(\\pi(X_i)\\). We can test our matched treatment and control groups to see if the balancing property holds for covariates \\(X_i\\).\n\n\n\n\n\n\nGenetic Matching\n\n\n\n\n\n\nWeighting Estimator\n\nIdentification with Weighting\nWe know that the ATE can be written as a weighted average, as shown in Equation 4 . We can rewrite the \\(\\tau_{ATE}\\) as follows using observed potential outcomes outcomes and conditional ignorability ( Equation 1 ).\n\\[\n\\begin{split}\n& = \\sum\\limits_{x \\in \\mathcal X} \\underbrace{(E(Y_{1i}|D_i = 1, X_i = x)}_{\\because \\text{ observed}} - \\underbrace{E(Y_{0i}|D_i = 0, X_i = x)}_{\\because \\text{ observed}})P(X_i = x) \\\\\n& = \\sum\\limits_{x \\in \\mathcal X}  (\\underbrace{E(Y_{1i}|X_i = x)}_{\\because \\text{ eq. (1)}} - \\underbrace{E(Y_{0i}|X_i = x)}_{\\because \\text{ eq. (1)}})P(X_i = x) \\\\\n& = \\underbrace{E[E(Y_{1i}|X_i = x) - E(Y_{0i}|X_i = x)]}_{\\text{definition of weighted average}}\n\\end{split}\n\\]\nLet us do an algebra trick - multiply both terms within the CATE by 1 (in blue):\n\\[\n\\begin{split}\n& = E \\left [E(Y_{1i}|X_i=x) \\color{blue}{\\frac{\\pi(X_i)}{\\pi(X_i)}}\\color{black} - (E(Y_{0i}|X_i=x) \\color{blue}{\\frac{1-\\pi(X_i)}{1-\\pi(X_i)}} \\right] \\\\\n& \\color{black} = E \\left[ \\frac{E(Y_{1i}|X_i = x) \\pi(X_i)}{\\pi(X_i)} -  \\frac{E(Y_{0i}|X_i = x) (1-\\pi(X_i))}{1-\\pi(X_i)} \\right]\n\\end{split}\n\\]\nWe know that propensity score \\(\\pi(X_i) := E(D_i|X_i = x)\\). Thus, we can convert the above to:\n\\[\n\\begin{split}\n& = E \\left[ \\frac{E(Y_{1i}|X_i = x)E(D_i|X_i = x)}{\\pi(X_i)} - \\frac{E(Y_{0i}|X_i = x)(1-E(D_i|X_i = x))}{1-\\pi(X_i)}\\right] \\\\\n& = E \\left[ \\frac{E(Y_{1i}|X_i = x)E(D_i|X_i = x)}{\\pi(X_i)} - \\frac{E(Y_{0i}|X_i = x)E(1-D_i|X_i = x)}{1-\\pi(X_i)}\\right]\n\\end{split}\n\\]\n\\[\n\\begin{align}\n& = E \\left[ \\frac{E(Y_{1i}D_i|X_i = x)}{\\pi(X_i)} - \\frac{E(Y_{0i}(1-D_i)|X_i = x)}{1 - \\pi(X_i)}\\right] && (\\text{property of expectation})\\\\\n& = E \\left[ E \\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} |X_i = x\\right) - E \\left( \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)} | X_i = x \\right) \\right] && (\\text{property of expectation})\\\\\n& = E\\left[ E\\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} - \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)} |X_i = x \\right) \\right]  && (\\text{property of expectation})\n\\end{align}\n\\]\n\\[\n\\begin{align}\n& = E\\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} - \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)}\\right) && (\\text{LIE: } E(X) = E[E(X|Y)] \\ ) \\\\\n& = E\\left( \\frac{Y_{i}D_i}{\\pi(X_i)} - \\frac{Y_{i}(1-D_i)}{1-\\pi(X_i)}\\right) && (\\text{observered outcome}) \\\\\n& = E \\left( \\frac{\\color{blue}{Y_i} \\color{black}D_i(1-\\pi(X_i))-\\color{blue}{Y_i}\\color{black}(1-D_i)\\pi(X_i)}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{getting common denom.}) \\\\\n& = E\\left( Y_i \\frac{D_i(1-\\pi(X_i))-(1-D_i)\\pi(X_i)}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{factor out }Y_i) \\\\\n& = E\\left( Y_i \\frac{D_i - D_i\\pi(X_i)-(\\pi(X_i) -D_i\\pi(X_i))}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{distribute out}) \\\\\n&  = E\\left( Y_i \\frac{D_i \\color{blue}{- D_i\\pi(X_i)} \\color{black}-\\pi(X_i) \\color{blue}{+ D_i\\pi(X_i)}}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{distribute out negative})\\\\\n& = E\\left( Y_i \\frac{D_i -\\pi(X_i) }{\\pi(X_i)(1-\\pi(X_i))}\\right) && (\\text{cancel out})\\\\\n\\end{align}\n\\tag{7}\\]\nAnd thus, we have identified the ATE.\n\n\n\nInverse Probability Weighting Estimator\nAn alternative use of propensity scores is weighting. As shown above, under conditional ignorability and common support, we can identify the ATE as:\n\\[\n\\tau_{ATE} = E\\left[ Y_i \\times \\underbrace{\\frac{D_i - \\pi(X_i)}{\\pi(X_i) (1 - \\pi(X_i))}}_{\\text{weight}}\\right]\n\\]\nThe inverse probability weighting (IPW) estimator is the sample estimator:\n\\[\n\\begin{split}\n\\hat\\tau_{ATE} & = \\frac{1}{N} \\sum\\limits_{i=1}^N \\left(Y_i \\frac{D_i - \\hat\\pi(X_i)}{\\hat\\pi(X_i) (1 - \\hat\\pi(X_i))} \\right) \\\\\n& = \\frac{1}{N} \\sum\\limits_{i=1}^N \\left(\\frac{D_i Y_i}{\\hat\\pi(X_i)} - \\frac{(1-D_i) Y_i}{1 - \\hat\\pi(X_i)} \\right)\n\\end{split}\n\\]\n\nThe second equation is equivalent to the first, shown by Equation 7 .\n\nEssentially, those who are unlikely to be treated but do get treated get weighted more, and individuals who are likely to be treated but do not get treated get weighted more.\n\n\n\n\n\n\nWeighting Estimator for ATT\n\n\n\n\n\nThe identification of the ATT under both conditional ignorability and common support are:\n\\[\n\\tau_{ATT} = \\frac{1}{Pr(D = 1)} \\times E\\left[ Y_i \\times \\underbrace{\\frac{D_i - \\pi(X_i)}{(1 - \\pi(X_i))}}_{\\text{weight}}\\right]\n\\]\nThe sample IPW estimator would be:\n\\[\n\\begin{split}\n\\hat\\tau_{ATT} & = \\frac{1}{N_1}\\sum\\limits_{i=1}^N \\left( Y_i \\frac{D_i - \\hat\\pi(X_i)}{1 - \\hat\\pi(X_i)} \\right) \\\\\n& = \\frac{1}{N_1} \\sum\\limits_{i=1}^N \\left( D_iY_i - (1-D_i)Y_i \\frac{\\hat\\pi(X_i)}{1 - \\hat\\pi(X_i)} \\right)\n\\end{split}\n\\]\n\n\n\nThe IPW estimator is asymptotically consistent, but has very poor small sample properties. They are highly sensitive to extreme values of \\(\\hat\\pi(X_i)\\). This generates high variance (inefficiency), and can produce significant bias under model mispecification.\n\n\n\n\n\n\nFalsification Tests\n\nTesting Assumptions with Falsification\nThe stronger (bolder) our assumptions for identification, the less credible our results are. Selection on Observables involves a very strong and hard to verify assumption: conditional ignorability. Can we really be sure that we have controlled for all confounders \\(X_i\\) needed to satisfy conditional ignorability?\nPlacebo tests are a type of falsification test to show evidence against our assumptions. Suppose that we make the assumption of conditional ignorability \\((Y_{0i}, Y_{1i}) \\perp D_i | X_i\\). Suppose we are concerned about the presence of another confounder \\(U\\) that is not included in \\(X_i\\).\n\n\n\n\n\nThe presence of \\(U\\) will falsify our conditional ignorability assumption, and means we cannot identify the causal effect of \\(D \\rightarrow Y\\).\n\n\n\n\n\n\nFalsification vs. Validation\n\n\n\n\n\nFalsification is a principle of trying to criticise our own research, rather than defend it. Falsification is about testing if our assumptions are not met. Failing a test provides evidence that our assumption is not met.\n\nEx. Covariates are balanced - thus there is no evidence that our assumptions are not met. We are not saying that our assumption is correct, just that there is no evidence against it.\n\nValidation is the opposite - we test to see if there is evidence in favour of our assumptions.\n\nEx. Covariates are balanced - thus our assumptions are met.\n\n\n\n\nFor falsification tests, we should not just pay attention to statistical significance - we must also pay attention to the magnitude of the point estimation.\n\n\n\nPlacebo Outcome Test\nA placebo outcome test utilities another alternative outcome variable \\(Y'\\) that is caused by our hypothesised unobserved confounder \\(U\\):\n\n\n\n\n\nWe can see that if \\(U\\) does not exist, \\(D\\) should have zero effect on the new outcome \\(Y'\\). Thus, if \\(U\\) is present, we should find a relationship between \\(D\\) and \\(Y'\\).\n\\[\nY'_i = \\gamma + \\delta D_i + \\varepsilon_i\n\\]\n\nIf we find that there is an effect of \\(D\\) on the new outcome \\(Y'\\) (non-zero \\(\\delta\\)), that is evidence that \\(U\\) exists, and is evidence to reject our conditional ignorability assumption (falsifies our design - red flag!).\nIf you do not find an effect of \\(D\\) on new outcome \\(Y'\\) (\\(\\delta = 0\\)) you find no evidence of \\(U\\), and no evidence to reject our conditional ignorability assumption (fails to falsify our design).\n\nWe must be sure that \\(Y\\) is not related to \\(Y'\\) except through \\(D\\) and \\(U\\). If this is true, we just run our original research design but replace \\(Y\\) with \\(Y'\\).\n\n\n\nPlacebo Treatment Test\nA placebo treatment test involves some other treatment \\(D'\\), that was assigned at the same time\n\n\n\n\n\nWe can see that if \\(U\\) does not exist, the effect of \\(D'\\) should have no effect on \\(Y\\). If \\(U\\) does exist, there should be some effect of \\(D'\\) on \\(Y\\).\n\\[\nY_i = \\gamma + \\delta D'_i + \\varepsilon_i\n\\]\n\nIf we find that there is an effect of \\(D'\\) on \\(Y\\) (non-zero \\(\\delta\\)), that is evidence that \\(U\\) exists, and is evidence to reject our conditional ignorability assumption (falsifies our design - red flag!).\nIf you do not find an effect of \\(D'\\) on \\(Y\\) (\\(\\delta = 0\\)) you find no evidence of \\(U\\), and no evidence to reject our conditional ignorability assumption (fails to falsify our design).\n\nWe must be sure that \\(Y\\) is not related to \\(D'\\) except through \\(D\\) and \\(U\\). If this is true, we just run our original research design but replace \\(D\\) with \\(D'\\).\n\n\n\n\n\n\nExtension: Partial Identification\n\nDecomposing the ATE\nWith falsification, we were concerned with what assumptions we needed to be not-false in order to identify the ATE. However, we can take a different approach - what can we learn about the ATE without any assumptions?\nLet us decompose the ATE into parts:\n\\[\n\\begin{align}\n\\tau_{ATE}  = & E(Y_{1i} - Y_{0i}) \\\\\n& \\\\\n= & E(Y_{1i} - Y_{0i} | D_i = 1) P(D_i = 1) \\\\\n& \\quad - E(Y_{1i} - Y_{0i}|D_i = 0)P(D_i = 0)  && (\\text{def. of weighted avg.})\\\\\n& \\\\\n= & [ \\color{blue}{E(Y_i |D_i = 1)} \\color{black}- \\color{red}{E(Y_{0i}|D_i = 1)} \\color{black}] P(D_i = 1) \\\\\n& \\quad + [ \\color{red}{E(Y_{1i}|D_i = 0)} \\color{black} - \\color{blue}{E(Y_i|D_i = 0)} \\color{black}]P(D_i = 0) && (\\text{observed + unobserved})\n\\end{align}\n\\]\nSome of the quantities are observed (in blue), and some of the quantities are unobserved (in red). Previously, we made assumptions (conditional ignorability, common support) to fill the unobserved quantities. But, we can make actually any assumption as possible.\n\n\n\nNonparametric Bounds\nOne way to fill in our unobserved outcomes through the “best” and “worst” possible outcomes. This allows us to construct a plausible range of the ATE.\nFirst, let us construct the worst-case scenario - the lowest possible \\(\\tau\\).\n\n\\(E(Y_{0i}|D_i = 1) = Y_H\\). Units in the treated \\(D_i=1\\), their potential outcome \\(Y_{0i}\\) will be the highest \\(Y\\) possible, \\(Y_H\\).\n\\(E(Y_{1i}|D_i = 0) = Y_L\\). Units in the control \\(D_i=0\\), their unobserved potential outcome \\(Y_{1i}\\) will be the lowest \\(Y\\) possible, \\(Y_L\\).\n\nThus, the lowest possible \\(\\tau\\) (sharp lower bound) is:\n\\[\n\\begin{split}\n\\tau_L = & [ \\color{blue}{E(Y_i |D_i = 1)} \\color{black}  - \\color{red}{Y_H} \\color{black}] P(D_i = 1) \\\\\n& \\quad + [ \\color{red}{Y_L} \\color{black} - \\color{blue}{E(Y_i|D_i = 0)} \\color{black}]P(D_i = 0)\n\\end{split}\n\\]\nNow, let us construct the best-case scenario - the highest possible \\(\\tau\\).\n\n\\(E(Y_{0i}|D_i = 1) = Y_L\\). Units in the treated \\(D_i=1\\), their potential outcome \\(Y_{0i}\\) will be the lowest \\(Y\\) possible, \\(Y_L\\).\n\\(E(Y_{1i}|D_i = 0) = Y_H\\). Units in the control \\(D_i=0\\), their unobserved potential outcome \\(Y_{1i}\\) will be the highest \\(Y\\) possible, \\(Y_H\\).\n\nThus, the highest possible \\(\\tau\\) (sharp upper bound) is:\n\\[\n\\begin{split}\n\\tau_H = & [ \\color{blue}{E(Y_i |D_i = 1)} \\color{black}  - \\color{red}{Y_L} \\color{black}] P(D_i = 1) \\\\\n& \\quad + [ \\color{red}{Y_H} \\color{black} - \\color{blue}{E(Y_i|D_i = 0)} \\color{black}]P(D_i = 0)\n\\end{split}\n\\]\nWe know that the true \\(\\tau_{ATE} \\in [\\tau_L, \\tau_H]\\).\n\n\n\nMonotone Treatment Selection Assumption\nOur extreme case from above is not very useful. However, we can layer on assumptions to lower the possible \\(\\tau\\) values.\nOne assumption is the Monotone Treatment Selection (MTS) assumption. This assumption basically says that potential outcomes for units in treatment, are always higher than for those in the control.\n\\[\n\\begin{split}\n& E(Y_{0i}|D_i = 0) ≤ \\overbrace{E(Y_{0i}|D_i = 1)}^{\\text{unobserved}} \\\\\n& \\underbrace{E(Y_{1i} |D_i = 0)}_{\\text{unobserved}} ≤ E(Y_{1i} |D_i = 1)\n\\end{split}\n\\]\nThis is basically saying that selection bias is one-direction.\nThis implies a tighter sharp upper bound on \\(\\tau\\).\n\\[\n\\begin{align}\n\\tau_H = & [ \\underbrace{E(Y_i |D_i = 1)}_{\\text{observed}}  - \\color{red}{E(Y_{0i}|D_i = 0)} \\color{black}] P(D_i = 1) \\\\\n& \\quad + [ \\color{red}{E(Y_{1i}|D_i =1)} \\color{black} - \\underbrace{E(Y_i|D_i = 0)}_{\\text{observed}}]P(D_i = 0) \\\\\n= & E(Y_i|D_i = 1) - E(Y_i|D_i = 0) && (\\text{def. of weighted avg.})\\\\\n\\end{align}\n\\]\nThis indicates that the upper bound of plausible \\(\\tau_{ATE}\\) values is the naive estimator of differences in observed outcomes.\nWe can also make the reverse assumption, where selection bias is in the opposite direction. This means a tighter sharp lower bound \\(\\underline\\tau\\). These assumptions help narrow our possible \\(\\tau_{ATE}\\) values, and can allow us to test if our estimated \\(\\hat\\tau\\) is reasonable (within the plausible bounds).\n\n\n\n\n\n\nImplementation in R\nFor all methods, you will need the tidyverse package:\n\nlibrary(tidyverse)\nlibrary(MatchIt)\nlibrary(estimatr)\n\nSee how to perform each estimator in R:\n\n\n\n\n\n\nDistance Matching\n\n\n\n\n\nFirst, let us conduct nearest neighbour matching with Mahalanobis distance by using the matchit() function.\n\nmatch_object = MatchIt::matchit(D ~ X1 + X2 + X3,\n                                data = my_data,\n                                method = \"nearest\", #distance matching\n                                distance = \"mahalanobis\")\n\n# for output summary\nsummary(match_object)\n\nSecond, let us save the matched data with the match.data() function.\n\nmatch_data &lt;- MatchIt::match.data(match_object,\n                                  weights = 'nn_weights')\n\nThird, we can test if matching worked by using a balance table and a love plot:\n\n# balance table\ncobalt::bal.tab(D ~ X1 + X2 + X3, \n                data = match_data, # from the 2nd step\n                weights = \"nn_weights\",\n                disp = c(\"means\", \"sds\"))\n\n#love plot\ncobalt::love.plot(match_object,\n                  data = my_data, #original dataset\n                  stars = 'raw')\n\nFinally, we can estimate the treatment effect. There are two options - either using a weighted regression, or using the matching algorithm:\n\n# using weighted regression\nestimate &lt;- lm_robust(Y ~ D,\n                      data = match_data, #data from step 2\n                      weights = nn_weights)\nsummary(estimate)\n\n## using the Matching package:\nestimate = Matching::Match(Y = my_data$Y, #outcome\n                           Tr = my_data$D, #treatment\n                           X = my_data[,c(\"X1\", \"X2\", \"X3\")], #covariates\n                           M=1, #number of neighbours\n                           BiasAdjust = TRUE, #for biased adjustment\n                           Weight = 2)\nsummary(estimate)\n\nYou will have the estimates that you can use.\n\n\n\n\n\n\n\n\n\nPropensity Score Matching\n\n\n\n\n\nFirst, we want to estimate propensity scores with a logistic regression (or a random forest):\n\n#logistic model\npscore_model = glm(D ~ X1 + X2,\n                   data = my_data,\n                   family = \"binomial\")\n\n# estimate propensity scores\nmy_data$pscore_estimate = predict(pscore_model,\n                                  my_data,\n                                  type = \"response\")\n\nNow, let us match with propensity scores:\n\n# match\nmatch_object = MatchIt::matchit(D ~ pscore_estimate,\n                                data = my_data,\n                                method = \"nearest\",\n                                distance = \"Mahalanobis\")\n\n# save matched data\nmatch_data &lt;- MatchIt::match.data(match_object,\n                                  weights = 'pscore_weights')\n\nThird, we can test if matching worked with a balance table and a love plot:\n\n#balance table\ncobalt::bal.tab(D ~ X1 + X2 + X3,\n                data = match_data, #matched data from step 2\n                weights = \"pscore_weights\",\n                disp = c(\"means\", \"sds\"))\n\n#love plot\ncobalt::love.plot(match_object,\n                  data = my_data, #original dataset\n                  addl = ~ X1 + X2 + X3,\n                  stars = 'raw')\n\nFinally, let us do the estimation:\n\nestimate &lt;- lm_robust(Y ~ D,\n                      data = match_data, #from step 2\n                      weights = pscore_weights)\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nInverse Probability Weighting\n\n\n\n\n\nFirst, we want to estimate propoensity scores with a logistic regression (or a random forest):\n\n#logistic model\npscore_model = glm(D ~ X1 + X2,\n                   data = my_data,\n                   family = \"binomial\")\n\n# estimate propensity scores\nmy_data$pscore_estimate = predict(pscore_model,\n                                  type = \"response\")\n\nSecond, we calculate the inverse probability weights based on the formula from earlier:\n\nmy_data$ipweight = ifelse(my_data$D == 1, # condition\n                       1/my_data$pscore_estimate,\n                       1/(1-my_data$pscore_estimate))\n\nFinally, we can estimate the ATE, or ATT, or use a weighted regression for the ATE:\n\n# ATE estimator\nmean((my_data$D * my_data$Y) * my_data$ipweight - ((1 - my_data$D) * my_data$Y) * my_data$ipweight)\n\n# ATT estimator\nsum(my_data$D * my_data$Y - (1 - my_data$D) * my_data$Y * (my_data$pscore_estimate/(1 - my_data$pscore_estimate)))/sum(my_data$D)\n\n# ATE with weighted regression\nestimate &lt;- lm_robust(Y ~ D, \n                      data = my_data,\n                      weights = ipweight)\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nOLS Estimator\n\n\n\n\n\nFor the OLS estimator, we can use the lm_robust() function:\n\nestimate &lt;- lm_robust(Y ~ D + X1 + X2 + X3,\n                      data = my_data)\nsummary(estimate)\n\nWe can also use the fixest package and the feols() function:\n\nlibrary(fixest)\n\nestimate &lt;- feols(Y ~ D + X1 + X2 + X3,\n                  data = my_data,\n                  se = \"hetero\")\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nFully Interacted Estimator\n\n\n\n\n\nFor the fully interacted estimator, we can use the lm_lin() function.\n\nestimate &lt;- lm_lin(Y ~ D,\n                   covariates = ~ X1 + X2 + X3,\n                   data = my_data)\nsummary(estimate)\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "6 Selection on Observables"
    ]
  },
  {
    "objectID": "quant5.html",
    "href": "quant5.html",
    "title": "Selection on Observables",
    "section": "",
    "text": "In the last chapter, we discussed randomisation. Randomisation is great, but, it requires specific circumstances of the research having control over the assignment mechanism. However, in the social sciences, this rarely occurs.\nThis chapter introduces the selection on observables framework, which allows us to identify causal effects in an observational setting by controlling for observable pre-treatment covariates. We discuss the main estimators, including regression, matching, and weighting.\nUse the right sidebar for quick navigation. R-code provided at the bottom.\n\n\nIdentification\n\nBlocking Backdoor Paths\nWithout randomisation, we need some other way to account for pre-treatment covariates that may be confounding and causing selection bias. Controlling for a set of nodes/confounders \\(X\\) can identify the causal effect of \\(D \\rightarrow Y\\), if:\n\nNo node within set \\(X\\) is a descendant of \\(D\\) (no element within \\(X\\) results from \\(D\\)).\nThe nodes within set \\(X\\) block all back-door paths from \\(D \\rightarrow Y\\).\n\n\n\n\n\n\nIn the figure above, let us block the backdoor paths between \\(D \\rightarrow Y\\):\n\nBackdoor path \\(D \\rightarrow X \\rightarrow Y\\). To block this path, we must control for \\(X\\).\nBackdoor path \\(D \\rightarrow V \\rightarrow Y\\). We do not need to control for \\(V\\), since it is post-treatment (a descendant of \\(D\\)). In fact, \\(V\\) is a bad control (see below).\n\nThus, to identify \\(D \\rightarrow Y\\) here, we only need to control for \\(X\\), and no other variable.\n\n\n\n\n\n\nGood and Bad Controls\n\n\n\n\n\nGood controls block backdoor paths, which facilitate identification of the causal effect.\nBad controls are when we control for post-treatment variables. For example, \\(P\\) below is a bad control, since it is caused by \\(D\\), so it is post-treatment.\n\n\n\n\n\nYou also never want to control variables that only predict \\(D\\). These are bad because controlling for these removes variation in \\(D\\) that could be useful.\nNeutral controls are ones that don’t identify the causal effect, but improve efficiency. For example, \\(Q\\) below affects \\(Y\\), but there is no backdoor path. Controlling \\(Q\\) will not help identification, but can control noise in \\(Y\\) which may increase efficiency.\n\n\n\n\n\n\n\n\n\n\n\nIdentification Assumptions\nOnce we have determined the set of confounders \\(X\\) that we need to control to block all backdoor paths, the assumptions needed for identification of causal effects are:\n\nConditional Ignorability (also known as exogeneity or independence): Among units with identical confounder values \\(X_i\\), treatment \\(D_i\\) is as-if randomly assigned. Or in other words, potential outcomes are independent from treatment within each specific confounder value \\(X_i = x\\).\n\n\\[\n(Y_{0i}, Y_{1i}) \\perp\\!\\!\\!\\!\\perp D_i  \\ | \\ X_i = x, \\quad \\forall \\ x \\in \\mathcal X\n\\]\nThis implies that for any given value of all confounders \\(X_i = x\\), we know that potential outcomes \\(Y_{di}\\) are equivalent between treatment and control:\n\\[\n\\begin{split}\nE(Y_{1i}|X_i = x) = E(Y_{1i}|D_i = 1, X_i = x) = E(Y_{1i}|D_i = 0, X_i = x) \\\\\nE(Y_{0i}|X_i = x) = E(Y_{0i}|D_i = 1, X_i = x) = E(Y_{0i}|D_i = 0, X_i = x)\n\\end{split}\n\\tag{1}\\]\n\nCommon Support: for any unit \\(i\\) with value of \\(X_i\\), there is a non-zero probability that they could be assigned to both control \\(D_i = 0\\) or treatment \\(D_i = 1\\).\n\n\\[\n0 &lt; P(D_i = 1 \\ | X_i = x) &lt; 1 \\quad \\forall \\ x \\in \\mathcal X\n\\]\n\n\n\n\n\n\nExample of Identification Assumptions\n\n\n\n\n\nImagine we have a theory that being abducted \\(D\\) causes turning out to vote.\nBlattman (2009) finds that age is the primary way violent groups chose to abduct individuals: abduction parties released young children and older adults, but kept all adolescent and young males.\nThat means our theory is that age \\(X\\) affects selection into treatment \\(D\\). Young children and older adults are less likely to get abducted \\(D\\), while adolescent and young males are more likely \\(D\\).\n\n\n\n\n\n\nIdentification of the ATE\nWith our assumptions above, we can identify the ATE. We start with the conditional average treatment effect, conditional on some value of confounders \\(X_i = x\\). Note the properties shown in Equation 1 .\n\\[\n\\begin{align}\n\\tau_{CATE}(x) & = E(Y_{1i} - Y_{0i} \\ | \\ X_i = x) \\\\\n& = E(Y_{1i}|X_i = x) - E(Y_{0i}|X_i = x) && (\\text{property of expectation}) \\\\\n& = E(Y_{1i}|D_i = 1, X_i = x) - E(Y_{0i}|D_i = 0X_i = x) &&( \\because \\text{equation (1)} \\ ) \\\\\n& = \\underbrace{E(Y_i|D_i = 1, X_i = x)}_{\\because \\text{ observable}} - \\underbrace{E(Y_i|D_i = 0, X_i = x)}_{\\because \\text{ observable}}\n\\end{align}\n\\tag{2}\\]\nNow, let us discuss the ATE, and plug in the CATE from Equation 2 to identify it:\n\\[\n\\begin{align}\n\\tau_{ATE} & = E(Y_{1i} - Y_{0i}) \\\\\n& = \\int \\underbrace{E(Y_{1i} - Y_{0i} \\ | \\ X_i = x)}_{\\tau_{CATE}(x)} d \\ \\underbrace{P(X_i = x)}_{\\text{weight}} && (\\text{weighted average})\\\\\n& = \\int(\\underbrace{E(Y_i|D_i = 1, X_i) - E(Y_i|D_i = 0, X_i)}_{\\because \\text{ equation (2)}})d \\ P(X_i = x)\n\\end{align}\n\\tag{3}\\]\nThus \\(\\tau_{ATE}\\) is identified as the weighted average of all the CATEs, who themselves are difference-in-means of the observed \\(Y_i\\) at every possible value of \\(X_i = x\\).\nWe assumed that the pre-treatment covariate \\(X\\) is continuous. This is why we need an integral. However, we can simplify this if \\(X\\) is discrete:\n\\[\n\\tau_{ATE} = \\sum\\limits_{x \\in \\mathcal X} ( E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)) P(X_i = x)\n\\tag{4}\\]\n\n\n\nIdentification of the ATT\nWe can weaken conditional ignorability, and still identify the ATT. Only \\(Y_{0i}\\) needs to be independent of \\(D_i\\) for units with the same covariates \\(X_i\\). Or in other words, \\((Y_{0i}) \\perp\\!\\!\\!\\perp D_i | X_i = x\\). This implies:\n\\[\nE(Y_{0i}|X_i = x) = E(Y_{0i}|D_i = 0, X_i = x) = E(Y_{0i}|D_i = 1, X_i = x)\n\\tag{5}\\]\nStart with the conditional ATT, using weakened conditional ignorability from Equation 5 :\n\\[\n\\begin{split}\n\\tau_{CATT}(x) & = E(Y_{1i}-Y_{0i}|D_i = 1, X_i = x) \\\\\n& = E(Y_{1i}|D_i = 1, X_i = x) - E(Y_{0i}|D_i = 1, X_i = x) \\\\\n& = E(Y_{1i}|D_i = 1, X_i = x) - \\underbrace{E(Y_{0i}|D_i = 0, X_i = x)}_{\\because \\text{ equation (5)}} \\\\\n& = \\underbrace{E(Y_i|D_i=1, X_i = x)}_{\\because \\text{ observable}} - \\underbrace{E(Y_1|D_i = 0, X_i x)}_{\\because \\text{ observable}}\n\\end{split}\n\\tag{6}\\]\nNow, look at the ATT, and plug in CATT from Equation 6 to identify it.\n\\[\n\\begin{align}\n\\tau_{ATT} & = E(Y_{1i} - Y_{0i}|D_i = 1) \\\\\n& = \\int \\underbrace{E(Y_{1i} - Y_{0i}|D_i = 1, X_i = x)}_{\\tau_{CATT}(x)}d \\ \\underbrace{P(X_i = x|D_i = 1)}_{P(X_i = x) \\text{ within treated}} \\\\\n& = \\int (\\underbrace{E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)}_{\\because \\text{ equation (6)}})d \\ P(X_i = x|D_i = 1)\n\\end{align}\n\\]\nWe can simplify this if \\(X\\) is discrete:\n\\[\n\\tau_{ATT} = \\sum\\limits_{x \\in \\mathcal X} ( E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)) Pr(X_i = x | D_i = 1)\n\\]\nEven when all assumptions are met for identification of the ATE, the \\(\\tau_{ATE}\\) can be different than the \\(\\tau_{ATT}\\). This is because the weights \\(Pr(X_i = x|D_i = 1)\\) for the ATT are different than the ATE \\(Pr(X_i = x)\\).\n\n\n\n\n\n\nParametric Estimators\n\nOrdinary Least Squares Estimator\nOLS is a natural approach for controlling for confounders \\(X\\), since \\(\\hat\\beta_{OLS}\\) estimates partial out the effects of covariates. OLS is a good estimator of \\(\\tau_{ATE}\\) under 2 conditions:\n\nConstant treatment effect: \\(\\tau_i = Y_{1i} - Y_{0i}\\) for all units \\(i\\).\nLinearity: Potential outcomes are linear, and can be written as:\n\n\\[\nY_i(d) = \\beta_0 + d\\beta_1 + \\mathbf X_i \\gamma + \\varepsilon_i \\quad \\text{for} \\quad d = 0, 1\n\\]\nWhy these conditions? Suppose we have the above linear potential outcomes. We can show:\n\\[\n\\begin{align}\n\\tau_i & = Y_{1i} - Y_{0i} && (\\text{definition of } \\tau_i) \\\\\n& = (\\beta_0 + (1)\\beta_1 + \\mathbf X_i \\gamma + \\varepsilon_i) - (\\beta_0 + (0)\\beta_1 + \\mathbf X_i \\gamma + \\varepsilon_i) && (\\text{plug in } Y_i(1), Y_i(0) \\ )\\\\\n& = (\\beta_0 + \\beta_1 + \\mathbf X_i \\gamma + \\varepsilon_i) - (\\beta_0 + \\mathbf X_i \\gamma + \\varepsilon_i) && (\\text{multiply}) \\\\\n& = \\beta_0 + \\beta_1 + \\mathbf X_i \\gamma + \\varepsilon_i - \\beta_0 - \\mathbf X_i\\gamma - \\varepsilon_i && (\\text{distribute negative sign})\\\\\n& = \\beta_1 && (\\text{cancel out terms})\n\\end{align}\n\\]\nWe also know that conditional ignorability implies zero-conditional mean. Thus \\(\\beta_1\\) is an unbiased and asymptotically consistent estimator of the ATE.\nYou should be cautious using OLS when assumption 2, linearity, is violated. OLS is the best linear estimator, but how far your data is from linearity will determine if the estimator is useful.\n\n\n\n\n\n\nNon-Linearity\n\n\n\n\n\nWhat if potential outcomes \\(Y_i(d)\\) is an unknown and non-linear function of \\(d\\) and \\(X_i\\).\nWe know the OLS is the best linear predictor of the conditional expectation function in terms of mean squared error. Thus, \\(\\beta_1\\) will provide the best linear approximation to the population regression function.\nThis does not mean it is good - just the best linear approximation.\n\n\n\nYou should not use OLS if you believe assumption 1, heterogeneity, is violated. The reasoning is explained below.\n\n\n\nOLS Bias under Heterogeneity\nWhat if there are heterogenous treatment effects (where \\(\\tau_i\\) is different between units)? Standard OLS in this case is no longer an unbiased estimator of the ATE.\nRecall the discrete identification of the ATE (in equation Equation 4 ) is a weighted average of CATEs:\n\\[\n\\hat\\tau_{ATE} = \\sum\\limits_{x \\in \\mathcal X} ( \\hat\\tau_{CATE}(x)) \\underbrace{P(X_i = x)}_{\\text{weight}} \\\\\n\\]\nOLS, when there are non constant treatment effects, can also be rewritten as a weighted average of CATEs:\n\\[\n\\hat\\beta_{OLS} = \\sum\\limits_{x \\in \\mathcal X} ( \\hat\\tau_{CATE}(x)) \\underbrace{ \\frac{Var(D_i|X_i = X)P(X_i = x)}{\\sum Var(D_i | X_i = x')P(X_i = x')} }_{\\text{weight}} \\\\\n\\]\nNotice how the weights are different. The weights in the OLS are the conditional variances of \\(D_i\\). This means that OLS is not an unbiased estimator of the ATE or ATT, but rather, a weighted average of the ATT and ATU.\nOLS, under heterogeneity, actually provides an unbiased estimator of the conditional variance weighted average treatment effect. This is not the same as the ATE or the ATT.\n\n\n\n\n\n\nConditional Variance Weighted Average Treatment Effect (CVW-ATE)\n\n\n\n\n\nThis estimand can also be described as a weighted average of the ATT (average treatment effect on the treated) and the ATU (average treatment effect on the untreated):\n\\[\n\\tau_{OLS} = w_1 \\cdot \\tau_{ATT} + w_0 \\cdot \\tau_{ATU}\n\\]\nWhere:\n\\[\n\\begin{split}\nw_1 & = \\frac{(1 - P(D=1)) V(\\pi(X)|D = 0)}{P(D=1)V(\\pi(X)|D=1) + (1-P(D=1)V(\\pi(X)|D=0)} \\\\\nw_0 & = 1 - w_1\n\\end{split}\n\\]\nThe reason for this is because regression is prone to extrapolation beyond common support - i.e. it can “estimate” potential outcomes for units that are not observed. This can lead to bias.\nThis is in contrast to the subclassification estimator, which cannot be computed if there are missing observable outcomes for a substratum/category of \\(X\\).\nThe weights of \\(D_i(X_i = x)\\) can also be seen as propensity scores of \\(\\pi(x)(1 - \\pi(x)\\). Therefore:\n\nWeights are higher for groups with propensity scores close to 0.5.\nWeights are low for groups with propensity scores close to 0 or 1.\nOLS minimises estimation uncertainty by downweighting groups of \\(X_i\\) where group-specific ATEs are less precisely estimated.\n\n\n\n\n\n\n\nFully Interacted Estimator\nThe Fully-Interacted Estimator, a newly developed large-sample regression estimator (Lin 2013), solves the heterogeneity bias in the OLS estimator. The fully-interacted estimator takes the form:\n\\[\nY_i = \\hat\\alpha + D_i \\widehat{\\tau}_{int} + (\\mathbf X_i - \\mathbf {\\bar X}) \\hat\\beta +D_i (\\mathbf X_i - \\mathbf{\\bar X}) \\hat\\gamma + \\hat\\varepsilon_i\n\\]\n\nWhere \\(X_i\\) are covariate values sufficient to satisfy conditional independence.\nWhere \\(\\bar X\\) contains the sample means of all \\(X_i\\) covariates.\n\nThis estimator \\(\\hat\\tau_{int}\\) is technically biased when estimating \\(\\tau_{ATE}\\). However, the bias is arbitrarily small in large samples under conditional ignorability.\nThis estimator thus allows us to accurately estimate the ATE even under heterogenous treatment effects, assuming our sample size is sufficiently large.\n\n\n\n\n\n\nOther Solutions to the OLS Bias under Heterogeneity\n\n\n\n\n\nThere are a few other solutions to this issue of OLS bias under heterogeneity:\n\nDoubly-robust estimation uses a weighted average of regression and IPW estimators, which will be asymptotically consistent as long as the regression model is correctly specified.\nMatching as pre-processing uses matching to make treatment and control groups similar, then runs regression models to estimate causal effects.\n\n\n\n\n\n\n\n\n\n\nNonparametric Estimators\n\nSubclassification Estimator\nUsing the discrete identification of the ATE shown in Equation 4 , we can instead use the sample equivalents to get the subclassification estimator:\n\\[\n\\hat\\tau_{ATE} = \\sum\\limits_{j=1}^M \\underbrace{(\\bar Y_{1j} - \\bar Y_{0j})}_{\\tau_{CATE}(j)} \\underbrace{\\frac{n_j}{n}}_{\\text{weight}}\n\\]\n\nWhere \\(M\\) is the number of levels/categories of \\(X\\), and \\(j\\) is one specific level/category of \\(X\\).\nWhere \\(n_j\\) is the number of units in a level/category \\(j\\) of \\(X\\).\nWhere \\(\\bar Y_{dj}\\) is the mean outcome for units with \\(D_i = d\\) in level/category \\(j\\) of \\(X\\).\n\nFor subclassificaion to be possible, within each level \\(j\\) of covariate \\(X\\), there must be at least one unit in control \\(D=0\\) and treatment \\(D=1\\).\nSubclassification is not a particularly popular estimator, because it only works if all covariates are discrete, and only if you have a manageable amount of categories and covariates.\n\n\n\n\n\n\nIntuitive Procedure of Subclassification\n\n\n\n\n\nMore intuitively, the procedure is as follows:\n\nChoose one specific value for all covaraites \\(X\\). Find the average treatment effect within this specific value of \\(X\\).\nMultiply that average treatment effect by the number of observations that meet this specific value of \\(X\\) divided by the total number of units.\nDo this for every possible values of all covaraites \\(X\\), then sum up all the weighted average treatment effects to get the overall ATE.\n\n\n\n\n\n\n\n\n\n\nSubclassification with Multiple Confounders\n\n\n\n\n\nLet us say we have 2 confounders, \\(X_1\\) and \\(X_2\\). Both confounders are categorical with 3 categories.\nWe would need to create \\(M=9\\) levels of strata, for every possible combination of values of \\(X_1\\) and \\(X_2\\). Then, we would estimate the within-strata average treatment effect, and weight them.\nThis illustrates how with large amounts of confounders, you will need a huge number of stratum. This makes subclassification infeasible in many cases.\n\n\n\n\n\n\n\n\n\nSubclassification for the ATT\n\n\n\n\n\nWhen pre-treatment covariate \\(X\\) is discrete, the identification result of the ATT is:\n\\[\n\\tau_{ATT} = \\sum\\limits_{x \\in \\mathcal X} ( E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)) P(X_i = x | D_i = 1)\n\\]\nWe can calculate this within our give sample to get the subclassificaiton estimator:\n\\[\n\\hat\\tau_{ATT} = \\sum\\limits_{j=1}^M(\\bar Y_{1j} - \\bar Y_{0j}) \\frac{n_{1j}}{n_1}\n\\]\n\nWhere \\(M\\) is the number of strata (levels/categories of \\(X\\)).\nWhere \\(n_j\\) is the number of units in a level/category \\(j\\) of \\(X\\).\nWhere \\(n_{1j}\\) is the number of treated cells \\(D = 1\\) in a level/category \\(j\\) of \\(X\\).\nWhere \\(\\bar Y_{dj}\\) is the mean outcome for units with \\(D_i = d\\) in level/category \\(j\\) of \\(X\\).\n\n\n\n\n\n\n\nMatching Estimator\nWe have a missing data problem in causal inference: we do not know all the potential outcomes. Matching “estimates” missing potential outcomes of a unit.\nFor each observation in the treated group, matching finds an observation in the untreated group that have the most similar values of a set of pre-treatment covariates \\(X\\). Thus, we have pairs of treatment-control observations that act as counterfactuals. We can estimate the ATT as the average difference in observed outcomes within the pairs:\n\\[\n\\hat\\tau_{ATT} = \\frac{1}{n_1} \\sum\\limits_{i:D_i = 1}(Y_i - \\widetilde{Y_i})\n\\]\n\nWhere \\(n_1\\) is the number of units in the treatment group.\nWhere \\(Y_i\\) is the unit’s observed \\(Y\\) in the treatment group.\nWhere \\(\\tilde Y_i\\) is unit \\(i\\)’s closest neighbour in the untreated group.\n\nSometimes, a treatment unit may not have one close control unit to match to. Instead, we could use a combination of control units to match to the treatment unit, and use the average \\(Y\\) of those combination of control units to approximate a more accurate match.\nSuppose we use \\(M_i\\) number of close control units to match to a treatment unit \\(i\\). Then, the matching estimator would be defined as follows:\n\\[\n\\hat\\tau_{ATT} = \\frac{1}{n_1} \\sum\\limits_{i:D_i = 1}(Y_i - \\left(\\frac{1}{M_i} \\sum\\limits_{m=1}^{M_i} \\widetilde{Y_{i_m}}\\right))\n\\]\nWhere \\(\\widetilde{Y_{i_m}}\\) is the obsered outcome for the \\(m\\)th match of unit \\(i\\).\n\n\n\n\n\n\nChoices during Matching\n\n\n\n\n\nWe have to make several choices when conducting matching.\n\nWhat covariates to match on. We generally want to select a set of pre-treatment covariates \\(X\\) such that these covariates ensure the conditional ignorability assumption is met.\nMatch with or without replacement. Matching with replacement means that once you have used one control unit to match to a treatment unit, you can still use that same control unit to match to another treatment unit. This has advantages since you can ensure better and closer matches. However, matching without replacement is also possible.\nHow many to match. You can decide to match multiple control units to one treatment unit, and use the average of the treatment units to approximate a true control unit. This may result in more accurate matches for treatment units that may not have a good single control unit to match to.\n\nWe can also choose to use more advanced matching methods, such as Mahalanobis Distance matching or Propensity Score matching, which are shown below. These are good for matching on more \\(X\\).\n\n\n\n\n\n\n\n\n\nWeaknesses of Matching\n\n\n\n\n\nMatching does not always create “perfect” matches. This means that the pairs matched together may not be identical to each other in terms of covariates \\(X\\) or potential outcomes.\nThe inability to find exact matches can cause bias, especially for the more covariates we match on (see below).\n\n\n\n\n\n\nMatching with Multiple Covariates\nConsider that we \\(k&gt;1\\) number of confounders \\(X\\). Now, we have to match observations in \\(k\\) variables, which implies we are in a multidimensional \\(\\mathbb R^k\\) space.\nThe most commonly used distance metric is Mahalanobis Distance - which measures the distance in \\(X_i\\) between units \\(i\\) and \\(j\\):\n\\[\nD_M (\\mathbf X_i, \\mathbf X_j) = \\sqrt{(\\mathbf X_i - \\mathbf X_j)^T \\boldsymbol\\Sigma_X^{-1} (\\mathbf X_i - \\mathbf X_j)}\n\\]\n\nWhere \\(\\boldsymbol \\Sigma_X\\) is the sample variance-covariance matrix of \\(\\mathbf X_i\\).\n\n\n\n\n\n\n\nEuclidean Distance\n\n\n\n\n\nEuclidean distance is another common distance metric:\n\\[\nD_E ( \\mathbf X_i, \\mathbf X_j) = \\sqrt{(x_{1i} - x_{1j})^2 + (x_{2i}-x_{2j})^2 + \\dots + (x_{ki}-x_{kj})^2}\n\\]\nEuclidean distance, while very simple, is not recommended. This is because Euclidean distance with non-normalised variables can get you very bizarre results that depend on the scale of the variables.\nThere are other distance metrics, but these are exceedingly rare in selection on observables.\n\n\n\nThere is one issue with matching in multi-dimensional spaces. It becomes very difficult to match every unit \\(i\\) on every covariate \\(X\\), even if we have a large number of observations.\n\n\n\n\n\n\nCurse of Dimensionality\n\n\n\n\n\nWhen we try to match on more than one \\(X\\) variable, we go from matching on a number line \\(\\mathbb R^1\\) to a \\(n\\)-dimensional space, \\(\\mathbb R^n\\).\nThe search space increases exponentially as you increase the number of dimensions.\n\nTake a look at the figure on the left. If we only match on a one dimensional plane (lets say the horizontal line between 0 and 1), we can see our red line covers approximately 30% of the horizontal line. But in 3 dimensions, our red box covers a significantly less proportion of the entire cube.\nThe figure on the right illustrates this. \\(d\\) represents the dimensions. We can see as the dimensions increase, the fraction of volume increases significantly slower relative to distance.\nThus, with a bigger space, the distance between two units increases, so you get worse matches.\n\n\n\nThis curse of dimensionality creates a bias problem - since we get non-exact matches. The more dimensions you add, the worse it becomes.\n\n\n\n\n\n\nMore on Bias\n\n\n\n\n\nThe poor matches caused by increased dimensionality inject error into our estimates of missing potential outcomes.\nThe bias term as you increase the number of dimensions \\(k\\), changes by \\(N^{(-1/k)}\\). This implies no \\(\\sqrt{n}\\) consistency for \\(k&gt;2\\).\nIf \\(N_0\\) (number of untreated units) is much larger than \\(N_1\\) (number of treated units), bias will typically be smaller.\nThere are ways to correct this bias, including Abadie and Imbens (2011) Bias Correction method.\nThere is a new method: Bias-corrected matching, which estimates bias ineherent to mathching estimators via regression, then subtracts it from the matching estimate to correct for it.\n\n\n\n\n\n\nPropensity Scores Matching\nPropensity Score matching is an alternative way to match over many dimensions. The propensity score is an unobserved property, defined as the probability of a unit \\(i\\) of receiving treatment:\n\\[\n\\pi(X_i) \\equiv P(D_i = 1|X_i)\n\\]\nWhen supposing the conditional ignorability and common support assumptions, the propensity score \\(\\pi(X_i)\\) has the balancing property: \\(D_i \\perp X_i \\ | \\ \\pi(X_i)\\). This implies that conditional ignorability holds on the propensity scores alone:\n\\[\n(Y_{1i}, Y_{0i}) \\perp\\!\\!\\!\\!\\perp D_i \\ | \\ \\pi(X_i)\n\\]\nThus, instead of conditioning on \\(X_i\\) as we did in selection on observables, we can instead condition on \\(\\pi (X_i)\\), and still identify the causal estimand.\nHowever, we do not actually observe \\(\\pi (X_i)\\). We estimate \\(\\pi (X_i)\\) with a binary response model, with outcome variable \\(D_i\\), and explanatory variables \\(X_i\\). This will get us a fitted probability \\(P(D_i = 1) = \\hat\\pi(X_i)\\).\nThen, once we have the propensity score estimates \\(\\hat\\pi(X_i)\\), we can do nearest neighbour matching with the propensity scores (in \\(\\mathbb R^1\\)). This will allow us to identify the \\(\\tau_{ATT}\\).\n\n\n\n\n\n\nBalance Checks\n\n\n\n\n\nThe accurate estimation of \\(\\tau_{ATT}\\) implies an accurate prediction of the propensity scores \\(\\pi(X_i)\\). We can test our matched treatment and control groups to see if the balancing property holds for covariates \\(X_i\\).\n\n\n\n\n\n\nGenetic Matching\n\n\n\n\n\n\nWeighting Estimator\n\nIdentification with Weighting\nWe know that the ATE can be written as a weighted average, as shown in Equation 4 . We can rewrite the \\(\\tau_{ATE}\\) as follows using observed potential outcomes outcomes and conditional ignorability ( Equation 1 ).\n\\[\n\\begin{split}\n& = \\sum\\limits_{x \\in \\mathcal X} \\underbrace{(E(Y_{1i}|D_i = 1, X_i = x)}_{\\because \\text{ observed}} - \\underbrace{E(Y_{0i}|D_i = 0, X_i = x)}_{\\because \\text{ observed}})P(X_i = x) \\\\\n& = \\sum\\limits_{x \\in \\mathcal X}  (\\underbrace{E(Y_{1i}|X_i = x)}_{\\because \\text{ eq. (1)}} - \\underbrace{E(Y_{0i}|X_i = x)}_{\\because \\text{ eq. (1)}})P(X_i = x) \\\\\n& = \\underbrace{E[E(Y_{1i}|X_i = x) - E(Y_{0i}|X_i = x)]}_{\\text{definition of weighted average}}\n\\end{split}\n\\]\nLet us do an algebra trick - multiply both terms within the CATE by 1 (in blue):\n\\[\n\\begin{split}\n& = E \\left [E(Y_{1i}|X_i=x) \\color{blue}{\\frac{\\pi(X_i)}{\\pi(X_i)}}\\color{black} - (E(Y_{0i}|X_i=x) \\color{blue}{\\frac{1-\\pi(X_i)}{1-\\pi(X_i)}} \\right] \\\\\n& \\color{black} = E \\left[ \\frac{E(Y_{1i}|X_i = x) \\pi(X_i)}{\\pi(X_i)} -  \\frac{E(Y_{0i}|X_i = x) (1-\\pi(X_i))}{1-\\pi(X_i)} \\right]\n\\end{split}\n\\]\nWe know that propensity score \\(\\pi(X_i) := E(D_i|X_i = x)\\). Thus, we can convert the above to:\n\\[\n\\begin{split}\n& = E \\left[ \\frac{E(Y_{1i}|X_i = x)E(D_i|X_i = x)}{\\pi(X_i)} - \\frac{E(Y_{0i}|X_i = x)(1-E(D_i|X_i = x))}{1-\\pi(X_i)}\\right] \\\\\n& = E \\left[ \\frac{E(Y_{1i}|X_i = x)E(D_i|X_i = x)}{\\pi(X_i)} - \\frac{E(Y_{0i}|X_i = x)E(1-D_i|X_i = x)}{1-\\pi(X_i)}\\right]\n\\end{split}\n\\]\n\\[\n\\begin{align}\n& = E \\left[ \\frac{E(Y_{1i}D_i|X_i = x)}{\\pi(X_i)} - \\frac{E(Y_{0i}(1-D_i)|X_i = x)}{1 - \\pi(X_i)}\\right] && (\\text{property of expectation})\\\\\n& = E \\left[ E \\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} |X_i = x\\right) - E \\left( \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)} | X_i = x \\right) \\right] && (\\text{property of expectation})\\\\\n& = E\\left[ E\\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} - \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)} |X_i = x \\right) \\right]  && (\\text{property of expectation})\n\\end{align}\n\\]\n\\[\n\\begin{align}\n& = E\\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} - \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)}\\right) && (\\text{LIE: } E(X) = E[E(X|Y)] \\ ) \\\\\n& = E\\left( \\frac{Y_{i}D_i}{\\pi(X_i)} - \\frac{Y_{i}(1-D_i)}{1-\\pi(X_i)}\\right) && (\\text{observered outcome}) \\\\\n& = E \\left( \\frac{\\color{blue}{Y_i} \\color{black}D_i(1-\\pi(X_i))-\\color{blue}{Y_i}\\color{black}(1-D_i)\\pi(X_i)}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{getting common denom.}) \\\\\n& = E\\left( Y_i \\frac{D_i(1-\\pi(X_i))-(1-D_i)\\pi(X_i)}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{factor out }Y_i) \\\\\n& = E\\left( Y_i \\frac{D_i - D_i\\pi(X_i)-(\\pi(X_i) -D_i\\pi(X_i))}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{distribute out}) \\\\\n&  = E\\left( Y_i \\frac{D_i \\color{blue}{- D_i\\pi(X_i)} \\color{black}-\\pi(X_i) \\color{blue}{+ D_i\\pi(X_i)}}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{distribute out negative})\\\\\n& = E\\left( Y_i \\frac{D_i -\\pi(X_i) }{\\pi(X_i)(1-\\pi(X_i))}\\right) && (\\text{cancel out})\\\\\n\\end{align}\n\\tag{7}\\]\nAnd thus, we have identified the ATE.\n\n\n\nInverse Probability Weighting Estimator\nAn alternative use of propensity scores is weighting. As shown above, under conditional ignorability and common support, we can identify the ATE as:\n\\[\n\\tau_{ATE} = E\\left[ Y_i \\times \\underbrace{\\frac{D_i - \\pi(X_i)}{\\pi(X_i) (1 - \\pi(X_i))}}_{\\text{weight}}\\right]\n\\]\nThe inverse probability weighting (IPW) estimator is the sample estimator:\n\\[\n\\begin{split}\n\\hat\\tau_{ATE} & = \\frac{1}{N} \\sum\\limits_{i=1}^N \\left(Y_i \\frac{D_i - \\hat\\pi(X_i)}{\\hat\\pi(X_i) (1 - \\hat\\pi(X_i))} \\right) \\\\\n& = \\frac{1}{N} \\sum\\limits_{i=1}^N \\left(\\frac{D_i Y_i}{\\hat\\pi(X_i)} - \\frac{(1-D_i) Y_i}{1 - \\hat\\pi(X_i)} \\right)\n\\end{split}\n\\]\n\nThe second equation is equivalent to the first, shown by Equation 7 .\n\nEssentially, those who are unlikely to be treated but do get treated get weighted more, and individuals who are likely to be treated but do not get treated get weighted more.\n\n\n\n\n\n\nWeighting Estimator for ATT\n\n\n\n\n\nThe identification of the ATT under both conditional ignorability and common support are:\n\\[\n\\tau_{ATT} = \\frac{1}{Pr(D = 1)} \\times E\\left[ Y_i \\times \\underbrace{\\frac{D_i - \\pi(X_i)}{(1 - \\pi(X_i))}}_{\\text{weight}}\\right]\n\\]\nThe sample IPW estimator would be:\n\\[\n\\begin{split}\n\\hat\\tau_{ATT} & = \\frac{1}{N_1}\\sum\\limits_{i=1}^N \\left( Y_i \\frac{D_i - \\hat\\pi(X_i)}{1 - \\hat\\pi(X_i)} \\right) \\\\\n& = \\frac{1}{N_1} \\sum\\limits_{i=1}^N \\left( D_iY_i - (1-D_i)Y_i \\frac{\\hat\\pi(X_i)}{1 - \\hat\\pi(X_i)} \\right)\n\\end{split}\n\\]\n\n\n\nThe IPW estimator is asymptotically consistent, but has very poor small sample properties. They are highly sensitive to extreme values of \\(\\hat\\pi(X_i)\\). This generates high variance (inefficiency), and can produce significant bias under model mispecification.\n\n\n\n\n\n\nFalsification Tests\n\nTesting Assumptions with Falsification\nThe stronger (bolder) our assumptions for identification, the less credible our results are. Selection on Observables involves a very strong and hard to verify assumption: conditional ignorability. Can we really be sure that we have controlled for all confounders \\(X_i\\) needed to satisfy conditional ignorability?\nPlacebo tests are a type of falsification test to show evidence against our assumptions. Suppose that we make the assumption of conditional ignorability \\((Y_{0i}, Y_{1i}) \\perp D_i | X_i\\). Suppose we are concerned about the presence of another confounder \\(U\\) that is not included in \\(X_i\\).\n\n\n\n\n\nThe presence of \\(U\\) will falsify our conditional ignorability assumption, and means we cannot identify the causal effect of \\(D \\rightarrow Y\\).\n\n\n\n\n\n\nFalsification vs. Validation\n\n\n\n\n\nFalsification is a principle of trying to criticise our own research, rather than defend it. Falsification is about testing if our assumptions are not met. Failing a test provides evidence that our assumption is not met.\n\nEx. Covariates are balanced - thus there is no evidence that our assumptions are not met. We are not saying that our assumption is correct, just that there is no evidence against it.\n\nValidation is the opposite - we test to see if there is evidence in favour of our assumptions.\n\nEx. Covariates are balanced - thus our assumptions are met.\n\n\n\n\nFor falsification tests, we should not just pay attention to statistical significance - we must also pay attention to the magnitude of the point estimation.\n\n\n\nPlacebo Outcome Test\nA placebo outcome test utilities another alternative outcome variable \\(Y'\\) that is caused by our hypothesised unobserved confounder \\(U\\):\n\n\n\n\n\nWe can see that if \\(U\\) does not exist, \\(D\\) should have zero effect on the new outcome \\(Y'\\). Thus, if \\(U\\) is present, we should find a relationship between \\(D\\) and \\(Y'\\).\n\\[\nY'_i = \\gamma + \\delta D_i + \\varepsilon_i\n\\]\n\nIf we find that there is an effect of \\(D\\) on the new outcome \\(Y'\\) (non-zero \\(\\delta\\)), that is evidence that \\(U\\) exists, and is evidence to reject our conditional ignorability assumption (falsifies our design - red flag!).\nIf you do not find an effect of \\(D\\) on new outcome \\(Y'\\) (\\(\\delta = 0\\)) you find no evidence of \\(U\\), and no evidence to reject our conditional ignorability assumption (fails to falsify our design).\n\nWe must be sure that \\(Y\\) is not related to \\(Y'\\) except through \\(D\\) and \\(U\\). If this is true, we just run our original research design but replace \\(Y\\) with \\(Y'\\).\n\n\n\nPlacebo Treatment Test\nA placebo treatment test involves some other treatment \\(D'\\), that was assigned at the same time\n\n\n\n\n\nWe can see that if \\(U\\) does not exist, the effect of \\(D'\\) should have no effect on \\(Y\\). If \\(U\\) does exist, there should be some effect of \\(D'\\) on \\(Y\\).\n\\[\nY_i = \\gamma + \\delta D'_i + \\varepsilon_i\n\\]\n\nIf we find that there is an effect of \\(D'\\) on \\(Y\\) (non-zero \\(\\delta\\)), that is evidence that \\(U\\) exists, and is evidence to reject our conditional ignorability assumption (falsifies our design - red flag!).\nIf you do not find an effect of \\(D'\\) on \\(Y\\) (\\(\\delta = 0\\)) you find no evidence of \\(U\\), and no evidence to reject our conditional ignorability assumption (fails to falsify our design).\n\nWe must be sure that \\(Y\\) is not related to \\(D'\\) except through \\(D\\) and \\(U\\). If this is true, we just run our original research design but replace \\(D\\) with \\(D'\\).\n\n\n\n\n\n\nExtension: Partial Identification\n\nDecomposing the ATE\nWith falsification, we were concerned with what assumptions we needed to be not-false in order to identify the ATE. However, we can take a different approach - what can we learn about the ATE without any assumptions?\nLet us decompose the ATE into parts:\n\\[\n\\begin{align}\n\\tau_{ATE}  = & E(Y_{1i} - Y_{0i}) \\\\\n& \\\\\n= & E(Y_{1i} - Y_{0i} | D_i = 1) P(D_i = 1) \\\\\n& \\quad - E(Y_{1i} - Y_{0i}|D_i = 0)P(D_i = 0)  && (\\text{def. of weighted avg.})\\\\\n& \\\\\n= & [ \\color{blue}{E(Y_i |D_i = 1)} \\color{black}- \\color{red}{E(Y_{0i}|D_i = 1)} \\color{black}] P(D_i = 1) \\\\\n& \\quad + [ \\color{red}{E(Y_{1i}|D_i = 0)} \\color{black} - \\color{blue}{E(Y_i|D_i = 0)} \\color{black}]P(D_i = 0) && (\\text{observed + unobserved})\n\\end{align}\n\\]\nSome of the quantities are observed (in blue), and some of the quantities are unobserved (in red). Previously, we made assumptions (conditional ignorability, common support) to fill the unobserved quantities. But, we can make actually any assumption as possible.\n\n\n\nNonparametric Bounds\nOne way to fill in our unobserved outcomes through the “best” and “worst” possible outcomes. This allows us to construct a plausible range of the ATE.\nFirst, let us construct the worst-case scenario - the lowest possible \\(\\tau\\).\n\n\\(E(Y_{0i}|D_i = 1) = Y_H\\). Units in the treated \\(D_i=1\\), their potential outcome \\(Y_{0i}\\) will be the highest \\(Y\\) possible, \\(Y_H\\).\n\\(E(Y_{1i}|D_i = 0) = Y_L\\). Units in the control \\(D_i=0\\), their unobserved potential outcome \\(Y_{1i}\\) will be the lowest \\(Y\\) possible, \\(Y_L\\).\n\nThus, the lowest possible \\(\\tau\\) (sharp lower bound) is:\n\\[\n\\begin{split}\n\\tau_L = & [ \\color{blue}{E(Y_i |D_i = 1)} \\color{black}  - \\color{red}{Y_H} \\color{black}] P(D_i = 1) \\\\\n& \\quad + [ \\color{red}{Y_L} \\color{black} - \\color{blue}{E(Y_i|D_i = 0)} \\color{black}]P(D_i = 0)\n\\end{split}\n\\]\nNow, let us construct the best-case scenario - the highest possible \\(\\tau\\).\n\n\\(E(Y_{0i}|D_i = 1) = Y_L\\). Units in the treated \\(D_i=1\\), their potential outcome \\(Y_{0i}\\) will be the lowest \\(Y\\) possible, \\(Y_L\\).\n\\(E(Y_{1i}|D_i = 0) = Y_H\\). Units in the control \\(D_i=0\\), their unobserved potential outcome \\(Y_{1i}\\) will be the highest \\(Y\\) possible, \\(Y_H\\).\n\nThus, the highest possible \\(\\tau\\) (sharp upper bound) is:\n\\[\n\\begin{split}\n\\tau_H = & [ \\color{blue}{E(Y_i |D_i = 1)} \\color{black}  - \\color{red}{Y_L} \\color{black}] P(D_i = 1) \\\\\n& \\quad + [ \\color{red}{Y_H} \\color{black} - \\color{blue}{E(Y_i|D_i = 0)} \\color{black}]P(D_i = 0)\n\\end{split}\n\\]\nWe know that the true \\(\\tau_{ATE} \\in [\\tau_L, \\tau_H]\\).\n\n\n\nMonotone Treatment Selection Assumption\nOur extreme case from above is not very useful. However, we can layer on assumptions to lower the possible \\(\\tau\\) values.\nOne assumption is the Monotone Treatment Selection (MTS) assumption. This assumption basically says that potential outcomes for units in treatment, are always higher than for those in the control.\n\\[\n\\begin{split}\n& E(Y_{0i}|D_i = 0) ≤ \\overbrace{E(Y_{0i}|D_i = 1)}^{\\text{unobserved}} \\\\\n& \\underbrace{E(Y_{1i} |D_i = 0)}_{\\text{unobserved}} ≤ E(Y_{1i} |D_i = 1)\n\\end{split}\n\\]\nThis is basically saying that selection bias is one-direction.\nThis implies a tighter sharp upper bound on \\(\\tau\\).\n\\[\n\\begin{align}\n\\tau_H = & [ \\underbrace{E(Y_i |D_i = 1)}_{\\text{observed}}  - \\color{red}{E(Y_{0i}|D_i = 0)} \\color{black}] P(D_i = 1) \\\\\n& \\quad + [ \\color{red}{E(Y_{1i}|D_i =1)} \\color{black} - \\underbrace{E(Y_i|D_i = 0)}_{\\text{observed}}]P(D_i = 0) \\\\\n= & E(Y_i|D_i = 1) - E(Y_i|D_i = 0) && (\\text{def. of weighted avg.})\\\\\n\\end{align}\n\\]\nThis indicates that the upper bound of plausible \\(\\tau_{ATE}\\) values is the naive estimator of differences in observed outcomes.\nWe can also make the reverse assumption, where selection bias is in the opposite direction. This means a tighter sharp lower bound \\(\\underline\\tau\\). These assumptions help narrow our possible \\(\\tau_{ATE}\\) values, and can allow us to test if our estimated \\(\\hat\\tau\\) is reasonable (within the plausible bounds).\n\n\n\n\n\n\nImplementation in R\nFor all methods, you will need the tidyverse package:\n\nlibrary(tidyverse)\nlibrary(MatchIt)\nlibrary(estimatr)\n\nSee how to perform each estimator in R:\n\n\n\n\n\n\nDistance Matching\n\n\n\n\n\nFirst, let us conduct nearest neighbour matching with Mahalanobis distance by using the matchit() function.\n\nmatch_object = MatchIt::matchit(D ~ X1 + X2 + X3,\n                                data = my_data,\n                                method = \"nearest\", #distance matching\n                                distance = \"mahalanobis\")\n\n# for output summary\nsummary(match_object)\n\nSecond, let us save the matched data with the match.data() function.\n\nmatch_data &lt;- MatchIt::match.data(match_object,\n                                  weights = 'nn_weights')\n\nThird, we can test if matching worked by using a balance table and a love plot:\n\n# balance table\ncobalt::bal.tab(D ~ X1 + X2 + X3, \n                data = match_data, # from the 2nd step\n                weights = \"nn_weights\",\n                disp = c(\"means\", \"sds\"))\n\n#love plot\ncobalt::love.plot(match_object,\n                  data = my_data, #original dataset\n                  stars = 'raw')\n\nFinally, we can estimate the treatment effect. There are two options - either using a weighted regression, or using the matching algorithm:\n\n# using weighted regression\nestimate &lt;- lm_robust(Y ~ D,\n                      data = match_data, #data from step 2\n                      weights = nn_weights)\nsummary(estimate)\n\n## using the Matching package:\nestimate = Matching::Match(Y = my_data$Y, #outcome\n                           Tr = my_data$D, #treatment\n                           X = my_data[,c(\"X1\", \"X2\", \"X3\")], #covariates\n                           M=1, #number of neighbours\n                           BiasAdjust = TRUE, #for biased adjustment\n                           Weight = 2)\nsummary(estimate)\n\nYou will have the estimates that you can use.\n\n\n\n\n\n\n\n\n\nPropensity Score Matching\n\n\n\n\n\nFirst, we want to estimate propensity scores with a logistic regression (or a random forest):\n\n#logistic model\npscore_model = glm(D ~ X1 + X2,\n                   data = my_data,\n                   family = \"binomial\")\n\n# estimate propensity scores\nmy_data$pscore_estimate = predict(pscore_model,\n                                  my_data,\n                                  type = \"response\")\n\nNow, let us match with propensity scores:\n\n# match\nmatch_object = MatchIt::matchit(D ~ pscore_estimate,\n                                data = my_data,\n                                method = \"nearest\",\n                                distance = \"Mahalanobis\")\n\n# save matched data\nmatch_data &lt;- MatchIt::match.data(match_object,\n                                  weights = 'pscore_weights')\n\nThird, we can test if matching worked with a balance table and a love plot:\n\n#balance table\ncobalt::bal.tab(D ~ X1 + X2 + X3,\n                data = match_data, #matched data from step 2\n                weights = \"pscore_weights\",\n                disp = c(\"means\", \"sds\"))\n\n#love plot\ncobalt::love.plot(match_object,\n                  data = my_data, #original dataset\n                  addl = ~ X1 + X2 + X3,\n                  stars = 'raw')\n\nFinally, let us do the estimation:\n\nestimate &lt;- lm_robust(Y ~ D,\n                      data = match_data, #from step 2\n                      weights = pscore_weights)\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nInverse Probability Weighting\n\n\n\n\n\nFirst, we want to estimate propoensity scores with a logistic regression (or a random forest):\n\n#logistic model\npscore_model = glm(D ~ X1 + X2,\n                   data = my_data,\n                   family = \"binomial\")\n\n# estimate propensity scores\nmy_data$pscore_estimate = predict(pscore_model,\n                                  type = \"response\")\n\nSecond, we calculate the inverse probability weights based on the formula from earlier:\n\nmy_data$ipweight = ifelse(my_data$D == 1, # condition\n                       1/my_data$pscore_estimate,\n                       1/(1-my_data$pscore_estimate))\n\nFinally, we can estimate the ATE, or ATT, or use a weighted regression for the ATE:\n\n# ATE estimator\nmean((my_data$D * my_data$Y) * my_data$ipweight - ((1 - my_data$D) * my_data$Y) * my_data$ipweight)\n\n# ATT estimator\nsum(my_data$D * my_data$Y - (1 - my_data$D) * my_data$Y * (my_data$pscore_estimate/(1 - my_data$pscore_estimate)))/sum(my_data$D)\n\n# ATE with weighted regression\nestimate &lt;- lm_robust(Y ~ D, \n                      data = my_data,\n                      weights = ipweight)\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nOLS Estimator\n\n\n\n\n\nFor the OLS estimator, we can use the lm_robust() function:\n\nestimate &lt;- lm_robust(Y ~ D + X1 + X2 + X3,\n                      data = my_data)\nsummary(estimate)\n\nWe can also use the fixest package and the feols() function:\n\nlibrary(fixest)\n\nestimate &lt;- feols(Y ~ D + X1 + X2 + X3,\n                  data = my_data,\n                  se = \"hetero\")\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nFully Interacted Estimator\n\n\n\n\n\nFor the fully interacted estimator, we can use the lm_lin() function.\n\nestimate &lt;- lm_lin(Y ~ D,\n                   covariates = ~ X1 + X2 + X3,\n                   data = my_data)\nsummary(estimate)\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "5 Selection on Observables"
    ]
  },
  {
    "objectID": "quant4.html",
    "href": "quant4.html",
    "title": "Selection on Observables",
    "section": "",
    "text": "In the last chapter, we discussed randomisation. Randomisation is great, but, it requires specific circumstances of the research having control over the assignment mechanism. However, in the social sciences, this rarely occurs.\nThis chapter introduces the selection on observables framework, which allows us to identify causal effects in an observational setting by controlling for observable pre-treatment covariates. We discuss the main estimators, including regression, matching, and weighting.\nUse the right sidebar for quick navigation. R-code provided at the bottom.\n\n\nIdentification\n\nBlocking Backdoor Paths\nWithout randomisation, we need some other way to account for pre-treatment covariates that may be confounding and causing selection bias. Controlling for a set of nodes/confounders \\(X\\) can identify the causal effect of \\(D \\rightarrow Y\\), if the following conditions are met:\n\nNo node within set \\(X\\) is a descendant of \\(D\\) (no element within \\(X\\) results from \\(D\\)).\nThe nodes within set \\(X\\) block all back-door paths from \\(D \\rightarrow Y\\).\n\n\n\n\n\n\nIn the figure above, let us block the backdoor paths between \\(D \\rightarrow Y\\):\n\nBackdoor path \\(D \\rightarrow X \\rightarrow Y\\). To block this path, we must control for \\(X\\).\nBackdoor path \\(D \\rightarrow V \\rightarrow Y\\). We do not need to control for \\(V\\), since it is post-treatment (a descendant of \\(D\\)). In fact, \\(V\\) is a bad control (see below).\n\nThus, to identify \\(D \\rightarrow Y\\) here, we only need to control for \\(X\\), and no other variable.\n\n\n\n\n\n\nGood and Bad Controls\n\n\n\n\n\nGood controls block backdoor paths, which facilitate identification of the causal effect.\nBad controls are when we control for post-treatment variables. For example, \\(P\\) below is a bad control, since it is caused by \\(D\\), so it is post-treatment.\n\n\n\n\n\nYou also never want to control variables that only predict \\(D\\). These are bad because controlling for these removes variation in \\(D\\) that could be useful.\nNeutral controls are ones that don’t identify the causal effect, but improve efficiency. For example, \\(Q\\) below affects \\(Y\\), but there is no backdoor path. Controlling \\(Q\\) will not help identification, but can control noise in \\(Y\\) which may increase efficiency.\n\n\n\n\n\n\n\n\n\n\n\nIdentification Assumptions\nOnce we have determined the set of confounders \\(X\\) that we need to control to block all backdoor paths, the assumptions needed for identification of causal effects are:\n\nConditional Ignorability (also known as exogeneity or independence): Among units with identical confounder values \\(X_i\\), treatment \\(D_i\\) is as-if randomly assigned. Or in other words, potential outcomes are independent from treatment within each specific confounder value \\(X_i = x\\).\n\n\\[\n(Y_{0i}, Y_{1i}) \\perp\\!\\!\\!\\!\\perp D_i  \\ | \\ X_i = x, \\quad \\forall \\ x \\in \\mathcal X\n\\]\nThis implies that for any given value of all confounders \\(X_i = x\\), we know that potential outcomes \\(Y_{di}\\) are equivalent between treatment and control:\n\\[\n\\begin{split}\nE(Y_{1i}|X_i = x) = E(Y_{1i}|D_i = 1, X_i = x) = E(Y_{1i}|D_i = 0, X_i = x) \\\\\nE(Y_{0i}|X_i = x) = E(Y_{0i}|D_i = 1, X_i = x) = E(Y_{0i}|D_i = 0, X_i = x)\n\\end{split}\n\\tag{1}\\]\n\nCommon Support: for any unit \\(i\\) with value of \\(X_i\\), there is a non-zero probability that they could be assigned to both control \\(D_i = 0\\) or treatment \\(D_i = 1\\).\n\n\\[\n0 &lt; P(D_i = 1 \\ | X_i = x) &lt; 1 \\quad \\forall \\ x \\in \\mathcal X\n\\]\n\n\n\n\n\n\nExample of Identification Assumptions\n\n\n\n\n\nImagine we have a theory that being abducted \\(D\\) causes turning out to vote.\nBlattman (2009) finds that age is the primary way violent groups chose to abduct individuals: abduction parties released young children and older adults, but kept all adolescent and young males.\nThat means our theory is that age \\(X\\) affects selection into treatment \\(D\\). Young children and older adults are less likely to get abducted \\(D\\), while adolescent and young males are more likely \\(D\\).\n\n\n\n\n\n\nIdentification of the ATE\nWith our assumptions above, we can identify the ATE. We start with the conditional average treatment effect, conditional on some value of confounders \\(X_i = x\\). Note the properties shown in Equation 1 .\n\\[\n\\begin{align}\n\\tau_{CATE}(x) & = E(Y_{1i} - Y_{0i} \\ | \\ X_i = x) \\\\\n& = E(Y_{1i}|X_i = x) - E(Y_{0i}|X_i = x) && (\\text{property of expectation}) \\\\\n& = E(Y_{1i}|D_i = 1, X_i = x) - E(Y_{0i}|D_i = 0X_i = x) &&( \\because \\text{equation (1)} \\ ) \\\\\n& = \\underbrace{E(Y_i|D_i = 1, X_i = x)}_{\\because \\text{ observable}} - \\underbrace{E(Y_i|D_i = 0, X_i = x)}_{\\because \\text{ observable}}\n\\end{align}\n\\tag{2}\\]\nNow, let us discuss the ATE, and plug in the CATE from Equation 2 to identify it:\n\\[\n\\begin{align}\n\\tau_{ATE} & = E(Y_{1i} - Y_{0i}) \\\\\n& = \\int \\underbrace{E(Y_{1i} - Y_{0i} \\ | \\ X_i = x)}_{\\tau_{CATE}(x)} d \\ \\underbrace{P(X_i = x)}_{\\text{weight}} && (\\text{weighted average})\\\\\n& = \\int(\\underbrace{E(Y_i|D_i = 1, X_i) - E(Y_i|D_i = 0, X_i)}_{\\because \\text{ equation (2)}})d \\ P(X_i = x)\n\\end{align}\n\\tag{3}\\]\nThus \\(\\tau_{ATE}\\) is identified as the weighted average of all the CATEs, who themselves are difference-in-means of the observed \\(Y_i\\) at every possible value of \\(X_i = x\\).\nWe assumed that the pre-treatment covariate \\(X\\) is continuous. This is why we need an integral. However, we can simplify this if \\(X\\) is discrete:\n\\[\n\\tau_{ATE} = \\sum\\limits_{x \\in \\mathcal X} ( E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)) P(X_i = x)\n\\tag{4}\\]\n\n\n\nIdentification of the ATT\nWe can weaken conditional ignorability, and still identify the ATT. Only \\(Y_{0i}\\) needs to be independent of \\(D_i\\) for units with the same covariates \\(X_i\\). Or in other words, \\((Y_{0i}) \\perp\\!\\!\\!\\perp D_i | X_i = x\\). This implies:\n\\[\nE(Y_{0i}|X_i = x) = E(Y_{0i}|D_i = 0, X_i = x) = E(Y_{0i}|D_i = 1, X_i = x)\n\\tag{5}\\]\nStart with the conditional ATT, using weakened conditional ignorability from Equation 5 :\n\\[\n\\begin{split}\n\\tau_{CATT}(x) & = E(Y_{1i}-Y_{0i}|D_i = 1, X_i = x) \\\\\n& = E(Y_{1i}|D_i = 1, X_i = x) - E(Y_{0i}|D_i = 1, X_i = x) \\\\\n& = E(Y_{1i}|D_i = 1, X_i = x) - \\underbrace{E(Y_{0i}|D_i = 0, X_i = x)}_{\\because \\text{ equation (5)}} \\\\\n& = \\underbrace{E(Y_i|D_i=1, X_i = x)}_{\\because \\text{ observable}} - \\underbrace{E(Y_1|D_i = 0, X_i x)}_{\\because \\text{ observable}}\n\\end{split}\n\\tag{6}\\]\nNow, look at the ATT, and plug in CATT from Equation 6 to identify it.\n\\[\n\\begin{align}\n\\tau_{ATT} & = E(Y_{1i} - Y_{0i}|D_i = 1) \\\\\n& = \\int \\underbrace{E(Y_{1i} - Y_{0i}|D_i = 1, X_i = x)}_{\\tau_{CATT}(x)}d \\ \\underbrace{P(X_i = x|D_i = 1)}_{P(X_i = x) \\text{ within treated}} \\\\\n& = \\int (\\underbrace{E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)}_{\\because \\text{ equation (6)}})d \\ P(X_i = x|D_i = 1)\n\\end{align}\n\\]\nWe can simplify this if \\(X\\) is discrete:\n\\[\n\\tau_{ATT} = \\sum\\limits_{x \\in \\mathcal X} ( E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)) Pr(X_i = x | D_i = 1)\n\\]\nEven when all assumptions are met for identification of the ATE, the \\(\\tau_{ATE}\\) can be different than the \\(\\tau_{ATT}\\). This is because the weights \\(Pr(X_i = x|D_i = 1)\\) for the ATT are different than the ATE \\(Pr(X_i = x)\\).\n\n\n\n\n\n\nParametric Estimators\n\nOrdinary Least Squares Estimator\nOLS is a natural approach for controlling for confounders \\(X\\), since \\(\\hat\\beta_{OLS}\\) estimates partial out the effects of covariates. OLS is a good estimator of \\(\\tau_{ATE}\\) under 2 conditions:\n\nConstant treatment effect: \\(\\tau_i = Y_{1i} - Y_{0i}\\) for all units \\(i\\).\nLinearity: Potential outcomes are linear, and can be written as:\n\n\\[\nY_i(d) = \\beta_0 + d\\beta_1 + \\mathbf X_i \\gamma + \\varepsilon_i \\quad \\text{for} \\quad d = 0, 1\n\\]\nWhy these conditions? Suppose we have the above linear potential outcomes. We can show:\n\\[\n\\begin{align}\n\\tau_i & = Y_{1i} - Y_{0i} && (\\text{definition of } \\tau_i) \\\\\n& = (\\beta_0 + (1)\\beta_1 + \\mathbf X_i \\gamma + \\varepsilon_i) - (\\beta_0 + (0)\\beta_1 + \\mathbf X_i \\gamma + \\varepsilon_i) && (\\text{plug in } Y_i(1), Y_i(0) \\ )\\\\\n& = (\\beta_0 + \\beta_1 + \\mathbf X_i \\gamma + \\varepsilon_i) - (\\beta_0 + \\mathbf X_i \\gamma + \\varepsilon_i) && (\\text{multiply}) \\\\\n& = \\beta_0 + \\beta_1 + \\mathbf X_i \\gamma + \\varepsilon_i - \\beta_0 - \\mathbf X_i\\gamma - \\varepsilon_i && (\\text{distribute negative sign})\\\\\n& = \\beta_1 && (\\text{cancel out terms})\n\\end{align}\n\\]\nThus, we see \\(\\tau_i = \\beta_1\\). We also know that conditional ignorability implies zero-conditional mean. Thus OLS is an unbiased and asymptotically consistent estimator of both \\(\\beta_1\\) and the ATE.\nYou should be cautious using OLS when assumption 2, linearity, is violated. We know the OLS is the best linear predictor of the conditional expectation function in terms of mean squared error. Thus, \\(\\beta_1\\) will provide the best linear approximation to the population regression function.\nThis does not mean it is good - just the best linear approximation. How far your actual data is away from linearity will determine how good OLS is here.\nYou should not use OLS if you believe assumption 1, heterogeneity, is violated. The reasoning is explained below.\n\n\n\nOLS Bias under Heterogeneity\nWhat if there are heterogenous treatment effects (where \\(\\tau_i\\) is different between units)? Standard OLS in this case is no longer an unbiased estimator of the ATE. Recall the discrete identification of the ATE (in equation Equation 4 ) is a weighted average of CATEs:\n\\[\n\\hat\\tau_{ATE} = \\sum\\limits_{x \\in \\mathcal X} ( \\hat\\tau_{CATE}(x)) \\underbrace{P(X_i = x)}_{\\text{weight}} \\\\\n\\]\nOLS, when there are non constant treatment effects, can also be rewritten as a weighted average of CATEs:\n\\[\n\\hat\\beta_{OLS} = \\sum\\limits_{x \\in \\mathcal X} ( \\hat\\tau_{CATE}(x)) \\underbrace{ \\frac{Var(D_i|X_i = X)P(X_i = x)}{\\sum Var(D_i | X_i = x')P(X_i = x')} }_{\\text{weight}} \\\\\n\\]\nNotice how the weights are different. The weights in the OLS are the conditional variances of \\(D_i\\). This means that OLS is not an unbiased estimator of the ATE or ATT, but rather, a weighted average of the ATT and ATU.\nOLS, under heterogeneity, actually provides an unbiased estimator of the conditional variance weighted average treatment effect. This is not the same as the ATE or the ATT. This estimand can also be described as a weighted average of the ATT and the ATU:\n\\[\n\\tau_{OLS} = w_1 \\cdot \\tau_{ATT} + w_0 \\cdot \\tau_{ATU}\n\\]\nWhere:\n\\[\n\\begin{split}\nw_1 & = \\frac{(1 - P(D=1)) V(\\pi(X)|D = 0)}{P(D=1)V(\\pi(X)|D=1) + (1-P(D=1)V(\\pi(X)|D=0)} \\\\\nw_0 & = 1 - w_1\n\\end{split}\n\\]\nThe reason for this is because regression is prone to extrapolation beyond common support - i.e. it can “estimate” potential outcomes for units that are not observed. This can lead to bias. This is in contrast to the subclassification estimator, which cannot be computed if there are missing observable outcomes for a substratum/category of \\(X\\).\nThe weights of \\(D_i(X_i = x)\\) can also be seen as propensity scores of \\(\\pi(x)(1 - \\pi(x)\\). Weights are higher for groups with propensity scores close to 0.5, and lower for groups with propensity scores close to 0 or 1. OLS minimises estimation uncertainty by downweighting groups of \\(X_i\\) where group-specific ATEs are less precisely estimated.\n\n\n\nFully Interacted Estimator\nThe Fully-Interacted Estimator, a newly developed large-sample regression estimator (Lin 2013), solves the heterogeneity bias in the OLS estimator. The fully-interacted estimator takes the form:\n\\[\nY_i = \\hat\\alpha + D_i \\widehat{\\tau}_{int} + (\\mathbf X_i - \\mathbf {\\bar X}) \\hat\\beta +D_i (\\mathbf X_i - \\mathbf{\\bar X}) \\hat\\gamma + \\hat\\varepsilon_i\n\\]\n\nWhere \\(X_i\\) are covariate values sufficient to satisfy conditional independence.\nWhere \\(\\bar X\\) contains the sample means of all \\(X_i\\) covariates.\n\nThis estimator \\(\\hat\\tau_{int}\\) is technically biased when estimating \\(\\tau_{ATE}\\). However, the bias is arbitrarily small in large samples under conditional ignorability.\nThis estimator thus allows us to accurately estimate the ATE even under heterogenous treatment effects, assuming our sample size is sufficiently large.\n\n\n\n\n\n\nOther Solutions to the OLS Bias under Heterogeneity\n\n\n\n\n\nThere are a few other solutions to this issue of OLS bias under heterogeneity:\n\nDoubly-robust estimation uses a weighted average of regression and IPW estimators, which will be asymptotically consistent as long as the regression model is correctly specified.\nMatching as pre-processing uses matching to make treatment and control groups similar, then runs regression models to estimate causal effects.\n\n\n\n\n\n\n\n\n\n\nNonparametric Estimators\n\nSubclassification Estimator\nUsing the discrete identification of the ATE shown in Equation 4 , we can instead use the sample equivalents to get the subclassification estimator:\n\\[\n\\hat\\tau_{ATE} = \\sum\\limits_{j=1}^M \\underbrace{(\\bar Y_{1j} - \\bar Y_{0j})}_{\\tau_{CATE}(j)} \\underbrace{\\frac{n_j}{n}}_{\\text{weight}}\n\\]\n\nWhere \\(M\\) is the number of levels/categories of \\(X\\), and \\(j\\) is one specific level/category of \\(X\\).\nWhere \\(n_j\\) is the number of units in a level/category \\(j\\) of \\(X\\).\nWhere \\(\\bar Y_{dj}\\) is the mean outcome for units with \\(D_i = d\\) in level/category \\(j\\) of \\(X\\).\n\nMore intuitively, the procedure is as follows:\n\nChoose one specific value for all covaraites \\(X\\). Find the ATE within this specific value of \\(X\\).\nMultiply that average treatment effect by the number of observations that meet this specific value of \\(X\\) divided by the total number of units.\nDo this for every possible values of all covaraites \\(X\\), then sum up all the weighted average treatment effects to get the overall ATE.\n\nFor subclassificaion to be possible, within each level \\(j\\) of covariate \\(X\\), there must be at least one unit in control \\(D=0\\) and treatment \\(D=1\\). Subclassification is not a particularly popular estimator, because it only works if all covariates are discrete, and only if you have a manageable amount of categories and covariates.\n\n\n\n\n\n\nSubclassification with Multiple Confounders\n\n\n\n\n\nLet us say we have 2 confounders, \\(X_1\\) and \\(X_2\\). Both confounders are categorical with 3 categories.\nWe would need to create \\(M=9\\) levels of strata, for every possible combination of values of \\(X_1\\) and \\(X_2\\). Then, we would estimate the within-strata average treatment effect, and weight them.\nThis illustrates how with large amounts of confounders, you will need a huge number of stratum. This makes subclassification infeasible in many cases.\n\n\n\n\n\n\n\n\n\nSubclassification for the ATT\n\n\n\n\n\nWhen pre-treatment covariate \\(X\\) is discrete, the identification result of the ATT is:\n\\[\n\\tau_{ATT} = \\sum\\limits_{x \\in \\mathcal X} ( E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)) P(X_i = x | D_i = 1)\n\\]\nWe can calculate this within our give sample to get the subclassificaiton estimator:\n\\[\n\\hat\\tau_{ATT} = \\sum\\limits_{j=1}^M(\\bar Y_{1j} - \\bar Y_{0j}) \\frac{n_{1j}}{n_1}\n\\]\n\nWhere \\(M\\) is the number of strata (levels/categories of \\(X\\)).\nWhere \\(n_j\\) is the number of units in a level/category \\(j\\) of \\(X\\).\nWhere \\(n_{1j}\\) is the number of treated cells \\(D = 1\\) in a level/category \\(j\\) of \\(X\\).\nWhere \\(\\bar Y_{dj}\\) is the mean outcome for units with \\(D_i = d\\) in level/category \\(j\\) of \\(X\\).\n\n\n\n\n\n\n\nMatching Estimator\nWe have a missing data problem in causal inference: we do not know all the potential outcomes. Matching “estimates” missing potential outcomes of a unit.\nFor each observation in the treated group, matching finds an observation in the untreated group that have the most similar values of a set of pre-treatment covariates \\(X\\). Thus, we have pairs of treatment-control observations that act as counterfactuals. We can estimate the ATT as the average difference in observed outcomes within the pairs:\n\\[\n\\hat\\tau_{ATT} = \\frac{1}{n_1} \\sum\\limits_{i:D_i = 1}(Y_i - \\widetilde{Y_i})\n\\]\n\nWhere \\(n_1\\) is the number of units in the treatment group.\nWhere \\(Y_i\\) is the unit’s observed \\(Y\\) in the treatment group.\nWhere \\(\\tilde Y_i\\) is unit \\(i\\)’s closest neighbour in the untreated group.\n\nSometimes, a treatment unit may not have one close control unit to match to. Instead, we could use a combination of control units to match to the treatment unit, and use the average \\(Y\\) of those combination of control units to approximate a more accurate match.\nSuppose we use \\(M_i\\) number of close control units to match to a treatment unit \\(i\\). Then, the matching estimator would be defined as follows:\n\\[\n\\hat\\tau_{ATT} = \\frac{1}{n_1} \\sum\\limits_{i:D_i = 1}(Y_i - \\left(\\frac{1}{M_i} \\sum\\limits_{m=1}^{M_i} \\widetilde{Y_{i_m}}\\right))\n\\]\nWhere \\(\\widetilde{Y_{i_m}}\\) is the obsered outcome for the \\(m\\)th match of unit \\(i\\).\n\n\n\n\n\n\nChoices during Matching\n\n\n\n\n\nWe have to make several choices when conducting matching.\n\nWhat covariates to match on. We generally want to select a set of pre-treatment covariates \\(X\\) such that these covariates ensure the conditional ignorability assumption is met.\nMatch with or without replacement. Matching with replacement means that once you have used one control unit to match to a treatment unit, you can still use that same control unit to match to another treatment unit. This has advantages since you can ensure better and closer matches. However, matching without replacement is also possible.\nHow many to match. You can decide to match multiple control units to one treatment unit, and use the average of the treatment units to approximate a true control unit. This may result in more accurate matches for treatment units that may not have a good single control unit to match to.\n\nWe can also choose to use more advanced matching methods, such as Mahalanobis Distance matching or Propensity Score matching, which are shown below. These are good for matching on more \\(X\\).\n\n\n\n\n\n\n\n\n\nWeaknesses of Matching\n\n\n\n\n\nMatching does not always create “perfect” matches. This means that the pairs matched together may not be identical to each other in terms of covariates \\(X\\) or potential outcomes.\nThe inability to find exact matches can cause bias, especially for the more covariates we match on (see below).\n\n\n\n\n\n\nMatching with Multiple Covariates\nConsider that we \\(k&gt;1\\) number of confounders \\(X\\). Now, we have to match observations in \\(k\\) variables, which implies we are in a multidimensional \\(\\mathbb R^k\\) space. The most commonly used distance metric is Mahalanobis Distance - which measures the distance in \\(X_i\\) between units \\(i\\) and \\(j\\):\n\\[\nD_M (\\mathbf X_i, \\mathbf X_j) = \\sqrt{(\\mathbf X_i - \\mathbf X_j)^T \\boldsymbol\\Sigma_X^{-1} (\\mathbf X_i - \\mathbf X_j)}, \\quad \\boldsymbol\\Sigma_X \\text{ is the variance-covariance matrix of X}\n\\]\nHowever, when we try to match on more than one \\(X\\) variable, we go from matching on a number line \\(\\mathbb R^1\\) to a \\(n\\)-dimensional space, \\(\\mathbb R^n\\). The search space increases exponentially as you increase the number of dimensions.\n\n\n\n\n\nIf we match on a one dimensional plane (say the horizontal line between 0 and 1 on the left), we can see our red line covers approximately 30% between 0 and 1. But in 3 dimensions, our red box covers a significantly less proportion of the entire cube. On the right, \\(d\\) represents the dimensions. We can see as \\(d\\) increases, the fraction of volume increases significantly slower relative to distance. Thus, with a bigger space, the distance between two units increases, so you get worse matches.\nThis curse of dimensionality creates a bias problem - since we get non-exact matches. The more dimensions you add, the worse it becomes.\n\n\n\n\n\n\nMore on Bias\n\n\n\n\n\nThe poor matches caused by increased dimensionality inject error into our estimates of missing potential outcomes.\nThe bias term as you increase the number of dimensions \\(k\\), changes by \\(N^{(-1/k)}\\). This implies no \\(\\sqrt{n}\\) consistency for \\(k&gt;2\\).\nIf \\(N_0\\) (number of untreated units) is much larger than \\(N_1\\) (number of treated units), bias will typically be smaller.\nThere are ways to correct this bias, including Abadie and Imbens (2011) Bias Correction method.\nThere is a new method: Bias-corrected matching, which estimates bias ineherent to mathching estimators via regression, then subtracts it from the matching estimate to correct for it.\n\n\n\n\n\n\nPropensity Scores Matching\nPropensity Score matching is an alternative way to match over many dimensions. The propensity score is an unobserved property, defined as the probability of a unit \\(i\\) of receiving treatment:\n\\[\n\\pi(X_i) \\equiv P(D_i = 1|X_i)\n\\]\nWhen supposing the conditional ignorability and common support assumptions, the propensity score \\(\\pi(X_i)\\) has the balancing property: \\(D_i \\perp X_i \\ | \\ \\pi(X_i)\\). This implies that conditional ignorability holds on the propensity scores alone:\n\\[\n(Y_{1i}, Y_{0i}) \\perp\\!\\!\\!\\!\\perp D_i \\ | \\ \\pi(X_i)\n\\]\nThus, instead of conditioning on \\(X_i\\) as we did in selection on observables, we can instead condition on \\(\\pi (X_i)\\), and still identify the causal estimand.\nHowever, we do not actually observe \\(\\pi (X_i)\\). We estimate \\(\\pi (X_i)\\) with a binary response model, with outcome variable \\(D_i\\), and explanatory variables \\(X_i\\). This will get us a fitted probability \\(P(D_i = 1) = \\hat\\pi(X_i)\\).\nThen, once we have the propensity score estimates \\(\\hat\\pi(X_i)\\), we can do nearest neighbour matching with the propensity scores (in \\(\\mathbb R^1\\)). This will allow us to identify the \\(\\tau_{ATT}\\).\nThe accurate estimation of \\(\\tau_{ATT}\\) implies an accurate prediction of the propensity scores \\(\\pi(X_i)\\). We can test our matched treatment and control groups to see if the balancing property holds for covariates \\(X_i\\).\n\n\n\nGenetic Matching\n\n\n\n\n\n\nWeighting Estimator\n\nIdentification with Weighting\nWe know that the ATE can be written as a weighted average, as shown in Equation 4 . We can rewrite the \\(\\tau_{ATE}\\) as follows using observed potential outcomes outcomes and conditional ignorability ( Equation 1 ).\n\\[\n\\begin{split}\n& = \\sum\\limits_{x \\in \\mathcal X} \\underbrace{(E(Y_{1i}|D_i = 1, X_i = x)}_{\\because \\text{ observed}} - \\underbrace{E(Y_{0i}|D_i = 0, X_i = x)}_{\\because \\text{ observed}})P(X_i = x) \\\\\n& = \\sum\\limits_{x \\in \\mathcal X}  (\\underbrace{E(Y_{1i}|X_i = x)}_{\\because \\text{ eq. (1)}} - \\underbrace{E(Y_{0i}|X_i = x)}_{\\because \\text{ eq. (1)}})P(X_i = x) \\\\\n& = \\underbrace{E[E(Y_{1i}|X_i = x) - E(Y_{0i}|X_i = x)]}_{\\text{definition of weighted average}}\n\\end{split}\n\\]\nLet us do an algebra trick - multiply both terms within the CATE by 1 (in blue):\n\\[\n\\begin{split}\n& = E \\left [E(Y_{1i}|X_i=x) \\color{blue}{\\frac{\\pi(X_i)}{\\pi(X_i)}}\\color{black} - (E(Y_{0i}|X_i=x) \\color{blue}{\\frac{1-\\pi(X_i)}{1-\\pi(X_i)}} \\right] \\\\\n& \\color{black} = E \\left[ \\frac{E(Y_{1i}|X_i = x) \\pi(X_i)}{\\pi(X_i)} -  \\frac{E(Y_{0i}|X_i = x) (1-\\pi(X_i))}{1-\\pi(X_i)} \\right]\n\\end{split}\n\\]\nWe know that propensity score \\(\\pi(X_i) := E(D_i|X_i = x)\\). Thus, we can convert the above to:\n\\[\n\\begin{split}\n& = E \\left[ \\frac{E(Y_{1i}|X_i = x)E(D_i|X_i = x)}{\\pi(X_i)} - \\frac{E(Y_{0i}|X_i = x)(1-E(D_i|X_i = x))}{1-\\pi(X_i)}\\right] \\\\\n& = E \\left[ \\frac{E(Y_{1i}|X_i = x)E(D_i|X_i = x)}{\\pi(X_i)} - \\frac{E(Y_{0i}|X_i = x)E(1-D_i|X_i = x)}{1-\\pi(X_i)}\\right]\n\\end{split}\n\\]\n\\[\n\\begin{align}\n& = E \\left[ \\frac{E(Y_{1i}D_i|X_i = x)}{\\pi(X_i)} - \\frac{E(Y_{0i}(1-D_i)|X_i = x)}{1 - \\pi(X_i)}\\right] && (\\text{property of expectation})\\\\\n& = E \\left[ E \\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} |X_i = x\\right) - E \\left( \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)} | X_i = x \\right) \\right] && (\\text{property of expectation})\\\\\n& = E\\left[ E\\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} - \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)} |X_i = x \\right) \\right]  && (\\text{property of expectation})\n\\end{align}\n\\]\n\\[\n\\begin{align}\n& = E\\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} - \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)}\\right) && (\\text{LIE: } E(X) = E[E(X|Y)] \\ ) \\\\\n& = E\\left( \\frac{Y_{i}D_i}{\\pi(X_i)} - \\frac{Y_{i}(1-D_i)}{1-\\pi(X_i)}\\right) && (\\text{observered outcome}) \\\\\n& = E \\left( \\frac{\\color{blue}{Y_i} \\color{black}D_i(1-\\pi(X_i))-\\color{blue}{Y_i}\\color{black}(1-D_i)\\pi(X_i)}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{getting common denom.}) \\\\\n& = E\\left( Y_i \\frac{D_i(1-\\pi(X_i))-(1-D_i)\\pi(X_i)}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{factor out }Y_i) \\\\\n& = E\\left( Y_i \\frac{D_i - D_i\\pi(X_i)-(\\pi(X_i) -D_i\\pi(X_i))}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{distribute out}) \\\\\n&  = E\\left( Y_i \\frac{D_i \\color{blue}{- D_i\\pi(X_i)} \\color{black}-\\pi(X_i) \\color{blue}{+ D_i\\pi(X_i)}}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{distribute out negative})\\\\\n& = E\\left( Y_i \\frac{D_i -\\pi(X_i) }{\\pi(X_i)(1-\\pi(X_i))}\\right) && (\\text{cancel out})\\\\\n\\end{align}\n\\tag{7}\\]\nAnd thus, we have identified the ATE.\n\n\n\nInverse Probability Weighting Estimator\nAn alternative use of propensity scores is weighting. As shown above, under conditional ignorability and common support, we can identify the ATE as:\n\\[\n\\tau_{ATE} = E\\left[ Y_i \\times \\underbrace{\\frac{D_i - \\pi(X_i)}{\\pi(X_i) (1 - \\pi(X_i))}}_{\\text{weight}}\\right]\n\\]\nThe inverse probability weighting (IPW) estimator is the sample estimator:\n\\[\n\\begin{split}\n\\hat\\tau_{ATE} & = \\frac{1}{N} \\sum\\limits_{i=1}^N \\left(Y_i \\frac{D_i - \\hat\\pi(X_i)}{\\hat\\pi(X_i) (1 - \\hat\\pi(X_i))} \\right) \\\\\n& = \\frac{1}{N} \\sum\\limits_{i=1}^N \\left(\\frac{D_i Y_i}{\\hat\\pi(X_i)} - \\frac{(1-D_i) Y_i}{1 - \\hat\\pi(X_i)} \\right)\n\\end{split}\n\\]\n\nThe second equation is equivalent to the first, shown by Equation 7 .\n\nEssentially, those who are unlikely to be treated but do get treated get weighted more, and individuals who are likely to be treated but do not get treated get weighted more.\n\n\n\n\n\n\nWeighting Estimator for ATT\n\n\n\n\n\nThe identification of the ATT under both conditional ignorability and common support are:\n\\[\n\\tau_{ATT} = \\frac{1}{Pr(D = 1)} \\times E\\left[ Y_i \\times \\underbrace{\\frac{D_i - \\pi(X_i)}{(1 - \\pi(X_i))}}_{\\text{weight}}\\right]\n\\]\nThe sample IPW estimator would be:\n\\[\n\\begin{split}\n\\hat\\tau_{ATT} & = \\frac{1}{N_1}\\sum\\limits_{i=1}^N \\left( Y_i \\frac{D_i - \\hat\\pi(X_i)}{1 - \\hat\\pi(X_i)} \\right) \\\\\n& = \\frac{1}{N_1} \\sum\\limits_{i=1}^N \\left( D_iY_i - (1-D_i)Y_i \\frac{\\hat\\pi(X_i)}{1 - \\hat\\pi(X_i)} \\right)\n\\end{split}\n\\]\n\n\n\nThe IPW estimator is asymptotically consistent, but has very poor small sample properties. They are highly sensitive to extreme values of \\(\\hat\\pi(X_i)\\). This generates high variance (inefficiency), and can produce significant bias under model mispecification.\n\n\n\n\n\n\nFalsification Tests\n\nTesting Assumptions with Falsification\nThe stronger (bolder) our assumptions for identification, the less credible our results are. Selection on Observables involves a very strong and hard to verify assumption: conditional ignorability. Can we really be sure that we have controlled for all confounders \\(X_i\\) needed to satisfy conditional ignorability?\nPlacebo tests are a type of falsification test to show evidence against our assumptions. Suppose that we make the assumption of conditional ignorability \\((Y_{0i}, Y_{1i}) \\perp D_i | X_i\\). Suppose we are concerned about the presence of another confounder \\(U\\) that is not included in \\(X_i\\).\n\n\n\n\n\nThe presence of \\(U\\) will falsify our conditional ignorability assumption, and means we cannot identify the causal effect of \\(D \\rightarrow Y\\).\n\n\n\n\n\n\nFalsification vs. Validation\n\n\n\n\n\nFalsification is a principle of trying to criticise our own research, rather than defend it. Falsification is about testing if our assumptions are not met. Failing a test provides evidence that our assumption is not met.\n\nEx. Covariates are balanced - thus there is no evidence that our assumptions are not met. We are not saying that our assumption is correct, just that there is no evidence against it.\n\nValidation is the opposite - we test to see if there is evidence in favour of our assumptions.\n\nEx. Covariates are balanced - thus our assumptions are met.\n\n\n\n\nFor falsification tests, we should not just pay attention to statistical significance - we must also pay attention to the magnitude of the point estimation.\n\n\n\nPlacebo Outcome Test\nA placebo outcome test utilities another alternative outcome variable \\(Y'\\) that is caused by our hypothesised unobserved confounder \\(U\\):\n\n\n\n\n\nWe can see that if \\(U\\) does not exist, \\(D\\) should have zero effect on the new outcome \\(Y'\\). Thus, if \\(U\\) is present, we should find a relationship between \\(D\\) and \\(Y'\\).\n\\[\nY'_i = \\gamma + \\delta D_i + \\varepsilon_i\n\\]\n\nIf we find that there is an effect of \\(D\\) on the new outcome \\(Y'\\) (non-zero \\(\\delta\\)), that is evidence that \\(U\\) exists, and is evidence to reject our conditional ignorability assumption (falsifies our design - red flag!).\nIf you do not find an effect of \\(D\\) on new outcome \\(Y'\\) (\\(\\delta = 0\\)) you find no evidence of \\(U\\), and no evidence to reject our conditional ignorability assumption (fails to falsify our design).\n\nWe must be sure that \\(Y\\) is not related to \\(Y'\\) except through \\(D\\) and \\(U\\). If this is true, we just run our original research design but replace \\(Y\\) with \\(Y'\\).\n\n\n\nPlacebo Treatment Test\nA placebo treatment test involves some other treatment \\(D'\\), that was assigned at the same time\n\n\n\n\n\nWe can see that if \\(U\\) does not exist, the effect of \\(D'\\) should have no effect on \\(Y\\). If \\(U\\) does exist, there should be some effect of \\(D'\\) on \\(Y\\).\n\\[\nY_i = \\gamma + \\delta D'_i + \\varepsilon_i\n\\]\n\nIf we find that there is an effect of \\(D'\\) on \\(Y\\) (non-zero \\(\\delta\\)), that is evidence that \\(U\\) exists, and is evidence to reject our conditional ignorability assumption (falsifies our design - red flag!).\nIf you do not find an effect of \\(D'\\) on \\(Y\\) (\\(\\delta = 0\\)) you find no evidence of \\(U\\), and no evidence to reject our conditional ignorability assumption (fails to falsify our design).\n\nWe must be sure that \\(Y\\) is not related to \\(D'\\) except through \\(D\\) and \\(U\\). If this is true, we just run our original research design but replace \\(D\\) with \\(D'\\).\n\n\n\n\n\n\nExtension: Partial Identification\n\nDecomposing the ATE\nWith falsification, we were concerned with what assumptions we needed to be not-false in order to identify the ATE. However, we can take a different approach - what can we learn about the ATE without any assumptions?\nLet us decompose the ATE into parts:\n\\[\n\\begin{align}\n\\tau_{ATE}  = & E(Y_{1i} - Y_{0i}) \\\\\n& \\\\\n= & E(Y_{1i} - Y_{0i} | D_i = 1) P(D_i = 1) \\\\\n& \\quad - E(Y_{1i} - Y_{0i}|D_i = 0)P(D_i = 0)  && (\\text{def. of weighted avg.})\\\\\n& \\\\\n= & [ \\color{blue}{E(Y_i |D_i = 1)} \\color{black}- \\color{red}{E(Y_{0i}|D_i = 1)} \\color{black}] P(D_i = 1) \\\\\n& \\quad + [ \\color{red}{E(Y_{1i}|D_i = 0)} \\color{black} - \\color{blue}{E(Y_i|D_i = 0)} \\color{black}]P(D_i = 0) && (\\text{observed + unobserved})\n\\end{align}\n\\]\nSome of the quantities are observed (in blue), and some of the quantities are unobserved (in red). Previously, we made assumptions (conditional ignorability, common support) to fill the unobserved quantities. But, we can make actually any assumption as possible.\n\n\n\nNonparametric Bounds\nOne way to fill in our unobserved outcomes through the “best” and “worst” possible outcomes. This allows us to construct a plausible range of the ATE.\nFirst, let us construct the worst-case scenario - the lowest possible \\(\\tau\\).\n\n\\(E(Y_{0i}|D_i = 1) = Y_H\\). Units in the treated \\(D_i=1\\), their potential outcome \\(Y_{0i}\\) will be the highest \\(Y\\) possible, \\(Y_H\\).\n\\(E(Y_{1i}|D_i = 0) = Y_L\\). Units in the control \\(D_i=0\\), their unobserved potential outcome \\(Y_{1i}\\) will be the lowest \\(Y\\) possible, \\(Y_L\\).\n\nThus, the lowest possible \\(\\tau\\) (sharp lower bound) is:\n\\[\n\\begin{split}\n\\tau_L = & [ \\color{blue}{E(Y_i |D_i = 1)} \\color{black}  - \\color{red}{Y_H} \\color{black}] P(D_i = 1) \\\\\n& \\quad + [ \\color{red}{Y_L} \\color{black} - \\color{blue}{E(Y_i|D_i = 0)} \\color{black}]P(D_i = 0)\n\\end{split}\n\\]\nNow, let us construct the best-case scenario - the highest possible \\(\\tau\\).\n\n\\(E(Y_{0i}|D_i = 1) = Y_L\\). Units in the treated \\(D_i=1\\), their potential outcome \\(Y_{0i}\\) will be the lowest \\(Y\\) possible, \\(Y_L\\).\n\\(E(Y_{1i}|D_i = 0) = Y_H\\). Units in the control \\(D_i=0\\), their unobserved potential outcome \\(Y_{1i}\\) will be the highest \\(Y\\) possible, \\(Y_H\\).\n\nThus, the highest possible \\(\\tau\\) (sharp upper bound) is:\n\\[\n\\begin{split}\n\\tau_H = & [ \\color{blue}{E(Y_i |D_i = 1)} \\color{black}  - \\color{red}{Y_L} \\color{black}] P(D_i = 1) \\\\\n& \\quad + [ \\color{red}{Y_H} \\color{black} - \\color{blue}{E(Y_i|D_i = 0)} \\color{black}]P(D_i = 0)\n\\end{split}\n\\]\nWe know that the true \\(\\tau_{ATE} \\in [\\tau_L, \\tau_H]\\).\n\n\n\nMonotone Treatment Selection Assumption\nOur extreme case from above is not very useful. However, we can layer on assumptions to lower the possible \\(\\tau\\) values.\nOne assumption is the Monotone Treatment Selection (MTS) assumption. This assumption basically says that potential outcomes for units in treatment, are always higher than for those in the control.\n\\[\n\\begin{split}\n& E(Y_{0i}|D_i = 0) ≤ \\overbrace{E(Y_{0i}|D_i = 1)}^{\\text{unobserved}} \\\\\n& \\underbrace{E(Y_{1i} |D_i = 0)}_{\\text{unobserved}} ≤ E(Y_{1i} |D_i = 1)\n\\end{split}\n\\]\nThis is basically saying that selection bias is one-direction.\nThis implies a tighter sharp upper bound on \\(\\tau\\).\n\\[\n\\begin{align}\n\\tau_H = & [ \\underbrace{E(Y_i |D_i = 1)}_{\\text{observed}}  - \\color{red}{E(Y_{0i}|D_i = 0)} \\color{black}] P(D_i = 1) \\\\\n& \\quad + [ \\color{red}{E(Y_{1i}|D_i =1)} \\color{black} - \\underbrace{E(Y_i|D_i = 0)}_{\\text{observed}}]P(D_i = 0) \\\\\n= & E(Y_i|D_i = 1) - E(Y_i|D_i = 0) && (\\text{def. of weighted avg.})\\\\\n\\end{align}\n\\]\nThis indicates that the upper bound of plausible \\(\\tau_{ATE}\\) values is the naive estimator of differences in observed outcomes.\nWe can also make the reverse assumption, where selection bias is in the opposite direction. This means a tighter sharp lower bound \\(\\underline\\tau\\). These assumptions help narrow our possible \\(\\tau_{ATE}\\) values, and can allow us to test if our estimated \\(\\hat\\tau\\) is reasonable (within the plausible bounds).\n\n\n\n\n\n\nImplementation in R\nFor all methods, you will need the tidyverse package:\n\nlibrary(tidyverse)\nlibrary(MatchIt)\nlibrary(estimatr)\n\nSee how to perform each estimator in R:\n\n\n\n\n\n\nDistance Matching\n\n\n\n\n\nFirst, let us conduct nearest neighbour matching with Mahalanobis distance by using the matchit() function.\n\nmatch_object = MatchIt::matchit(D ~ X1 + X2 + X3,\n                                data = my_data,\n                                method = \"nearest\", #distance matching\n                                distance = \"mahalanobis\")\n\n# for output summary\nsummary(match_object)\n\nSecond, let us save the matched data with the match.data() function.\n\nmatch_data &lt;- MatchIt::match.data(match_object,\n                                  weights = 'nn_weights')\n\nThird, we can test if matching worked by using a balance table and a love plot:\n\n# balance table\ncobalt::bal.tab(D ~ X1 + X2 + X3, \n                data = match_data, # from the 2nd step\n                weights = \"nn_weights\",\n                disp = c(\"means\", \"sds\"))\n\n#love plot\ncobalt::love.plot(match_object,\n                  data = my_data, #original dataset\n                  stars = 'raw')\n\nFinally, we can estimate the treatment effect. There are two options - either using a weighted regression, or using the matching algorithm:\n\n# using weighted regression\nestimate &lt;- lm_robust(Y ~ D,\n                      data = match_data, #data from step 2\n                      weights = nn_weights)\nsummary(estimate)\n\n## using the Matching package:\nestimate = Matching::Match(Y = my_data$Y, #outcome\n                           Tr = my_data$D, #treatment\n                           X = my_data[,c(\"X1\", \"X2\", \"X3\")], #covariates\n                           M=1, #number of neighbours\n                           BiasAdjust = TRUE, #for biased adjustment\n                           Weight = 2)\nsummary(estimate)\n\nYou will have the estimates that you can use.\n\n\n\n\n\n\n\n\n\nPropensity Score Matching\n\n\n\n\n\nFirst, we want to estimate propensity scores with a logistic regression (or a random forest):\n\n#logistic model\npscore_model = glm(D ~ X1 + X2,\n                   data = my_data,\n                   family = \"binomial\")\n\n# estimate propensity scores\nmy_data$pscore_estimate = predict(pscore_model,\n                                  my_data,\n                                  type = \"response\")\n\nNow, let us match with propensity scores:\n\n# match\nmatch_object = MatchIt::matchit(D ~ pscore_estimate,\n                                data = my_data,\n                                method = \"nearest\",\n                                distance = \"Mahalanobis\")\n\n# save matched data\nmatch_data &lt;- MatchIt::match.data(match_object,\n                                  weights = 'pscore_weights')\n\nThird, we can test if matching worked with a balance table and a love plot:\n\n#balance table\ncobalt::bal.tab(D ~ X1 + X2 + X3,\n                data = match_data, #matched data from step 2\n                weights = \"pscore_weights\",\n                disp = c(\"means\", \"sds\"))\n\n#love plot\ncobalt::love.plot(match_object,\n                  data = my_data, #original dataset\n                  addl = ~ X1 + X2 + X3,\n                  stars = 'raw')\n\nFinally, let us do the estimation:\n\nestimate &lt;- lm_robust(Y ~ D,\n                      data = match_data, #from step 2\n                      weights = pscore_weights)\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nInverse Probability Weighting\n\n\n\n\n\nFirst, we want to estimate propoensity scores with a logistic regression (or a random forest):\n\n#logistic model\npscore_model = glm(D ~ X1 + X2,\n                   data = my_data,\n                   family = \"binomial\")\n\n# estimate propensity scores\nmy_data$pscore_estimate = predict(pscore_model,\n                                  type = \"response\")\n\nSecond, we calculate the inverse probability weights based on the formula from earlier:\n\nmy_data$ipweight = ifelse(my_data$D == 1, # condition\n                       1/my_data$pscore_estimate,\n                       1/(1-my_data$pscore_estimate))\n\nFinally, we can estimate the ATE, or ATT, or use a weighted regression for the ATE:\n\n# ATE estimator\nmean((my_data$D * my_data$Y) * my_data$ipweight - ((1 - my_data$D) * my_data$Y) * my_data$ipweight)\n\n# ATT estimator\nsum(my_data$D * my_data$Y - (1 - my_data$D) * my_data$Y * (my_data$pscore_estimate/(1 - my_data$pscore_estimate)))/sum(my_data$D)\n\n# ATE with weighted regression\nestimate &lt;- lm_robust(Y ~ D, \n                      data = my_data,\n                      weights = ipweight)\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nOLS Estimator\n\n\n\n\n\nFor the OLS estimator, we can use the lm_robust() function:\n\nestimate &lt;- lm_robust(Y ~ D + X1 + X2 + X3,\n                      data = my_data)\nsummary(estimate)\n\nWe can also use the fixest package and the feols() function:\n\nlibrary(fixest)\n\nestimate &lt;- feols(Y ~ D + X1 + X2 + X3,\n                  data = my_data,\n                  se = \"hetero\")\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nFully Interacted Estimator\n\n\n\n\n\nFor the fully interacted estimator, we can use the lm_lin() function.\n\nestimate &lt;- lm_lin(Y ~ D,\n                   covariates = ~ X1 + X2 + X3,\n                   data = my_data)\nsummary(estimate)\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "4 Selection on Observables"
    ]
  },
  {
    "objectID": "quant3.html",
    "href": "quant3.html",
    "title": "Randomised Controlled Trials",
    "section": "",
    "text": "Up until now, we have focused on theory of statistics and causal inference. But now, we are ready to dive into methodology - different designs to measure and identify causal effects.\nThis chapter introduces the “gold standard” of causal inference: randomised controlled trials. This chapter also covers extensions, such as stratified experiments and survey experiments.\nUse the right sidebar for quick navigation.\n\n\nRandomisation\n\nRandomised Experiments\nExperiments are a research design where the assignment mechanism is controlled by the researcher.\nRandomised Experiments use randomisation as the assignment mechanism. Treatment values are assigned to \\(N\\) units at random, with both known and positive probabilities of being assigned to treatment and control groups.\nQuick notation for randomised experiments:\n\nWe have \\(N\\) total number of units in our experiment.\nA randomly subset of \\(N_1\\) units are assigned to treatment \\(D = 1\\).\nThe remaining \\(N_0 = N - N_1\\) are assigned to control.\n\n\\(N_1\\), the number of individuals assigned to treatment, does not necessarily need to be 50% (although this is quite a common number).\nAlso, when you fix the number of units to be treated at \\(N_1\\) before randomisation, technically, not all units have an independent probability of being selected. This is because once you have assigned \\(N_1\\) individuals to treatment, we know the remaining individuals must be assigned to control. This usually is not a huge issue.\nYou can use bernoulli randomisation (simple randomisation) to avoid this issue. Bernoulli gives every individual a certain chance of being selected based on a bernoulli distribution. This does mean that with different randomisation trials, we will get different numbers of treated individuals for each trial.\n\n\n\nIdentification Assumptions\nRandomisation implies that assignment probabilities do not depend on the potential outcomes. The potential outcome values do not affect our chances of being selected for treatment.\n\\[\nP(D=1|Y_0, Y_1) = P(D=1)\n\\]\nOr in other words, treatment is independent of potential outcomes (also unconfounded or ignorability):\n\\[\n(Y_1, Y_0)  \\perp\\!\\!\\!\\!\\perp D\n\\]\nThis implies that \\(E(Y_{0i})\\) is the same between treatment and control groups, and \\(E(Y_{1i})\\) is also the same between treatment and control:\n\\[\n\\begin{split}\n& E(Y_{0i} | D_i = 1) = E(Y_{0i} | D_i = 0) = E(Y_{0i})\\\\\n& E(Y_{1i} | D_i = 1) = E(Y_{1i} | D_i = 0) = E(Y_{1i})\n\\end{split}\n\\tag{1}\\]\n\n\n\nCausal Identification\nLet us return to our naive estimator, and our problem of selection bias. Using the above property in Equation 1, we can simplify:\n\\[\n\\begin{align}\n& \\underbrace{E(Y_{1i}|D_i = 1)- E(Y_{0i}|D_i = 1)}_{\\tau_{ATT}} + \\underbrace{E(Y_{0i}|D_i = 1) - E(Y_{0i} | D_i = 0)}_{\\text{Selection Bias}} \\\\\n& = \\underbrace{E(Y_{1i}|D_i = 1)- E(Y_{0i}|D_i = 1)}_{\\tau_{ATT}} + \\underbrace{E(Y_{0i}) - E(Y_{0i})}_{\\text{Selection Bias}} && (\\because \\text{eq. (1)} \\ )\\\\\n& = \\underbrace{E(Y_{1i}|D_i = 1)- E(Y_{0i}|D_i = 1)}_{\\tau_{ATT}} + 0\n\\end{align}\n\\]\nThus, under randomisation, selection bias is equal to 0, and thus our comparison of observed outcomes is now an unbiased estimator of \\(\\tau_{ATT}\\). Now look at the formula for the ATT estimand. We can simplify as follows using Equation 1:\n\\[\n\\begin{align}\n\\tau_{ATT} & = E(Y_{1i} -Y_{0i}|D_i = 1)\\\\\n& = E(Y_{1i} |D_i = 1) - E(Y_{0i} | D_i = 1) \\\\\n& = E(Y_{1i} ) - E(Y_{0i}) && (\\because \\text{equation (1)} \\ )\\\\\n& = \\underbrace{E(Y_{1i} - Y_{0i})}_{\\tau_{ATE}}\n\\end{align}\n\\]\nAnd now we see that \\(\\tau_{ATT}\\) and \\(\\tau_{ATE}\\) are equivalent under randomisation, and we can identify the \\(\\tau_{ATE}\\) with our observed data.\n\n\n\n\n\n\nGraphical Identification\n\n\n\n\n\nLet us look at a direct acyclic graph:\n\n\n\n\n\nBecause we are randomly assigning treatment \\(D\\), we are exogenously determining \\(D\\). Thus, values of \\(D\\) are not being caused by \\(U\\), they are being caused by randomisation.\nThus, we can eliminate the arrow between \\(U \\rightarrow D\\). This allows us to estimate \\(D \\rightarrow Y\\) without any confounders.\n\n\n\n\n\n\nThe Balancing Property\nRandomisation balances all observed and unobserved pre-treatment characteristics between units between the treatment and control. This is because not only is \\((Y_1, Y_0) \\perp\\!\\!\\!\\!\\perp D\\), but also any covariate \\(X\\) is also independent of treatment: \\(X \\perp\\!\\!\\!\\!\\perp D\\).\nThis means that if randomisation is successful, we should expect minimal differences between control and treatment groups for all pre-treatment characteristics values.\nWe can test this assumption by finding the average \\(X\\) values for both control and treatment groups, and see if there are any statistical significant differences in \\(X\\) between control and treatment. This is typically done with a t-test or a regression:\n\\[\nX_i = \\alpha + \\gamma D_i + \\varepsilon_i \\quad \\text{test if } \\gamma \\text{ is significant}\n\\]\nIn any one sample, we actually are likely to have some imbalances in \\(X\\) between control and treatment simply due to chance. You could control for imbalanced covariates, but you do not have to (we will discuss this later).\nYou can adopt other randomisation procedures, such as stratified randomisation, to guarantee balance on \\(X\\).\n\n\n\nInternal Validity\nInternal validity is the validity of the research design in identifying causal effects. Randomisation can be complicated by a few factors:\n\nMissing data (often due to individuals dropping out). We are concerned that there is some covariate that is causing some people to drop out, which re-introduces selection bias.\nMeasurement Problems: Hawthorne Effect - subjects know what you are studying, and will change their behaviour as a result.\nNon-Compliance: Some units assigned to treatment might not take the treatment, and some units assigned to control may take the treatment (this can often be dealt with by using an instrumental variable design).\n\nSome of these issues are very hard to overcome, so it is important to identify these limitations in our experiments. One way to check validity is to do a balance check between key covariates.\n\n\n\nExternal Validity\nExternal validity has little to do with causal identifiction with the subjects in our experiment. Instead, External validity asks if we can generalise our conclusions from our subjects, to other subjects outside our experiment. Can we extrapolate our estimates to to other populations? This is important - if we cannot extrapolate, some results may be very niche.\nFor example, if we measured the effect of migration on tolerance for our subjects in India, can we say the same effect is true of someone in Japan, the US, or Europe?\nRandomisation does not help with external validity - the ability to extrapolate our results to external situations. To extrapolate to a greater population, our actual sample of observations in our experiment, should be representative of the greater population. This is called 𝑋-Validity: we can study this with data - to see how representative our population is compared to the population.\nNon-representative programme of treatment is another threat: Sometimes, treatments will differ between areas. For example, if we are encouraging people to migrate to test how that changes their tolerance, how are the governmental/ngo/private agencies working with you affecting the process. Would less capable agencies create different effects?\nThis is called \\(C\\)-validity, and we cannot measure this with data, unless you redo your experiment in another context.\n\n\n\n\n\n\nCausal Estimation\n\nDifference in Means Estimator\nOur causal estimand is the Average Treatment Effect (ATE):\n\\[\n\\tau_{ATE} = \\E(Y_{1i}) - \\E(Y_{0i})\n\\]\nEarlier in the causal identification section, we illustrated that there is no selection bias. That means we can estimate the ATE using the difference-in-means estimator, by taking the sample mean \\(Y\\) of the treatment group, minus the sample mean \\(Y\\) of the control group:\n\\[\n\\hat\\tau_{ATE} = \\bar Y_1 - \\bar Y_0\n\\]\nThis is an unbiased estimator because selection bias is eliminated with randomisation. This is also an asymptotically consistent estimator due to the law of large numbers.\n\n\n\nOrdinary Least Squares Estimator\nWe can also estimate the \\(\\tau_{ATE}\\) with a regression:\n\\[\nY_i = \\hat\\gamma + \\hat\\tau D_i + \\hat\\varepsilon_i\n\\]\nRemember that OLS is the best approximation of the conditional expectation function \\(\\E(Y_i|X_i)\\). Thus, we can write the regression as:\n\\[\n\\E(Y_i|D_i) = \\hat\\gamma + \\hat\\tau D_i\n\\]\nNow, let us find the difference between treatment \\(E(Y_i|D_i =1)\\) and control \\(E(Y_i|D_i = 0)\\):\n\\[\n\\begin{split}\n& \\E(Y_i|D_i = 1) - \\E(Y_i|D_i = 0) \\\\\n= & \\ \\hat\\gamma + \\hat\\tau(1) - (\\hat\\gamma + \\hat\\tau(0)) \\\\\n= & \\ \\hat\\gamma + \\hat\\tau - \\hat\\gamma \\\\\n= & \\ \\hat\\tau\n\\end{split}\n\\]\nThus, the difference in means is equivalent to \\(\\hat\\tau\\) regression coefficient. Here, \\(\\hat\\tau\\) is our estimator of the ATE. This gives the same estimate as the difference-in-means estimator. Furthermore, \\(\\hat\\gamma\\) is equivalent to the average \\(Y\\) in the control group \\(\\bar Y_0\\).\nWe do not need to include covariates. This is because randomisation allows us to meet the conditions of random sampling and zero-conditional mean neccesary for the unbiasedness of OLS.\nHowever, there are several reasons one might want to include pre-treatment covariates:\n\nCan increase precision (reduce standard error), by getting better predictions of \\(Y\\).\nCan control for observable imbalance that was observed in the balance tables. Many researchers will compare a model without and with an imbalanced covariate, to show that the covariate does not matter significantly.\nCan allow for estimation of heterogenous treatment effects by including interactions in the model.\n\nWe should not include post-treatment covariates. Anything that is measured post-treatment could be measuring a treatment effect (something that results from the treatment \\(D\\)). This may “model away” your treatment effect.\n\n\n\n\n\n\nStatistical Inference\n\nStandard Errors\nStandard errors are needed if we want to conduct classical hypothesis testing. If we use the OLS estimator, we can use robust standard errors, as we derived in the last chapter.\nIf we use the difference-in-means estimator, we can calculate the standard errors with the following formula:\n\\[\n\\widehat{se}(\\hat\\tau) = \\sqrt{\\frac{\\hat\\sigma_1^2}{N_1} + \\frac{\\hat\\sigma_0^2}{N_0}}, \\quad \\hat\\sigma^2_d = \\sum\\limits_{D_i = d}\\frac{Y_i - \\bar Y_d}{N_d} \\quad \\forall \\ d \\in \\{0, 1\\}\n\\]\nWe can also use Nonparametric Bootstrap to create our sampling distribution and standard errors for statistical inference, instead of relying on asymptotic normality of the standard t-test.\nFor blocked experiments (see later), you should randomly sample blocks, not units, to create your bootstrap re-samples. For example, if your data is clustered in cities, you should re-sample by cities.\n\n\n\nClassical Hypothesis Testing\nWe can use a t-test for statistical inference. We first estimate the \\(\\hat\\tau_{ATE}\\) and robust standard error \\(\\widehat{rse}(\\hat\\tau_{ATE})\\).\nThen, we state our hypotheses, which are normally:\n\\[\nH_0 : \\tau_{ATE} = 0, \\quad H_1:  \\tau_{ATE} ≠ 0\n\\]\nFinally, we can calculate the t-test statistic \\(\\hat\\tau /\\widehat{rse}(\\hat\\tau)\\), refer to the relevant t-distribution, and calculate the p-value.\nGenerally, we use a statistical significance level of \\(\\alpha = 0.05\\), so we reject the null if \\(|t|&gt;1.96\\).\nFor more complex randomisation schemes, you will need different standard errors. For example, if you use a cluster randomisation scheme, you might need clustered standard errors.\nWe can also use Nonparametric Bootstrap to create our sampling distribution for statistical inference, instead of relying on asymptotic normality of the standard t-test.\n\n\n\nRandomisation Inference\nConsider a new sharp null hypothesis, that all individual causal effects are zero (not just the average causal effect):\n\\[\nH_0^s : Y_1 = Y_0, \\quad H_A^s : Y_1 ≠ Y_0\n\\]\nAssuming \\(H_0\\) is true, we can actually fully construct the potential outcomes \\(Y_{0i}\\) and \\(Y_{1i}\\), since we know every unit has 0 individual treatment effect. Then, we can “recreate” the sampling distribution without asymptotic properties.\nFirst, we want to calculate the total number of randomisations possible. If we have \\(N\\) total units, and \\(N_1\\) in the treatment group and \\(N_0\\) in the control group, we can calculate all possible randomisation permutations as follows:\n\\[\n\\begin{pmatrix} N \\\\ N_1 \\end{pmatrix} = \\frac{N!}{N_1 ! N_0 !}\n\\]\nThis is the total number of assignments possible given \\(N\\), \\(N_1\\), and \\(N_0\\). Then, we can calculate the \\(\\widehat{\\tau_j}\\) of every possible randomisation assignment. The figure below shows this:\n\n\n\n\n\nNow, plot all \\(\\widehat{\\tau_j}\\) in our sampling distribution:\n\n\n\n\n\nLet us say our sample \\(\\hat\\tau = 6\\). We would simply find the area under the curve that is above \\(\\hat\\tau = 6\\), and below \\(-\\hat\\tau = -6\\). This would be our p-value.\n\n\n\n\n\n\nPros/Cons of Randomisation Inference\n\n\n\n\n\nRandomisation Inference is assumption free - we do not need asymptotic properties or hypothetical sampling distributions.\nSince we also do not need asymptotic property inferences, we can do inference with very small samples as well.\nDownsides: the randomisation inference only tests if the sharp null hypothesis is true, but sometimes, that might not be something we want to test.\n\n\n\n\n\n\n\n\n\nExtension: Other Randomisations\n\nStratified Randomisation\nStratified (also called blocked or conditional) randomisation are when randomisation occurs separately within levels of some covariates(s) \\(X\\). Generally, you separate your sample of \\(N\\) units into \\(J\\) subgroups. For example, you could split people up into male or female, and random sample within each group, rather than everyone together.\nWhy would you want to do this? Let us say you have 4 subjects, with pre-treatment potential outcomes of \\(Y_{0i} = \\{2, 2, 8, 8 \\}\\).\nIf you just randomly assign, then there is a 33% chance that you end up with the random assignment where \\(\\{8, 8\\}\\) are placed in one group, and \\(\\{2, 2\\}\\) are placed in another group.\nThis is a concern: our treatment and control groups would be very imbalanced in this situation, which violates our independence assumption.\nWith blocking, we could divide our sample into \\(J = 2\\) subgroups, having group 1 being \\(\\{2, 2\\}\\), and group 2 being \\(\\{8, 8\\}\\). Then, we randomly sample one from each group into the treatment. This way, we are guaranteed better balance. This can prevent imbalances as normal randomisation can have a high probability (in certain situations) of creating imbalances.\n\n\n\n\n\n\nEstimation with Stratification\n\n\n\n\n\nTo estimate the ATE, you will need a weighted average of the ATE for each subgroup \\(j\\), with the weights being the proportion of units each group \\(j\\) accounts for:\n\\[\n\\tau_{ATE} = \\sum\\limits_{j=1}^J \\frac{N_j}{N} \\tau_j\n\\]\n\n\n\n\n\n\nCluster Randomisation\nCluster randomisation is when we randomly assign units (or have individuals naturally) in groups. Every unit within a group (called a cluster) will get the same treatment. We randomly sample the groups to get the treatment or control.\nFor example, we could randomise development treatment at the village level, or randomise treatment of a cirriculum at the school level.\nThe main reason for this is to prevent SUTVA violations.\nFor example, imagine you are testing the effects of a new curriculum. If you randomise by each student, students will talk to their friends, and treated individuals may teach control individuals about the new curriculum. But by randomising by school (either an entire school gets or does not get the new curriculum), this concern is not a huge issue.\nIf you do implement cluster randomisation, you should note that units within each cluster are likely not independent of each other. Thus, you should use clustered standard errors to account for this.\n\n\n\n\n\n\nExtension: Survey Experiments\n\nFraming/Endorsement Experiments\n\n\nPriming Experiments\n\n\nList Experiments\n\n\n\n\n\n Back to top",
    "crumbs": [
      "3 Randomised Controlled Trials"
    ]
  },
  {
    "objectID": "model4.html",
    "href": "model4.html",
    "title": "Unsupervised Learning Methods",
    "section": "",
    "text": "This chapter introduces unsupervised learning methods commonly used in Data Science. First, we will discuss Principle Components Analysis, a popular dimensional reduction approach. Then, we will discuss cluster analysis, a popular method of learning more about our data.\nUse the right sidebar for navigation. R-code provided at the bottom.\n\n\nPrinciple Components Analysis\n\nIntroduction\nPrinciple Components Analysis (PCA) combines multiple observed variables into one or new observed variables (called principle components).\n\n\n\n\n\n\n\n\n\n\n\nExample: Trust in Institutions\n\n\n\n\n\nA survey asked respondents to respond, on a scale of 0-10, how much they trust the institutions.\n\nParliament: \\(x_1\\)\nLegal: \\(x_2\\)\nPolice: \\(x_3\\)\nPoliticians: \\(x_4\\)\nPolitical Parties: \\(x_5\\)\nEuropean Parliament: \\(x_6\\)\nUnited Nations: \\(x_7\\)\n\nWe don’t care about each variable individually. We want to summarise them into one variable that describes trust in institutions.\nWe can use PCA to create \\(y_1\\) (and \\(y_2\\), and more).\n\n\n\nPCA can be used for dimensional reduction (reducing the number of variables), or for summarising multiple variables into one measure.\n\n\n\nObserved Variables\nWe have \\(p\\) observed variables \\(x_1, \\dots, x_p\\), measured for each unit in a sample of data.\nEach of these observered variables \\(x_i\\) can be summarised by:\n\nTheir sample means \\(\\bar x_i\\).\nTheir sample variances \\(var(x_i)\\) and standard deviations \\(s_i = \\sqrt{Var(x_i)}\\).\nSample correlations \\(Corr(x_i, x_k)\\) for each pair of observed variables \\(i ≠ k\\).\nTotal variance of the \\(p\\) variables: \\(Var(x_1) + Var(x_2)+ \\dots + Var(x_p)\\).\n\nThe sample correlations \\(Corr(x_i, x_k)\\) can be summarised in a covariance or correlation matrix. We notate the covariance or correlation matrix as \\(\\boldsymbol\\Sigma\\).\nBelow is an example of a correlation matrix between 7 variables (the same 7 in the example above).\n\n\n\n\n\n\n\n\nPrinciple Components\nPCA takes these original variables \\(x_1, \\dots, x_p\\), and calculates a set of new variables (principle components) \\(y_1, \\dots, y_p\\). Each principle component \\(y_j\\) are linear combinations of the original variables:\n\\[\n\\begin{split}\ny_1 = & a_{11}x_1 + a_{21}x_2 + \\dots + a_{p1}x_p \\\\\ny_2 = & a_{12}x_1 + a_{22}x_2 + \\dots + a_{p2}x_p \\\\\n& \\vdots \\\\\ny_p = & a_{1p}x_1 + a_{2p}x_2 + \\dots + a_{pp}x_p\n\\end{split}\n\\]\nWith \\(a_{ij}\\) being the weights of the linear combinations. The sum of all the weights for each principle component should be 1: \\(\\sum_i a_{ij}^2 = 1\\) for each \\(j = 1, 2, \\dots, p\\).\nWe can rewrite each principle component \\(y_j\\) in terms of linear algebra:\n\\[\ny_j = \\mathbf a_j^\\mathsf{T}\\mathbf x = \\underbrace{\\begin{pmatrix}\na_{1j} & a_{2j} &\\dots & a_{pj}\n\\end{pmatrix}}_{\\text{weight vector for } y_i} \\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_p\n\\end{pmatrix}\n\\]\nAnd all principle components \\(\\mathbf y = (y_1, \\dots, y_p)\\) can be expressed as:\n\\[\n\\mathbf y = \\mathbf A^\\mathsf{T} \\mathbf x, \\quad \\mathbf A = \\begin{pmatrix} \\mathbf a_1 & \\mathbf a_2 & \\dots & \\mathbf a_p \\end{pmatrix}\n\\]\nThe PCs together have the same total variance as the original variables: \\(\\sum Var(y_i) = \\sum Var(x_i)\\). Thus, the PCs carry the same information/variation as the original variables, just with a different distribution. The principle components are also numbered, with each subsequent principle component having less variance: \\(Var(y_j) ≥ Var(y_{j+1})\\).\nThe principle components are all uncorrelated with each other: \\(Corr(y_j , y_k) = 0, \\ \\forall \\ j ≠ k\\). This also implies a property with weights: \\(\\sum_i a_{ij}a_{ik} = 0\\) for every pair \\(j ≠ k\\). Thus, the difference components convey distinct aspects of the data.\n\n\n\nCalculating Principle Components\nThe weights \\(a_{ij}\\) are calculated from eigenvalue decomposition of the covariance matrix \\(\\boldsymbol\\Sigma\\) of variables \\(x_1, \\dots, x_p\\).\nWe assume that \\(\\boldsymbol\\Sigma\\) has \\(p\\) distinct positive eigenvalues, denoted \\(\\lambda_1 &gt; \\dots, \\lambda_p &gt; 0\\). Each eigenvalue corresponds to a eigenvector \\(\\mathbf a_j\\), which serves as the weight vector for the \\(i\\)th principle component:\n\\[\n\\boldsymbol\\Sigma \\mathbf a_j = \\lambda_j \\mathbf a_j\n\\]\nWe can apply eigenvalue decomposition to matrix \\(\\boldsymbol\\Sigma\\):\n\\[\n\\boldsymbol\\Sigma = \\mathbf{ADA}^{-1}\n\\]\nWhere \\(\\mathbf Q\\) is made up of the eigenvectors of matrix \\(\\boldsymbol\\Sigma\\), and \\(\\mathbf D\\) is a diagonal matrix with the eigenvalues \\(\\lambda\\) on its diagonals:\n\\[\n\\mathbf D = \\begin{pmatrix}\n\\lambda_1 & 0 & \\dots & 0 \\\\\n0 & \\lambda_2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\lambda_p\n\\end{pmatrix}, \\quad \\mathbf A = \\begin{pmatrix} \\mathbf a_1 & \\mathbf a_2 & \\dots & \\mathbf a_p \\end{pmatrix}\n\\]\nThe principle components will be as follows:\n\\[\n\\mathbf y = \\mathbf A^\\mathsf{T} \\mathbf x\n\\]\n\n\n\nStandardisation of Items\nThe results of PCA will be affected by the variances of individual variables. If \\(Var(x_1) &gt; Var(x_2)\\), \\(x_1\\) will receive a larger weight in PCA.\nThis means that if we have different measurement scales for variables, we can get very different PCA results. This is very similar to the issue of comparing covariances, which generally need to be standardised into correlation coefficients.\nTo prevent measurement scales from affecting PCA, we will standardise the observations for each variable, creating a new variable \\(z_{it}\\) (\\(t\\) is observations \\(t = 1, \\dots, n\\) in the sample):\n\\[\nz_{it} = \\frac{x_{it} - \\bar x_i}{s_i}\n\\]\nA standardised variable has sample mean 0, and standard deviation and variance of 1. Thus, the total variance of all standardised variables \\(z_i\\) is equal to the number of variables:\n\\[\n\\sum\\limits_{i=1}^p Var(z_i) = p\n\\]\nWe can also avoid the standardisation step by performing PCA on the correlation matrix instead of the covariance matrix (since correlation coefficients are already standardised).\n\n\n\n\n\n\nInterpreting Principle Components\n\nChoosing Number of Components\nThe variance of \\(y_j\\), is equivalent to the \\(j\\)th eigenvalue: \\(Var(y_j) = \\lambda_j\\). Principle components are listed in decreasing order of variance \\(\\lambda_1, ≥ \\lambda_2 ≥ \\dots ≥ \\lambda_p\\). The proportion of the first \\(q\\) principle components is thus:\n\\[\n\\frac{\\lambda_1 + \\lambda_2 + \\dots + \\lambda_q}{\\lambda_1 + \\lambda_2 + \\dots + \\lambda_p}\n\\]\nYou could just choose the first principle component \\(y_1\\), and use it. However, the best practice is to retain as few as possible principle components, without losing a significant amount of variability. There are a few different rules that people often use to determine how many principle components to use:\n\nRetain as many components \\(q\\) such that it explains 70-80% of the variation.\nRetain components with large eigenvalues \\(\\lambda_j\\), generally at least 0.7 if using the covariance matrix, and 1 for the correlation matrix.\nOmit components once \\(\\lambda_j\\) stops decreasing significantly (using a scree plot).\n\n\n\n\n\n\n\nScree Plot of \\(\\lambda_j\\)\n\n\n\n\n\nBelow is a scree plot of \\(\\lambda_j\\) on the \\(y\\) axis, and the principle component on the \\(y\\) axis.\n\n\n\n\n\nWe can see after the first two principle components, the plot becomes relatively flat. Thus, we can keep components 1 and 2, and we can ignore the rest.\n\n\n\nGenerally, you will normally keep around 2 or 3 principle components following these guidelines. You should also consider the interpretation of the principle components when deciding to keep the components.\n\n\n\nWeights and Component Loadings\nEach principle component is a weighted sum of the original variables, with \\(a_{ij}\\) being weights:\n\\[\ny_j = a_{1j}x_1 + a_{2j}x_2 + \\dots + a_{pj}x_p\n\\]\nThus, a larger weight for a variable means that variable contributes more, and a smaller weight for a variable means that the variable contributes less.\n\n\n\n\n\n\nExample of Interpreting Weights\n\n\n\n\n\nBelow is a table of weights:\n\nWe can see that it seems that police is contributing the least to the first principle component \\(y_1\\).\n\n\n\nWe can also consider the weights in a normalised form, called component loadings:\n\\[\na^*_{ij} = \\sqrt{\\lambda_j}  \\ a_{ij} = sd(y_j) a_{ij}\n\\]\nWhen PCA is based on the correlation matrix, we will have the property that the component loadings equals the correlation between the given variable \\(x_i\\) and the principle component \\(y_j\\):\n\\[\na^*_{ij} = Corr(x_i, y_j)\n\\]\nThis property can help us interpret what higher values of \\(y_j\\) and lower values of \\(y_j\\) mean for each principle component.\n\n\n\n\n\n\nExample of Component Weights\n\n\n\n\n\nBelow is a table of component weights:\n\nWe can see that all of the component weights for the first component are positive. That means that the first principle component \\(y_1\\) is positively correlated with all \\(x_i\\).\nWe know that all \\(x_i\\) are scales of 0 - 10 on how much they trust the institution in question. Thus, we know that for principle component \\(y_1\\), that higher values mean higher trust in institutions.\nFor the second principle component \\(y_2\\), there are some positives and some negatives. The negative correlations are with legal, police, and UN, and the positive correlations are with parliament, politicians, political parties, and european parliament.\nWe could interpret this as the second principle component \\(y_2\\) increases, the trust in legal/police decreases, but the trust in political institutions increase. Thus, \\(y_2\\) could be a measure of trust in legal/police versus political institutions.\nFor the third principle component, we can see only the trust in european parliament and UN is negative. Thus, we could interpret \\(y_2\\) as a measure of trust in national versus international institutions.\n\n\n\n\n\n\nWeights and Correlations of Original Variables\nPCA is derived from the sample correlation matrix. Thus, the weights also reflect patterns in the correlations of the original variables \\(x_i\\).\nPatterns between the weights of the first principle component and correlation matrix include the following:\n\nIf the correlations of all variables is positive, then the first principle component will be positive. This is because all the variables that are correlated mean they are measuring the “same thing” in the same direction.\nVariables which on average have the largest correlations with other variables will get the largest weights. This makes sense, since variables with the largest correlations seem to be measuring the “concept” the most.\nIf a variable has 0 correlation with the other variables, it will get a 0 weight, since it is measuring a completely different thing than the other variables.\n\n\n\n\n\n\n\nCorrelation Matrix and the First Principle Component\n\n\n\n\n\nBelow is a correlation matrix between variables:\n\nWe can see the variable politicians (highlighted in blue) have the largest correlation with the other variables on average.\nWe can see the variable police (highlighted in yellow) have the smallest correlation with other variables on average. It might be the “odd one out”, not as lined up as the rest, so it gets a smaller weight in the first principle component.\n\n\n\nThe second principle component is the contrast of the first principle component (remember, the covariance between the two is 0).\nIt will identify two subgroups of variables, such that each subgroup will have high correlations within them, but the correlation between groups is lower.\n\n\n\nPrinciple Component Scores\nFor each unit \\(i\\) (individual), we can calculate a component score for the \\(j\\)th principle component:\n\\[\ny_j = a_{1j}z_1 + a_{2j}z_2 + \\dots + a_{pj}z_p\n\\]\nBy doing this for each individual unit \\(i\\), we now have a new variable \\(y_j\\) that we can use for further analysis. Such analyses can include:\n\nWe could plot the principle component scores for the first and second principle component score, and look for clusters with similar values.\nWe could use the principle component scores as summary statistics for some phenomena.\nWe could also use the scores of \\(y_j\\) and input into a regression model or another statistical model.\n\n\n\n\n\n\n\nCluster Analysis\n\n\n\n\nImplementation in R\nYou will need packages psych and GPArotation.\n\nlibrary(psych)\nlibrary(GPArotation)\n\nTo estimate the principle components, you should use the princomp() command:\n\npca_object &lt;- princomp(~x1 + x2 + x3,\n                      data = mydata,\n                      cor = TRUE, #to do on corr matrix\n                      scores = TRUE, #calculate scores\n                      na.action = na.exclude)\nsummary(newObject)\n\nThe summary command will provide the standard deviation of each principle component, the proportion of total variance each component explains, and the cumulative proportion of variance explained.\n\n\n\n\n\n\nObtaining Variances/Eigenvalues\n\n\n\n\n\nThe variances of each principle component (also the eigenvalues) are the square of the standard deviation. We can obtain as following:\n\npca_object$sdev^2\n\n\n\n\n\n\n\n\n\n\nScreeplot\n\n\n\n\n\nTo create a screeplot to help identify how many principle components to keep, we use the screeplot() command:\n\nscreeplot(pca_object, type='l', main=\"\")\n\n\n\n\n\n\n\n\n\n\nWeights and Component Loadings\n\n\n\n\n\nWe can calculate weights for each principle component by using the loadings() command:\n\npca_weights &lt;- loadings(pca_object)\nprint(pca_loadings, cutoff = 0, digits = 4)\n\nFor component loadings (weights in a normalised form), we first get the square root of the eigenvalues \\(\\lambda_i\\), then we multiply our weights with those square roots.\n\nsqrt_lambda &lt;- pca_object$sdev\nprint(t(t(pca_weights)*sqrt_lambda), cutoff = 0, digits = 4)\n\n\n\n\n\n\n\n\n\n\nComponent Scores\n\n\n\n\n\nWe can access component scores in the pca_object we originally created:\n\npca_object$score\n\nThis will give you a table, with the rows being different units in the data, and the columns being different principle component scores for each principle component.\nIf you are wanting to just view the scores, it is recommended to subset the data if you have too many observations:\n\npca_object$score[1:10,] #first 10 units\n\nYou can also subset the number of principle components:\n\npca_object$score[,1:3] #first 3 PC\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Unsupervised Learning Methods"
    ]
  },
  {
    "objectID": "model5.html",
    "href": "model5.html",
    "title": "Factor Analysis Models",
    "section": "",
    "text": "PCA, as we discussed in the last chapter, is essentially an algorithm based method of dimensional reduction. In this chapter, we will discuss Factor Analysis, a model-based method. These models specify some probability model that represents an approximation of the data-generating process, that allow us to model latent variables.\nUse the right sidebar for navigation. R-code provided at the bottom.\n\n\nFactor Analysis\n\nFactor Analysis Models\nFactor Analysis models are Latent Variable Models, with a few characteristics:\n\n\n\n\n\n\nExplanation of Latent Variables\n\n\n\n\n\nA latent variable is a variable that is not observed. Instead, we observe several variables that are indicators of the latent variable.\nA measurement model represents how the observed indicators measure the true concept of interest, the latent variable.\n\n\n\n\n\nAn example of latent variables is the measurement of personality traits.\nWe can measure personality traits by asking survey respondents many different questions, and asking them to respond on a scale of 0-10 on each question.\nBut, these answers to the questions are all measuring one latent variable - the personality.\n\n\n\n\nAll observed indicator variables (items) and latent variables (factors) are treated as continuous variables.\nAll distributions of the variables are specified as normal distributions.\nAll the observed variables (items) are treated as measures of latent variables.\nThe latent variables (factors) are on an equal footing with each other - associations between factors are represented with correlations.\n\nWe can decide how many factors we need to properly measure the latent variables. We can assign predictions (factor scores) of the latent variable to create constructs of the latent variable to use in statistical analysis.\n\n\n\n\n\n\nExample of Factor Analysis\n\n\n\n\n\nBelow is a model of a factor analysis with 2 factors that are correlated (double sided arrow).\n\n\n\n\n\n\n\n\n\n\n\nExploratory and Confirmatory Analysis\nThere are two types of factor analysis: Exploratory factor analysis and confirmatory factor analysis.\nExploratory factor analysis (EFA):\n\nWe do not assume any number of factors, or what the factor pattern will look like.\nOur aim is to find the smallest number of interpretable factors needed to explain the correlations between the observed items.\nModels have minimum number of constraints on model parameters.\n\nConfirmatory factor analysis (CFA):\n\nModels have more than the minimum number of parameter constraints.\nCan be used to study how well hypothesized measurement models fit the data.\n\n\n\n\n\n\n\nExploratory Factor Analysis\n\nOne-Factor Model Specification\nWe denote the latent variable factor (common factor) with \\(\\xi\\). This latent variable is normally distributed with mean \\(k\\) and variance \\(\\phi\\):\n\\[\n\\xi \\sim \\mathcal N(k, \\phi)\n\\]\nWe denote our observed indicator variables (items) by \\(x_1, \\dots, x_p\\). Each item \\(x_i\\) is related to the latent factor \\(\\xi\\) with a linear regression model:\n\\[\nx_i = \\tau_i + \\lambda_i \\xi + \\delta_i\n\\]\n\\(\\lambda_i\\) is the slope, also called the factor loadings, which determine the associations between \\(\\xi\\) and \\(x_i\\). \\(\\delta_i\\) is the error term, called the unique factors.\n\n\n\n\n\n\n\n\nAssumptions and Identification\nThere are several assumptions that the exploratory factor analysis model makes:\n\nError terms \\(\\delta_i\\) for each regression model between \\(\\xi\\) and \\(x_i\\) is normally distributed with a mean of 0: \\(\\delta_i \\sim \\mathcal N(0, \\theta_{ii})\\).\nError terms \\(\\delta_1, \\dots, \\delta_p\\) are uncorrelated with each other. This implies that correlations between the \\(x_1, \\dots, x_p\\) are entirely explained by the factor. In other words, all \\(x_i\\) are conditionally independent given \\(\\xi\\).\nFactor \\(\\xi\\) is uncorrelated with the error terms \\(\\delta_i\\).\n\nWe also generally fix \\(k\\) and \\(\\phi\\) in \\(\\xi \\sim \\mathcal N(k, \\phi)\\) such that \\(\\xi\\) is a standard normal distribution. This is due to a problem of unique identification:\n\\[\n\\xi \\sim \\mathcal N(k = 0, \\ \\phi = 1)\n\\]\n\n\n\n\n\n\nProblem of Unique Identification\n\n\n\n\n\nMany different values of \\(k, \\phi, \\tau_i, \\lambda_i\\) can give the same observed means, variances, and covariances of the items \\(x_1, \\dots, x_p\\). Thus, we have many potential solutions to the problem, which means it is difficult to estimate.\nThus, we need to fix \\(k\\) and \\(\\phi\\) of \\(\\xi \\sim \\mathcal N(k, \\phi)\\) by assumption. The standard assumption is \\(k = 0\\) and \\(\\phi =1\\), so \\(\\xi \\sim \\mathcal N(0, 1)\\). This makes \\(\\xi\\) take a standard normal distribution.\n\n\n\n\n\n\n\n\n\nOther Choices of Assumptions\n\n\n\n\n\nWe can also, instead of assuming \\(k\\) and \\(\\phi\\) in \\(\\xi \\sim \\mathcal N(k, \\phi)\\), we can instead assume the size of \\(\\tau_i\\) and \\(\\lambda_i\\). This also allows for a unique identification of the model.\nThe most common alternative assumption is to choose one item (generally \\(x_1\\)), and set \\(\\tau_1 = 0\\) and \\(\\lambda = 1\\). Since \\(x_1\\) is normally distributed, this implies that \\(x_i \\sim \\mathcal N(k, \\phi + \\theta_{11})\\).\n\n\n\n\n\n\nEstimation Process\nWe know that our latent factor \\(\\xi \\sim \\mathcal N(k, \\phi)\\). We know that \\(x_i\\) is related to \\(\\xi\\) by \\(x_i = \\tau_i + \\lambda_i \\xi + \\delta_i\\), a linear regression.\nThus, the expected value/mean of \\(x_i\\) is given by \\(E(x_i|\\xi) = \\tau_i + \\lambda_i \\xi\\). Since \\(x_i\\) is also assumed to be normally distributed, we can determine the distribution of each item \\(x_i\\):\n\\[\nx_i \\sim \\mathcal N(\\tau_i + \\lambda_i \\xi, \\ \\lambda_i^2\\phi +\\theta_{ii})\n\\]\nWe can construct a theoretical covariance matrix of all the items \\(x_1, \\dots x_p\\), where the diagonals are the variances:\n\\[\n\\begin{pmatrix}\nVar(x_1) = \\lambda_1^2\\phi + \\theta_{11} & Cov(x_1, x_2) = \\lambda_1\\phi\\lambda_2 & \\dots & Cov(x_1, x_p) = \\lambda_1\\phi\\lambda_p \\\\\nCov(x_2, x_1) = \\lambda_2\\phi\\lambda_1 & Var(x_2) = \\lambda_2^2 \\phi + \\theta_{22}& \\dots & Cov(x_2, x_p) = \\lambda_1\\phi\\lambda_p \\\\\n\\dots & \\dots & \\ddots & \\vdots \\\\\nCov(x_p, x_1) = \\lambda_p\\phi\\lambda_1 & Cov(x_p, x_2) = \\lambda_p\\phi\\lambda_2 & \\dots & Var(x_p) = \\lambda_p^2 \\phi + \\theta_{pp}\n\\end{pmatrix}\n\\]\nIf we fix \\(\\xi \\sim \\mathcal N(0, 1)\\), that implies \\(\\phi = 1\\). This allows us to simplify the above theoretical covariance matrix:\n\\[\n\\begin{pmatrix}\nVar(x_1) = \\lambda_1^2 + \\theta_{11} & Cov(x_1, x_2) = \\lambda_1\\lambda_2 & \\dots & Cov(x_1, x_p) = \\lambda_1\\lambda_p \\\\\nCov(x_2, x_1) = \\lambda_2\\lambda_1 & Var(x_2) = \\lambda_2^2  + \\theta_{22}& \\dots & Cov(x_2, x_p) = \\lambda_1\\lambda_p \\\\\n\\dots & \\dots & \\ddots & \\vdots \\\\\nCov(x_p, x_1) = \\lambda_p\\lambda_1 & Cov(x_p, x_2) = \\lambda_p\\lambda_2 & \\dots & Var(x_p) = \\lambda_p^2  + \\theta_{pp}\n\\end{pmatrix}\n\\]\nThe estimation process is to find the values of \\(\\lambda_i\\) and \\(\\theta_{ii}\\), that make the above hypothetical covariance matrix, as close as possible to our observed covariance matrix from our sample with \\(x_1, \\dots, x_p\\). This is generally done with maximum likelihood estimation.\n\n\n\nInterpretation of Factor Loadings\nRecall the relationship between each item \\(x_i\\) and the latent factor \\(\\xi\\):\n\\[\nx_i = \\tau_i + \\lambda_i \\xi + \\delta_i\n\\]\nThe estimated factor loading, \\(\\widehat{\\lambda_i}\\), is the estimated covariance between the observed item \\(x_i\\), and the latent factor \\(\\xi\\). If item \\(x_i\\) has been standardised to a standard normal distribution, \\(\\widehat{\\lambda_i}\\) is also the correlation between \\(x_i\\) and \\(\\xi\\).\n\n\n\n\n\n\nAdditional Note on Multiple Factors\n\n\n\n\n\nWhen there are two or more factors, these interpretations of \\(\\widehat{\\lambda_i}\\) only hold if the multiple factors are uncorrelated.\nFor correlated factors, the covariances and correlations between the factors are still dependent on \\(\\widehat{\\lambda_i}\\), but they need to be calculated with an additional calculation.\n\n\n\nInterpretation of the latent factor \\(\\xi\\) is based on the items \\(x_i\\) that have large (positive or negative) loadings \\(\\widehat{\\lambda_i}\\) on the factor \\(\\xi\\). For example, take this example below:\n\n\n\n\n\nWe can see all the items are positively correlated with the factor. Thus, we could interpret the personality factor \\(\\xi\\) as a measure of status/power-oriented personality. We can see that admire and success carry larger weights (more important for the factor), while rich and respect carry less weight.\n\n\n\n\n\n\nRotation of Factors\n\n\n\n\n\nIn a one-factor model, the direction of the factor \\(\\xi\\) is not identified, so it can be chosen freely.\nFor example, we can change the sign of the values of the factors from \\(\\xi\\) to \\(-\\xi\\).\nRotating the factors would simply inverse the signs of the loading from \\(\\lambda_i\\) to \\(-\\lambda_i\\).\n\n\n\n\n\n\nCommunality and Reliability\nWhen we set our model such that \\(\\xi \\sim \\mathcal N(0, 1)\\), the model implies that the variances of \\(x_i\\) is:\n\\[\nVar(x_i) = \\lambda_i^2 + \\theta_{ii}\n\\]\n\\(\\lambda_i^2\\) is the part of the variance of \\(x_i\\) explained by the common factor \\(\\xi\\), known as the communality of \\(x_i\\). \\(\\theta_{ii}\\) is the residual variance, also called the specific variance.\n\\(\\theta_{ii}\\) is the unique variance (or error variance), which is the the part of the variance in \\(x_i\\) not explained by the factor \\(\\xi\\).\nThe proportion \\(\\lambda_i^2/ (\\lambda_i^2 + \\theta_{ii})\\) is called the reliability \\(\\rho_i\\) of \\(x_i\\). It is the \\(R^2\\) of the measurement model for \\(x_i\\) on \\(\\xi\\): the proportion of variance in \\(x_i\\) that is explained by \\(\\xi\\).\nWhen all \\(x_i\\) are standardised to have a variance \\(Var(x_i) = 1\\), then the communality \\(\\lambda_i^2\\) is equal to the reliability: \\(\\lambda_i^2 = 1 - \\theta_{ii}\\).\nThe mean of all the communalities of each item \\(x_1, \\dots, x_p\\) is the proportion of total variance of all the items explained by the common factor \\(\\xi\\).\n\n\n\nFactor Scores\nOnce we have estimated the factor analysis model, we can then use \\(x_1, \\dots, x_k\\) values for a individual, and predict their latent variable values \\(\\xi\\), called factor scores. These are calculated as a linear combination of the observed items:\n\\[\n\\tilde\\xi = w_0 + w_1x_1 + w_2x+2 + \\dots + w_px_p\n\\]\nThe coefficients \\(w_1, \\dots w_p\\) depend on the model parameters. Coefficients \\(w_i\\) are highest for items \\(x_i\\) which are the strongest measures of \\(\\xi\\) according to the model.\n\\(w_0\\) is the intercept. \\(w_0 = 0\\) if both \\(\\xi\\) has a mean of 0, and all \\(x_i\\) are standardised (to a standard normal distribution \\(\\mathcal N(0, 1)\\)) to have a mean of 0.\nThese weights \\(w\\) are determined by three different situations:\n\nCongeneric Measures: This is when the communality \\(\\lambda_i^2\\) and unique variance \\(\\theta_{ii}\\) are differerent for different items \\(x_i\\) and \\(x_k\\). this indicates that the items measure the same factor, but on different scales, and with different amount of errors and reliabilities \\(\\rho\\).\nTau-equivalent measures are when the communality \\(\\lambda_i^2\\) are equivalent for different items \\(x_i\\) and \\(x_k\\), but unique variance \\(\\theta_{ii}\\) is different. That implies the items measure the same factor on the same scale, but with different amounts of error and reliability \\(\\rho\\).\nParallel Measures are when the communality \\(\\lambda_i^2\\) and unique variance \\(\\theta_{ii}\\) are equivalent for different items \\(x_1\\) and \\(x_k\\). These items measure the same factor, at the same scale, with the same amount of error.\n\nIf items \\(x_i, x_k\\) are parallel measures, they recieve equal weights \\(w_i = w_k\\) in the calculation of factor scores for \\(\\xi\\).\n\n\n\n\n\n\nMultiple Factor EFA\n\nModel Specification\nWe can have a model with \\(q\\) number of latent factors \\(\\boldsymbol\\xi = (\\xi_1, \\dots, \\xi_q)\\). The model for the distribution of vector \\(\\boldsymbol\\xi\\) is:\n\\[\n\\boldsymbol\\xi \\sim \\mathcal N(\\boldsymbol k, \\boldsymbol\\Phi)\n\\]\n\nWhere \\(\\boldsymbol k = (k_1, k_2, \\dots, k_q)\\).\nWhere \\(\\boldsymbol\\Phi\\) is a matrix with diagonal elements being variances \\(Var(\\xi_j) = \\phi_{jj}\\), and other elements being covariances \\(Cov(\\xi_j, \\xi_k) = \\phi_{jk}\\).\n\nThis implies that each latent factor \\(\\xi_j\\) is a standard normal distribution:\n\\[\n\\xi_j \\sim \\mathcal N(k_j, \\phi_{jj})\n\\]\nNow, the measurement model of how \\(x_i\\) is related to each factor \\(\\xi_j\\) is as follows:\n\\[\nx_i = \\tau_i + \\lambda_{i1} \\xi_1 + \\lambda_{i2} \\xi_2 + \\dots + \\lambda_{iq} \\xi_q + \\delta_i\n\\]\nThe assumptions remain the same from the first model:\n\nError term \\(\\delta_i\\) is normally distributed with a mean of 0: \\(\\delta_i \\sim \\mathcal N(0, \\theta_{ii})\\). The variance \\(\\theta_{ii}\\) depends on which factor \\(x_i\\) we are using.\nError terms \\(\\delta_{1j}, \\dots, \\delta_{pj}\\) are uncorrelated with each other. This implies that correlations between the items, are entirely explained by the factor. In other words, all \\(x_i\\) are conditionally independent given \\(\\xi_j\\).\nAll factors \\(\\xi_j\\) is uncorrelated with the error terms \\(\\delta_i\\).\n\n\n\n\nModel Identification\nFor a given number of items \\(p\\), you must have a sufficiently small number of factors \\(q\\). Generally, the maximum amount of factors \\(q\\) is given by:\n\\[\ndf = \\frac{(p-q)^2 - (p+q)}{2} ≥ 0\n\\]\nAs in the 1-factor model, we need to specify the scales of the factors. The most conventional way is to set all means of all factors \\(\\xi_j\\) to 0, \\(k_j = 0\\), and set all variances of all factors \\(\\xi_j\\) to 1, \\(\\phi_{jj} = 1\\). Thus, every factor will be defined as:\n\\[\n\\xi_j \\sim \\mathcal N(k_j = 0, \\ \\Phi_{jj} = 1)\n\\]\nFixing \\(\\Phi_{jj} = 1\\) means our variance-covariance matrix \\(\\boldsymbol\\Phi\\) has diagonals of 1. However, the non-diagonal elements \\(Cov(\\xi_j, \\xi_k) = \\phi_{jk}\\) are not defined, so these covariances between factors are parameters that we need to estimate.\n\n\n\n\n\n\nThe Heywood Case\n\n\n\n\n\nEven when the model is formally identified, the number of factors \\(q\\) can be “too large”.\nThe Heywood Case is the name of an estimated variance of \\(x_i\\), \\(\\theta_{ii}\\), which is 0 or even negative for some observed variable \\(x_i\\).\nIt is possible that this means that \\(x_i\\) is a perfect measure of the factors \\(\\xi_j\\). However, it is far more likely that this indicates that the model has too many factors .\nThe best way to deal with this is to fit the model with one fewer factor. You can also use a confirmatory factor analysis model which sets some loadings to 0.\n\n\n\n\n\n\nFactor Rotation\nChoosing the scale of the latent factors does not fully resolve their identification. This is because there are actually infinitely many rotations of our latent factors, that all produce the same fit.\n\n\n\n\n\n\nDetails on the Rotation Identification Issue\n\n\n\n\n\nSuppose we start with two factors, \\(\\xi_1\\) and \\(\\xi_2\\). Let us transform them to 2 new factors with some linear combinations with coefficients \\(a\\):\n\\[\n\\begin{split}\n& \\xi_1^* = a_{11}\\xi_1 + a_{12}\\xi_2 \\\\\n& \\xi_2^* = a_{21} \\xi_1 + a_{22} \\xi_2\n\\end{split}\n\\]\nThis transformation can be interpreted as a rotation (change in coordinate axes) of the space of these factors. Both pairs \\((\\xi_1, \\xi_2)\\) and \\((\\xi_1^*, \\xi_2^*)\\) both produce the exactly same fit for the observed items. Thus, this causes a unique identification issue.\nIn fact, any choice of coefficients \\(a\\) (there are infinitely many of them) will produce the same model fit.\n\n\n\nGenerally, we choose the rotation based on the interpretability of the resulting factors. Interpretation is easiest when each factor has high loadings for some variables, and small (near 0) loadings for all the rest. This allows us to clearly identify what each factor is representing.\nOrthogonal factors (perpendicular to each other in vector space) imply that the factors are uncorrelated. However, some rotations can be oblique rotations, which allow the factors to be correlated.\n\n\n\n\n\n\nVisualisation of Orthogonal and Oblique Rotations\n\n\n\n\n\nThe below illustrates orthogonal and oblique rotations of factors:\n\n\n\n\n\nWe can see that the obliquely rotated axes are not exactly perpendicular to each other.\n\n\n\nIf the main goal of analysis is interpretation, we generally want to use an oblique rotation, as they are easier to interpret. This also shows if there are correlations between the factors.\nFor data reduction purposes, the orthogonal rotation can be useful, as it avoids multicollinearity.\n\n\n\nFactor Interpretation\nInterpretation is very similar to the one-factor models.\nInterpretation of a factor \\(\\xi\\) is based on the items \\(x_i\\) that have large (positive or negative) loadings \\(\\widehat{\\lambda_{ij}}\\) on the factor \\(\\xi\\).\nWhen there are two or more factors, these interpretations of \\(\\widehat{\\lambda_{ij}}\\) (which were the covariance between \\(x_i\\) and \\(\\xi_j\\)) only hold if the multiple factors are uncorrelated.\nFor correlated factors, the covariances and correlations between the factors are still dependent on \\(\\widehat{\\lambda_i}\\), but they are not exactly the value of \\(\\widehat{\\lambda_i}\\).\nWe can still calculate our factor scores as before.\n\\[\n\\tilde\\xi_j = w_{0j} + w_{1j}x_1 + w_{2j}x+2 + \\dots + w_{pj}x_p\n\\]\n\n\n\n\n\n\nConfirmatory Factor Analysis\n\nIntroduction\nExplanatory factor analysis allows us to learn from our data. However, we also make some arbitrary restrictions for indentification:\n\nWe rely on arbitrary guidelines/personal judgement to determine how many factors \\(\\xi\\) to include.\nWe rely on personal judgement to interpret factors by determining which loadings \\(\\widehat{\\lambda}_i\\) are large and small.\nWe have to make arbitrary rotation decisions (oblique or orthogonal).\n\nConfirmatory Factor Analysis is a different approach. Instead of letting the data “speak for itself”, CFA allows us to test our own theories that we already have about the relationships of indicators to factors.\nWe can test theories about relationships between factors \\(\\xi\\) by constraining them according to our theory. For example, based on our theories, we might set some component loadings to zero, and sometimes, set measurement parameters for different items \\(x_i, x_k\\) equal to each other.\n\n\n\n\n\n\nExample of Confirmatory Factor Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nModel Specification\nThe model specification of CFA is almost identical to multiple-factor EFA.\nRemember our factor analysis model, with observed items \\(x_1, \\dots, x_p\\), and latent factors \\(\\xi_1, \\dots, \\xi_q\\) is composed of a series of regressions between one item \\(x_i\\) and all the factors \\(\\xi_1, \\dots, \\xi_q\\):\n\\[\nx_i = \\tau_i + \\lambda_{i1} \\xi_1 + \\lambda_{i2} \\xi_2 + \\dots + \\lambda_{iq} \\xi_q + \\delta_i  \n\\]\nAs with EFA, we assume that all latent factors \\(\\boldsymbol\\xi = \\xi_1, \\dots, \\xi_q\\) are normally distributed with means \\(\\boldsymbol\\kappa = \\kappa_1, \\dots, \\kappa_q\\) and variances \\(\\boldsymbol\\Phi = \\phi_1, \\dots \\phi_q\\):\n\\[\n\\boldsymbol\\xi \\sim \\mathcal N(\\boldsymbol\\kappa, \\boldsymbol\\Phi)\n\\]\nAnd as with EFA, the error terms \\(\\delta_i\\) are normally distributed \\(\\delta_i \\sim \\mathcal N(0, \\theta_{ii})\\), and \\(\\delta_i\\) is uncorrelated with the \\(\\xi_j\\)’s.\nThe estimation process is identical to EFA estimation that was previously discussed.\n\n\n\nRotation and Scales of Factors\nIn EFA, all loadings \\(\\lambda_{ij}\\) (coefficients in the regression of \\(x_i\\) on \\(\\xi_1, \\dots, \\xi_q\\)) are non-zero, or the minimum number of 0’s to fix a specific ration.\nIn CFA, We can fix the factor rotation by setting enough 0 loadings (putting some \\(\\lambda_i = 0\\)), based on theories on which factors might not have any influence on a certain indicator \\(x_i\\).\nA 0 loading implies that that specific \\(\\xi_j\\) is not being measured by an observed item \\(x_i\\) (since a coefficient of 0 implies no correlation between \\(x_i\\) and \\(\\xi_j\\)). Ideally, for CFA, we want a simple structure where each item \\(x_i\\) has only one non-0 loading. This makes interpretation quite easy, as each item will only be measuring one factor.\n\n\n\n\n\n\nVisualisation of the Ideal Structure\n\n\n\n\n\nLet us say we have 8 items, and 2 factors. The ideal structure of our loadings is that each of the 8 items only has one non-zero loading. Or in other words, each item is only measuring one of the two factors.\nGraphically, such a structure would be visualised as:\n\n\n\n\n\nThis makes interpretation of the factors much easier. Below are the factor loadings of this model in EFA and CFA:\n\n\n\n\n\nWe can clearly see that the first factor is measuring some combination of rich, admire, success, and respect, while the second factor is measuring friend, equal, nature, and care.\n\n\n\n\n\n\n\n\n\nFurther Conditions for Identification\n\n\n\n\n\nIf you want a simple structure where each item \\(x_i\\) only has one non-zero loading, and you assume that errors \\(\\delta_i\\) are all uncorrelated with each other, you will need a certain factor-item ratio.\n\nFor a 1-factor CFA model, you must have at least 3 observed items.\nFor a model with 2 or more factors, you must have at least 2 items per factor.\n\nFurther conditions exist for more specific cases. Generally, the computer software will give you a warning message when a model cannot be identified.\n\n\n\nIn CFA, we can also fix the scales of individual factors through two ways:\n\nWe could, just like in EFA, assume that factors are standard normal distributions \\(\\xi_j \\sim \\mathcal N(0,1)\\). This option is the more common one, since it is simple.\nOr, each factor \\(\\xi_j\\) can be standardised to have the same scale of one item \\(x_i\\). This is done by choosing a \\(\\xi_j\\) to standardise to \\(x_i\\), going into that \\(x_i\\)’s regression, and setting the coefficient of \\(\\xi_j\\), \\(\\lambda_{ij} = 1\\) in that \\(x_i\\)’s regression.\n\n\n\n\n\n\n\nModel Selection\n\nSelection Choices in Factor Analysis\nFor both exploratory and confirmatory factor analysis, we have choices to make.\n\nFor exploratory factor analysis, we need to decide how many factors.\nFor confirmator factor analysis, we need to decide the number of factors, and what parameter constraints to use (what components to set equal to 0).\n\nGenerally, we judge models on a few set of criteria:\n\nThe error variance of each item, or reliability of each item. If the reliability is very low, we may not want to include it. We will show a few tests to determine this later.\nInterpretation: once we have selected the number of factors, can those factors be interpreted?\n\nGenerally, if we have any concerns with the above criteria, we will typically omit items with poor reliability, or just omit items such that the smaller number of interpretable factors fits the model better.\nThere are a number of statistical tests and metrics to help us make these decisions.\n\n\n\nGlobal Goodness of Fit Test\nThe estimation process of factor analysis involes finding parameters to estimate the covariance matrix \\(\\boldsymbol\\Sigma\\). The likelihood ratio test compares the fit of two models, relating to how well the fit the covariance matrix \\(\\boldsymbol\\Sigma\\) of the observed items \\(x_1, \\dots, x_p\\).\nFor the global goodness of fit test, we want to estimate the “goodness” of one model we have fitted. The two models in the test are:\n\nThe Saturated “Full” Model \\(f\\) , which reproduces the sample covariance matrix \\(\\boldsymbol\\Sigma\\) perfectly, using \\(\\frac{p(p+1)}{2}\\) parameters.\nOur fitted model \\(R\\) (also called the restricted model), which produces a fitted covariance matrix \\(\\hat{\\boldsymbol\\Sigma}\\), using \\(\\nu_r\\) number of parameters. All the parameters used in the fitted model are also present in the saturated model (which means our fitted model is “nested” in the saturated model).\n\nThe hypothesis of the test is:\n\n\\(H_0\\): the fitted/restricted model \\(r\\) is correct.\n\\(H_1\\): the fitted/restricted model \\(r\\) is incorrect.\n\nSo unlike many other hypothesis tests, we actually do not want to reject the null hypothesis \\(H_0\\).\n\n\n\n\n\n\nDetails on the Global Goodness of Fit Test\n\n\n\n\n\nThe test statistic is the likelihood ratio statistic:\n\\[\nL^2 = 2(L-L_r)\n\\]\nWhere \\(L_r\\) is the log-likelihood for the fitted/restricted model, and \\(L\\) is the log-likelihood for the saturated/full model. Essentially, \\(L^2\\) is comparing the difference between the sample covariance matrix \\(\\boldsymbol\\Sigma\\) and our fitted covariance matrix \\(\\hat{\\boldsymbol\\Sigma}\\).\nThe degrees of freedom of the test is the difference in number of parameters between the two models:\n\\[\n\\text{df} = \\nu_f - \\nu_r \\ = \\  \\frac{p(p+1)}{2} - \\nu_r\n\\]\nOnce we calculate the test-statistic \\(L^2\\), we consult a \\(\\chi^2\\) distribution with the above degrees of freedom.\nIf the p-value is small (p&lt;0.05), we reject \\(H_0\\), and conclude that \\(\\boldsymbol\\Sigma\\) and \\(\\hat{\\boldsymbol\\Sigma}\\) are not very similar, and the restricted fitted model is not a good fit of the data.\nIf the p-value is not small (p&gt;0.05), we refuse to reject \\(H_0\\), conclude that \\(\\boldsymbol\\Sigma\\) and \\(\\hat{\\boldsymbol\\Sigma}\\) are similar, and the restricted fitted model is a good fit of the data.\n\n\n\nOwn drawback of this goodness-of-fit test is that it is sensitive to sample size. That means the larger the sample size, the more likely the test rejects the restricted model, even if the differences between the two models is minimal.\n\n\n\nGlobal Fit Indicies\nAs discussed above, the global goodness-of-fit test is sensitive to sample size. It also assumes we are testing against exact fit in the population, which may not make sense.\nThere are some alternative fit indicies that address this limitation of the goodness-of-fit tests, that are more specialised. These statistics are often all reported alongside a factor analysis model, but are not too important to understand.\n\n\n\n\n\n\nRoot Mean Square Error of Approximation (RMSEA)\n\n\n\n\n\nRMSEA is a global goodness-of-fit metric that determines how well a fitted model does. The value is calculated as:\n\\[\n\\text{RMSEA} = \\sqrt{\\frac{T_m - \\text{df}_m}{\\text{df}_m(N-1)}}\n\\]\n\nWhere \\(T_m\\) is the overal goodness of fit \\(\\chi^2\\) statistic of the implied model.\nWhere \\(\\text{df}_m\\) is the corresponding degrees of freedom.\nWhere \\(N\\) is the sample size.\n\nA RMSEA of 0 is a perfect fit. Values smaller than 0.05 indicate close fit. Values greater than 0.1 indicate poor fit.\n\nAlthough these cutoffs can be questionable for smaller sample sizes (less than 100).\n\nWe can conduct statistical hypothesis tests of \\(H_0 : RMSEA = 0.05\\). This means the null hypothesis assumes a close fit - and if our null hypothesis is rejected, that means we have a poor fit (when doing a one tailed test towards higher values of RMSEA).\n\n\n\n\n\n\n\n\n\nStandard Root Mean Square Residual (SRMR)\n\n\n\n\n\nThe standard root mean square residual is the average covariance residual of our estimates to the actual sample covariances.\nThe smaller the value, the better the fit. Generally, values less tha 0.08 are indications of good fit.\n\n\n\n\n\n\n\n\n\nTucker and Lewis Index (TLI)\n\n\n\n\n\nThe Tucker and Lewis Index compares our fitted model \\(m\\) with some baseline model \\(b\\). It is as follows:\n\\[\nTLI = \\frac{\\frac{T_b}{df_b} - \\frac{T_m}{df_m}}{T_b \\ df_b - 1}\n\\]\n\nWhere \\(T_b\\) and \\(T_m\\) are the chi-squared statistics for the baseline model \\(b\\) and the implied model \\(m\\).\n\\(df\\) are the degrees of freedom.\n\nValues less than 0.9 indicate poor fit. 1 indicates a very good fit. If you get a value over 1, that may indicate overfitting.\nThis statistic does not differ too much with sample size.\n\n\n\n\n\n\n\n\n\nComparative Fit Index (CFI)\n\n\n\n\n\nThe Comparative Fit Index compares our fitted model \\(m\\) with some baseline model \\(b\\). It is as follows:\n\\[\nCFI = \\frac{(T_b - df_b) - (T_m - df_m)}{T_b - df_b}\n\\]\n\nWhere \\(T_b\\) and \\(T_m\\) are the chi-squared statistics for the baseline model \\(b\\) and the implied model \\(m\\).\n\\(df\\) are the degrees of freedom.\n\nThe values are always between 0 and 1. Values closer to 1 indicate a good fit.\nThis statistic does not differ too much with sample size.\n\n\n\n\n\n\nNested Likelihood Ratio Test\nIn the global goodness-of-fit test, we compared one fitted model to an “accurate” model.\nHowever, we can also compare two fitted models, just like the F-test in Linear Regression or Likelihood Ratio test in Logistic Regression.\nWe will have two models that we fit: one is a smaller (restricted model) \\(M_0\\), and one is the larger (less restricted model) \\(L_1\\). Our hypotheses are:\n\nNull hypothesis: \\(H_0 : M_0\\).\nAlternate Hypothesis: \\(H_1: M_1\\).\n\nThis essentially tests if the additional parameters in the larger model are statistically significant - i.e. are they worth including.\n\nFor exploratory factor analysis, a larger model is a model with more factors. The test is to see if including an extra factor makes the model statistically significantly better.\nFor confirmatory factor analysis, we can do the same tests as EFA. We can also test if setting additional loadings to 0 in the larger model is statistically significant.\n\n\n\n\n\n\n\nDetails on Likelihood Ratio Tests\n\n\n\n\n\nThe test statistic is the likelihood ratio statistic:\n\\[\nL^2 - 2(L_1- L_0)\n\\]\nWhere \\(L_r\\) is the log-likelihood for the restricted model \\(M_0\\), and \\(L\\) is the log-likelihood for the less-restricted model \\(M_1\\). Essentially, \\(L^2\\) is comparing the difference between the fitted covariance matrices \\(\\hat{\\boldsymbol\\Sigma}\\) for both models.\nThe degrees of freedom of the test is the difference in number of parameters between the two models:\n\\[\n\\text{df} = \\nu_1 - \\nu_0\n\\]\nOnce we calculate the test-statistic \\(L^2\\), we consult a \\(\\chi^2\\) distribution with the above degrees of freedom.\nIf the p-value is small (p&lt;0.05), we reject \\(H_0\\), and conclude that \\(\\boldsymbol\\Sigma\\) and \\(\\hat{\\boldsymbol\\Sigma}\\) are not very similar, and the larger model \\(M_1\\) is the better model (and the additional parameters are statistically significant).\nIf the p-value is not small (p&gt;0.05), we refuse to reject \\(H_0\\), conclude that \\(\\boldsymbol\\Sigma\\) and \\(\\hat{\\boldsymbol\\Sigma}\\) are similar, and the larger model \\(M_1\\) is no better than the smaller model \\(M_0\\) (and the additional parameters are not statistically significant.\n\n\n\n\n\n\nTests of Single Coefficients\nYou can also conduct singificance tests of individual parameter estimates. This is useful for CFA, if you want to determine if a loading should be 0 or not. The hypotheses are:\n\nNull hypothesis: \\(H_0 : \\lambda_i = 0\\).\nAlternate Hypothesis: \\(H_1 : \\lambda_i ≠ 0\\).\n\nThe CFA software will produce standard errors for the parameter estimates. Using these standard errors, we can conduct \\(z\\)-tests (or Wald tests):\n\\[\nz = \\frac{\\widehat{\\lambda_i}}{se(\\widehat{\\lambda_i})}, \\quad W = \\left( \\frac{\\widehat{\\lambda_i}}{se(\\widehat{\\lambda_i})} \\right)^2\n\\]\nFor the \\(z\\) test, you consult a standard normal distribution to find the p-values. For the wald test, consult a \\(\\chi^2\\) distribution with 1 degree of freedom.\n\nIf p&lt;0.05, we can reject the null hypothesis, and conclude that \\(\\lambda_i\\) is not 0, and should not be set to 0 in our CFA.\nIf p&gt;0.05, we cannot reject the null, so we cannot conclude that \\(\\lambda_i\\) is not 0, and thus, we can set it to 0 in our CFA.\n\n\n\n\nInformation Criteria Statistics\nInformation Criteria Statistics (the same as in Logistic regression) can be used to compare different models, and how good they fit the model.\nThe two most common are:\n\nAkaike’s Information Criterion: \\(AIC = L^2 - 2df\\), where \\(L^2\\) is the likelihood ratio statistic of the fitted restricted model against the full/satruated model, and \\(df\\) is the degrees of freedom.\nBayesian Information Criterion: \\(BIC = L^2 - \\log(N) df\\), where \\(N\\) is the sample size.\n\nSmaller values indicates a better model. Generally, these values cannot be interpreted on their own - they need to be interpreted in relation to other models (comparison/ordinal, not cardinal). BIC rewards having fewer parameters more strongly than AIC.\n\n\n\n\n\nImplementation in R\nFor exploratory factor analysis, you will need the package psych and GPArotation:\n\nlibrary(psych)\nlibrary(GPArotation)\n\nBefore starting factor analysis, you want a dataset with only complete observations (no NA’s) for the variables you are items for factor analysis:\n\nno_na &lt;- apply(mydata, 1, FUN=function(x){all(!is.na(x))})\nmydata &lt;- mydata[no_na,]\n\nAlso subset the data so that only the items you want to use are in the dataframe.\n\n\n\n\n\n\nEFA with One Factor\n\n\n\n\n\nWe can use the fa() command to conduct factor analysis:\n\nfa_object &lt;- fa(mydata, nfactors=1, fm=\"ml\")\nprint(fa_object)\n\nThe output provides\n\n\n\n\n\n\n\n\n\nEFA with Two Factors\n\n\n\n\n\nWe can use the fa() command to conduct factor anlaysis.\nFor a non-rotated (orthogonal) rotation, the code is as follows:\n\nfa_object &lt;- fa(mydata, nfactors=1, fm=\"ml\", rotate = \"none\")\nprint(fa_object)\n\nFor a oblique rotation, the code is as follows:\n\nfa_object &lt;- fa(mydata, nfactors=1, fm=\"ml\", rotate = \"oblimin\")\nprint(fa_object)\n\nThe output provides a table of the loadings \\(\\lambda_i\\) for each item. ML1 represents the first factor \\(\\xi_1\\), and ML2 represents the second factor \\(\\xi_2\\), and so on…\nFor the oblique rotation, there is also a table of correlations between all the factors \\(\\xi_j\\).\n\n\n\n\n\n\n\n\n\nFactor Scores\n\n\n\n\n\nTo calculate factor scores, we can extract them from the fa_object in which we stored our factor analysis.\n\nfa_object$scores\n\nThis will give you a table, with the rows being different units in the data, and the columns being different factor scores for each factor.\nIf you are wanting to just view the scores, it is recommended to subset the data if you have too many observations:\n\nfa_object$scores[1:10,] #first 10 units\n\nYou can also subset the number of factors (although if you don’t need extra factors, you would just specify less when estimating):\n\npca_object$scores[,1:2] #first 2 factors\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Factor Analysis Models"
    ]
  },
  {
    "objectID": "model3.html",
    "href": "model3.html",
    "title": "Forecasting and Prediction Models",
    "section": "",
    "text": "This chapter covers methods for forecasting and prediction. We start with time-series models for forecasting future values. Then, we explore a series of machine learning models that help with prediction/classification tasks.\nUse the right sidebar for navigation. R-code provided at the bottom.\n\n\nTime Series Models\n\nTime Series and Stationary Processes\nTime series are a set of data collected at successive time points \\(t \\in \\mathbb Z\\). Each observation \\(X_t\\) is at time point \\(t\\). For example, a time series with \\(T\\) number of time periods would be \\((X_1, X_2, \\dots, X_T)\\). Examples of time series include monthly unemployment data, daily currency exchange rates, and monthly politician approval rates.\nThe goal of time series modelling is to predict \\(X_t\\) with past data. There are several factors that might affect \\(X_t\\):\n\nPrevious values of \\(X_t\\) in the time series.\nLong-term trends: these are long-term movements in the mean of a section of the time series. This can be an upward trend, a downward trend, or a stationary trend (no changes).\nSeasonality: cyclical repeating patterns in certain periods, for example, business cycles, or sales being higher during the holiday season.\nRandomness: other random/systematic fluctuations.\n\nOne important idea is a (weak) stationary process, where there is zero trend over time. A weak stationary process is when the expected value and variance of our time series is constant over different (but same-sized) groups of time periods: \\(E(X_t) = \\mu\\), \\(Var(X_t) = \\sigma^2\\).\nMost time-series data will not be weak stationary. However, we can transform most time-series data into weak stationary processes through differencing. We refer to the new differenced time series as \\(X'_t\\):\n\nZero differencing \\(d=0\\) is no differencing: \\(X_t' = X_t\\). We just use our original time-series.\nFirst order differencing \\(d = 1\\) is \\(X_t' = X_t - X_{t-1}\\). Basically we take our \\(X_t\\) at time period \\(t\\), and subtract away the previous time period \\(X_{t-1}\\).\nSecond order differencing \\(d = 2\\) is \\(X'_t = (X_t - X_{t-1}) - (X_{t-1}-X_{t-2})\\).\n\nAny further order of differencing \\(d\\) is increasingly uncommon. The point of differencing is that we keep increasing \\(d\\) until we have a weak stationary process. We denote a time series that has been differenced at \\(d\\)-order with \\(X_i'^d\\).\n\n\n\nMA and AR Models\nMA (Moving Average) models are when \\(X_t\\) at time \\(t\\) depends on past error terms \\(\\epsilon_{t-1}, \\dots, \\epsilon_{t-q}\\). This implies that \\(X_t\\) is dependent on short-term noise/fluctuation. The model is specified with stationarity assumed for \\(X'_t\\):\n\\[\n\\begin{align}\nMA(q) \\ : \\ X_t'^d & = \\omega +  \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2} + \\dots + \\theta_q \\varepsilon_{t-q} + \\varepsilon_t \\\\\n& = \\omega + \\sum\\limits_{i=1}^q \\theta_i \\epsilon_{t-i} \\ + \\varepsilon_t\n\\end{align}\n\\]\nWhere \\(\\omega\\) is some constant (the mean of the time series), and \\(\\theta_1, \\dots, \\theta_q\\) are the coefficients. This model is estimated with maximum likelihood. To choose \\(q\\), we find the model with the lowest AIC or BIC.\nAR (Auto-regressive) models are when \\(X_t\\) at time \\(t\\) depends on past values \\(X_{t-1}, \\dots, X_{t-p}\\). This implies that \\(X_t\\) is dependent on previous values in the time series, for example, in variables with momentum (ex. stock prices, sales). The model is specified with stationarity assumed for \\(X'_t\\):\n\\[\n\\begin{align}\nAR(p) \\ : \\ X_t'^d& = \\omega + \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\dots + \\phi_p X_{t-p} + \\varepsilon_t \\\\\n& = \\omega + \\sum\\limits_{i=1}^p \\phi_i X_{t-i} \\ + \\varepsilon_t\n\\end{align}\n\\]\nLet us demonstrate a unique property of autoregressive models. Consider AR(1):\n\\[\nAR(1) : X_t' = \\omega + \\phi_1 X_{t-1} + \\varepsilon_i\n\\]\nKnowing this, we can also predict \\(X'_{t-1}\\), and then plug this back into our original \\(X'_t\\). Then, we can do the same with \\(X'_{t-2}\\), plug it in, and then \\(X'_{t-3}\\), and so on \\(k\\) number of times, until we can get the following model:\n\\[\nX'_t = \\phi^kX_{t-k} + \\sum\\limits_{j=0}^{k-1} \\phi_j \\varepsilon_{t-j}\n\\]\nAnd when \\(|\\phi| &lt; 1\\), the model is equivalent to a moving average model where \\(q = ∞\\):\n\\[\nX'_t = \\sum\\limits_{j=0}^∞ \\phi_j \\varepsilon_{t-j} = MA (∞)\n\\]\n\n\n\nARIMA Model\nThe most popular model is the ARIMA (Auto-regressive Integrated Moving Average Model), which combines AR and MA models. ARIMA assumes that we have stationary trends:\n\\[\n\\text{ARIMA}(p, d, q) \\ : \\ X_t'^d = \\underbrace{\\sum\\limits_{i=1}^p \\phi_iX_{t-i}}_{\\text{AR}} + \\underbrace{\\sum\\limits_{i=1}^q \\theta_i\\varepsilon_{t-i}}_{\\text{MA}} + \\varepsilon_t\n\\]\n\n\\(X_t'^d\\) is \\(d\\)-ordered differenced time series that is stationary.\nThe AR (Autoregressive) section is how previous values of \\(X\\) before time period \\(t\\) predict the current \\(X_t\\). \\(p\\) is the number of previous time periods to include (which we have to choose).\nThe MA (Moving Average) section is how previous values of error \\(\\epsilon\\) before time period \\(t\\) predict the current \\(\\epsilon_t\\). \\(q\\) is the number of previous time periods to include (which we have to choose).\n\nThe model is estimated with maximum likelihood estimation. We can choose \\(p\\) and \\(q\\) by first choosing a variety of \\(p\\) and \\(q\\), estimating each model, and choosing the model (and \\(p\\) and \\(q\\)) that has the least AIC or BIC value.\nARIMA is a non-seasonal models, since seasonality is technically non-stationary by definition. An extension of this model for seasonaility is called SARIMA.\nARIMA is very popular for a multitude of reasons.\n\nFirst, most situations in the real world have both AR and MA components: generally, \\(X_t\\) is determined by some combination of past \\(X_t\\) and errors.\nARIMA is a general framework. An \\(\\text{ARIMA}(p, 0, 0) = \\text{AR}(p)\\), and a \\(\\text{ARIM}(0,0,q) = \\text{MA}(q)\\). This allows us to adapt ARIMA to a wide variety of data without being forced into one specific model selection.\nARIMA has quite good accuracy in time series forecasting, even compared to machine learning and modern techniques.\n\n\n\n\nSeasonality Differencing and SARIMA\nSARIMA (Seasonal Auto-regressive Integrated Moving Average) models are the seasonal extension: we add seasons by doing additional seasonal differencing.\nA seasonal differenced is taking \\(X_t\\), and subtracting the \\(X\\) value from the same point in the last season. Let us say each season is \\(S\\) number of time periods long. So, a first-order seasonal difference would be \\(Z_t = X_t - X_{t-S}\\).\nWe can also combine a seasonal differenced \\(Z_t\\) with a normal difference. For example, if \\(d= 1\\) and \\(D=1\\):\n\\[\nZ^{*d}_t = Z_t - Z_{t-1} = (X_t - X_{t-S}) - (X_{t-1} - X_{t-1-S}), \\qquad \\text{where } Z_t = X_t - X_{t-S}\n\\]\nA SARIMA model will take the following form:\n\\[\n\\text{SARIMA}(p, q, d)(P, Q,D)S \\ : \\ Z_t^{*d} = \\underbrace{\\overbrace{\\sum\\limits_{i=1}^p \\phi_iX_{t-i}}^{\\text{AR}} + \\overbrace{\\sum\\limits_{i=1}^q \\theta_i\\epsilon_{t-i}}^{\\text{MA}}}_{\\text{ARIMA}} + \\underbrace{\\overbrace{\\sum\\limits_{i=1}^P \\Phi_iX_{t-Si}}^{\\text{AR}} + \\overbrace{\\sum\\limits_{i=1}^Q \\Theta_i\\epsilon_{t-Si}}^{\\text{MA}}}_{\\text{Seasonality}} + \\epsilon_t\n\\]\nWhere \\(Z^{*d}\\) is a \\(d\\)-order difference of \\(D\\)-order seasonal differences \\(Z_t'^D\\). Note that the seasonality section’s parameters and model components are all specified with capital letters.\nChoosing \\(p, q, P, Q\\) is the same as in a ARIMA model - we should estimate models with different combinations, and see which model has the lowest AIC and BIC.\n\n\n\n\n\n\nTrees and Random Forests\n\nRegression Trees\nRegression Trees are an alternative way to obtain predictions of an outcome \\(y\\) from explanatory variables \\(x\\). Instead of modelling how \\(y\\) changes with every unit increase in \\(x\\), regression trees instead model stratification.\nTree-based methods will divide the independent variable \\(x_j\\) into 2 regions, splitting \\(x\\) at some value \\(x = s\\). For example, if we had an \\(x\\) variable such that \\(x \\in [0, 100]\\), we could split the variable at \\(x = s\\) to create two regions: \\(x^{(1)} \\in [0, s]\\) and \\(x^{(2)} \\in [s, 100]\\).\nThen, tree-based methods will calculate the mean \\(y\\) value in each region created: \\(\\bar y^{(1)}\\) and \\(\\bar y^{(2)}\\). These means will be the predictions of \\(\\hat y\\).\n\nIf a unit \\(i\\) has an \\(x\\) value that falls into region \\(x^{(1)} \\in [0, s]\\), their predicted \\(\\hat y = \\bar y^{(1)}\\).\nIf a unit \\(i\\) has an \\(x\\) value that falls into region \\(x^{(2)} \\in [s,100]\\), their prediction \\(\\hat y = \\bar y^{(2)}\\).\n\nBut how do we decide which threshold \\(s\\)? For continuous \\(y\\) variables, to determine the threshold \\(s\\) in which to split \\(x\\), the algorithm of tree regressions will find the optimal threshold \\(x = s\\) that reduces the residual sum of squares (RSS) of the predictions the most:\n\\[\nRSS = \\sum\\limits_{i=1}^n(y_i - \\hat y_i)^2\n\\]\nEssentially, the computer tests every possible threshold value of \\(x_j = s\\), and finds the threshold that reduces the sum of squares the most.\nFor binary \\(y\\) variables, trees will determine the threshold \\(s\\) in which to split \\(x\\), based on the threshold \\(x=s\\) that reduces the classification error rate. The classification error is essentially how many predictions \\(\\hat y\\) do not match to the actual \\(y\\).\n\n\n\n\n\n\nProcedure of Growing a Tree\n\n\n\n\n\nThe process of growing a tree is as follows. Assume now we have more than one explanatory variable.\n\nTest all possible thresholds \\(x_j = s\\) for each \\(x_j \\in \\{x_1, \\dots, x_k\\}\\).\nIdentify the specific variable \\(x_p \\in \\{x_2, \\dots, x_k\\}\\) which has a specific threshold \\(x_p = s^*\\) that has the greatest reduction in residual sum of squares (or classification error rate), for all combinations of \\(x_j\\) and threshold \\(s\\).\nNow, divide that specific \\(x_p\\) at that specific threshold \\(s^*\\), creating two regions \\(x_p^{(1)}, x_p^{(2)}\\).\nNow, we repeat the process of testing all possible thresholds \\(x_j = s\\) for each \\(x_j \\in \\{x_1, \\dots, x_k \\}\\) and finding the specific variable-threshold \\(x_m = s^{**}\\) that results the in the greatest reduction in RSS (or classification error rate) within \\(x_p^{(1)}\\). Create 2 more groups from within \\(x_p^{(1)}\\).\nThen, do the same for \\(x_p^{(2)}\\). Create 2 more groups from with \\(x_p^{(2)}\\).\nKeep on going, finding the variable-thresholds which reduce the RSS the most from each previous subregion we have created.\nContinue doing this until some stopping criteria. Find the average \\(y\\) in each of the groups you have, and those are the predicted \\(\\hat y\\).\n\nIt is possible for a variable to occur multiple times with different thresholds.\n\n\n\n\n\n\n\n\n\nExample of Growing a Tree\n\n\n\n\n\nBelow is an example of growing a tree:\n\n\n\n\n\nWe can see that the first (top) split is in the variable taxpercent, at threshold 34.2935. That means that specific variable-threshold split reduced the RSS the most of any variable-threshold combination.\nNotice how after the first split, we only divide each subregion. We do not go back to the top/whole data set.\nNotice how taxpercent re-appears again on the right side of the tree. It is possible for a variable to occur multiple times.\nAt the end, you can see the numbers at the bottom. These are called leaves, and are the final categories/groups of the tree with their mean \\(y\\) labelled. Those mean \\(y\\) will be the predictions for each observation that falls into that group.\n\n\n\nThe earlier a variable is split, the more influential the variable is. We will discuss this idea further later in the chapter.\n\n\n\nLimitations and Bootstrap Aggregation\nOne of the best things about decision trees is that they incorporate interactions between variables. Lower-level splits of variables are interacting with higher-level splits of variables. This allows regression trees to be excellent for non-linear predictions. Regression trees are also great for visualisation, and are quite easy to explain visually without invoking complex statistics or mathematics.\nThe downside of regression trees is that they have extremely high variance. If you just slightly change the data, your predicted results will be completely different, and even the order of variables and thresholds will completely change. This is called overfitting. Overfitting causes simple regression trees to be poor predictors of out-of-sample data.\nBootstrap Aggregation is a solution to this variance and overfitting problem. This procedure builds on a simple statistical idea: in a set of \\(n\\) samples \\(Z_1, \\dots, Z_n\\) each with variance \\(\\sigma^2\\), the variance of the means \\(\\bar Z\\) of all the samples is \\(\\frac{\\sigma^2}{n}\\). Since \\(n\\) is in the denominator, that implies increasing the number of samples \\(n\\) will reduce the variance of our predictions.\nHowever, we typically only have one sample of data. How can we increase \\(n\\) if we only have one sample? The answer is Bootstrap Sampling. Essentially, we sample with replacement from our original sample.\nTo create a bootstrap sample, we choose 1 observation at random from our original sample. We add that observation to our bootstrap sample, and replace it back into our original sample. Then, we draw another observation, add it to our bootstrap sample, and replace it back. We do this \\(n\\) times (\\(n\\) being the sample size of our sample).\nIf we do this procedure multiple times, we will end up with a few similar, but slightly different data sets. We are essentially replicating the process of obtaining new data sets, without gathering more data. Now, with our multiple data sets, we can use bootstrap aggregation to reduce our variance.\n\n\n\nBagging and Random Forest\nBagging models apply bootstrap sampling to tree regressions.\n\nWe first generate \\(B\\) number of bootstrap samples.\nThen, we train a tree on each different sample \\(b\\), creating a prediction function \\(f_b(x) = \\hat y_b\\) for each sample, which specifies what values of \\(x\\) result in what predicted \\(\\hat y\\) according to the tree.\nThen we average all of the sample predictions together to obtain our final prediction function \\(f_{bag}(x)\\).\n\n\\[\nf_{bag}(x) = \\frac{1}{B}\\sum\\limits_{b=1}^Bf_b(x)\n\\]\nBagging reduces the variance of trees, and is one of the most accurate prediction methods, frequently outperforming both linear and non-linear methods.\nHowever, bagging is still not perfect. This is because bagging trees are heavily correlated: in general, the trees will have the same top-level \\(x_p\\) variable, especially if there is one very strong predictor in our explanatory variables. This is an issue - highly correlated trees, even if averaged, do not reduce the variance as much as we might need.\nRandom Forests solve this issue by not only bootstrapping observations like Bagging does, but also sampling a subset of explanatory variables for each tree. For example, let us say you a set of explanatory variables \\(\\mathcal X = \\{x_1, \\dots  x_k\\}\\). For every bootstrap sample of observations \\(b\\) that we did in bagging, Random forest will also sample a subset of explanatory variables \\(\\mathcal X_b\\), which contains only \\(g&lt;k\\) number of explanatory variables.\nThis means that each tree Random Forest produces only has a subset of \\(g\\) explanatory variables, not the total number of \\(k\\) explanatory variables. This means that sometimes, influential variables \\(x_p\\) will not be included in the subset \\(\\mathcal X_b\\), which will allow for other predictors to get a chance to shine.\nThus, Random Forest will have less correlated trees, and thus, will typically have more accurate predictions.\nGenerally, the size of the subset of explanatory variables \\(\\mathcal X_b\\) will typically be \\(g=\\sqrt{k}\\), the square root of the total number of explanatory variables. However, you can play around with this (discussed in the next section).\n\n\n\nModel Selection\nThere are a few choices you must make when specifying the model you are using.\n\nShould you use Random Forest or Bagging? Typically, Random Forest is better, but this is not always the case.\nIf you do choose Random Forest, what size of the subset of explanatory variables \\(\\mathcal X_b\\) should you use? Typically, the standard is to use the square root of the number of explanatory variables, but this is not a fixed rule.\n\nFor prediction tasks, Mean of squared residuals is a general measure of how well the model performed. It is the mean of the squared errors, where the errors are the differences between the actual dataset \\(y_i\\) value and the predicted \\(\\hat y_i\\) value. In R, the mean of squared residuals metric is calculated on testing data - testing the model on units that were not included in a specific bootstrap sample. Thus, this is a good measure of how predictive your model is, without worrying about overfitting.\nFor classification tasks, the simplest metric to help you make the decision is the error rate. This is simply the percentage of observations the model got wrong (predicted one category when it should have been the other). We can also dig into more detail:\n\nThe False Positive Rate is the observations that are \\(y=0\\), but our model predicted incorrectly as \\(\\hat y = 1\\).\nThe False Negative Rate is the observations that are \\(y=1\\), but our model predicted incorrectly as \\(\\hat y = 0\\).\n\nThe inverses of false positives/negatives are specificity and sensitivity:\n\nSpecificity is the percentage of observations that are \\(y=0\\), that our model correctly predicted as \\(\\hat y =0\\).\nSensitivity is the percentage of observations that are \\(y=1\\), that our model correctly predicted as \\(\\hat y = 1\\).\n\nThe metric on which to focus on depends on our goals of classification. Choosing the right model requires some subjectivity. For example, if we are trying to predict if a patient has a serious disease, we probably want more false positives rather than under-detecting people who are actually sick. But for judicial systems, since we do not want to put an innocent person in jail, we might prefer lower false positives, and more false negatives.\n\n\n\nImportance Statistics\nOne of the downsides of Bagging and Random Forest is that they are harder to interpret than traditional statistical models and also regression trees.\nRegression Trees produce a nice diagram, with the most important variables near the top. However, Bagging and Random Forest average hundreds or thousands of different trees, so we cannot really draw a diagram out.\nThis is where importance statistics comes in. To calculate importance, we “remember” the reduction in RSS (or error rate) every time we split a tree, for every bagging sample \\(b\\). Then, we figure out which variables on average reduce the RSS (or error rate) the most.\n\n\n\n\n\nAbove is an example of the importance plot. This allows us to determine which explanatory variables are the most influential in determining the predictions, which can be useful in interpretation.\n\n\n\nExtension: Causal Forests\n\n\n\n\n\n\nNaive Bayes Classifier\n\n\n\nImplementation in R\nYou will need the randomForest and tidyverse packages.\n\nlibrary(randomForest)\nlibrary(tidyverse)\n\nFor replication purposes (if you are publishing a paper), you may also want to set a random seed, which will ensure your bootstrap and variable sampling remains consistent in replication.\n\nset.seed(1234) #any number will work\n\n\n\n\n\n\n\nBagging Model\n\n\n\n\n\nYou can train a bagging model with the randomForest() function:\n\nmodel &lt;- randomForest(Y ~ x1 + x2 + x3 + x4,\n                      data = mydata,\n                      na.action = na.omit,\n                      mtry = 4, #equal to k\n                      importance = TRUE)\n\n#call object to see output\nmodel\n\nThe output will contain summary statistics.\nNote: you can also do \\(Y \\sim .\\) in the formula. The \\(.\\) symbolises that you want to include all other variables not \\(Y\\) within your dataframe into your model. This speeds up the process of writing every single explanatory variable out.\n\n\n\n\n\n\n\n\n\nRandom Forest Model\n\n\n\n\n\nYou can train a random forest model with the randomForest() function:\n\nmodel &lt;- randomForest(Y ~ x1 + x2 + x3 + x4,\n                      data = mydata,\n                      na.action = na.omit,\n                      mtry = 2, #equal to sqrt of k\n                      importance = TRUE)\n\n#call object to see output\nmodel\n\nThe output will contain summary statistics.\nNote: you can also do \\(Y \\sim .\\) in the formula. The \\(.\\) symbolises that you want to include all other variables not \\(Y\\) within your dataframe into your model. This speeds up the process of writing every single explanatory variable out.\n\n\n\n\n\n\n\n\n\nPredictions\n\n\n\n\n\nYou probably want to make predictions with your model (that is kind of the point of these models).\nWe can use the predict() function to generate predictions:\n\nmypredictions &lt;- predict(model, newdata = my_new_data)\n\nmy_new_data is a dataframe with a bunch of explanatory variable values (for every explanatory variable) for a collection of observations, that you wish to predict \\(\\hat y\\) for.\nmodel is the name of the object in which you stored your trained model to.\n\n\n\n\n\n\n\n\n\nImportance Plot\n\n\n\n\n\nTo see the importance of each explanatory variable, we can use the varImpPlot() function:\n\nvarImpPlot(model,\n           main = \"Title of the Graph\",\n           type = 2)\n\nDo not change type = 2. That specifies something technical that you do not need to worry about.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Forecasting and Prediction Models"
    ]
  },
  {
    "objectID": "quant1.html",
    "href": "quant1.html",
    "title": "The Classical Linear Model",
    "section": "",
    "text": "Before we start with causal inference, we need to understand the classical linear model, its mechanics, and how we can achieve unbiased estimates of relationships. This chapter discusses the classical linear models, the properties of the ordinary least squares estimator, statistical inference in a regression setting, and model specification issues.\nUse the right sidebar for quick navigation. R-code is provided at the bottom.\n\n\nThe Linear Model\n\nModel Specification\nThere is some random outcome variable \\(Y_i\\), and \\(p\\) explanatory variables \\(X_{i1}, X_{i2}, \\dots, X_{ip}\\). The population linear model is specified as a conditional expectation function:\n\\[\n\\E(Y_i|X_i) = \\beta_0 + \\beta_1 X_{i1} + \\dots + \\beta_p X_{ip}\n\\]\nWhere \\(\\beta_0, \\dots, \\beta_p\\) are population parameters to be estimated. We can also specify linear regression not as the expected \\(Y\\) as above, but for each observation of \\(Y_i\\):\n\\[\nY_i = \\beta_0 + \\beta_1X_{i1} + \\dots + \\beta_pX_{ip} + \\eps_i \\ = \\  \\beta_0 + \\sum\\limits_{j=1}^p \\beta_jX_{ij} + \\eps_i\n\\]\nWhere \\(\\eps_i\\) is the error term representing the variation in \\(Y_i\\) that is not explained by the \\(p\\) explanatory variables (either a missing variable in our model, or some random noise). This model implies for each observation/individual \\(i=1,\\dots ,n\\) in the population, with values \\((y_i, x_{i1} \\dots, x_{ip})\\) has a regression equation:\n\\[\n\\begin{align}\ny_1 = & \\ \\beta_0 + \\beta_1x_{11} + \\dots + \\beta_px_{1p} + \\eps_1 \\\\\ny_2 = & \\ \\beta_0 + \\beta_1x_{21} + \\dots + \\beta_px_{2p} + \\eps_2 \\\\\n& \\qquad \\vdots \\qquad \\qquad \\vdots \\\\\ny_n = & \\ \\beta_0 + \\beta_1x_{n1} + \\dots + \\beta_px_{np} + \\eps_n\n\\end{align}\n\\]\nWe can write this system of regression equations in linear algebra form:\n\\[\ny = X\\beta + \\eps \\quad \\iff \\quad \\begin{pmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\\end{pmatrix}  =\n\\begin{pmatrix}1 & x_{11} & \\dots & x_{1p} \\\\1 & x_{21} & \\dots & x_{2p} \\\\\\vdots & \\vdots & \\vdots & \\vdots \\\\1 & x_{n1} & \\dots & x_{np}\\end{pmatrix}\n\\begin{pmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p\\end{pmatrix}\n+ \\begin{pmatrix}\\eps_1 \\\\ \\eps_2 \\\\ \\vdots \\\\ \\eps_n\\end{pmatrix}\n\\]\nOur goal is to estimate \\(\\beta\\) to get our estimates \\(\\hat\\beta\\), which allows us to create predicted fitted-values model \\(\\hat y = X\\hat\\beta\\).\n\n\n\nOrdinary Least Squares Estimator\nTo estimate the population parameters \\(\\beta_0, \\dots, \\beta_p\\), we use our sample data, and try to find the values \\(\\hat\\beta_0, \\dots, \\hat\\beta_p\\) that minimise the square sum of residuals (SSR): \\(\\sum(Y_i - \\hat Y_i)^2\\) .We will define the SSR as function \\(S\\):\n\\[\n\\begin{align}\nS(\\hat\\beta) &  = (y - \\hat y)^\\top (y - \\hat y) && (\\Sigma(Y - \\hat Y_i)^2 \\text{ in linear algebra})\\\\\n& = (y - \\color{blue}{X \\hat\\beta}\\color{black} )^\\top (y - \\color{blue}{X \\hat\\beta}\\color{black})  && (\\because \\color{blue}{\\hat y = X \\hat\\beta}\\color{black})\\\\\n& = y^\\top y - \\hat\\beta^\\top X^\\top y - y^\\top X \\hat\\beta +  \\hat\\beta^\\top X^\\top X \\hat\\beta && (\\text{distribute out}) \\\\\n& = y^\\top y \\ \\color{blue}{-  2 \\hat\\beta^\\top X^\\top y} \\color{black}  +  \\underbrace{\\hat\\beta^\\top X^\\top X \\hat\\beta}_{\\text{quadratic}} && (\\because \\color{blue}{- \\hat\\beta^\\top X^\\top y - y^\\top X \\hat\\beta = - 2 \\hat\\beta^\\top X^\\top y} \\color{black})\n\\end{align}\n\\]\nNow, let us take the gradient to find the first order condition:\n\\[\n\\frac{\\partial S(\\hat\\beta)}{\\partial \\hat\\beta} = -2 X^\\top y + 2 X^\\top X \\hat\\beta = 0\n\\]\nWhen assuming \\(X^\\top X\\) is invertable (which is true if \\(X\\) is full rank), we can isolate \\(\\hat\\beta\\) to find the solution to OLS:\n\\[\n\\begin{align}\n-2 X^\\top y + 2 X^\\top X \\hat\\beta & = 0 \\\\\n2 X^\\top X \\hat\\beta & = 2 X^\\top y && ( + 2X^\\top y \\text{ to both sides}) \\\\\n\\hat\\beta & = (2X^\\top X)^{-1} -2 X^\\top y && (\\times (2X^\\top X)^{-1} \\text{ to both sides}) \\\\\n\\hat\\beta & = (X^\\top X)^{-1} X^\\top y && (2^{-1}, 2 \\text{ cancel out})\n\\end{align}\n\\tag{1}\\]\nVector \\(\\hat\\beta\\) is our coefficient estimates derived from OLS.\n\n\n\n\n\n\nAlternative Derivation for Simple Linear Regression\n\n\n\n\n\nCurrently, we are deriving the first order conditions for multiple linear regression using linear algebra. For simple linear regression (with one explanatory variable), we can use summation notation. Recall our sum of squared residuals in summation form:\n\\[\nSSR = S(\\hat\\beta_0, \\hat\\beta_1)= \\sum\\limits_{i=1}^n(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i)^2\n\\]\nWe want to minimise the SSR in respect to both \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\). We can do this by finding our first order conditions:\n\\[\n\\begin{align}\n\\frac{\\partial S(\\hat\\beta_0, \\hat\\beta_1)}{\\partial \\hat\\beta_0} & = \\sum\\limits_{i=1}^n(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i) = 0 \\\\\n\\frac{\\partial S(\\hat\\beta_0, \\hat\\beta_1)}{\\partial \\hat\\beta_1} & = \\sum\\limits_{i=1}^n X_i(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i) = 0 \\\\\n\\end{align}\n\\]\nThese conditions create a system of equations, which you can solve for the OLS solutions of \\(\\widehat{\\beta_0}\\) and \\(\\widehat{\\beta_1}\\). I will not show it step by step, as it is tedious (and not that important). The OLS solutions are\n\\[\n\\begin{align}\n\\hat\\beta_0 & = \\bar Y - \\widehat{\\beta_1} \\bar X \\\\\n\\hat\\beta_1 & = \\frac{\\sum_{i=1}^n(X_i - \\bar X)(Y_i - \\bar Y)}{\\sum_{i=1}^n(X_i - \\bar X)^2} = \\frac{Cov(X_i, Y_i)}{\\V Y_i}\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nProof OLS Estimates are Equal with \\(Y_i\\) and \\(\\E(Y_i|X_i)\\)\n\n\n\n\n\nBefore, we specified the linear model in terms of both \\(Y_i\\) and the conditional expectation function \\(\\E(Y_i|X_i)\\). However, I only derived OLS estimates in respect to \\(Y_i\\). For those interested, this is proof the OLS estimates of both are equivalent.\nBest-approximation of a conditional expectation function is defined by the lowest mean-squared error (MSE). Let us prove OLS on \\(Y_i\\) gets the same \\(\\beta_0, \\dots, \\beta_p\\) as the best linear approximation of \\(\\E(Y_i|X_i)\\). Take this very simple CEF and its MSE:\n\\[\n\\begin{align}\n\\E(Y_i|X_i) & = b_0 + b_1X_i \\\\\nMSE & = \\E(Y_i - \\E(Y_i|X_i))^2 \\\\\n& =  \\E(Y_i - (\\color{blue}{b_0 + b_1X_i}\\color{black}))^2  && (\\because \\color{blue}{\\E(Y_i|X_i) = b_0 + b_1X_i}\\color{black})\\\\\n& = \\E(Y_i - b_0 - b_1 X_i) && \\text{(distribute negative sign)}\n\\end{align}\n\\]\nThe first order conditions are (using chain rule and partial derivatives):\n\\[\n\\begin{split}\n& \\E(Y_i - b_0 - b_1X_i) = 0 \\\\\n& \\E(X_i(Y_i - b_0 - b_1X_i) = 0\n\\end{split}\n\\]\nNow, recall our OLS minimisation conditions (simple linear regression, presented in the extra info box above). Since by definition, average/expectation is \\(\\E(x) = \\frac{1}{n} \\sum x_i\\), we can rewrite as:\n\\[\n\\begin{split}\n& \\sum\\limits_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i) = \\ n \\times \\E(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i) = 0 \\\\\n& \\sum\\limits_{i=1}^n X_i (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i) = \\  n \\times \\E(X_i(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i)) = 0\n\\end{split}\n\\]\nAnd since anything multiplied to a zero turns into zero, we can ignore the \\(n\\) in the first order condition. Thus, we can see, are the exact same minimisation conditions as the conditional expectation function. Thus, OLS is the best approximation of the CEF. This property is very useful for interpreting our regression results.\n\n\n\n\n\n\nOrthogonal Projection of OLS\nWe can use the OLS solution from Equation 1 to get our fitted values \\(\\hat{y}\\) and residuals \\(\\hat\\eps\\):\n\\[\n\\begin{align}\n\\hat y & = X\\hat\\beta = X \\color{blue}{(X^\\top X)^{-1}X^\\top y} && \\color{black}(\\because \\color{blue}{\\hat\\beta = (X^\\top X)^{-1}X^\\top y} \\color{black}) \\\\\n& = \\color{red}{P}\\color{black}y && (\\because \\color{red}{P:= X(X^\\top X)^{-1}X^\\top}) \\\\\n\\hat\\eps & = y - \\hat y  = y - \\color{blue}{Py} && \\color{black}( \\because \\color{blue}{\\hat y = Py}\\color{black}) \\\\\n& = (I-P)y && (\\text{factor out y}) \\\\\n& = \\color{purple}{M}\\color{black}y && (\\because \\color{purple}{M:= I - P}\\color{black})\n\\end{align}\n\\]\nMatrix \\(\\color{red}{P}\\), called the projection matrix, is a matrix operator that performs the linear mapping \\(y \\rightarrow \\hat{ y}\\). Matrix \\(\\color{purple}{M}\\), called the residual maker, is a matrix operator that performs the linear mapping \\(y \\rightarrow \\hat{\\eps}\\).\n\n\n\n\n\n\nMatrix Properties of \\(P\\) and \\(M\\)\n\n\n\n\n\nBoth \\(\\color{red}{P}\\) and \\(\\color{purple}{M}\\) are symmetric matrices: \\(P^\\top = P, \\ M^\\top = M\\). They are also both idempotent matrices: \\(PP = P, \\ MM = M\\). We can prove this second statement using the first (I will only do it for \\(P\\), but the same applies for \\(M\\):\n\\[\n\\begin{align}\nPP & = X(X^\\top X)^{-1} \\underbrace{X^\\top X(X^\\top X)^{-1}}_{= I} X^\\top \\\\\n& = X(X^\\top X)^{-1} X^\\top = P\n\\end{align}\n\\]\n\\(\\color{red}{ P}\\) and \\(\\color{purple}{ M}\\) are also orthogonal to each other - i.e. \\(P^\\top M = 0\\):\n\\[\n\\begin{align}\nP^\\top M & = \\color{blue}{P}\\color{black}M && (\\because \\color{blue}{P^\\top = P}\\color{black}) \\\\\n& = P(\\color{blue}{I-P}\\color{black}) && (\\because \\color{blue}{M:= I - P}\\color{black}) \\\\\n& = P - PP && \\text{(distribute out)} \\\\\n& = P - \\color{blue}{P} && \\color{black}(\\because \\color{blue}{PP = P}\\color{black}) \\\\\n& = 0\n\\end{align}\n\\]\n\n\n\nFitted values \\(\\hat{y}\\) are a linear combination of our explanatory variables \\(X\\). By the definition of vector spaces, that means our explanatory variable vectors \\(x_1, x_2, \\dots, x_p\\) span a space that includes our fitted values vector \\(\\hat{y}\\).\nSo, what \\(\\color{red}{ P}\\) is doing is taking our original data vector \\(y\\), and projecting it into the space spanned by our explanatory variables \\(X\\) (called the column space). We can see in the figure below, our observed \\(y\\) vector is being projected onto the blue plane spanned by \\(X\\) to create our fitted values vector \\(\\hat{y}\\).\n\n\n\n\n\nResidual maker \\(\\color{purple}{M}\\) projects \\(y\\) onto the space orthogonal to the column space of \\(X\\) to get our residuals \\(\\hat{\\eps}\\). We can see in the figure the residuals vector (notated \\(\\mathbf e\\) in the figure) is orthogonal/perpendicular to the space of \\(\\mathbf X\\).\n\n\n\n\n\n\nInterpreting the Model\n\nRegression Anatomy and Controlling\nWe can split up matrix \\(X\\) into two matrices - \\(X_1\\) containing the regressors we care about, and \\(X_2\\) containing regressors we do not care about. Vector \\(\\beta\\) will be split in the same way. Our partitioned model is:\n\\[\ny = X_1 \\beta_1 + X_2 \\beta_2 + \\eps\n\\]\nRecall our “residual maker” matrix \\(M\\). First, note a unique property: \\(\\color{red}{MX = 0}\\). Now, let us define the residual making matrix for the second part of the regression \\(M_2\\):\n\\[\nM_2 = I - X_2 (X_2^\\top X_2)^{-1}X_2^\\top\n\\]\nNow, let us multiply both sides of our above partitioned model by \\(M_2\\):\n\\[\n\\begin{align}\nM_2 y & = M2(X_1\\beta_1 + X_2\\beta_2 + \\eps) \\\\\nM_2 y & = M_2X_1 \\beta_1 + M_2 X_2 \\beta_2 + M_2 \\eps && \\text{(multiply out)} \\\\\nM_2 y & = M_2 X_1 \\beta_1 + M_2 \\eps && (\\because M_2X_2 = 0, \\ \\because \\color{red}{MX = 0}\\color{black})\n\\end{align}\n\\]\nNow, let us denote \\(\\tilde{y} := M_2 y\\), \\(\\tilde{X}_1: = M_2 X_1\\), and error \\(\\tilde\\eps := M_2 \\eps\\). Then we get the following regression equation and OLS coefficient estimates:\n\\[\n\\tilde y = \\tilde X_1 \\beta_1 + \\tilde\\eps\n\\]\n\\[\n\\hat\\beta_1 = (\\tilde X_1^\\top \\tilde X_1)^{-1}\\tilde X_1 ^\\top \\tilde y\n\\]\nRemember that vector \\(\\hat{\\beta}_1\\) is our coefficient estimates for \\(X_1\\), the portion of \\(X\\) we are interested in. This is equivalent to the coefficient estimates had we not partitioned the model.\nNotice how in the formula, we have \\(\\tilde{X}_1\\). What is \\(\\tilde{X}_1 := M_2 X_1\\)? Well, we know that \\(M_2 X_2 = 0\\). That tells us that any part of \\(X_1\\) that was correlated to \\(X_2\\) also became 0. Thus, \\(\\tilde{X}_1\\) is the part of \\(X_1\\) that is uncorrelated with \\(X_2\\). Essentially, we are partialling out the effect of other variables. This is why we can “control” for other variables when focusing on the coefficient of one (or a few) variables.\n\n\n\nInterpretation of Coefficients\nAbove, we showed OLS coefficients partial out (control) for the other control variables in the regression. Here, we formalise the interpretations. I define \\(\\hat\\beta_j \\in \\{\\hat\\beta_1, \\dots, \\hat\\beta_p\\}\\), multiplied to \\(X_{ij} \\in \\{X_{i1}, \\dots, X_{ip}\\}\\). \\(\\hat\\beta_0\\) is the intercept.\n\n\n\n\n\n\n\n\n\nContinuous \\(X_{ij}\\)\nBinary \\(X_{ij}\\)\n\n\n\\(\\hat\\beta_j\\)\nFor every one unit increase in \\(X_{ij}\\), there is an expected \\(\\hat\\beta_j\\) unit change in \\(Y_i\\), holding all other explanatory variables constant.\nThere is a \\(\\hat\\beta_j\\) unit difference in \\(Y_i\\) between category \\(X_{ij} = 1\\) and category \\(X_{ij} = 0\\), holding all other explanatory variables constant.\n\n\n\\(\\hat\\beta_0\\)\nWhen all explanatory variables equal 0, the expected value of \\(Y_i\\) is \\(\\hat\\beta_0\\).\nFor category \\(X_{ij} = 0\\), the expected value of \\(Y_i\\) is \\(\\hat\\beta_0\\) (when all other explanatory variables equal 0).\n\n\n\nNote: these interpretations are not causal effects, just correlations. We also have not discussed if the actual estimates of \\(\\hat\\beta_j\\) are reliable (which will be covered in the classical least squares theory below).\n\n\n\n\n\n\nStandardised Interpretations\n\n\n\n\n\nSometimes, unit change is not very useful - as it depends on how the variable is measured. For example, what does a 5 unit change in democracy mean? Is that big, small? It is hard to tell.\nInstead, we can look at the change in standard deviations. For a one standard deviation \\(\\sigma_X\\) increase in \\(X_{ij}\\), there is an expected \\(\\frac{\\beta_j\\sigma_X}{\\sigma_Y}\\)-standard deviation change in \\(Y_i\\). The proof is provided below.\nProof: For simplicity, let us use a simple linear regression \\(\\E(Y_i|X_i) = \\beta_0 + \\beta_1 X_i\\):\n\\[\n\\begin{align}\n& \\E \\left(\\frac{Y_i}{\\sigma_Y} | X_i = x + \\sigma_X \\right ) - \\E \\left(\\frac{Y_i}{\\sigma_Y} | X_i = x \\right ) \\\\\n& = \\frac{\\E(Y_i|X_i = x+ \\sigma_X)}{\\sigma_Y} - \\frac{\\E(Y_i|X_i = x)}{\\sigma_Y} &&\\text{(property of expectation)} \\\\\n& = \\frac{\\E(Y_i|X_i = x+ \\sigma_X) - \\E(Y_i|X_i = x)}{\\sigma_Y} && \\text{(combine into 1 fraction)}\\\\\n& = \\frac{\\beta_0 + \\beta_1(x+\\sigma_X) - [\\beta_0 + \\beta_1(x)]}{\\sigma_Y} && \\text{(plug in regression models)}\\\\\n& = \\frac{\\beta_1\\sigma_X}{\\sigma_Y} && \\text{(cancel and simplify)}\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nCategorical Explanatory Variables\n\n\n\n\n\nTake an explanatory variable \\(X_i\\), which has \\(g\\) number of categories \\(1, \\dots, g\\). To include \\(X_i\\) in our regression, we would create \\(g-1\\) dummy (binary) variables, to create the following regression model:\n\\[\n\\E(Y_i|X_i) = \\beta_0 + \\sum\\limits_{j=1}^{g-1} \\beta_j X_{ij}\n\\]\n\nCategories \\(1, \\dots, g-1\\) get there own binary variable \\(X_{i1}, \\dots, X_{ig-1}\\).\nCategory \\(g\\) (the reference category) does not get its own variable. We can change which category we wish to be the reference.\n\nInterpretation is as follows (category \\(j\\) is any one of category \\(1, \\dots, g-1\\)).\n\n\\(\\beta_j\\) is the difference in expected \\(Y_i\\) between category \\(j\\) and the reference category \\(g\\).\n\\(\\beta_0\\) is the expected \\(Y_i\\) of the reference category \\(g\\).\nThus, category \\(j\\) has an expected \\(Y_i\\) of \\(\\beta_0 + \\beta_j\\).\n\n\n\n\n\n\n\n\n\n\nBinary \\(Y_i\\): Linear Probability Model\n\n\n\n\n\nThe standard linear model assumes a continuous \\(Y_i\\) variable. However, we can adapt the linear model to fit binary \\(Y_i\\) variables. When \\(Y_i\\) is binary and only has values \\(Y_i \\in \\{0, 1\\}\\), our linear model is actually no longer a predictor of \\(Y_i\\), since our regression will output values that are not 0 and 1.\nInstead, our linear model will now predict the probability of unit \\(i\\) having \\(Y_i = 1\\). The is due to the conditional expectation interpretation of regression, and the expectation of the binomial distribution:\n\\[\n\\begin{align}\n\\E(Y_i|X_i) & = \\underbrace{0 \\times \\P(Y_i = 0|X_i) \\ + \\ \\P(Y_i = 1|X_i)}_{\\text{a weighted avg. formula}} \\\\\n& = \\P(Y_i=1|X_i)\n\\end{align}\n\\]\nThus, we can rewrite our linear model with the primary outcome being \\(\\P(Y_i = 1|X_i)\\). This model is called the linear probability model:\n\\[\n\\P(Y_i = 1|X_i) = \\beta_0 + \\beta_1X_{i1} + \\dots + \\beta_pX_{ip} + \\eps_i\n\\]\nOur interpretations of coefficients also slightly change.\n\n\n\n\n\n\n\n\n\nContinuous \\(X_{ij}\\)\nBinary \\(X_{ij}\\)\n\n\n\\(\\hat\\beta_j\\)\nFor every one unit increase in \\(X_{ij}\\), there is an expected \\(\\hat\\beta_j \\times 100\\) percentage point change in the probability of a unit being in category \\(Y_i=1\\), holding all other explanatory variables constant.\nThere is a \\(\\hat\\beta_j\\times 100\\) percentage point difference in the probability of a unit being in category \\(Y_i=1\\) between category \\(X_{ij} = 1\\) and category \\(X_{ij} = 0\\), holding all other explanatory variables constant.\n\n\n\\(\\widehat{\\beta_0}\\)\nWhen all explanatory variables equal 0, the expected probability of a unit being in category \\(Y_i=1\\) is \\(\\hat\\beta_0 \\times 100\\)\nFor category \\(X_{ij} = 0\\), the expected probability of a unit being in category \\(Y_i=1\\) is \\(\\hat\\beta_j \\times 100\\) (when all other explanatory variables equal 0).\n\n\n\n\n\n\n\n\n\nGoodness of Fit with R-Squared\nRecall our fitted values equation, shown previously, can be rewritten with the projection matrix \\(P\\):\n\\[\n\\begin{align}\n\\hat y = X(X^\\top X)^{-1} X^\\top y \\ = \\ \\color{blue}{P}\\color{black}y && (\\because \\color{blue}{P := X(X^\\top X)^{-1} X^\\top})\n\\end{align}\n\\]\nOne thing we might be interested in is how well our model \\(Py\\) explains the actual \\(y\\). One way we can do this is the scalar product: the scalar product \\(y^\\top Py\\) describes the shadow the actual \\(y\\) casts on our projected model. However, this value will change based on the scale of our \\(y\\) variable. Thus, we will divide it by \\(y^\\top y\\), which is the “maximum” shadow possible (perfect shadow). This ratio is called \\(R^2\\).\n\\[\nR^2 = \\frac{y^\\top Py}{y^\\top y}\n\\]\nWe can also reason about \\(R^2\\) in a another way. The total amount of variation in \\(y\\) is called the total sum of squares (SST). The part of \\(y\\) we cannot explain is the Sum of Squared Residuals (SSR) that we minimised for OLS esimtation. That implies that the remaining part SST-SSR is the variation in \\(y\\) that our model explains, called the sum of explained squares (SSE). \\(R^2\\) can be though of the ratio of explained variation in \\(y\\) by our model to the total variation in \\(y\\):\n\\[\nR^2 = \\frac{SSE}{SST} = \\frac{SST - SSR}{SST} = 1 - \\frac{SSR}{SST} = 1 - \\frac{\\sum (Y_i - \\hat Y_i)^2}{\\sum(Y_i - \\bar Y)^2}\n\\]\nR-Squared (\\(R^2\\)) measures the proportion of variation in \\(y\\) that is explained by our explanatory variables. R-Squared is always between 0 and 1 (0%-100%). Higher values indicate our model better explains the variation in \\(y\\).\n\n\n\n\n\n\nClassical Least Squares Theory\n\nThe Classical Assumptions\nThe classical linear model has several assumptions:\n1) Linearity in Parameters. This means that the linear model must be able to be written in the form \\(y = X\\beta + \\eps\\). This does not mean the best-fit line must be linear (as we will explore later).\n2) Independent and Identically Distributed (i.i.d.). Essentially, this means any two observations \\(i\\) and \\(j\\) are sampled from the same random variable distribution with the same probabilities.\n3) No Perfect Multicolinearity. This means that no explanatory variables \\(X_{i1}, \\dots, X_{ip}\\) can be written as an exact linear combination of other explanatory variables in the model. This is needed for the OLS formula.\n4) Zero Conditional Mean \\(\\E(\\eps|X) = 0\\). This can be broken down into two parts. First, \\(\\E(\\eps) = 0\\): This is always met since if it is not 0, you can adjust \\(\\beta_0\\) until it is 0. More importantly, the assumption of exogeneity \\(\\E(X^\\top \\eps) = 0\\). This means that all regressors \\(X_{ij}\\), and any combination of regressors, should be uncorrelated with the error term \\(\\eps\\).\n5) Spherical Errors. This is an assumption made on the variance-covariance matrix of the error terms \\(\\eps_i\\):\n\\[\n\\underbrace{\\V(\\eps|X)}_{\\mathrm{cov. \\ matrix}} = \\begin{pmatrix}\n\\V\\eps_1 & cov(\\eps_1, \\eps_2) & cov(\\eps_1, \\eps_3) & \\dots \\\\\ncov(\\eps_2, \\eps_1) & \\V\\eps_2 & cov(\\eps_2, \\eps_3) & \\dots \\\\\ncov(\\eps_3, \\eps_1) & cov(\\eps_3, \\eps_2) & \\V\\eps_3 & \\vdots \\\\\n\\vdots & \\vdots & \\dots & \\ddots\n\\end{pmatrix} = \\sigma^2 I_n = \\begin{pmatrix}\n\\sigma^2 & 0 & \\dots & 0 \\\\\n0 & \\sigma^2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & 0 \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{pmatrix}\n\\tag{2}\\]\nThis assumption can also be broken into two parts. No Autocorrelation is the assumption that all error terms are uncorrelated \\(cov(\\eps_i, \\eps_j) = 0\\), and is directly a result of the earlier i.d.d. assumption. The second part is Homoscedasticity, which says that all the variances of each error term \\(\\V(\\eps_i)\\) is constant at some value \\(\\sigma^2\\), which implies \\(X\\) has no impact on the variance of \\(\\eps_i\\).\nIf homoscedasticity is violated but ‘no autocorrleation’ is still met, we have heteroscedasticity. We will discuss heteroscedasticity later below, and we will not worry too much about autocorrelation here.\n\n\n\n\n\n\nVisualising Homoscedasticity\n\n\n\n\n\nAn easy way to identify homoscedasticity is to look at a residual plot (just the plot of all \\(\\hat\\eps_i\\)):\n\n\n\n\n\nNotice how the homoscedasticity residuals seem to have the same up-down variance, no matter the value of \\(X_i\\).\nThe heteroscedasticity (when homoscedasticity is violated) residuals have a clear pattern - the up-down variance is smaller when \\(X_i\\) is smaller, and the up-down variance is larger when \\(X_i\\) is larger.\nEssentially, if you see a pattern in the residual plot, it is likely heteroscedasticity.\n\n\n\n\n\n\nUnbiasedness of OLS\nOLS is an unbiased estimator of the relationship between any \\(X_{ij}\\) and \\(Y_i\\) under the first 4 classical assumptions: linearity, i.i.d., no perfect multicollinearity, and zero-conditional mean.\nLet us prove OLS is unbiased - i.e. \\(\\E\\hat\\beta = \\beta\\) under the 4 classical assumptions. Let us manipulate our OLS solution:\n\\[\n\\begin{align}\n\\hat\\beta & = (X^\\top X)^{-1} X^\\top y \\\\\n& = (X^\\top X)^{-1} X^\\top(\\color{blue}{X\\beta + \\eps}\\color{black}) && (\\because \\color{blue}{y = X\\beta + \\eps}\\color{black}) \\\\\n& = \\underbrace{(X^\\top X)^{-1} X^\\top X}_{= \\ I}\\beta + (X^\\top X)^{-1}X^\\top \\eps && \\text{(multiply out)} \\\\\n& = \\beta + (X^\\top X)^{-1}X^\\top \\eps\n\\end{align}\n\\tag{3}\\]\nNow, let us take the expectation of \\(\\hat\\beta\\) conditional on \\(X\\). Remember condition 4, \\(\\E(\\eps | X) = 0\\):\n\\[\n\\E(\\hat\\beta | X) = \\beta + (X^\\top X)^{-1} \\underbrace{\\E(\\eps | X)}_{= \\ 0} \\  = \\ \\beta\n\\]\nNow, we can use the law of iterated expectations (LIE) to conclude this proof:\n\\[\n\\begin{align}\n\\E \\hat\\beta & = \\E(\\E(\\hat\\beta|X)) && (\\because \\mathrm{LIE}) \\\\\n& = \\E(\\color{blue}{\\beta}\\color{black}) && (\\because \\color{blue}{\\E(\\hat\\beta | X = \\beta)}\\color{black}) \\\\\n& = \\beta && \\text{(expectation of a constant)}\n\\end{align}\n\\]\nThus, OLS is unbiased under the 4 conditions above. This is extremely desirable, as in causal inference (which is the purpose of this course), we will want unbiased estimators that can accurately find the causal effects of one variable on another. In fact, most of the methods we cover in later chpaters will be about trying to satisfy these conditions.\n\n\n\nVariance of the OLS Estimator\nWe want to find the variance of our estimator’s sampling distribution, \\(\\V(\\hat\\beta | X)\\), under all 5 of the classical assumptions. First, let us start off where we left off in Equation 3:\n\\[\n\\begin{align}\n& \\hat\\beta = \\beta + (X^\\top X)^{-1} X^\\top \\eps \\\\\n& \\V(\\hat\\beta | X) = \\V(\\beta + (X^\\top X)^{-1} X^\\top \\eps)\n\\end{align}\n\\]\n\n\n\n\n\n\nLemma: Property of Variance\n\n\n\n\n\nLemma: If \\(\\eps\\) is an \\(n\\) dimensional vector of random variables, \\(c\\) is an \\(m\\) dimensional vector, and \\(B\\) is an \\(n \\times m\\) dimensional matrix with fixed constants, then the following is true (I will not prove this lemma here, but it is provable):\n\\[\n\\V(c + B\\eps) =  B \\V(\\eps) B^\\top\n\\tag{4}\\]\n\n\n\n\\(\\beta\\) is a vector of fixed constants. \\((X^\\top X)^{-1} X^\\top \\eps\\) can be imagined as a matrix of fixed constants, since we are conditioning the variance on \\(X\\) (so for each \\(X\\), it is fixed). With the Lemma above:\n\\[\n\\begin{align}\n\\V (\\hat\\beta | X) & = (X^\\top X)^{-1}X^\\top \\V(\\eps|X) [(X^\\top X)^{-1}X^\\top]^{-1} && \\text{(lemma)} \\\\\n& = (X^\\top X)^{-1}X^\\top \\V(\\eps|X) \\color{blue}{X(X^\\top X)^{-1}} && \\color{black}(\\because \\color{blue}{[(X^\\top X)^{-1}X^\\top]^{-1} = X(X^\\top X)^{-1}} \\color{black})\n\\end{align}\n\\tag{5}\\]\nNow, assuming spherical errors in Equation 2, we can conclude the derivation.\n\\[\n\\begin{align}\n\\V (\\hat\\beta | X) & = (X^\\top X)^{-1}X^\\top \\color{blue}{\\sigma^2I_n}\\color{black}{X} (X^\\top X)^{-1} && (\\because \\color{blue}{\\V(\\eps|X) = \\sigma^2 I_n}\\color{black}) \\\\\n& = \\color{blue}{\\sigma^2}\\color{black}{\\underbrace{(X^\\top X)^{-1}X^\\top X}_{= \\ I}(X^\\top X)^{-1}} && \\text{(rearrange and simplify)} \\\\\n& = \\sigma^2 (X^\\top X)^{-1}\n\\end{align}\n\\]\nThis is the variance of the sampling distribution of \\(\\hat\\beta\\). We will use this for the Gauss-Markov theorem (below), and also statistical inference later.\n\n\n\nGauss-Markov Theorem\nThe Gauss-Markov Theorem states that if all 5 classical assumptions are met, the OLS estimator is the best linear unbiased estimator (BLUE) - the unbiased linear estimator with the lowest variance. Any linear estimator takes the form \\(\\tilde{\\beta} = Cy\\), including OLS. For any linear estimator \\(\\tilde{\\beta} = Cy\\) to be unbiased, we need to assume \\(\\color{red}{CX = I}\\).\n\n\n\n\n\n\nProof \\(CX = I\\) For a Unbiased Linear Estimator\n\n\n\n\n\nFor any linear estimator \\(\\tilde{\\beta} = Cy\\) to be unbiased, we need to assume \\(\\color{red}{CX = I}\\). The proof of this is as follows:\n\\[\n\\begin{align}\n\\tilde\\beta =  C & (\\color{blue}{C\\beta + \\eps}\\color{black}) && (\\because \\color{blue}{y = X\\beta + \\eps}\\color{black}) \\\\\n=  C & X\\beta + C\\eps && \\text{(multiply out)} \\\\\n\\E(\\tilde\\beta | X) & = \\E(C X\\beta + C\\eps) \\\\\n& = CX\\beta + C \\underbrace{\\E(\\eps | X)}_{= \\ 0} && \\text{(take constants out of exp.)} \\\\\n& = CX\\beta \\\\\n& = \\color{red}{I}\\color{black}\\beta = \\beta && (\\because \\color{red}{CX = I}\\color{black}) \\\\\n\\E \\tilde\\beta & = \\E( \\E(\\tilde\\beta|X)) && \\text{(law of iterated expect.)} \\\\\n& = \\E(\\color{blue}{\\beta}\\color{black}) && (\\because \\color{blue}{\\E(\\tilde\\beta|X) = \\beta}\\color{black}) \\\\\n& = \\beta && \\text{(expect. of a constant)}\n\\end{align}\n\\]\nThus, we have shown \\(\\color{red}{CX = I}\\) is a necessary condition for any linear estimator \\(\\tilde{\\beta} = Cy\\) to be unbiased.\n\n\n\nNow, let us calculate the variance of \\(\\tilde{\\beta}\\), taking into consideration the lemma (Equation 4) used in the OLS variance:\n\\[\n\\begin{align}\n\\V(\\tilde\\beta | X) & = \\V(Cy|X) \\\\\n& = \\V(C(\\color{blue}{X\\beta + \\eps}\\color{black})|X) && (\\because \\color{blue}{y = X\\beta + \\eps}\\color{black}) \\\\\n& = \\V(\\underbrace{CX}_{= I}\\beta + C\\eps | X) && \\text{(multiply out)} \\\\\n& = \\V(\\beta + C\\eps | X) \\\\\n& = C \\V(\\eps | X) C^\\top && \\text{(using lemma)} \\\\\n& = C \\color{blue}{\\sigma^2 I_n} \\color{black} C^\\top && (\\mathrm{homoscedasticity} \\ \\color{blue}{\\V(\\eps|X) = \\sigma^2 I_n}\\color{black}) \\\\\n& = \\sigma^2 CC^\\top && \\text{(rearrange and simplify)}\n\\end{align}\n\\]\nNow, we want to show that the variance of the OLS estimator \\(\\hat{\\beta}\\) (under homoscedasticity) is smaller than any linear estimator \\(\\tilde{\\beta}\\). Let us find the difference between the variances of estimator \\(\\tilde{\\beta}\\) and \\(\\hat{\\beta}\\). Note: since \\(\\color{red}{CX = I}\\), the following is also true: \\(\\color{red}{ X^\\top C^\\top = (CX)^\\top = I}\\).\n\\[\n\\begin{align}\n\\V(\\tilde\\beta | X)  - \\V(\\hat\\beta|X) & = \\sigma^2 CC^\\top - \\sigma^2 (X^\\top X)^{-1} \\\\\n& = \\sigma^2(CC^\\top - (X^\\top X)^{-1}) && (\\text{factor out }\\sigma^2) \\\\\n& = \\sigma^2(CC^\\top - \\color{red}{CX}\\color{black}(X^\\top X)^{-1} \\color{red}{X^\\top C^\\top}\\color{black}) && (\\because \\color{red}{X^\\top C^\\top = CX = I}\\color{black}) \\\\\n& = \\sigma^2 C(I - X(X^\\top X)^{-1} X^\\top) C^\\top && (\\text{factor out }C, C^\\top) \\\\\n& = \\sigma^2 C \\color{blue}{M}\\color{black}C^\\top && (\\text{residual maker matrix } \\color{blue}{M}\\color{black})\n\\end{align}\n\\]\nSince \\(\\sigma^2 CM C^\\top\\) is positive semi-definite (I will not prove this, but it is provable with the properties of \\(M\\) introduced earlier), we know that \\(V(\\tilde{\\beta}| X) &gt; V(\\hat{\\beta}| X)\\). Thus, OLS is BLUE under the Gauss-Markov Theorem.\n\n\n\nAsymptotic Consistency of OLS\nWe know OLS is unbiased under the first 4 classical assumptions: linearity, i.i.d., no perfect multiocllinearity, and zero-conditional mean. OLS is also an asymptotically consistent estimator of \\(\\beta_j\\) under the first 3 classical assumptions, and one weakened version of the zero-conditional mean.\nFor asymptotic consistency, we replace zero-conditional mean with zero-mean and exogeneity: \\(\\E(\\eps_i) = 0\\), and \\(Cov(x_i, \\eps_i) = 0\\), which implies \\(E(X_i \\eps_i) = 0\\). This means that no regressor should be correlated with \\(\\eps\\). This is weaker than Zero-Conditional mean, since it means a function of regressors can be correlated with \\(\\eps_i\\).\n\n\n\n\n\n\nLemma: Vector Notation\n\n\n\n\n\nThe following statements are true (with \\(x_i\\) being a vector and \\(\\eps_i\\) being a scalar):\n\\[\n\\begin{split}\n& X^\\top X = \\sum\\limits_{i=1}^n x_i x_i^\\top\\\\\n&  X^\\top \\mathbf \\eps = \\sum\\limits_{i=1}^n x_i \\eps_i\n\\end{split}\n\\]\n\n\n\nLet us start of where we left of from Equation 3. Using vector notation, law of large numbers, and zero-mean and exogeneity condition:\n\\[\n\\begin{align}\n\\hat\\beta & = \\beta + (X^\\top X)^{-1} X^\\top \\eps \\\\\n& = \\beta \\left( \\sum\\limits_{i=1}^n x_i x_i^\\top \\right)^{-1} \\left( \\sum\\limits_{i=1}^n x_i \\eps_i \\right) && \\text{(vector notation)} \\\\\n& = \\beta + \\left( \\frac{1}{n}\\sum\\limits_{i=1}^n x_i x_i^\\top \\right)^{-1} \\left( \\frac{1}{n} \\sum\\limits_{i=1}^n x_i \\eps_i \\right) && (\\left(\\frac{1}{n}\\right)^{-1}, \\frac{1}{n} \\text{ cancel out})\n\\end{align}\n\\]\n\\[\n\\begin{align}\n\\mathrm{plim}\\hat\\beta & = \\beta + \\left( \\mathrm{plim} \\frac{1}{n}\\sum\\limits_{i=1}^n x_i x_i^\\top \\right)^{-1} \\left( \\mathrm{plim}\\frac{1}{n} \\sum\\limits_{i=1}^n x_i \\eps_i \\right) \\\\\n& = \\beta + (\\E(x_i x_i^\\top))^{-1} \\underbrace{\\E(x_i \\eps_i)}_{= 0} = \\beta && \\text{(law of large numbers)}\n\\end{align}\n\\]\nThus, OLS is asymptotically consistent under the 4 conditions above. Note that it is possible for OLS to be consistent but biased (if we only meet the weakened zero-mean and exogeneity condition, and not the full zero-conditional mean condition). However, asymptotic consistency is still valuable if we have large sample sizes.\n\n\n\n\n\n\nGeneralised Least Squares\n\nWeakening Spherical Errors\nSo far, we have assumed that the classical assumptions are met. However, this is often not the case, especially with the final assumption: spherical errors (homoscedasticity + no autocorrelation).\nHeteroscedasticity is when homoscedasticity is violated - which implies each \\(i\\) (based on its \\(X\\) values) has its own error: \\(\\V(\\eps_i|X) = \\sigma_i^2\\). We still assume no autocorrelation.\n\\[\n\\V(\\eps| X) = \\Omega = \\begin{pmatrix}\n\\sigma^2_1 & 0 & \\dots & 0 \\\\\n0 & \\sigma^2_2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & 0 \\\\\n0 & 0 & \\dots & \\sigma^2_n\n\\end{pmatrix}\n\\]\nWhat are the implications of heteroscedasticity? First, heteroscedasticity does not bias OLS, as the unbiasedness proof only relies on the first 4 classical assumptions. Second, heteroscedasticity means OLS is no longer BLUE - i.e. there exists are more efficient unbiased linear estimator (the generalised least squares estimator).\nThird, and most relevant to us, is that the OLS variance formula (and standard errors) are no longer valid. Instead, we have to use the Huber-White Standard Errors (also called robust standard errors). This is because if we recall from Equation 5, we originally simplified this equation using the homoscedasticity assumption. Instead, for robust standard errors, we start from Equation 5, and plug in our error covariance matrix \\(\\Omega\\) from above:\n\\[\n\\V(\\hat{\\beta}| X)  = (X^\\top X)^{-1} X^\\top \\color{blue}{\\begin{pmatrix}\n\\sigma^2_1 & 0 & \\dots & 0 \\\\\n0 & \\sigma^2_2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & 0 \\\\\n0 & 0 & \\dots & \\sigma^2_n\n\\end{pmatrix}}\\color{black} X ( X^\\top X)^{-1}\n\\]\nThe other assumption of spherical errors, autocorrelation, is a common problem in time-series and data that has spatial elements, and just like heteroscedasticity, this means that OLS is no longer BLUE (the generalised least squares will be BLUE), and that we will need special autocorrelation + heteroscedasticity (HAC) robust standard errors.\n\n\n\nWeighted Least Squares\nLet us say that the heteroscedastic variance of the errors takes the form \\(\\V(\\eps_i|X_i) = \\sigma^2 h(X_i)\\), where \\(\\sigma^2\\) is some unknown constant, and \\(h(X_i)\\) is some known function of the regressors that determines the variance of the error. Using a lemma \\(\\V(a(x) \\eps | x) = a(x)^2 \\V(\\eps|x)\\), we can determine that:\n\\[\n\\V \\left( \\frac{\\eps_i}{\\sqrt{h(X_i)}} | X_i \\right) = \\left( \\frac{1}{\\sqrt{h(X_i)}}\\right)^2 \\V (\\eps_i|X_i ) =\\sigma^2\n\\]\nSo this tells us that if we create a new error term \\(\\eps_i^* = \\eps_i / \\sqrt{h(X_i)}\\), our variance of the new error term \\(\\eps_i^*\\) will be homoscedastic. What this implies is that if we divide all terms of our regression by the heteroscedastic errors \\(\\sigma^2_i = \\sigma^2 h(x_i)\\), we can get a new regression with homoscedasticity met. Let us define \\(\\Omega^{-1/2}\\) as the inverse of the square root of the heteroscedastic covariance matrix of errors.\n\\[\n\\Omega^{-1/2} = \\begin{pmatrix}\n1/\\sigma_1 & 0 & \\dots & 0 \\\\\n0 & 1/\\sigma_2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots  \\\\\n0 & 0 & \\dots & 1/\\sigma_n\n\\end{pmatrix}\n\\] Using this formula, we can write the transformed regression equation as:\n\\[\n\\underbrace{\\Omega^{-1/2}y}_{y^*}  = \\underbrace{\\Omega^{-1/2}X}_{X^*} \\beta+ \\underbrace{\\Omega^{-1/2}\\eps}_{\\eps_i^*}\n\\]\nAnd since this is a homoscedastic regression, we can use OLS and have a BLUE estimator:\n\\[\n\\begin{align}\n\\hat\\beta & = (X^{*\\top} X^*)^{-1}X^{*\\top}y^* && \\text{(OLS estimator solution)}  \\\\\n& = \\left[ (\\Omega^{-1/2}X)^\\top (\\Omega^{-1/2}X)\\right]^{-1} (\\Omega^{-1/2}X)^\\top (\\Omega^{-1/2}y) && (\\text{plug in }X^*, y^*) \\\\\n& = [X^\\top \\Omega^{-1/2}\\Omega^{-1/2}X]^{-1}X^\\top\\Omega^{-1/2}\\Omega^{-1/2}y && \\text{(matrix transposes)}\\\\\n& =(X^\\top \\Omega^{-1} X)^{-1}X^\\top \\Omega^{-1}y && (\\because \\Omega^{-1/2} \\Omega^{-1/2} = \\Omega^{-1})\n\\end{align}\n\\]\nThis weighted least squares estimator is BLUE. This estimator, under the name of the generalised least squares (GLS) estimator, can also be applied to autocorrelated \\(\\Omega\\) and is BLUE in that case. However, in most cases, we do not know the structure of \\(\\Omega\\), so we will need to estimate it by either assuming some \\(h(X_i)\\), or using OLS estimation first to get OLS residuals to build a structure for \\(\\Omega\\). Despite this, weighted least squares is generally more efficient than OLS. However, for autocorrelation feasible GLS tends to be worse than OLS due to difficulties in estimating \\(\\Omega\\).\n\n\n\n\n\n\nStatistical Inference\n\nNormality and Standard Errors\nWe need to know the form of the \\(\\hat\\beta\\) sampling distribution. We have two options here:\n\nIf our sample size is sufficiently large, we can invoke the central limit theorem, which says that the sampling distribution of \\(\\hat\\beta\\) is approximately normal if our sample size \\(n\\) is large enough.\nOr, we can impose a condition of normality of the error terms: \\(\\eps | X \\sim \\mathcal N(0, \\sigma^2 I)\\). This will ensure the sampling distribution of \\(\\hat\\beta\\) is normally distributed. (Note, the mean 0 and variance \\(\\sigma^2 I\\) come from the classical assumptions of zero-conditional mean and spherical errors).\n\nNow, we will need the standard error of the sampling distribution. We know the standard error is the square root of the variance of the sampling distribution, which we derived (assuming the classical assumptions are met) as:\n\\[\n\\V(\\hat\\beta | X) = \\sigma^2 (X^\\top X)^{-1}\n\\]\nWe do not know the value of \\(\\sigma^2\\) as it is a population parameter. So, we estimate it with an unbiased estimator \\(s^2\\):\n\\[\n\\sigma^2 \\approx s^2 = \\frac{\\hat{\\eps}^\\top \\hat{\\eps}}{n-k-1}\n\\]\nWhile this estimator is unbiased, the estimator \\(s^2\\) has variance. The implication of this is that we can no longer use the standard normal distribution for our sampling distribution, and instead, use the t-distribution, which has fatter tails and a lower peak to account for this variance in \\(s^2\\).\nIf we believe heteroscedasticity is violated (which we assume by default), we should use the heteroscedasticity variance of OLS:\n\\[\n\\V(\\hat{\\beta}| X)  = (X^\\top X)^{-1} X^\\top \\Omega\\color{black} X ( X^\\top X)^{-1}\n\\]\nThe \\(\\sigma^2_i\\) in the \\(\\Omega\\) matrix can be estimated with \\(s_i^2 = \\hat\\eps_i^2\\). The robust standard errors can be derived by taking the square root. We typically use robust-standard errors in causal inference by default, unless we can prove homoscedasticity.\n\n\n\nHypothesis Testing\nWith our (robust) standard errors, we can run hypothesis tests on our coefficients to see if we have a relationship between two variables \\(X_{ij}\\) and \\(Y_i\\). Our typical hypotheses are:\n\n\\(H_0 : \\beta_j = 0\\) (i.e. there is no relationship between \\(X_{ij}\\) and \\(Y_i\\)).\n\\(H_1:\\beta_j ≠ 0\\) (i.e. there is a relationship between \\(X_{ij}\\) and \\(Y_i\\)).\n\nFirst, we calculate the t-test statistic, where \\(H_0\\) is the value set in the null hypothesis (typically 0):\n\\[\nt = \\frac{\\hat\\beta_j - H_0}{\\widehat{se}(\\hat\\beta_j)}\n\\]\nNow, we consult a t-distribution of \\(n-k-1\\) degrees of freedom. We find points \\(t\\) and \\(-t\\) on our t-distribution, and highlight the areas under the curve further away from the 0 at these two points. In the figure below, \\(t = 2.228\\):\n\n\n\n\n\nThe area highlighted, divided by the entire area under the curve, is the p-value. The p-value we get is the probability of getting a test statistic equally or more extreme than the one we got, given the null hypothesis is true.\n\nIf \\(p&lt;0.05\\), we believe the probability of a null hypothesis is low enough, such that we reject the null hypothesis (that there is no relationship between \\(X_{ij}\\) and \\(Y_i\\)), and conclude our alternate hypothesis (that there is a relationship between \\(X_{ij}\\) and \\(Y_i\\)).\nIf \\(p &gt; 0.05\\), we cannot reject the null hypothesis, and cannot reject there is no relationship between \\(X_{ij}\\) and \\(Y_i\\).\n\n\n\n\n\n\n\nConfidence Intervals\n\n\n\n\n\nThe 95% confidence intervals of coefficients have the following bounds:\n\\[\n(\\hat\\beta_j - 1.96 \\widehat{se}(\\hat\\beta_j), \\ \\ \\hat\\beta_j + 1.96 \\widehat{se}(\\hat\\beta_j))\n\\]\n\nThe 1.96 is an approximation assuming a normal distribution. The actual confidence intervals (calculated by computers) will use a t-distribution of \\(n-k-1\\), which will result in a slightly different multiplicative factor.\n\nThe confidence interval means that under repeated sampling and estimating \\(\\hat\\beta_j\\), 95% of the confidence intervals that we construct will include the true \\(\\beta_j\\) value in the population.\nIf the confidence interval contains 0, we cannot conclude a relationship between \\(X_{ij}\\) and \\(Y_i\\), as 0 is a plausible value of \\(\\beta_j\\). These results will always match those of the t-test.\n\n\n\n\n\n\nF-Tests\nF-tests are used to test more than one coefficient at a time. For example, you might want to test if two variables or coefficients are jointly significant. The utility of this will become more clear when we talk about categorical explanatory variables and polynomial transformations. Our hypotheses in a F-test will be:\n\n\\(M_0 : Y_i = \\beta_0 + \\sum\\limits_{j=1}^g \\beta_{j} X_{ij} + \\eps_i\\) (the smaller null model with \\(g\\) variables).\n\\(M_a : Y_i = \\beta_0 + \\sum\\limits_{j=1}^g \\beta_{j} X_{ij} + \\sum\\limits_{j=g+1}^p \\beta_{j} X_{ij} + \\eps_i\\) (the bigger model with the original \\(g\\) variables + additional variables up to \\(p\\)).\n\nF-tests compare the R-squared of the two models through the F-statistic:\n\\[\nF = \\frac{(SSR_0 - SSR_a) / (k_a - k_0)}{SSR_a /(n - k_a - 1)}\n\\]\nWe then consult a F-distribution with \\(k_a - k_0\\) and \\(n-k_a - 1\\) degrees of freedom, obtaining a p-value (in the same way as the t-test). The p-value we get is the probability of getting a test statistic equally or more extreme than the one we got, given the null hypothesis is true.\n\nIf \\(p&lt;0.05\\), the we believe the probability of the null hypothesis is low enough, such that we reject the null hypothesis (that \\(M_0\\) is the better model), and conclude our alternate hypothesis (that \\(M_a\\) is a better model). This also means the extra coefficients in \\(M_a\\) are jointly statistically significant.\nIf \\(p &gt; 0.05\\), we cannot reject the null hypothesis, and cannot reject that \\(M_0\\) is a better model. Thus, the extra coefficients in \\(M_a\\) are jointly not statistically significant.\n\n\n\n\n\n\n\nModel Specification Issues\n\nOmitted Variable Bias\nFrom the regression anatomy theorem, we know that \\(\\hat\\beta_j\\) is the relationship of \\(Y_i\\) and the part of \\(X_{ij}\\) that is uncorrelated with all the other explanatory variables. That implies that if we omit a variable that is correlated with both \\(X_{ij}\\) and \\(Y_i\\), that we will get a different (biased) coefficient estimate. This is called omitted variable bias.\nSuppose there is some variable \\(Z_i\\) that we have not included in a “short” regression. The actual, “true” regression of the population, would include this confounder \\(Z_i\\)\n\\[\n\\underbrace{y = X\\beta + \\eps}_{\\text{short regression}}\n\\qquad \\underbrace{y = X\\beta + z\\delta + \\eps}_{\\text{true regression with z} }\n\\]\nThe OLS estimate of the “short regression” excluding confounder \\(Z_i\\) is:\n\\[\n\\begin{align}\n\\hat\\beta & = (X^\\top X)^{-1}X^\\top y \\\\\n& = (X^\\top X)^{-1}X^\\top(\\color{blue}{X\\beta + z\\delta + \\eps}\\color{black}) && (\\because \\color{blue}{y = X\\beta + z\\delta + \\eps}\\color{black}) \\\\\n& = \\underbrace{(X^\\top X)^{-1}X^\\top X}_{= \\ I}\\beta + (X^\\top X)^{-1}X^\\top z\\delta + (X^\\top X)^{-1} X^\\top \\eps && \\text{(multiply out)} \\\\\n& = \\beta + (X^\\top X)^{-1}X^\\top z\\delta + (X^\\top X)^{-1} X^\\top \\eps\n\\end{align}\n\\]\nNow, let us find the expected value of \\(\\hat\\beta\\), which is conditional on \\(X, z\\), and simplify (using zero conditional mean):\n\\[\n\\begin{align}\n\\E(\\hat\\beta | X, z) & = \\beta + (X^\\top X)^{-1}X^\\top z \\delta + (X^\\top X)^{-1} X^\\top \\underbrace{\\E(\\eps | X, z)}_{= 0} \\\\\n& = \\beta + (X^\\top X)^{-1}X^\\top z \\delta\n\\end{align}\n\\]\nNow, what if we had a regression of outcome variable being the confounder \\(z\\), on the explanatory variables \\(X\\), such that \\(z = X\\eta + u\\). Our OLS estimate would have the solution:\n\\[\n\\hat\\eta = (X^\\top X)^{-1} X^\\top z\n\\]\nNow, we can plug \\(\\hat\\eta\\) into our expected value of \\(\\hat\\beta\\). Assume our estimator \\(\\hat{\\eta}\\) is unbiased:\n\\[\n\\begin{align}\n\\E(\\hat\\beta | X, z) & = \\beta + (X^\\top X)^{-1}X^\\top z \\delta\\\\\n& = \\beta + \\color{blue}{\\hat\\eta}\\color{black}\\delta && (\\because \\color{blue}{\\hat\\eta = (X^\\top X)^{-1} X^\\top z }\\color{black}) \\\\\n\\E\\hat\\beta & = \\E(\\E(\\hat\\beta|X, z)) && \\text{(law of iterated expect.)} \\\\\n& = E(\\color{blue}{\\beta + \\hat\\eta \\delta}\\color{black}) && (\\because \\color{blue}{\\E(\\hat\\beta|X, z) = \\beta + \\hat\\eta\\delta} \\color{black}) \\\\\n& = \\beta + \\E\\hat\\eta \\ \\delta && \\text{(take out constants from exp.)} \\\\\n& = \\beta + \\eta\\delta && (\\text{unbiased estimator } \\E\\hat\\eta = \\eta)\n\\end{align}\n\\]\nThus, we can see by not including confounder \\(z\\) in our “short regression”, the estimator is now biased by \\(\\hat\\eta \\delta\\). In the next chapter when we start discussing causality, we will see omitted confounders as a huge issue in our estimation.\n\n\n\nFunctional Form Specification\nThere are several functional form decisions we have to make for our model: Interactions, polynomial transformations, and logarithmic transformations.\nHeterogeneity is when we believe the magnitude of the relationship between \\(X_{i1}\\) and \\(Y_i\\) in the population is affected by another variable \\(X_{i2}\\), called the moderating variable. An interaction between \\(X_{i1}\\) and the moderating variable \\(X_{i2}\\) means they are multiplied in the regression equation:\n\\[\nY_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\underbrace{\\beta_3 X_{i1} X_{i2}}_{\\text{interaction}}\n\\]\n\n\n\n\n\n\nInterpreting Interaction Effects\n\n\n\n\n\nIn an interaction, \\(\\hat\\beta_0\\) is still the expected \\(Y_i\\) when all explanatory variables equal 0. The other coefficient’s interpretations are:\n\n\n\n\n\n\n\n\n\nBinary \\(X_{i2}\\)\nContinuous \\(X_{i2}\\)\n\n\nBinary \\(X_{i1}\\)\nWhen \\(X_{i2} = 0\\), the effect of \\(X_{i1}\\) (going from 0 to 1) on \\(Y_i\\) is \\(\\hat\\beta_1\\).\nWhen \\(X_{i2} = 1\\), the effect of \\(X_{i1}\\) (going from 0 to 1) on \\(Y_i\\) is \\(\\hat\\beta_1 + \\hat\\beta_3\\).\nThe effect of \\(X_{i1}\\) (going from 0 to 1) on \\(Y_i\\) is \\(\\hat\\beta_1 + \\hat\\beta_3 X_{i2}\\).\n\n\nContinuous \\(X_{i1}\\)\nWhen \\(X_{i2} = 0\\), for every increase in one unit of \\(X_{i1}\\), there is an expected \\(\\widehat{\\beta_1}\\) unit change in \\(Y_i\\).\nWhen \\(X_{i2} = 1\\), for every increase in one unit of \\(X_{i1}\\), there is an expected \\(\\hat\\beta_1+ \\hat\\beta_3\\) change in \\(Y_i\\).\nFor every increase of one unit in \\(X_{i1}\\), there is an expected \\(\\hat\\beta_1 + \\hat\\beta_3 X_{i2}\\) change in \\(Y_i\\).\n\n\n\nThe hypothesis test for \\(\\hat\\beta_3\\) tests the null hypothesis that there is no interaction in the population between \\(X_{i1}\\) and \\(X_{i2}\\). If our coefficient \\(\\hat\\beta_3\\) is statistically significant, we can conclude that this null is wrong, and there indeed is an interaction. If \\(\\hat\\beta_3\\) is insignificant, we fail to reject the null, and can drop the interaction effect from our regression.\n\n\n\nSometimes the relationship between two variables in the population is not a straight linear line. The most common form of polynomial transformation is the quadratic transformation:\n\\[\nY_i = \\beta_0 + \\beta_1X_i + \\beta_2 X_i^2 + \\eps_i\n\\]\nNote that while \\(X_i\\) is non-linear, the actual regression is still linear in parameters. We can see this because it can still be written as \\(y = X\\beta + \\eps\\) when you consider \\(X_i\\) and \\(X_i^2\\) to be two different explanatory variables.\n\n\n\n\n\n\nInterpreting Polynomial Transformations\n\n\n\n\n\nOur estimated \\(\\hat\\beta_0\\) remains the expected value of \\(Y_i\\) when all explanatory variables equal 0. Unfortunately, the \\(\\hat\\beta_1\\) and \\(\\hat\\beta_2\\) coefficients are not directly interpretable.\n\n\\(\\hat\\beta_2\\)’s sign can tell us if the best-fit parabola opens upward or downward.\nThe significance of \\(\\hat\\beta_2\\) also indicates if the quadratic term is statistically significant. If it is not, we can remove the transformation.\n\nWe can interpret two things about the quadratic transformation:\n\nFor every one unit increase in \\(X_i\\), there is an expected \\(\\hat\\beta_1 + 2 \\hat\\beta_2X_i\\) unit increase in \\(Y_i\\).\nThe minimum/maximum point in the best-fit parabola occurs at \\(X_i = - \\hat\\beta_1/2 \\hat\\beta_2\\)\n\n\n\n\nLogarithmic transformations are often used to change skewed variables into normally distributed variables. Often, skewed variables will violate homoscedasticity, which means the Gauss-Markov theorem of OLS being BLUE no longer applies. By applying a logarithmic transformation, you can meet homoscedasticity and retain the BLUE properties of OLS. We have 3 types of logarithmic transformations:\n\nLinear-Log model: \\(Y_i = \\beta_0 + \\beta_1 \\log X_i + \\eps_i\\).\nLog-Linear model: \\(\\log Y_i = \\beta_0 + \\beta_1 X_i + \\eps_i\\).\nLog-Log model: \\(\\log Y_i = \\beta_0 + \\beta_1 \\log X_i + \\eps_i\\).\n\n\n\n\n\n\n\nInterpreting Logarithmic Transformations\n\n\n\n\n\nThe coefficient interpretations become a little more complex with logarithms:\n\n\n\n\n\n\n\n\n\n\\(X_i\\)\n\\(\\log (X_i)\\)\n\n\n\\(Y_i\\)\nLinear Model:\nWhen \\(X_i\\) increases by one unit, there is an expected \\(\\hat\\beta_1\\) unit change in \\(Y_i\\).\nLinear-Log Model:\nWhen \\(x\\) increases by 10%, there is an expected \\(0.096 \\hat\\beta_1\\) unit change in \\(Y_i\\).\n\n\n\\(\\log (Y_i)\\)\nLog-Linear Model:\nFor every one unit increase in \\(X_i\\), the expected \\(Y_i\\) is multiplied by \\(e^{\\hat\\beta_1}\\).\nLog-Log Model:\nMultiplying \\(X_i\\) by \\(e\\) will multiply the expected value of \\(Y_i\\) by \\(e^{\\hat\\beta_1}\\).x\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation in R\nYou will need package fixest and estimatr.\n\nlibrary(fixest)\nlibrary(estimatr)\n\nRegression with normal standard errors can be done with the lm() function:\n\nmodel &lt;- lm(y ~ x1 + x2 + x3, data = mydata)\nsummary(model)\n\nRegression with robust standard errors can be done with the feols() function or lm_robust() function:\n\n# feols\nmodel &lt;- feols(y ~ x1 + x2 + x3, data = mydata, se = \"hetero\")\nsummary(model)\n\n# lm robust\nmodel &lt;- lm_robust(y ~ x1 + x2 + x3, data = mydata)\n\nOutput will include coefficients, standard errors, p-values, and more.\n\n\n\n\n\n\nBinary and Categorical Variables\n\n\n\n\n\nYou can include binary and categorical variables by using the as.factor() function:\n\nfeols(y ~ x1 + as.factor(x2) + x3, data = mydata, se = \"hetero\")\n\nYou can do the same for \\(y\\) or \\(x\\). Just remember, \\(y\\) cannot be a categorical variable (use multinomial logsitic regression instead).\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n\n\n\n\nYou can include one-way fixed effects by adding a | after your regression formula in feols():\n\nmodel &lt;- feols(y ~ x1 + x2 + x3 | cluster,\n               data = mydata, se = \"hetero\")\nsummary(model)\n\nYou can add two-way fixed effects as follows:\n\nmodel &lt;- feols(y ~ x1 + x2 + x3 | unit + year,\n               data = mydata, se = \"hetero\")\nsummary(model)\n\n\n\n\n\n\n\n\n\n\nInteraction Effects\n\n\n\n\n\nTwo interact two variables, use * between them. This will automatically include both the interaction term, and the two variables by themselves.\n\nfeols(y ~ x1 + x2*x3, data = mydata, se = \"hetero\")\n\nIf for some reason, you only want the interaction term, but not the variables by themselves, you can use a colon : between the two variables:\n\nfeols(y ~ x1 + x2:x3, data = mydata, se = \"hetero\")\n\n\n\n\n\n\n\n\n\n\nPolynomial Transformations\n\n\n\n\n\nTo conduct a polynomial transformation, you can use the I() function. The second argument is the degree of the polynomial:\n\nfeols(y ~ x1 + I(x2, 3), data = mydata, se = \"hetero\") #cubic for x2\n\n\n\n\n\n\n\n\n\n\nLogarithmic Transformations\n\n\n\n\n\nThe best way to do a logarithmic transformation is to create a new variable that is the log of the variable you want to transform using the log() function, before you even start the regression:\n\nmydata$x1_log &lt;- log(mydata$x1)\n\n\n\n\n\n\n\n\n\n\nConfidence Intervals\n\n\n\n\n\nTo find the confidence intervals for coefficients, first estimate the model with lm() or feols() as shown previously, then use the confint() command:\n\nconfint(model)\n\n\n\n\n\n\n\n\n\n\nF-Tests\n\n\n\n\n\nTo run a f-test, use the anova() command, and input your two different models, with the null model going first.\n\nanova(model1, model2)\n\nNote: F-tests only work with models that are run with homoscedastic standard errors. Robust standard errors will not work.\n\n\n\n\n\n\n\n\n\nLaTeX Regression Tables\n\n\n\n\n\nYou can use the texreg package to make nice regression tables automatically.\n\nlibrary(texreg)\n\nThe syntax for texreg() is as follows:\n\ntexreg(l = list(model1, model2, model3),\n       custom.model.names = c(\"model 1\", \"model 2\", \"model 3\"),\n       custom.coef.names = c(\"intercept\", \"x1\", \"x2\"),\n       digits = 3)\n\nYou can replace texreg() with screenreg() if you want a nicer regression table in the R-console.\nNote: you must have the same amount of model names as total models in your texreg, and you must have the same amount of coeficient names as the total amount of coefficients in all of your models.\n\n\n\n\n\n\n\n\n\nPrediction\n\n\n\n\n\nWe can use the predict() function to generate fitted value predictions in R:\n\nmy_predictions &lt;- predict(model, newdata = my_new_data)\n\nmy_new_data is a dataframe with a bunch of explanatory variable values (for every explanatory variable) for a collection of observations, that you wish to predict \\(\\hat y\\) for.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "1 The Classical Linear Model"
    ]
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "Kevin’s PSPE Resources",
    "section": "Topics",
    "text": "Topics\nThe topics in this collection focus on methodology in political science and political economy. Topics include:\n\nQuantitative Methods for Causal Inference. R-code is provided for implementation.\nOther Materials, including a Guide to Game Theory, and some Supplementary Mathematics.\nFurther Statistical Models (other regressions, multivariate latent models, etc.). R-code is provided for implementation purposes."
  },
  {
    "objectID": "index.html#topics-in-this-collection",
    "href": "index.html#topics-in-this-collection",
    "title": "Kevin’s PSPE Resources",
    "section": "Topics in this Collection",
    "text": "Topics in this Collection\n\nQuantitative Analysis, focusing on designs to recover causal estimates, but also with other methods for descriptive and predictive inference.\nA guide to Game Theory, Background Mathematics, and Background Statistics for PSPE.\n\nThis collection is a work in progress, some pages may not be complete."
  },
  {
    "objectID": "calc.html",
    "href": "calc.html",
    "title": "Calculus Reference",
    "section": "",
    "text": "This chapter is a reference for the essentials of linear algebra for political science (for both statistics/quantitative methods and game theory).\nUse the right sidebar for easy navigation.\n\n\nDifferential Calculus\n\nDefinition of Derivative\nA derivative function \\(f'\\) is one that outputs the slope of the function \\(f\\) at any input. So if we wanted to know the slope of function \\(f\\) at \\(f(a)\\), we would evaluate \\(f'(a)\\).\nWe know the definition of slope between two points is:\n\\[\n\\frac{\\Delta y}{\\Delta x} = \\frac{y_2 -y_1}{x_2 - x_1} = \\frac{f(x_2) - f(x_1)}{x_2 - x_1}\n\\]\nWe can rewrite \\(x_1, x_2\\) relative to a single point of \\(x\\), as \\(x_1 = x\\), and \\(x_2 = x + h\\), where \\(h\\) is the distance between \\(x_1\\) and \\(x_2\\). Thus, we get:\n\\[\n\\frac{\\Delta x}{\\Delta y} = \\frac{f(x + h) - f(x)}{x - (x +h)} = \\frac{f(x+h) - f(x)}{h}\n\\]\nHowever, we do not want to calculate the slope between two points. The derivative function \\(f'\\) is the slope at one point. We can mimic this by finding the value of the slope as \\(h\\) approaches 0.\n\\[\nf'(x) = \\lim\\limits_{h \\rightarrow 0}\\frac{f(x+h) - f(x)}{h}\n\\]\nThis is the definition of a derivative. We can solve any derivative with this definition by plugging in a function into the definition and solving.\nWe can also take the derivative of a derivative (second-order derivative), notated \\(f''(x)\\).\n\n\n\nDifferentiation Rules\nWe can solve any derivative with the definition above. However, that is tedious. So, we have a series of rules to help us. You can check these rules by plugging them into the definition above.\n\\[\n\\begin{array}{c|c|c}\n\\text{Function} & \\text{Derivative} & \\text{Notes} \\\\ \\hline\nc & 0 & \\text{c is a constant} \\\\ \\hline\ncx & c & \\text{c is a constant} \\\\ \\hline\nx^k & kx^{k-1} & x^k \\text{ is a monomial (power rule)} \\\\ \\hline\nf(x)±g(x) & f'(x) ± g'(x) \\ & \\text{(sum rule)} \\\\ \\hline\nf(x)g(x) & f'(x)g(x) + g'(x)f(x) & \\text{(product rule)}\\\\ \\hline\n\\frac{f(x)}{g(x)} & \\frac{f'(x)g(x) + g'(x)f(x)}{(g(x))^2} & \\text{(quotient rule)} \\\\ \\hline\nf[g(x)] & f'[g(x)] \\cdot g'(x) & \\text{(chain rule)} \\\\ \\hline\ne^x & e^x \\\\ \\hline\ne^{u(x)} & e^{u(x)} \\cdot u'(x) \\\\ \\hline\nb^x & \\ln (b) \\cdot b^x & \\text{b is some constant base} \\\\ \\hline\n\\ln(x) & \\frac{1}{x} \\\\ \\hline\n\\ln(x^k) & \\frac{k}{x} \\\\ \\hline\n\\ln(u(x)) & \\frac{u'(x)}{u(x)} \\\\ \\hline\n\\sin(x) & \\cos(x) \\\\ \\hline\n\\cos(x) & -\\sin(x)\n\\end{array}\n\\]\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\n\n\n\n\n\n\nPartial Derivatives\n\n\nGradient\n\n\nTotal Differentiation\n\n\nJacobian Matrix\n\n\nHessian Matrix\n\n\n\n\nOptimisation\n\nMinima and Maxima\n\n\nConcavity\n\n\nSingle Variable Optimisation\n\n\nOptimisation with Constraints\n\n\n\n\n\nIntegral Calculus\n\nIndefinite Integrals\n\n\nIntegration Rules\n\n\nRiemann Sums\n\n\nFundamental Theorem of Calculus\n\n\nIntegration by Substitution\n\n\nIntegration by Parts\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Calculus Reference"
    ]
  },
  {
    "objectID": "math.html",
    "href": "math.html",
    "title": "Background Mathematics",
    "section": "",
    "text": "This chapter is a reference for the essentials of Mathematics for Political Science, mainly consisting of linear algebra, calculus, and basic probability. This reference is not meant to teach these concepts. If you want a good math course for political scientists/social scientists, see David Siegel’s youtube channel (scroll through the playlists).\nUse the right sidebar for easy navigation.\n\n\nVectors and Matrices\n\nVector Algebra\nA scalar is any single element or component, like a real number (ex. \\(x_1 \\in \\mathbb{R}\\)). A vector is a collection of scalars: \\((x_1 \\ x_2 \\ x_3 \\ x_4)\\). Each scalar is considered a element/component.\n\n\n\n\n\n\nVector Geometry\n\n\n\n\n\nVectors can be visualised graphically. Each element corresponds to a distance in a direction. For example, take the vector \\((3 \\ 2)\\). Graphically:\n\n\n\n\n\nThe dimension of a vector is the number of components/scalars/elements in the vector. If a vector has 3 dimensions, the graphical representation will be in 3 dimensions, and so on.\nThe norm of a vector is its length (when thinking geometrically):\n\\[\n|| \\mathbf{x}|| = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2}\n\\]\nNormalizing a vector is scalar multiplying (see below) the vector by \\(1/||\\mathbf{x} ||\\). This results in the norm of the vector equaling 1. This can be useful if you want to standardise and compare vectors.\n\n\n\nVector addition and subtraction is done just by adding the respective elements to each other:\n\\[\n\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} + \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 1+3 \\\\ 2+4 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 6 \\end{pmatrix}\n\\]\nVector scalar multiplication is done by multiplying all elements of the vector by the scalar:\n\\[\n4 \\times \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 4 \\times 1 \\\\ 4 \\times 2 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 8 \\end{pmatrix}\n\\]\nScalar product, also known as dot product, takes two vectors and creates a scalar.\n\\[\n\\mathbf a \\cdot \\mathbf b = \\sum_i a_ib_i = a_1 b_1 + a_2 b_2+ \\dots a_n b_n\n\\]\n\nEssentially, multiply each respective element with each other. Then sum all of the products.\n\n\n\n\n\n\n\nExample of Scalar Product\n\n\n\n\n\nLet us do an example of scalar product:\n\\[\n\\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix} \\cdot \\begin{pmatrix} 4 \\\\ 5 \\end{pmatrix}\n\\]\nWe multiply each respective element with each other, then sum all of the products:\n\\[\n2 \\times 4 + 3 \\times 5 = 8 + 15 = 23\n\\]\n\n\n\n\n\n\n\n\n\nThe Geometry of Vector Algebra\n\n\n\n\n\nVector addition can be viewed graphically. Take two 2-dimensional vectors \\(\\mathbf A\\) and \\(\\mathbf B\\):\n\n\n\n\n\nVector Scalar Multiplication can also be visualised graphically: it just multiplies the length of the vector by the scalar (and if the scalar is negative, the direction switches 180 degrees).\nDot product calculates the projection/shadow of vector \\(\\mathbf a\\) on vector \\(\\mathbf b\\). In the figure below, the dot product calcualtes the length of the blue-highlighted line segment:\n\n\n\n\n\nFrom here, we can tell if the two vectors are perpendicular, then the dot product would be 0. This is useful for measures of similarity/correlation.\n\n\n\n\n\n\nTypes of Matrices\nA matrix is a collection of scalars, that are put in a \\(n \\times m\\) order with \\(n\\) number of rows, and \\(m\\) number of columns. Each element by the matrix can be denoted \\(a_{ij}\\), which is the element in the \\(i\\)th row and \\(j\\)th column:\n\\[\n\\mathbf A_{2 \\times 3} = \\begin{pmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23}\n\\end{pmatrix}\n\\]\nThere are several very common types of matrices that you need to know.\n\n\n\n\n\n\nSquare and Zero Matrix\n\n\n\n\n\nSquare Matrix is a matrix that have an equal number of rows and columns.\n\\[\n\\mathbf A_{2 \\times 2} = \\begin{pmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{pmatrix}\n\\]\nThese are useful because many matrix manipulations, like inversions and determinants.\nZero Matrix is a square matrix with all 0’s.\n\n\n\n\n\n\n\n\n\nDiagonal, Identity, and Lower/Upper Triangular Matrix\n\n\n\n\n\nDiagonal matrices only have elements along the top-left bottom-right diagonal.\n\\[\n\\mathbf A_{3 \\times 3} = \\begin{pmatrix}\na_{11} & 0 & 0 \\\\\n0 & a_{22} & 0 \\\\\n0 & 0 & a_{33} \\\\\n\\end{pmatrix}\n\\]\nAn identity matrix (notated \\(\\mathbf I\\))is a diagonal matrix, but all the diagonal elements equal 1:\n\\[\n\\mathbf I_{2 \\times 2} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\\]\n\nAny matrix times \\(\\mathbf I\\) equals itself (like a 1 in normal multiplication).\n\nA Lower/Upper Triangular Matrix is a matrix where only has values above/below the diagonal. For example, the following is a lower triangular matrix:\n\\[\n\\mathbf A_{3 \\times 3} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n3 & 4 & 0 \\\\\n3 & 3 & 4\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nSubmatrix\n\n\n\n\n\nA submatrix is a matrix if you were to remove a row and a column (that is specified by an element).\nFor example, take this 3 by 3 matrix:\n\\[\n\\mathbf A_{3 \\times 3} = \\begin{pmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{pmatrix}\n\\]\nLet us find the submatrix of \\(a_{21}\\). This means we will eliminate the 2nd row, and 1st column:\n\\[\n\\mathbf A_{2 \\times 2} = \\begin{pmatrix}\na_{12} & a_{13} \\\\\na_{32} & a_{33}\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nPermutation Matrix\n\n\n\n\n\nA permutation matrix is a matrix that only has one non-zero element in each row and column.\n\\[\n\\mathbf A_{3 \\times 3} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{pmatrix}\n\\]\nThe identity matrix is a permutation matrix.\n\n\n\n\n\n\n\n\n\nSingular/Non-Singular Matrix\n\n\n\n\n\nA singular matrix is one who’s determinant is zero. These cannot be inverted.\nA non-singular matrix is one who’s determinant is not zero. These can be inverted. For non-singular matrices:\n\\[\nAA^{-1} = I\n\\]\n\n\n\n\n\n\n\n\n\nBlock/Partitioned/Block Diagonal Matrix\n\n\n\n\n\nA block or partitioned matrix is a matrix which contains matrices within.\n\\[\n\\mathbf A_{4 \\times 4} = \\begin{pmatrix}\n\\mathbf A_{2 \\times 2} & \\mathbf B_{2 \\times 2} \\\\\n\\mathbf C_{2 \\times 2} & \\mathbf D_{2 \\times 2}\n\\end{pmatrix}\n\\]\n\nNote how the block matrix is 4 by 4, since if we expand out each matrix within, we would get a 4 by 4 matrix.\n\nA block diagonal matrix is a block/partitioned matrix with only matrices on its diagonal:\n\\[\n\\mathbf A_{4 \\times 4} = \\begin{pmatrix}\n\\mathbf A_{2 \\times 2} & 0 \\\\\n0 & \\mathbf D_{2 \\times 2}\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nOrthogonal/Orthonormal Matrix\n\n\n\n\n\nAn orthogonal matrix is one with columns perpendicular to each other (when treating each column as a vector). In other words, the dot product of any two columns is zero.\n\\[\n\\mathbf A_{3 \\times 3} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 5 \\\\\n0 & 3 & 0\n\\end{pmatrix}\n\\]\n\nThe identity matrix is also orthogonal.\nAny matrix with one element in each row and column will be orthogonal.\n\nAn orthonormal matrix is an orthogonal matrix but the lengths/norms of all the columns is 1:\n\\[\n\\mathbf A_{3 \\times 3} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nSymmetrical and Idempotent Matrix\n\n\n\n\n\nA symmetrical matrix is one where the transpose is equivalent to itself:\n\\[\n\\mathbf A^\\mathsf{T} = \\mathbf A\n\\]\nAn Idempotent matrix is one where when multiplied to itself, it produces itself:\n\\[\n\\mathbf{AA} = \\mathbf A\n\\]\n\n\n\n\n\n\nMatrix Transpose and Trace\nThe matrix transpose is a matrix flipped along its diagonal. It is denoted either \\(\\mathbf A^\\mathsf{T}\\) or \\(\\mathbf A '\\). In other words, the rows and column locations of each element are inverted (essentially elements \\(a^\\mathsf{T}_{ij} = a_{ji}\\)):\n\\[\n\\begin{pmatrix}\n2 & 3 & 5 \\\\\n1 & 4 & 6\n\\end{pmatrix}^\\mathsf{T} = \\begin{pmatrix}\n2 & 1 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{pmatrix}\n\\]\n\nNotice how the first column became the first row, the second column became the second row.\nYou can also get the transpose of a vector.\n\n\n\n\n\n\n\nProperties of Transposes\n\n\n\n\n\nThe Vector Property says that the dot product of vectors can be written with transposes:\n\\[\n\\mathbf a \\cdot \\mathbf b = \\mathbf a^\\mathsf{T}\\mathbf b\n\\]\nThe Inverse Property says that the transpose of a transpose is the original matrix:\n\\[\n\\left(\\mathbf A^\\mathsf{T} \\right)^\\mathsf{T} = \\mathbf A\n\\]\nThe Addition Property states that the transpose of a sum of two matrices, is equal to the individual transposes of both matrices added together:\n\\[\n(\\mathbf A + \\mathbf B)^\\mathsf{T} = \\mathbf A^\\mathsf{T} + \\mathbf B^\\mathsf{T} = \\mathbf B^\\mathsf{T} + \\mathbf A^\\mathsf{T}\n\\]\nThe Multiplication Property says the following (note the order of multiplication):\n\\[\n( \\mathbf{AB})^\\mathsf{T} = \\mathbf B^\\mathsf{T} \\mathbf A^\\mathsf{T}\n\\]\nThe Symmetrical Property says that a matrix that is symmetrical does not change when inversed:\n\\[\n\\mathbf A^\\mathsf{T} = \\mathbf A \\quad \\text{s.t.} \\quad \\mathbf A \\text{ is symmetrical}\n\\]\nThe Inverse Transpose Property says the following about inverses and transposes:\n\\[\n\\left( \\mathbf A^{-1} \\right)^\\mathsf{T} = \\left( \\mathbf A^\\mathsf{T} \\right)^{-1}\n\\]\n\n\n\n\n\n\n\n\n\nSymmetrical Matrix\n\n\n\n\n\nA symmetrical matrix is one where the transpose is equivalent to itself:\n\\[\n\\mathbf A^\\mathsf{T} = \\mathbf A\n\\]\n\n\n\nThe trace of \\(\\mathbf A\\) is a sum of all diagonal elements. Traces are used in Eigenvalues.\n\\[\nTr(\\mathbf A) = \\sum_i a_{ii} = a_{11} + a_{22} + \\dots\n\\]\n\n\n\n\n\n\nProperty of Traces\n\n\n\n\n\nThe Addition Property of traces states that the trace of the sum of two matrices is equivalent to the sum of the traces of each individual matrix:\n\\[\nTr(\\mathbf A + \\mathbf B) =Tr(\\mathbf A) + Tr(\\mathbf B) = Tr(\\mathbf B) + Tr(\\mathbf A)\n\\]\nThe Transpose Property says the trace of the transpose is equal to the trace of the original (since the diagonal remains the same):\n\\[\nTr(\\mathbf A^\\mathsf{T}) = Tr(\\mathbf A)\n\\]\nThe Multiplication Property says that the trace of multiplication has the commutative property (only for two matrices):\n\\[\nTr(\\mathbf{AB}) = Tr(\\mathbf{BA})\n\\]\n\n\n\n\n\n\nMatrix Algebra\nMatrix addition/subtraction is the same as vector addition - add the respective elements together:\n\\[\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{pmatrix} + \\begin{pmatrix}\n5 & 6 \\\\\n7 & 8\n\\end{pmatrix} = \\begin{pmatrix}\n1 + 5 & 2 +6 \\\\\n3+7 & 4 + 8\n\\end{pmatrix} = \\begin{pmatrix}\n6 & 8 \\\\\n10 & 12\n\\end{pmatrix}\n\\]\nMatrix scalar multiplication is the same as vector scalar multiplication - multiply each element by the scalar:\n\\[\n3 \\times \\begin{pmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{pmatrix} = \\begin{pmatrix}\n3 \\times 1 & 3 \\times 2 \\\\\n3 \\times 3 & 3 \\times 4\n\\end{pmatrix} = \\begin{pmatrix}\n3 & 6 \\\\\n9 & 12\n\\end{pmatrix}\n\\]\nMatrix Plain Multiplication is a little more complicated. Let us say you want to multiply \\(\\mathbf A\\) and \\(\\mathbf B\\) to get a new matrix \\(\\mathbf C\\). The elements of \\(\\mathbf C\\) are calculated as follows:\n\\[\nc_{ij} = \\sum_ka_{ik}b_{kj} = a_{i1}b_{1j} + a_{i2}b_{2j} + a_{i3}b_{3j}\\dots\n\\]\nIn other words, \\(c_{ij}\\) is the dot product of the \\(i\\)th row of \\(\\mathbf A\\), and the \\(j\\)th column of \\(\\mathbf B\\).\nMatrix multiplication is only possible when the number of columns in \\(\\mathbf A\\) is equal to the number of rows in \\(\\mathbf B\\). So for example, we can multiply \\(\\mathbf A_{2 \\times 3}\\) and \\(\\mathbf B_{3 \\times 4}\\). We cannot multiply \\(\\mathbf A_{2 \\times 3}\\) and \\(\\mathbf B_{2 \\times 3}\\).\nThe dimensions of product \\(\\mathbf C\\) is the number of rows in \\(\\mathbf A\\) and the number of columns in \\(\\mathbf B\\). So for example, if we multiply \\(\\mathbf A_{2 \\times 3}\\) and \\(\\mathbf B_{3 \\times 4}\\), we will get \\(\\mathbf C_{2 \\times 4}\\).\n\n\n\n\n\n\nExample of Matrix Multiplication\n\n\n\n\n\nLet us solve the following problem:\n\\[\n\\begin{pmatrix}\n2 & 1 \\\\\n3 & 5 \\end{pmatrix} \\begin{pmatrix}\n6 & 1 \\\\\n2 & 3 \\end{pmatrix} = \\mathbf C\n\\]\nLet us do each dot product for each element of \\(\\mathbf C\\):\n\n\\(c_{11}\\) is the dot product of the 1st row of the 1st matrix, and the 1st column of the 2nd matrix: \\((2 \\ 1) \\cdot (6 \\ 2)\\). That means \\(c_{11} = 2 \\times 6 + 1 \\times 2 = 12+2 = 14\\).\n\\(c_{12}\\) is the dot product of the 1st row of the 1st matrix, and the 2nd column of the 2nd matrix: \\((2 \\ 1) \\cdot (1 \\ 3)\\). That means \\(c_{12} = 2 \\times 1 + 1 \\times 3 = 2 + 3 = 5\\).\n\\(c_{21}\\) is the dot product of the 2nd row of the 1st matrix, and the 1st column of the 2nd matrix: \\((3 \\ 5) \\cdot (6 \\ 2)\\). That means \\(c_{21} = 3 \\times 6 + 5 \\times 2 = 18 + 10 = 28\\).\n\\(c_{22}\\) is the dot product of the 2nd row of the 1st matrix, and the 2nd column of the 2nd matrix: \\((3 \\ 5) \\cdot (1 \\ 3)\\). That means \\(c_{22} = 3 \\times 1 + 5 \\times 3 = 3 + 15 = 18\\).\n\nThus, we now have our answer:\n\\[\n\\begin{pmatrix}\n2 & 1 \\\\\n3 & 5 \\end{pmatrix} \\begin{pmatrix}\n6 & 1 \\\\\n2 & 3 \\end{pmatrix} = \\begin{pmatrix}\n14 & 5 \\\\\n28 & 18 \\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nProperties of Matrix Algebra\n\n\n\n\n\nThe Associative property applies to addition/subtraction and multiplication:\n\\[\n\\begin{split}\n& (\\mathbf A + \\mathbf B) + \\mathbf C = \\mathbf A + (\\mathbf B + \\mathbf C) \\\\\n& (\\mathbf A \\mathbf B)\\mathbf C = \\mathbf A(\\mathbf B \\mathbf C)\n\\end{split}\n\\]\nThe Distributive Property states the following is true:\n\\[\n(\\mathbf A + \\mathbf B) \\mathbf C = \\mathbf A \\mathbf C + \\mathbf B \\mathbf C\n\\]\nThe Commutative Property applies only to addition/subtraction, not multiplication. Commutative property also applies to dot products.\n\\[\n\\begin{split}\n& \\mathbf A + \\mathbf B = \\mathbf B + \\mathbf A \\\\\n& \\mathbf a \\cdot \\mathbf b = \\mathbf b \\cdot \\mathbf a\n\\end{split}\n\\]\nMatrix Multiplication does not have the commutative property: \\(\\mathbf A \\mathbf B ≠ \\mathbf B \\mathbf A\\). Although there are two exceptions: \\(\\mathbf A \\mathbf I = \\mathbf I \\mathbf A\\), and \\(\\mathbf A \\mathbf A^{-1} = \\mathbf A^{-1}  \\mathbf A\\).\n\n\n\n\n\n\n\n\n\nKronecker Product\n\n\n\n\n\nTake the Kronecker Product of \\(\\mathbf A\\) and \\(\\mathbf B\\):\n\\[\n\\mathbf A \\otimes \\mathbf B = \\mathbf C\n\\]\nLet us define \\(\\mathbf A\\) and \\(\\mathbf B\\) as the following:\n\\[\n\\mathbf A_{2 \\times 2} = \\begin{pmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{pmatrix}, \\ \\mathbf B_{2 \\times 2} = \\begin{pmatrix}\nb_{11} & b_{12} \\\\\nb_{21} & b_{22}\n\\end{pmatrix}\n\\]\nThe resulting Kronecker Product \\(\\mathbf C\\) would be defined as a block matrix:\n\\[\n\\mathbf C_{4 \\times 4} = \\begin{pmatrix}\na_{11}\\mathbf B & a_{12} \\mathbf B \\\\\na_{21} \\mathbf B & a_{22} \\mathbf B\n\\end{pmatrix}\n\\]\nEssentially, we treat \\(\\mathbf A\\) as a collection of scalars. We scalar multiply each scalar element of \\(\\mathbf A\\) by the matrix of \\(\\mathbf B\\).\nIf \\(\\mathbf A\\) has dimensions \\(n \\times m\\), and \\(\\mathbf B\\) has dimensions \\(p \\times q\\), then \\(\\mathbf C\\) will have dimensions \\(np \\times mq\\).\n\n\n\n\n\n\nDeterminants and Laplace Expansion\nDeterminants tell us if a matrix is singular (and thus has no inverse). If the determinant is 0, then the matrix is singular. The determinant is only computable for square matrices. For a 2 by 2 matrix:\n\\[\n|\\mathbf A_{2 \\times 2}| = \\left| \\begin{pmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{pmatrix} \\right | = a_{11} a_{22} - a_{12}a_{21}\n\\]\nFor 3 by 3, there is a method called the butterfly method to find the determinant.\n\\[\n\\begin{split}\n| \\mathbf A| = & a_{11}a_{22}a_{33} + a_{12} a_{23} a_{31} + a_{13}a_{21}a_{32} \\\\\n& \\qquad -a_{31}a_{22}a_{13} - a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33}\n\\end{split}\n\\]\nFor anything larger than a 3 by 3 matrix, we should us a Laplace expansion to find the determinant. First, you choose a row or column of the matrix.\n\nFor every element in that row or column, find the submatrix of that element.\nCalculate the determinant of each of the submatrices. This is called the minor.\nNow, convert the minors to cofactors. The cofactor is the minor times \\((-1)^{i+j}\\).\nThen, take each element, multiply by its cofactor. Sum all of these products together.\n\nThe final sum is the determinant of the matrix.\n\n\n\n\n\n\nExample of Laplace Expansion\n\n\n\n\n\nFor example, take this matrix:\n\\[\n\\mathbf A = \\begin{pmatrix}\n1 & 2 & 1 \\\\\n0 & 1 & 1 \\\\\n5 & 3 & 0\n\\end{pmatrix}\n\\]\nLet us expand over the 1st row \\((1 \\ 2 \\ 1 )\\). We expand over the submatrices of each element in that row.\n\nFor \\(a_{11} = 1\\), the submatrix is \\(\\begin{pmatrix} 1 & 1 \\\\ 3 & 0 \\end{pmatrix}\\), and the determinant/minor of that is \\(1 \\times 0 - 1 \\times 3 = -3\\).\nFor \\(a_{12} = 2\\), the submatrix is \\(\\begin{pmatrix} 0 & 1 \\\\ 5 & 0 \\end{pmatrix}\\). The determinant/minor of that is \\(0 - 5 = -5\\).\nFor \\(a_{13} = 1\\), the submatrix is \\(\\begin{pmatrix} 0 & 1 \\\\ 5 & 3 \\end{pmatrix}\\). The determinant/minor of that is \\(0 - 5 = -5\\).\n\nNow, let us find the cofactors \\((-1)^{i + j} \\times \\text{minor}\\):\n\nFor \\(a_{11}\\), the cofactor is \\((-1)^2 \\times -3 = 1 \\times -3 = -3\\).\nFor \\(a_{12}\\), the cofactor is \\((-1)^3 \\times -5 = -1 \\times -5 = 5\\).\nFor \\(a_{13}\\), the cofactor is \\((-1)^4 \\times -5 = 1 \\times -5 = -5\\).\n\nNow, take each element, multiply by its cofactor. Sum all of these products together.\n\\[\n1(-3) + 2(5) + 1(-5) = -3 + 10 - 5 = 2\n\\]\nThus, the determinant of the matrix is \\(2\\).\n\n\n\nThis works for any matrix of any size, for any row or any column. So, you should choose rows/columns with more 0’s, since these will cancel out more terms.\n\n\n\n\n\n\nProperties of Determinants\n\n\n\n\n\nThe Transpose Property states that the determinant of a transpose is equal to the determinant of the original:\n\\[\n\\det(\\mathbf A^\\mathsf{T}) = \\det(\\mathbf A)\n\\]\nThe Identity Property states that the determinant of an identity matrix is 1:\n\\[\n\\det (\\mathbf I)=1\n\\]\nThe Multiplication Property states that the determinant of a product is equal to the individual determinants multiplied:\n\\[\n\\det (\\mathbf {AB}) = \\det (\\mathbf A) \\det (\\mathbf B)\n\\]\nThe Inverse Property says that the determinant of an inverse is the inverse of the determinant of the original matrix:\n\\[\n\\det(\\mathbf A^{-1}) = \\frac{1}{\\det(\\mathbf A)}\n\\]\nThe Triangular/Diagonal Property is the product of all diagonal elements:\n\\[\n\\det (\\mathbf A) = \\prod_i a_{ii}\n\\]\n\n\n\n\n\n\nMatrix Inverse\nIf you take a matrix \\(\\mathbf A\\), and multiply by the inverse \\(\\mathbf A^{-1}\\), the result will be the identity matrix \\(\\mathbf{I}\\).\nYou can invert any square matrix that does not have a determinant of a 0. This is because the inverse is defined as the following:\n\\[\n\\mathbf A^{-1} = \\frac{1}{|\\mathbf A|} \\mathbf C^\\mathsf{T}\n\\]\n\nWhere \\(| \\mathbf A|\\) is the determinant of the matrix \\(\\mathbf A\\), and \\(\\mathbf C^\\mathsf{T}\\) is the transpose of the cofactor matrix (consisting of the cofactor of every element of \\(\\mathbf A\\).\n\n\n\n\n\n\n\nExample of a 2 by 2 Matrix Inverse\n\n\n\n\n\nLet us solve for the matrix inverse of a 2 by 2 matrix.\n\\[\n\\mathbf A_{2 \\times 2} = \\begin{pmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{pmatrix}\n\\]\nWe know the determinant of \\(\\mathbf A\\) with the formula for 2 by 2 matrix determinants:\n\\[\n| \\mathbf A | = a_{11}a_{22} - a_{12} a_{21}\n\\]\nNow, let us find the cofactors (note, the determinant of a scalar is just the scalar):\n\n\\(c_{11} = (-1)^{1+1}a_{22} = a_{22}\\)\n\\(c_{12} = (-1)^{1+2}a_{21} = -a_{21}\\)\n\\(c_{21} = (-1)^{2+1} a_{12} = -a_{12}\\)\n\\(c_{22} = (-1)^{2+2} a_{11} = a_{11}\\)\n\nThus, our cofactor matrix is:\n\\[\n\\mathbf C_{2 \\times 2} = \\begin{pmatrix}\na_{22} & -a_{21} \\\\\n-a_{12} & a_{11}\n\\end{pmatrix}\n\\]\nThe transpose of the cofactor matrix is thus (flipping rows to columns):\n\\[\n\\mathbf C^\\mathsf{T} = \\begin{pmatrix}\na_{22} & -a_{12} \\\\\n-a_{21} & a_{11}\n\\end{pmatrix}\n\\]\nThus, the inverse is:\n\\[\n\\mathbf A^{-1} = \\frac{1}{|\\mathbf A|} \\mathbf C^\\mathsf{T} = \\frac{1}{a_{11}a_{22} - a_{12} a_{21}} \\begin{pmatrix}\na_{22} & -a_{12} \\\\\n-a_{21} & a_{11}\n\\end{pmatrix}\n\\]\nThus, for example, the following is true:\n\\[\n\\mathbf A = \\begin{pmatrix}\n3 & 1 \\\\\n5 & 2\n\\end{pmatrix}, \\ \\mathbf A^{-1}  = \\begin{pmatrix}\n2 & -1 \\\\\n-5 & 3\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nProperties of Inverses\n\n\n\n\n\nThe Inverse Property states that the inverse of an inverse is the original matrix:\n\\[\n\\left( \\mathbf A^{-1} \\right)^{-1} = \\mathbf A\n\\]\nThe Multiplication Property states the inverse of a product is the following (note the order of the multiplication):\n\\[\n(\\mathbf{AB})^{-1}=\\mathbf B^{-1} \\mathbf A^{-1}\n\\]\nThe Scalar Multiplication Property states that the scalar product is the following:\n\\[\n(c \\mathbf A)^{-1} = \\frac{1}{c} \\mathbf A^{-1}, \\quad \\text{s.t.} \\quad  c ≠ 0\n\\]\n\n\n\n\n\n\n\n\n\nLinear Algebra\n\nLinear Mappings and Combinations\nA mapping is any rule that maps elements from one set to another. A function \\(f\\) is a mapping \\(f: A \\rightarrow B\\). A linear mapping is a mapping that is linear, which must meet the following properties:\n\n\\(f(a+b) = f(a) + f(b)\\)\n\\(f(ca) = c f(a)\\)\n\nWe can represent linear mappings for finite sets by matrices. Let us say \\(\\mathbf X_{n \\times m}\\) is a matrix, and \\(\\mathbf y_{m}\\) is a vector. Below is a linear mapping of \\(\\mathbf y_m \\rightarrow \\mathbf z_m\\).\n\\[\n\\mathbf {Xy}_m = \\mathbf z_m\n\\]\nA linear combination is a combination of vectors that is linear (i.e. vectors can be added, and scalar multiplied). For example, this is a linear combination:\n\\[\nt \\mathbf x + (1-t) \\mathbf y\n\\]\nLinear combinations either represents lines (in \\(\\mathbb R^2\\)), planes (in \\(\\mathbb R^3\\)), and hyperplanes (a plane of one less dimensions than the space) in higher dimensions. Now, let us take some linear combination:\n\\[\na_1 \\mathbf x_1 + a_2 \\mathbf x_2+\\dots + a_n \\mathbf x_n\n\\]\nThis set of vectors \\((\\mathbf x_1, \\dots, \\mathbf x_n)\\) is linearly independent if you cannot go from one vector \\(\\mathbf x_j \\in \\{ \\mathbf x_1, \\dots, \\mathbf x_n \\}\\), and linearly transform it (by adding/subtracting/multiplying a constant) into another vector.\n\n\n\n\n\n\nExample of Linear Independence\n\n\n\n\n\nLet us say we have these two vectors:\n\\[\n\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n\\]\nAre these linearly independent? That means I cannot use a linear transformation to go from one to another.\nNo, there is no constant you can multiply to get from vector 1 to vector 2, and there are no other vectors to add/subtract to to go from one to another.\nNow consider these two vectors:\n\\[\n\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix}\n\\]\nThese are not independent - you can multiply the first vector by a scalar of 2 to get the second vector.\n\n\n\nThis can be complicated to see in higher dimensions (since it is hard to consider how multiple vectors can be combined to match another). There, we use the matrix rank (see below).\n\n\n\nVector Spaces and Matrix Rank\nA collection of spanning vectors spans some space, if you can write any vector in that space, as a linear combination of the spanning vectors. For example, take vector \\(\\mathbf e_1 = (1 \\ 0)\\) and \\(\\mathbf e_2 = (0 \\ 1)\\). These vectors span some space including \\(\\mathbf z\\), if vector \\(\\mathbf z\\) can be written as:\n\\[\n\\mathbf z = a \\mathbf e_1 + b \\mathbf e_2, \\quad \\text{e.x.} \\quad \\mathbf z = (a \\ b)\n\\]\nThis allows us to write vectors in terms of the core vectors, and to understand the dimension of the space. The dimension of the space is the smallest number of linearly independent vectors that span the space.\n\n\n\n\n\n\nExample of Dimensionality\n\n\n\n\n\nTake these two vectors:\n\\[\n\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\]\nThese are linearly independent - no factor multipled can get the other vector. Thus, this is 2 dimensional space\nNow, let us add a third vector.\n\\[\n\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}\n\\]\nIs the third vector linearly independent? No. We can write the third vector with a combination of the other two:\n\\[\n\\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix} = 3 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + 2 \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\]\nThus, the third vector is in the space spanned by the first 2 vectors. The first two vectors spans this space, and thus, it is 2 dimensional space.\n\n\n\nGenerally, the dimensionality of the space matches the number of vectors that span the space (so a 2 dimensional space is often spanned by 2 vectors, 3 spanned by 3, etc.). Dimensionality of a vector space is not always the same as the dimensionality of the vectors.\nIt can be difficult to find if vectors are linearly independent in higher dimensions. Instead, we can stack the row vectors into a matrix (we can also do them in columns):\n\\[\n\\begin{pmatrix} \\mathbf x_1 \\\\ \\mathbf x_2 \\\\ \\mathbf x_3 \\end{pmatrix} =\n\\begin{pmatrix} x_{11} & x_{12} & x_{13} \\\\\nx_{21} & x_{22} & x_{23} \\\\\nx_{31} & x_{32} & x_{33} \\end{pmatrix}\n\\]\nThe Rank of a matrix is the number of linearly independent rows/columns in a matrix. If the Rank is equal to the total number of rows/columns (all rows/columns are linearly independent), the matrix has full rank. A matrix with full rank is non-singular, and thus, can be inverted, and has a non-zero determinant.\nThus, if we find the determinant of the matrix, if it is 0, the vectors are not linearly independent, and if it is not 0 , they are linearly independent. We can also know a matrix is not full rank, if the space of the dimension is less than the number of vectors (as explained above).\n\n\n\nSystems of Equations\nYou can write any system of linear equations as a matrix times a vector. Let us take this set of equations:\n\\[\n\\begin{cases}\na_{11}x_1 + a_{12} x_2 + a_{13} x_3 = y_1 \\\\\na_{21}x_1 + a_{22}x_2 + a_{23}x_3 = y_2 \\\\\na_{31}x_1 + a_{32}x_2 + a_{33}x_3 = y_3\n\\end{cases}\n\\]\nWe can write this system of equations as follows:\n\\[\n\\begin{pmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}\n= \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix}\n\\]\n\\[\n\\mathbf A\\mathbf x = \\mathbf y\n\\]\nMatrix inversion is a way to solve a system of equations. You can solve for \\(\\mathbf x\\) by inverting matrix \\(\\mathbf A\\) (assuming matrix a is full rank):\n\\[\n\\begin{align}\n\\mathbf{Ax} & = \\mathbf y \\\\\n\\color{blue}{\\mathbf{A}^{-1}}\\color{black}{\\mathbf{Ax}} & = \\color{blue}{\\mathbf A^{-1}}\\color{black}{\\mathbf y} && (\\text{multiply both sides by } \\color{blue}{\\mathbf A^{-1}}\\color{black}) \\\\\n\\mathbf x &= \\mathbf A^{-1}\\mathbf y && (\\mathbf A^{-1} \\mathbf A\\text{ inverses cancel})\n\\end{align}\n\\]\n\n\n\n\n\n\nExample of Matrix Inversion\n\n\n\n\n\nTake this system of equations:\n\\[\n\\begin{cases}\n3x - 7y = -11 \\\\\n5x + 10y = 25\n\\end{cases}\n\\]\nWe can write this in linear algebra:\n\\[\n\\begin{pmatrix}\n3 & -7 \\\\\n5 & 10\n\\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} =\n\\begin{pmatrix} -11 \\\\ 25 \\end{pmatrix}\n\\]\nNow, let us find the inverse of the first matrix:\n\\[\n\\mathbf A^{-1} = \\frac{1}{|\\mathbf A|}\\mathbf C^\\mathsf{T}\n\\]\nWe know the determinant \\(|\\mathbf A| = 3(10) - (-7)(5) = 65\\).\nNow, let us find the cofactor matrix:\n\n\\(c_{11} = (-1)^{1+1}\\times 10 = 10\\)\n\\(c_{12} = (-1)^{1+2} \\times 5 = -5\\)\n\\(c_{21} = (-1)^{2+1} \\times -7 = 7\\)\n\\(c_{22} = (-1)^{2+2} \\times 3 = 3\\)\n\nThus, the cofactor matrix transposed should be:\n\\[\n\\mathbf C^\\mathsf{T} = \\begin{pmatrix} 10 & -5 \\\\ 7 & 3 \\end{pmatrix}^\\mathsf{T} = \\begin{pmatrix} 10 & 7 \\\\ -5 & 3 \\end{pmatrix}\n\\]\nThus, the inverse of matrix \\(\\mathbf{A}\\) should be:\n\\[\n\\mathbf A^{-1} = \\frac{1}{65} \\begin{pmatrix} 10 & 7 \\\\ -5 & 3 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{13} & \\frac{7}{65} \\\\ -\\frac{1}{13} & \\frac{3}{65} \\end{pmatrix}\n\\]\nWe know the solution should be:\n\\[\n\\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{13} & \\frac{7}{65} \\\\ -\\frac{1}{13} & \\frac{3}{65} \\end{pmatrix} \\begin{pmatrix} -11 \\\\ 25 \\end{pmatrix}\n\\]\nThus, doing matrix multiplication to obtain \\(x\\) and \\(y\\):\n\n\\(x =\\frac{2}{13}(-11) + \\frac{7}{65}(25) = -\\frac{22}{13} + \\frac{35}{13} = \\frac{13}{13}=1\\)\n\\(y=-\\frac{1}{13}(-11) + \\frac{3}{65}(25) = \\frac{11}{13}+ \\frac{15}{13} = \\frac{26}{13} = 2\\)\n\nThe solution to this system of equations is: \\((1, 2)\\).\n\n\n\n\n\n\n\n\n\nCramer’s Rule\n\n\n\n\n\nCramer’s rule is another rule to solve equations, only for square matrices. It is not super commonly used. Given the system of equations:\n\\[\n\\mathbf{Ax} = \\mathbf y\n\\]\nThe element \\(x_i\\) is defined as:\n\\[\nx_i = \\frac{|\\mathbf B_i|}{\\mathbf A}\n\\]\nWhere \\(\\mathbf B_i\\) is a matrix obtained by taking the matrix \\(\\mathbf A\\), and replacing the \\(i\\)th column with the column vector \\(\\mathbf y\\).\n\n\n\n\n\n\n\n\n\nIssue with Unique Identification of Solutions\n\n\n\n\n\nA unique solution exists when you have the same amount of equations as unknowns, and the equations are non-contradictory. Overdetermined systems are when there are more equations than unknowns, which might contradict each other. Underdetermined systems are when there are not enough equations compared to unknowns, so we cannot solve it.\nTake this system of linear equations:\n\\[\n\\begin{cases}\nx+y=1 \\\\\n2x + 2y = 2\n\\end{cases}\n\\]\nWe can see that the two equations are a common factor of each other. Or in other words, these two vectors are non-linearly independent.\nWe know when solving for these equations, we cannot actually solve for a unique solution.\nWe know if a matrix is full rank, then all rows/columns are linearly independent.\n\n\n\n\n\n\nEigenvector Decomposition.\nGeometrically, vector \\(\\mathbf x\\) points in some direction. Then matrix \\(\\mathbf A\\) lineally maps it into a different vector \\(\\mathbf y\\), which might change the direction or change its norm. An eigenvector of matrix \\(\\mathbf A\\) is a vector \\(\\mathbf x\\), that does not change its direction (stays on the same linear line) when you apply the mapping operator \\(\\mathbf A\\):\n\\[\n\\mathbf{Ax} = \\lambda \\mathbf x\n\\]\nThe \\(\\lambda\\) is an eigenvalue that corresponds to the eigenvector. We can compute them by manipulating the above:\n\\[\n\\begin{split}\n\\mathbf{Ax} = \\lambda \\mathbf {Ix} \\\\\n\\mathbf{Ax} - \\lambda \\mathbf {Ix} & = 0 \\\\\n(\\mathbf A - \\lambda \\mathbf I) \\mathbf x & = 0\n\\end{split}\n\\]\n\\((\\mathbf A - \\lambda \\mathbf I)\\) is an singular matrix, meaning determinant \\(|(\\mathbf A - \\lambda \\mathbf I)| = 0\\). All values of \\(\\lambda\\) that solve this determinant equation will be eigenvalues of the matrix.\n\n\n\n\n\n\nStep by Step Calculating Eigenvalues\n\n\n\n\n\nFor example, let us say we have the matrix \\(\\mathbf{A}\\).\n\\[\n\\mathbf A = \\begin{pmatrix} 2 & 3 \\\\ 2 & 1 \\end{pmatrix}\n\\]\nWe know \\(|(\\mathbf A - \\lambda \\mathbf I)| = 0\\).\n\\[\n\\mathbf A - \\lambda \\mathbf I = \\begin{pmatrix} 2 - \\lambda & 3 \\\\ 2 & 1 - \\lambda \\end{pmatrix}\n\\]\nNow solve \\(|(\\mathbf A - \\lambda \\mathbf I)| = 0\\).\n\\[\n\\begin{split}\n0 & = |(\\mathbf A - \\lambda \\mathbf I)| \\\\\n0 & = (2 - \\lambda)(1- \\lambda) - 2(3) \\\\\n0 & = 2 -2 \\lambda - \\lambda + \\lambda^2 - 6 \\\\\n0 & = \\lambda ^2 - 3\\lambda - 4 \\\\\n0 & = (\\lambda - 4)(\\lambda + 1) \\\\\n\\lambda & = 4, -1\n\\end{split}\n\\]\n\n\n\nFor every \\(\\lambda\\), you will have an eigenvector that makes \\(\\mathbf{Ax} = \\lambda \\mathbf x\\) true. From our example in the info box:\n\\[\n\\begin{split}\n& \\mathbf{Ax} = 4 \\mathbf x \\\\\n& \\mathbf{Ax} = -1 \\mathbf x\n\\end{split}\n\\]\n\\(\\mathbf x\\) is a vector of 2 elements, \\(x_1, x_2\\) - but we only have one equation for each eigenvalue pair - an underdetermined system. Thus, we typically define \\(x_1 = 1\\) so there is a unique solution when computing \\(\\mathbf x\\).\n\n\n\n\n\n\nExample of Calculating Eigenvectors\n\n\n\n\n\nLet us continue the same example from before.\n\\[\n\\mathbf A = \\begin{pmatrix} 2 & 3 \\\\ 2 & 1 \\end{pmatrix}, \\quad \\lambda = 4, -1, \\quad \\mathbf x = \\begin{pmatrix} 1 \\\\ c \\end{pmatrix}\n\\]\nLet us solve for the eigenvalue of \\(\\lambda = 4\\).\n\\[\n\\begin{split}\n\\mathbf{Ax} & = 4 \\mathbf x \\\\\n\\begin{pmatrix} 2 & 3 \\\\ 2 & 1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ c \\end{pmatrix} & = 4\\begin{pmatrix} 1 \\\\ c \\end{pmatrix} \\\\\n\\begin{pmatrix} 2+3c \\\\ 2+c \\end{pmatrix} & = \\begin{pmatrix} 4 \\\\ 4c \\end{pmatrix}\n\\end{split}\n\\]\nThat gives us two equations:\n\\[\n\\begin{split}\n& 2 + 3c = 4 \\\\\n& 2 + c = 4c\n\\end{split}\n\\]\nThe answer is \\(c = \\frac{2}{3}\\) (both answers give us the equation).\nThus, the eigenvector with \\(\\lambda = 4\\) is:\n\\[\n\\mathbf x = \\begin{pmatrix} 1 \\\\ \\frac{2}{3} \\end{pmatrix}\n\\]\nWe can do the same for \\(\\lambda = -1\\), and we will get eigenvector:\n\\[\n\\mathbf x = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n\\]\n\n\n\nMatrix decomposition is to take a matrix \\(\\mathbf A\\), and decompose it into other matrices, that when multiplied together, get the original matrix. If matrix \\(\\mathbf A\\) has unique eigenvalues, you can write:\n\\[\n\\mathbf A = \\mathbf {QDQ}^{-1}, \\quad\n\\mathbf D = \\begin{pmatrix} \\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{pmatrix}, \\quad\n\\mathbf Q = \\begin{pmatrix} \\mathbf x_1 & \\mathbf x_2\\end{pmatrix}\n\\]\nWhere \\(\\mathbf Q\\) is made up of eigenvectors of \\(\\mathbf A\\), and \\(\\mathbf D\\) is a diagonal matrix with the \\(\\lambda\\) on the diagonal:\n\n\n\nQuadratic Forms\nA quadratic form takes the following form:\n\\[\n\\mathbf x^\\mathsf{T} \\mathbf A \\mathbf x\n\\]\nWhere matrix \\(\\mathbf A\\) is a square matrix.\nIf \\(\\mathbf x^\\mathsf{T} \\mathbf A \\mathbf x &gt; 0\\) for all values, then the matrix is positive definite. If \\(\\mathbf x^\\mathsf{T} \\mathbf A \\mathbf x &lt; 0\\), then negative definite. If \\(\\mathbf x^\\mathsf{T} \\mathbf A \\mathbf x ≤ 0\\) it is positive semi-definite.\nThis will be useful for optimisation and multivariable calculus. We will specifically see this in deriving the OLS estimator.\n\n\n\n\n\n\nSingle and Multivariable Calculus\n\nDerivatives and Partial Derivatives\nDerivatives \\(f'(x)\\) calculate the slope of the original function \\(f(x)\\) at a point.\n\n\n\n\n\n\nFormal Definition of a Derivative\n\n\n\n\n\nThe slope between two points on a function \\(y = f(x)\\) can be defined as follows:\n\\[\n\\frac{\\Delta f(x)}{\\Delta x} = \\frac{f(x_2) - f(x_1)}{x_2 - x_1} = \\frac{f(x+h) - f(x)}{x + h -x} = \\frac{f(x+h) - f(x)}{h}\n\\] We want the slope at one point, not at two. So we can make the distance between the two points \\(h\\) approach zero:\n\\[\nf'(x) = \\lim\\limits_{h \\rightarrow 0}\\frac{f(x+h)-f(x)}{h}\n\\]\nThis is the formal definition of a derivative, and any derivative can be calculated this way.\n\n\n\nSome quick derivative rules (where \\(c\\) is some constant, and \\(k\\) is some constant exponent):\n\\[\n\\begin{array}{|c|c|}\nf(x) & f'(x) \\\\ \\hline\nc & 0 \\\\\ncx & c \\\\\ncu(x) & c'u(x) \\\\\nx^k & kx^{k-1} \\\\\nu[v(x)] & u'[v(x)] \\cdot v'(x) \\\\\nu(x)+v(x) & u'(x) + v'(x) \\\\\nu(x)v(x) & u'(x)v(x) + v'(x)u(x) \\\\\n\\frac{u(x)}{v(x)} & \\frac{u'(x)v(x) + v'(x)u(x)}{(v(x))^2} \\\\\n\\end{array}\n\\qquad \\qquad\n\\begin{array}{|c|c|}\nf(x) & f'(x) \\\\ \\hline\ne^x & e^x \\\\\ne^{u(x)} & e^{u(x)} \\cdot u'(x) \\\\\nc^x & \\ln (c) \\cdot c^x \\\\\n\\ln(x) & \\frac{1}{x} \\\\\n\\ln(x^k) & \\frac{k}{x} \\\\\n\\ln(u(x)) & \\frac{u'(x)}{u(x)} \\\\\n\\sin(x) & \\cos(x) \\\\\n\\cos(x) & -\\sin(x)\n\\end{array}\n\\]\n\n\n\n\n\n\nExample 1 (Product and Power Rule)\n\n\n\n\n\nLet us find the derivative of \\(f(x) = (x^3)(2x^4)\\). There are two ways to do this.\n\nMethod 1: Product Rule\nProduct rule says that \\([f(x)g(x)]' = f'(x)g(x) + g'(x)f(x)\\). We can define \\(f(x):=x^3\\) and \\(g(x) := 2x^4\\).\nUsing these definitions, we can find the derivative of our defined \\(f(x)\\) and \\(g(x)\\):\n\n\\(f'(x) = 3x^2\\) using power rule.\n\\(g'(x) = 8x^3\\) using the rule \\([cf(x)]' = f'(x)\\) and power rule.\n\nUsing this info, we can now put it into the product rule form and simplify:\n\\[\n\\begin{align}\nf'(x) & = (3x^2)(2x^4) + (8x^3)(x^3) && \\text{(product rule)} \\\\\n& = 6x^6 + 8x^6 && \\text{(multiply with properties of exponents)} \\\\\n& = 14x^6\n\\end{align}\n\\]\n\n\nMethod 2: Power Rule\nInstead of using product rule (which I did more as a demonstration), we can just simplify our original expression and use power rule:\n\\[\n\\begin{align}\nf(x) & = (x^3)(2x^4) \\\\\nf(x) & = 2x^7 && \\text{(multiply with property of exponents)} \\\\\nf'(x) & = 2[x^7]' && ([cf(x)]' = cf'(x)) \\\\\nf'(x) & = 2(7x^6) && \\text{(power rule)} \\\\\nf'(x) & = 14x^6 && \\text{(multiply out)}\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\n\nExample 2 (Chain Rule)\n\n\n\n\n\nLet us find the derivative of \\(f(x) = (3x^2 + 5x - 7)^6\\).\nThis is a composite function, so we can use chain rule \\([f(g(x))]' = f'(g(x)) \\cdot g'(x)\\). Let us define \\(f(x) := x^6\\) and \\(g(x) := 3x^2 + 5x - 7\\).\nWe can quickly determine that:\n\n\\(f'(x) = 6x^5\\) by power rule.\n\\(g'(x) = 6x + 5\\) by power rule and sum rule.\n\nNow, let us put it into the form of chain rule and simplify:\n\\[\n\\begin{align}\nf'(x) & = 6(3x^2 + 5x - 7)^5 \\cdot (6x + 5) && \\text{(chain rule)} \\\\\nf'(x) & = 36x(3x^2 + 5x - 7)^5 + 30(3x^2 + 5x - 7)^5 && \\text{(multiply out)}\n\\end{align}\n\\]\n\n\n\nPartial derivatives are derivatives in functions with multiple variables \\(x_1, \\dots, x_n\\), but we only care about one of these variables \\(x_j\\). We denote them with the partial sign \\(\\frac{\\partial y}{\\partial x_j}\\). Essentially, we just pretend the other variables are constants (treat them like you would the number 5), and take the derivative as normal.\n\n\n\n\n\n\nPartial Derivatives Example\n\n\n\n\n\nLet us find the partial derivative in respect to \\(x\\) of function \\(f(x,z) = 5x^2z^3 + 2x + 3z +5\\).\nSince we are taking the partial derivative in respect to \\(x\\), we can pretend that \\(z\\) is just some constant, like the number 5:\n\\[\n\\begin{align}\n\\frac{\\partial f(x,z)}{\\partial x} & = \\frac{\\partial}{\\partial x}[5x^2z^3 + 2x + 3z +5]\\\\\n& = \\frac{\\partial}{\\partial x}[5x^2z^3] + \\frac{\\partial}{\\partial x}[2x] + \\frac{\\partial}{\\partial x}[3z] + \\frac{\\partial}{\\partial x}[5] && \\text{(by sum rule)} \\\\\n& = \\frac{\\partial}{\\partial x}[5x^2z^3] + \\frac{\\partial}{\\partial x}[2x] + 0 + 0  && \\text{(derivative of constants is 0)} \\\\\n& = \\frac{\\partial}{\\partial x}[5x^2z^3] + 2 && ([cx]' = c)\\\\\n& = 5z^3 \\frac{\\partial}{\\partial x}[x^2] + 2&& ([cf(x)]' = cf'(x)) \\\\\n& = 5z^3[2x] + 2 && \\text{(power rule)} \\\\\n& = 10z^3x + 2 && \\text{(multiply out)}\n\\end{align}\n\\]\n\n\n\n\n\n\nThe Gradient\n\n\n\nUnconstrained Optimisation\nMaxima and Minima are extrema of a function \\(f(x)\\). A local maximum/minimum is some \\(x\\) value that corresponds with the maximum/minimum \\(f(x)\\) value in its neighbourhood (in nearby \\(x\\) values). A global maximum/minimum is some \\(x\\) value that corresponds with the maximum/minimum \\(f(x)\\) value within the entire domain of the function.\nMaxima and Minima (both global and local) always occur at a stationary point. A stationary point is defined as a point where the first derivative \\(f'(x) = 0\\). Maxima occur at a stationary point when a function is concave, i.e. the second derivative \\(f''(x) &lt; 0\\). Minima occur at a stationary point where a function is convex, i.e. the second derivative \\(f''(x) &gt; 0\\).\nThus, to optimise a function (find the \\(x\\) value that produces the minima/maximima), our procedure is the following:\n\nFind \\(f'(x)\\). Then, set \\(f'(x) = 0\\) (first order condition). Solve for the \\(x\\) values that makes \\(f'(x) = 0\\) true (there can be multiple). Let us denote those \\(x\\) values as \\(x_s\\).\nFind \\(f''(x)\\). Now, plug in our solved \\(x_s\\)’s into our second derivative \\(f''(x_s)\\). If it is negative, we have a maxima. If it is positive, we have a minima.\nIf we are interested in the actual function value at the extrema, we can plug them back into the original function, \\(f(x_s)\\).\n\nIf we want to find the maximum within some sub-domain of the function \\([a, b]\\), we should also plug in \\(f(a)\\) and \\(f(b)\\) into the original function to see if these edge-cases of the sub-domain are higher/lower than our calculated maxima/minima.\n\n\n\n\n\n\nExample of Unconstrained Maximisation\n\n\n\n\n\n\n\n\n\n\n\n\nConstrained Optimisation\n\n\n\nIndefinite and Definite Integrals\nIn evaluating indefinite integrals, we always include a \\(+C\\) (integration constant). Common rules include:\n\\[\n\\begin{array}{|c|c|}\nf(x) & \\int f(x) = F(x) \\\\ \\hline\nc & cx + C\\\\\nc u(x) & c \\cdot \\int u(x)dx \\\\\nu(x) + v(x) & \\int u(x)dx + \\int v(x)dx \\\\\nx^k & \\frac{x^{k+1}}{k+1} + C \\\\\n[u(x)]^k u'(x) & \\frac{1}{k + 1}[u(x)]^{k+1} + C\n\\end{array}\n\\qquad \\qquad\n\\begin{array}{|c|c|}\nf(x) & \\int f(x) = F(x) \\\\ \\hline\nx^{-1} & \\ln (x) + C \\\\\n\\frac{u'(x)}{u(x)} & \\ln(f(x)) + C \\\\\ne^x & e^x + C\\\\\ne^{u(x)}u'(x) & e^{u(x)} + C \\\\\n\\cos(x) & \\sin(x) + C \\\\\n\\end{array}\n\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\nLet us find the integral of \\(f(x) = e^xe^{e^x} + 3x^2\\).\nFirst, we know that \\(\\int [u(x) v(x)]dx = \\int u(x)dx + \\int v(x)dx\\). Thus, we can split our above into \\(u(x) := e^e^{e^x}\\) and \\(v(x) := 3x^2\\).\nFirst, let us find \\(\\int u(x)dx\\). We know the rule from above \\(\\int [e^{u(x)} u'(x)]dx = e^{u(x)} + C\\).\n\nWe can rearrange our original part into the rule form as \\(\\int e^{e^x}e^xdx\\). Since this now fits the rule, we know $u(x)dx = e{ex} + C $.\n\nNow, let us find \\(\\int v(x)dx\\). Using reverse power rule \\(\\int x^k dx = \\frac{x^{k+1}}{k+1} + C\\), we can quickly deduce that \\(\\int v(x)dx = 3(\\frac{1}{3}x^3) = x^3 + C\\).\nThus, putting the two back together, we get:\n\\[\n\\int \\left[e^x e^{e^x} + 3x^2 \\right]dx = e^{e^x} + x^3 + C\n\\]\n\n\n\nThere are two more complex ways to solve integrals as well:\n\\[\n\\begin{align}\n& \\int u[v(x)]v'(x)dx = U(v(x)) + C && \\text{integration by substitution} \\\\\n& \\int u(x) v'(x)dx = u(x) v(x) - \\int u'(x) v(x)dx && \\text{integration by parts}\n\\end{align}\n\\]\n\n\n\n\n\n\nExample of Integration by Substitution\n\n\n\n\n\nLet us find the integral of \\(f(x) = (x+4)^5\\).\nWe know integration by substitution says:\n\\[\n\\int u[v(x)]v'(x)dx = U(v(x)) + C\n\\]\nFor integration by substitution, we want to find some \\(u(x)\\) and \\(v(x)\\) we can define to rearrange our original equation, where \\(\\int u(x)dx\\) is not too difficult to find.\nFor this example, we could define \\(u(x) := x^5\\) and \\(v(x) := x +4\\). These substitutions will make the \\(u[v(x)]\\) look like \\((x+4)^5\\). However, if we take a look at the integration by substitution, we will note that our left hand side is in the form \\(u[v(x)] v'(x)\\). Luckily, our originally definitions still work, since \\(v'(x) = 1\\).\nNow, we have this form, we can use the integration by substitution formula, which tells us the integra will be \\(U(v(x)) + C\\).\nWe see that we have to find \\(U(x) = \\int u(x)dx\\). We know \\(u(x) := x^5\\), so we can use reverse power rule \\(\\int x^k dx = \\frac{x^{k+1}}{k+1} + C\\), and see this is \\(\\frac{1}{6}x^6 + C\\).\nNow, we need to fund \\(U[v(x)]\\), so we insert \\(v(x) = x + 4\\) inside to get our final answer:\n\\[\n\\int (x+4)^5 dx = \\frac{1}{6}(x+4)^2 + C\n\\]\n\n\n\n\n\n\n\n\n\nExample of Integration by Parts\n\n\n\n\n\nLet us find the integral of \\(f(x) = xe^xdx\\).\nWe know integration by parts says:\n\\[\n\\int u(x) v'(x)dx = u(x) v(x) - \\int u'(x) v(x)dx\n\\]\nFor integration by parts, we want to find some \\(u(x)\\) and \\(v'(x)\\) we can define to rearrange our original equation, where \\(\\int u'(x)v(x)dx\\) is not too difficult to find.\nWe can define \\(u(x) := x\\), and \\(v'(x) := e^x\\). This means our left side of the rule \\(u(x)v'(x) = xe^x\\) as we need it to be.\nNow, we know to solve this by integration by parts, we need to find \\(\\int u'(x)v(x)\\). First, we know \\(u'(x) = 1\\). Thus, we need to find \\(\\int 1v(x)\\). We know \\(\\int e^x = e^x\\), so \\(\\int u'(x)v(x) = e^x\\).\nNow, we have all the parts we need to put into our integraion by parts formula for the answer:\n\\[\n\\int xe^x dx = x \\cdot e^x - e^x + C\n\\]\n\n\n\nDefinite integrals are notated \\(\\int_a^b f(x)\\). They denote the area under the curve \\(f(x)\\) between points \\(a\\) and \\(b\\). The Fundamental Theorem of Calculus allows us to find the value of definite integrals.\n\\[\n\\int\\limits_a^b f(x)dx = F(b) - F(a)\n\\]\nThis is a useful property for calculating the area under distributions, which are also probabilities.\n\n\n\n\n\n\nSets and Probability\n\nBasic Set Theory\nA set is a collection of distinct objects (generally numbers). We notate a set with a capital letter \\(A\\), and an element/object within a set with a lowercase \\(a\\). The most common sets you will see include:\n\n\\(\\mathbb R\\): The set of all real numbers (not imaginary)\n\\(\\mathbb Z\\): The set of all integers {…, -1, 0, 1, 2, …}.\n\\(\\mathbb N\\): The set of all natural numbers {1,2,3,…}.\n\\(\\omega\\) The set of all possible outcomes of some event. This depends on the event, but is quite widely used in probability.\n\\(\\varnothing\\): An empty set with no elements.\n\nWe can define sets by their elements. Below is set \\(A\\) with 3 elements \\(a, b, c\\):\n\\[\nA = \\{a, b, c \\}\n\\]\nWe can also define sets by some interval/ranges/conditions. Below, we define set \\(A\\) as some elements \\(x\\) such that (\\(:\\)) \\(x\\) is in between -1 and 3, and \\(x\\) is in all real numbers \\(\\mathbb R\\).\n\\[\nA = \\{x : -1 &lt; x &lt; 3, x \\in \\mathbb R\\}\n\\]\nAn element \\(a\\) in set \\(A\\) is denoted \\(a \\in A\\). If element \\(a\\) is not in set \\(A\\), we say \\(a \\notin A\\). You will also see the notation \\(a \\in [c, d]\\) or \\(a \\in (c, d)\\), which represents element \\(a\\) is in the interval between \\(c\\) and \\(d\\).\nThe intersection between two sets \\(A\\) and \\(B\\), denoted \\(A \\cap B\\), is a new set of only the elements that are both in \\(A\\) and \\(B\\) at the same time. The union between two sets \\(A\\) and \\(B\\), denoted \\(A \\cup B\\), is a set of elements that are either in \\(A\\) or \\(B\\) or both. It is the combination of elements of both sets.\nThe notation \\(A \\subset B\\) means \\(A\\) is a subset of \\(B\\). This means that all elements of \\(A\\) are present in \\(B\\) as well, and \\(A\\) has less elements than \\(B\\) does.\nYou will also see the set complement \\(A^c\\), which refers to all elements that are not in set \\(A\\) but are a part of the set of possible elements (which will depend on the context).\n\n\n\nBasics of Probability\nAn experiment is some process that has several different possible outcomes. The sample space \\(\\Omega\\) of the experiment is the set of all possible outcomes of the experiment.\nAn event \\(A\\) is some subset of outcomes of the sample space, \\(A \\in \\Omega\\). The probability of event \\(A\\) occuring, that we notate as \\(P(A)\\), is the chance of some subset of outcomes \\(A\\) occuring in relation to the set of all outcomes \\(\\Omega\\).\nLet us have two events \\(A\\) and \\(B\\). The following probabilities are:\n\n\\(P(A \\cap B)\\): the probability that \\(A\\) and \\(B\\) both happen. This is called a join probability.\n\\(P(A \\cup B)\\): the probability that \\(A\\) or \\(B\\) occurs - at least one occurs.\n\\(P(A^c)\\): the probability that not-\\(A\\) occurs - so the probability that anything other than \\(A\\) occurs. By definition, \\(P(A^c)  = 1 - P(A)\\).\n\nProbabilities have a few axioms:\n\nFor any event \\(A\\), there must be a non-negative probability: \\(P(A) ≥ 0\\).\nThe probability of all possible outcomes (sample space) is 1: \\(P(\\Omega) = 1\\).\n\nIf we have mutually exclusive events (i.e. if \\(A\\) occurs than \\(B\\) cannot occur at the same time), then the probability either \\(A\\) or \\(B\\) occuring is:\n\\[\nP(A \\cup B) = P(A)+Pr(B), \\quad \\text{more than 2 events}: P\\left(\\bigcup\\limits_{i=1}^n A_i\\right) = \\sum\\limits_{i=1}^n P(A_i)\n\\]\nIf we have independent events (i.e. if \\(A\\) occurs, that does not affect the probability of \\(B\\) occuring), then the probability that both \\(A\\) and \\(B\\) occuring is:\n\\[\nP (A \\cap B) = P(A) P(B), \\quad \\text{more than 2 events}: P\\left(\\bigcap\\limits_{i=1}^n A_i \\right) = \\prod\\limits_{i=1}^n P(A_i)\n\\]\n\n\n\nBayes’ Rule\nConditional Probability is the probability of one event, given another has already occured. The conditional probability of \\(A\\) given \\(B\\) has occured is denoted \\(P(A|B)\\):\n\\[\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nThe law of Total Probability helps us calculate the probability of an event \\(A\\), by considering different mutually exclusive events that partition the sample space \\(\\Omega\\).\nFor example, perhaps you want to calculate the overall smoking rate \\(A\\). Assuming all humans can be divided into male or female, and no one can be both male and female, you can calculate the total smoking rate of the population, if you know the conditional probabilities of smoking for males and females, and their proportion in the population.\n\\[\nP(A) = P(A|B_M) P(B_M) + P(A|B_F)P(B_F)\n\\]\n\nWhere \\(B_M\\) is male and \\(B_F\\) is female.\n\nFurthermore, we know that events \\(B\\) and \\(B^c\\) completely partition the sample space \\(\\Omega\\). Thus, we can also write the law of total probability as:\n\\[\nP(A) = P(A|B)P(B) + P(A|B^c)P(B^c)\n\\]\nUsing the definition of conditional probability and the law of total probability, we can create Bayes’ Rule, one of the most important rules in statistics and probability:\n\\[\n\\begin{align}\nP(A|B) & = \\frac{P(B|A)P(A)}{P(B)} \\\\\n& = \\frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|A^c)P(A^c)} && \\text{by law of total probability}\n\\end{align}\n\\]\n\n\n\n\n\n\nDeriving Bayes’ Rule\n\n\n\n\n\nLet us start off with the definition of conditional probability:\n\\[\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nLet us solve for \\(P(A \\cap B)\\) to get:\n\\[\nP(A\\cap B) = P(A|B)Pr(B)\n\\]\nNow, let us take the opposite conditional probability:\n\\[\nP(B|A) = \\frac{P(B \\cap A)}{P(A)}\n\\]\nLet us solve for \\(P(B \\cap A)\\):\n\\[\nP(B\\cap A) = P(B|A) P(A)\n\\]\n\nBy the commutative property of sets, we know that:\n\\[\nP(A \\cap B) = P(B \\cap A)\n\\]\nThus, the following must also be true.\n\\[\nP(A|B)P(B) = P(B|A)P(A)\n\\]\nNow, let us solve for \\(P(A|B)\\):\n\\[\nP(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n\\]\nThus, we have proved Bayes’ Rule. We can plug in the law of total probability below to expand.\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Background Mathematics"
    ]
  },
  {
    "objectID": "math.html#quadratic-forms",
    "href": "math.html#quadratic-forms",
    "title": "Essential Mathematics",
    "section": "Quadratic Forms",
    "text": "Quadratic Forms\nA quadratic form takes the following form:\n\\[\n\\mathbf x^\\mathsf{T} \\mathbf A \\mathbf x\n\\]\nWhere matrix \\(\\mathbf A\\) is a square matrix.\nIf \\(\\mathbf x^\\mathsf{T} \\mathbf A \\mathbf x &gt; 0\\) for all values, then the matrix is positive definite. If \\(\\mathbf x^\\mathsf{T} \\mathbf A \\mathbf x &lt; 0\\), then negative definite. If \\(\\mathbf x^\\mathsf{T} \\mathbf A \\mathbf x ≤ 0\\) it is positive semi-definite.\nThis will be useful for optimisation and multivariable calculus. We will specifically see this in deriving the OLS estimator.",
    "crumbs": [
      "Essential Mathematics"
    ]
  },
  {
    "objectID": "ztest.html",
    "href": "ztest.html",
    "title": "zTest",
    "section": "",
    "text": "\\[\n\\begin{align}\n\\E(\\hat\\beta | X, z) & = \\beta + (X^\\top X)^{-1}X^\\top z \\delta\\\\\n& = \\beta + \\color{blue}{\\hat\\eta}\\color{black}\\delta && (\\because \\color{blue}{\\hat\\eta = (X^\\top X)^{-1} X^\\top z }\\color{black}) \\\\\n\\E\\hat\\beta & = \\E(\\E(\\hat\\beta|X, z)) && \\text{(law of iterated expect.)} \\\\\n& = E(\\color{blue}{\\beta + \\hat\\eta \\delta}\\color{black}) && (\\because \\color{blue}{\\E(\\hat\\beta|X, z) = \\beta + \\hat\\eta\\delta} \\color{black}) \\\\\n& = \\beta + \\E\\hat\\eta \\ \\delta && \\text{(take out constants from exp.)} \\\\\n& = \\beta + \\eta\\delta && (\\text{unbiased estimator } \\E\\hat\\eta = \\eta)\n\\end{align}\n\\]\n\\[\n\\hat\\beta_1 = (\\tilde X_1^\\top \\tilde X_1)^{-1}\\tilde X_1 ^\\top \\tilde y\n\\]\n\n\n\n Back to top"
  },
  {
    "objectID": "quant1.html#alternative-derivation-for-simple-linear-regression",
    "href": "quant1.html#alternative-derivation-for-simple-linear-regression",
    "title": "Classical Linear Model",
    "section": "Alternative Derivation for Simple Linear Regression",
    "text": "Alternative Derivation for Simple Linear Regression\nCurrently, we are deriving the first order conditions for multiple linear regression using linear algebra.\nFor simple linear regression (with one explanatory variable), we can use summation notation. Recall our sum of squared residuals in summation form:\n\\[\nSSR = S(\\hat\\beta_0, \\hat\\beta_1)= \\sum\\limits_{i=1}^n(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i)^2\n\\]\nWe want to minimise the SSR in respect to both \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\). We can do this by finding our first order conditions:\n\\[\n\\begin{align}\n\\frac{\\partial S(\\hat\\beta_0, \\hat\\beta_1)}{\\partial \\hat\\beta_0} & = \\sum\\limits_{i=1}^n(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i) = 0 \\\\\n\\frac{\\partial S(\\hat\\beta_0, \\hat\\beta_1)}{\\partial \\hat\\beta_1} & = \\sum\\limits_{i=1}^n X_i(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i) = 0 \\\\\n\\end{align}\n\\]\nThese conditions create a system of equations, which you can solve for the OLS solutions of \\(\\widehat{\\beta_0}\\) and \\(\\widehat{\\beta_1}\\). I will not show it step by step, as it is tedious (and not that important). The OLS solutions are\n\\[\n\\begin{align}\n\\hat\\beta_0 & = \\bar Y - \\widehat{\\beta_1} \\bar X \\\\\n\\hat\\beta_1 & = \\frac{\\sum_{i=1}^n(X_i - \\bar X)(Y_i - \\bar Y)}{\\sum_{i=1}^n(X_i - \\bar X)^2} = \\frac{Cov(X_i, Y_i)}{\\V Y_i}\n\\end{align}\n\\]",
    "crumbs": [
      "1 Classical Linear Model"
    ]
  },
  {
    "objectID": "stats.html",
    "href": "stats.html",
    "title": "Background Statistics",
    "section": "",
    "text": "This chapter is about the very building blocks of statistics. We start off with a discussion over probability, random variables, and distributions. Then, we talk about the basics of statistical inference and hypothesis testing. We conclude by discussing correlations between variables and the basics of regression.\nUse the right sidebar to navigate quickly.\n\n\nRandom Variables\n\nRandom Variables and Distributions\nRandom variables are outcomes that have some randomness/uncertainty. There is a set of possible outcomes \\(\\Omega\\), called the sample space. Within the sample space there exists events \\(\\omega \\in \\Omega\\) (potential outcomes). We do not know which event will be observed, but we know each outcome \\(\\omega\\)’s probability of realisation \\(\\P(\\omega \\in \\Omega)\\).\nWe can represent \\(\\omega \\in \\Omega\\), and the associated probabilities with each outcome \\(\\P(\\omega)\\), in a distribution. On the horizontal axis is all \\(\\omega \\in \\Omega\\), and the vertical axis is \\(\\P(\\omega)\\).\n\n\n\n\n\nContinuous random variables \\(\\omega \\in [a, b]\\) can take any possible value within a range \\(a\\) to \\(b\\), including any decimal. Discrete random variables can only take a certain set of values \\(\\omega \\in \\{a, b, \\dots \\}\\), not any possible decimal. For example, a dice roll can only take the values of 1, 2, 3,…, and not 3.23478. A discrete random variable with only 2 potential outcomes \\(\\omega \\in \\{0, 1 \\}\\) is a binary random variable.\nWe will denote random variables with a capital letter (ex. \\(Z\\), \\(X\\)). In later chapters, we will denote them with a subscript \\(Z_i, X_i\\) to distinguish them from matrices. The realisation of a random variable (the outcome) is denoted with lower case letters (ex. \\(z, x\\)).\n\n\n\nProbability Density Functions\nWe can describe these distributions mathematically. A probability density function is a function \\(\\varphi\\) that takes one potential outcome as an input, and spits out the probability of that outcome. For example, the probability density function of a dice \\(z \\in \\{ 1, 2, 3, 4, 5, 6\\}\\) is:\n\\[\n\\P(z) = \\varphi(z) = 1/6\n\\]\nWe can see that if we want to see the probability of getting a 5 on our dice roll, we plug in \\(z=5\\) to get \\(\\varphi(5) = 1/6\\), which is the probability of rolling a 5.\nFor continuous random variables, it does not make sense to calculate the probability of one specific outcome \\(z\\). For example, let us say your random variable is the time it will take you to get to school tomorrow. We really do not care about the probability of taking 13.4357 minutes to get to school. What we do care about is a range - lets say the probability of getting to school between 13 and 14 minutes.\nMore mathematically, the probability density function of a continuous random variable gives you the probability of an outcome between \\(a\\) and \\(b\\):\n\\[\n\\P(a&lt;z&lt;b) =\\int\\limits_a^b \\varphi(z)dz\n\\]\nThis integral is important - it tells us that the area under a distribution gives us the probability of an event.\n\n\n\n\n\n\n\n\n\n\n\nCumulative Density Functions\n\n\n\n\n\nCumulative density functions measure the cumulative probability below a certain point.\nFor example, to get the probability of anything below \\(a\\) occurring as an outcome, our cumulative density function is:\n\\[\n\\P(z&lt;a) = \\int\\limits_{-∞}^a \\varphi(z)dz\n\\]\nThis will be useful in some cases to calculate values for probability density functions.\n\n\n\n\n\n\nExpectation and Variance\nThe expectation, also called the mean or expected value, is the “average” outcome value that you would expect to get from the distribution. It is essentially our “best guess” of what the random variable’s outcome would be.\nExpectation for a discrete random variable \\(X\\) is calculated as a weighted sum of all potential outcomes \\(x_i\\) and their respective probabilities given by the probability density function \\(p(x_i)\\):\n\\[\n\\E Z  = \\sum\\limits_{i=1}^n(z_i \\times \\varphi(z_i))\n\\]\nFor a continuous random variable \\(X\\), the idea is the same but with an integral:\n\\[\n\\E Z = \\int\\limits_{-∞}^∞ (z \\times \\varphi(z))dz\n\\]\nExpectations have a few unique algebraic properties:\n\nThe expectation of a constant is itself: \\(\\E c = c\\).\nExpectations can be added: \\(\\E(X+Y) = \\E X + \\E Y\\).\nExpectations multiplied with constants: \\(\\E(cX) = c \\cdot \\E X\\).\nLaw of iterated expectations: \\(\\E(X) = \\E[\\E(X|Y)]\\).\n\nVariance measures the spread of a distribution. It is essentially the average distance of each individual outcome from the expected value \\(\\mu\\) of the random variable:\n\\[\n\\V Z = \\sigma^2= \\E(z - \\E Z)^2\n\\]\nThe standard deviation \\(\\sigma\\) is simply the square root of variance.\n\n\n\nConditional Distributions\nLet us say we have two random variables \\(X\\) and \\(Y\\), each with their own respective distributions.\nOften in statistics, we are interested in how two variables interact with each other. For example, we are interested in how democracy affects economic growth. Or how education affects income.\nConditional distributions are a distribution of one random variable \\(X\\), given we hold another variable \\(Y\\) fixed at some value.\nFor example, imagine \\(X\\) is income and \\(Y\\) is age. The conditional distribution of \\(X|Y\\) is the distribution of income \\(X\\) at every specific age \\(Y\\). For example, \\(X|Y = 20\\) is the distribution of income \\(X\\) for 20 year olds. \\(X|Y=60\\) would be the distribution of income \\(X\\) for 60 year olds.\nConditional distributions have all the same properties as normal distributions. The most important of these is the conditional expectation, which we denotate \\(\\E(X|Y)\\). In the context of above, \\(\\E(X|Y=20)\\) would be the expected income for a 20 year old.\nThe reason conditional expectations are so important is because they illustrate how one variable affects another. If we see a pattern in going between \\(\\E(X|Y=20)\\), \\(\\E(X|Y = 21)\\), and \\(\\E(X|Y = 24)\\), we might be tempted to say that increasing age \\(Y\\) has some effect on income \\(X\\).\n\n\n\n\n\n\nDistributions\n\nThe Normal Distribution\nThe normal distribution is for continuous random variables, and takes a famous “bell shape” (hence why it is also called the bell curve). Every normal distribution and its probability density function (PDF) can be defined by two parameters: the mean and the variance:\n\\[\nZ \\sim \\mathcal N(\\mu, \\sigma^2), \\quad \\P(a&lt;z&lt;b) = \\int\\limits_a^b \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{\\left( -\\frac{(z - \\E Z)^2}{2 \\sigma^2}\\right)}dx\n\\]\nWhere \\(\\mathcal N\\) represents the normal distribution, \\(\\mu\\) represents the expected value \\(\\E Z\\) of the distribution, and \\(\\sigma^2\\) represents the variance. \\(Z\\) is the variable we have assigned the normal distribution to. The figure below shows how normal distributions change when you alter \\(\\mu\\) and \\(\\sigma^2\\).\n\n\n\n\n\n\n\n\nProperties of the Normal Distribution\nNormal Distributions have a unique property - within any standard deviations \\(\\sigma\\) from the mean, every single normal distribution contains the same amount of area under the curve (which is also the probability):\n\n\n\n\n\nThese properties don’t just apply to 1, 2, or 3 standard deviations away from the mean. They apply to any amount of standard deviations (such as 1.23478 standard deviations), and there are online tables/calculators that can derive this. This means that we can find the probability between any two points of a normal distribution, just by knowing the mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nAnother property of the normal distribution is that we can manipulate them as follows:\n\nWe can add a constant \\(c\\) to every single outcome/value in a normal distribution. The resulting distribution will still be a normal distribution, just with its mean shifted by \\(c\\). Or in other words: \\(Z \\sim \\mathcal N(\\mu + c, \\sigma^2)\\).\nWe can multiply by constant \\(c\\) to every single outcome/value in a normal distribution. The resulting distribution will still be a normal distribution, just with its mean multiplied by \\(c\\), and its standard deviation multipled by \\(c^2\\). Or in other words: \\(Z \\sim \\mathcal N(c \\mu, (c \\mu)^2)\\).\n\n\n\n\nThe Standard Normal Distribution\nThe standard normal distribution (often called the \\(Z\\)-distribution) is a special version of the normal distribution, with a mean of 0 and a variance of 1:\n\\[\nZ \\sim\\mathcal N(0, 1), \\quad \\P(a&lt;z&lt;b) = \\int\\limits_a^b \\frac{e^{-\\frac{z^2}{2}}}{\\sqrt{2\\pi}}dz = \\int\\limits_a^b\\phi(z)dz\n\\]\nNote: we denote the PDF of the standard normal as \\(\\phi\\), and the CDF of the standard normal as \\(\\Phi\\).\n\n\n\n\n\nThe best part about the standard normal distribution is that we can transform any other normal distribution into a standard normal. Recall the properties of adding and multiplying constants to a normal distribution from above. That means we can apply a transformation to every value/unit in a normal distribution \\(X\\) to get a standard normal distribution \\(Z\\):\n\\[\n\\text{if } X \\sim \\mathcal N(\\mu, \\sigma^2), \\quad Z = \\frac{x - \\E X}{\\sigma} \\sim \\mathcal N(0,1)\n\\]\nOr in other words, for every value of \\(x \\in X\\), subtract the mean, then divide by the standard deviation. If you do this for every value \\(x \\in X\\), you will get a standard normal.\n\n\n\nThe T-Distribution\nThe T-distribution looks very similar to that of the normal distribution, with the same bell curve shape. However, the t-distribution’s tails are slightly fatter than the normal distribution, and the peak is slightly shorter.\n\n\n\n\n\nThe key difference between the t-distribution and the normal distribution is the parameters. Unlike the normal distribution, which has 2 parameters, the t-distribution has only one parameter: degrees of freedom.\nThe t-distribution is always centered on 0. Higher degrees of freedom means less thick tails. As degrees of freedom get larger (past 30), it almost perfectly fits a normal distribution.\nThe importance of the T-distribution is that it is often used for statistical inference when the normal distribution, for whatever reason, cannot be used. This is often because with the normal distribution, we need to know variance \\(\\sigma^2\\), but we do not need to know this to use the T-distribution.\n\n\n\nBernoulli and Binomial Distribution\nA bernoulli trial is an experiment that has two possible outcomes: a success \\(z= 1\\) and a failure \\(z = 0\\). We denote the probability of a success as \\(p\\), and the probability of a failure as \\(q = 1-p\\). For example, a coin flip could be seen as a bernoulli trial, with \\(p = 0.5\\).\nFor example, below is a bernoulli distribution with \\(p = 0.15\\):\n\n\n\n\n\nThe probability density function of a bernoulli distribution is:\n\\[\n\\varphi(z) = \\begin{cases}\np \\quad \\text{if } z = 1 \\\\\n1 - p \\quad \\text{if } z= 0\n\\end{cases}\n\\]\nThe expected value of the bernoulli trial is equal to the probability of a a success \\(p\\).\n\\[\n\\E Z = \\P (z = 1) = p\n\\]\nThe bernoulli distribution is a special case of the Binomial distribution, which is basically the question of how many success you will get with \\(n\\) number of trials. When \\(n=1\\) (just one coin flip), we get the bernoulli distribution.\nA bernoulli distribution is often employed in randomisation of treatment during experiments, as we will see later.\n\n\n\n\n\n\nEstimators and Statistical Inference\n\nEstimators and Sampling Distributions\nAn estimand is the true value of some true parameter \\(\\theta\\) in the population we are trying to measure.\nWe often do not have data on the population. We typically have a sample from the population, and use an estimator (procedure) to produce a sample estimate \\(\\hat\\theta\\). Estimators and estimates are denoted with either a hat \\(\\hat\\theta\\) or tilde \\(\\tilde\\theta\\).\n\nFor example, if we wanted to find the average trust in institutions in the UK, we cannot possibly ask 70 million people. So, instead, we take a sample from the population, and produce a sample estimate.\n\nHowever, because of sampling variability (not all random samples will be identical), each sample \\(n\\) will have a different estimate \\(\\hat\\theta_n\\).\nImagine if we keep taking \\(N\\) number of samples, we will have \\(N\\) number of estimates \\(\\hat\\theta_1, \\hat\\theta_2, \\dots, \\hat\\theta_N\\). Thus, any specific estimate \\(\\theta_n\\) from sample \\(n\\) can be thought of as a random draw from the sampling distribution \\(\\hat\\theta_1, \\hat\\theta_2, \\dots, \\hat\\theta_N\\).\n\n\n\n\n\n\nExample of a Sampling Distribution\n\n\n\n\n\nLet us say we want to find the mean salary of all individuals in the UK. The true value of the mean salary for every individual is \\(\\theta\\).\nHowever, asking all 60 million people is nearly impossible. So, we take a randomly sample of 1000 individuals, and then find the sample mean. Our estimator is thus the sample mean estimator.\nOur first sample of 1000 individuals yields an estimate \\(\\hat\\theta_1\\). If we take another sample, we will get slightly different people in this sample, and get another estimate \\(\\hat\\theta_2\\). We keep taking samples, and get more and more estimates \\(\\hat\\theta_3, \\hat\\theta_4, \\dots, \\hat\\theta_n\\).\nWe plot all of these samples into a distribution as follows:\n\n\n\n\n\nThis indicates the potential estimates we can get. If we were to conduct only one sample, we would essentially be selecting a random \\(\\hat\\theta_i\\) value from this distribution.\nThe sampling distribution of an estimator is the key property of estimators. The two parameters of interest from this sampling distribution are its expectation and variance.\n\n\n\nWe can describe sampling distributions with their expectation and variance.\n\n\n\nFinite Sample Properties of Estimators\nAn estimator of a parameter is unbiased, if its estimates \\(\\hat\\theta_n\\) have an expectation equal to the true population value of the parameter: \\(\\E \\hat\\theta_n = \\theta\\). Or in other words, if we repeatedly sample and use the estimator, on average, the estimates will be equal to the true population value (the sampling distribution will have an expectation of the true population value).\nWe want an unbiased estimator, because if \\(\\E \\hat\\theta_n = \\theta\\), that means our “best guess” of the estimator value is the true parameter value \\(\\theta\\). That means any one estimate \\(\\hat\\theta_n\\) is on average, correct.\nUnbiasedness is not the only desirable property of estimators - we also care about the variance. After all, if we have two unbiased estimators, the one with less variance will be on average, closer to the true population value, for any one estimate \\(\\hat\\theta\\).\n\n\n\n\n\n\nExample of the Importance of Variance\n\n\n\n\n\nFor example, let us say the true population parameter is \\(\\theta = 0\\). We will have two estimators: estimator \\(A\\) and estimator \\(B\\):\n\nEstimator \\(A\\), after two samples (for simplicity), produces estimates -1 and 1.\nEstimator \\(B\\), after two samples, produces estimates -100 and 100.\n\nBoth estimators are unbiased \\(\\E \\hat\\theta_n = 0\\). However, clearly, estimator \\(A\\) is, on average, closer to \\(\\theta =0\\) than estimator \\(B\\). This is because while both estimators are unbiased, estimator \\(A\\) has a smaller variance than estimator \\(B\\) - that is on average, estimator \\(A\\)’s estimators are more closely “packed around” the expectation of the estimator.\n\n\n\nThe variance of an estimator (and variance of the sampling distribution) can be quantified as:\n\\[\n\\V \\hat\\theta_n = \\E[(\\hat\\theta_n - \\E \\hat\\theta_n)^2]\n\\]\nAn efficient estimator is one that, on average, has the closest estimated value \\(\\hat\\theta_n\\) to the true population parameter. If two estimators are both unbiased, the one with lower variance is more efficient. Efficiency can be quantified as the estimator with the lowest mean squared error:\n\\[\n\\mathrm{MSE}(\\hat\\theta_n) = \\E[(\\hat\\theta_n - \\theta)^2] =\\V \\hat\\theta_n + \\underbrace{(\\E \\theta_n - \\theta)}_{\\mathrm{bias}}\n\\]\nWe generally want an efficient estimator, since gives us the closest guess to the true population parameter \\(\\theta\\).\n\n\n\n\n\n\nEfficient but Biased\n\n\n\n\n\nInterestingly, it is possible for a biased estimator to be more efficient than an unbiased estimator.\nThis is particularly the case when the biased estimator has a slight bias but small variance, while the unbiased estimator has a giant variance. In this case, the biased estimator is producing estimates \\(\\hat\\theta\\) that on average, are closer to the true population parameter \\(\\theta\\).\n\n\n\n\n\n\nAsymptotic Properties of Estimators\nAsymptotic properties are properties of estimators as the sample size \\(n\\) approaches infinity.\nAn estimator is asymptotically consistent, if as we increase sample size towards infinity, the estimate will become more and more concentrated around the true population value \\(\\theta\\). At \\(n = ∞\\), our sampling distribution collapses to just one value, the true population value \\(\\theta\\). Mathematically:\n\\[\n\\P(|\\hat\\theta_n - \\theta|&gt; \\varepsilon) \\rightarrow 0, \\text { as } n \\rightarrow ∞\n\\]\nThat means with a consistent estimator, increasing the sample size will get us more and more accurate results.\n\n\n\n\n\n\nBiased but Consistent\n\n\n\n\n\nAn estimator can be both biased, but consistent. In smaller sample sizes, the estimator might not be on average correct, but over a large enough sample size, it will become “unbiased”.\nFor example, in the figure below, we can see that this estimator is biased at small values of \\(n\\), but as \\(n\\) increases, it becomes more consistent, collapsing its distribution around the true \\(\\theta\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaw of Large Numbers and Consistency\n\n\n\n\n\nThe law of large numbers states that the sample average of a random sample, is a consistent estimator of the population mean.\nFor example, let us say we have a random variable \\(x\\). We take a random sample of \\(n\\) units, so our sample is \\((x_1, \\dots, x_n)\\).\n\nLet us define \\(\\bar x_n\\) as our sample average.\nLet us define \\(\\mu\\) as the true population mean of variable \\(x\\).\n\nThe law of large numbers states that:\n\\[\nplim( \\bar x_n) = \\mu\n\\]\n\nWhere \\(plim\\) states that as \\(n\\) approaches infinity, the probability distribution of \\(\\bar x_n\\) collapses around \\(\\mu\\).\n\nWhy is this the case? This sample mean estimator is calculated simply through the formula for mean:\n\\[\n\\bar x_n = \\frac{1}{n}\\sum\\limits_{i=1}^n x_i\n\\]\nLet us define the variance of our sample of \\(x_1, \\dots, x_n\\) as \\(Var(x_i) = \\sigma^2\\). We can now find the variance of our sampling distribution of estimator \\(\\bar x_n\\):\n\\[\n\\begin{split}\nVar(\\bar x_n) & = Var\\left( \\frac{1}{n}\\sum\\limits_{i=1}^n x_i \\right) \\\\\n& = \\frac{1}{n^2} Var \\left(\\sum\\limits_{i=1}^n x_i\\right) \\\\\n& = \\frac{1}{n^2} \\sum\\limits_{i=1}^n Var(x_i) \\\\\n& = \\frac{1}{n^2} \\sigma^2 \\\\\n& = \\frac{\\sigma^2}{n}\n\\end{split}\n\\]\nAnd as sample size \\(n\\) increases to infinity, we get:\n\\[\n\\lim\\limits_{n \\rightarrow ∞} Var(\\bar x_n) = \\lim\\limits_{n \\rightarrow ∞} \\frac{\\sigma^2}{n} = 0\n\\]\nThus, the variance of our estimator \\(\\bar x_n\\) shrinks to zero, so as sample size increases to infinity \\(n\\), the sampling distribution of estimator \\(\\bar x_n\\) collapses around the true population mean.\n\n\n\nAnother asymptotic property of estimators, as sample size \\(n\\) approaches infinity, is that the sampling distribution approaches a normal distribution. The central limit theorem establishes asymptotic normality of estimators. Let us say we have \\(N\\) number of random variables \\(\\hat\\theta_1, \\dots, \\hat\\theta_N\\) (estimates are realisations of random variables). The central limit theorem states that:\n\\[\nP(z_n &lt; w) \\rightarrow \\Phi(z) \\quad \\text{as } n \\rightarrow ∞\n\\]\n\nWhere \\(z_n\\) is a transformed version of the random variable \\(\\hat\\theta_n\\), defined as \\(z_n = \\frac{\\bar\\theta_n - \\E \\hat\\theta_n}{\\sigma / \\sqrt{n}}\\).\nWhere \\(\\P(z_n &lt; z)\\) is the cumulative density function of the random variable \\(w_n\\).\nWhere \\(\\Phi(z)\\) is the cumulative density function (cdf) of the standard normal distribution \\(\\mathcal N(0, 1)\\).\n\nThe importance of CLM comes from the fact that as we increase sample size, our sampling distribution becomes more and more normally distributed. Recall that we know that the normal distribution has unique properties of distribution that allow us to calculate the probability between standard deviations (and any point). Since CLM says that our sampling distribution is normally distributed, we can apply the same properties to learn about the probabilities of getting a certain sample estimates \\(\\hat\\theta_n\\).\n\n\n\nIntuition of Hypothesis Testing\nWe have status-quo theory, called the null hypothesis \\(H_0\\). A status quo theory is the generally accepted value of the true population parameter \\(\\theta\\) in our field. However, you might want to prove this status-quo theory wrong, and have an alternative hypothesis \\(H_1\\). You need some way to prove that the status-quo null hypothesis is wrong.\n\n\n\n\n\n\nExample of Hypotheses\n\n\n\n\n\nLet us say we are interested in measuring the relationship between democracy and economic growth.\nThe status-quo theory is that there is no relationship - since we need to prove there is a relationship. Thus, our null hypothesis is \\(H_0 : \\theta = 0\\) (0 representing 0 relationship).\nOur alternative hypothesis might be \\(H_1: \\theta ≠ 0\\) - i.e. there is a relationship between democracy and economic growth.\nTo prove our alternative hypothesis, we need to disprove the null hypothesis.\n\n\n\nWe cannot know if the null hypothesis is wrong. However, we can calculate the probability of getting a certain sample estimate \\(\\hat\\theta_n\\) assuming the null hypothesis is true. If we assume an unbiased estimator, that means our hypothetical sampling distribution should have an expected value of our null hypothesis: \\(\\E \\hat\\theta_n = H_0\\).\nThen, we can gather a sample, and calculate some sample estimate \\(\\hat\\theta\\). Using some math (we will see this later with regression), we can also estimate the variance of the sampling distribution.\nThus, we now know the expectation of the sampling distribution \\(\\E \\hat\\theta_n = H_0\\) and the variance. We also know that the sampling distribution is normally distributed by the central limit theorem. Thus, using the properties of the normal distribution, we can calculate the probability of getting our sample estimate \\(\\hat\\theta\\) or an estimate even further from the \\(H_0\\) than our \\(\\hat\\theta\\).\n\n\n\n\n\nFor example, above, our sample estimate \\(\\hat\\theta = -2.228\\), and our null hypothesis is \\(\\theta = 0\\). The yellow parts highlighted are the probability of getting an sample estimate \\(\\hat\\theta_n\\) as extreme or more extreme than our \\(\\hat\\theta = -2.228\\).\nIf this probability of getting our estimate \\(\\hat\\theta\\) or more extreme is very small (less than 5% typically), we can believe either one of two things:\n\nWe got extremely lucky (less than 5% chance) and got a estimate very far from the null hypothesis.\nOr, we typically believe that we were not actually lucky, but instead, the null hypothesis value of \\(\\theta\\) is incorrect.\n\nThus, conclusion 2 essentially means we reject the null hypothesis, and conclude our alternative hypothesis.\n\n\n\nMechanics of Hypothesis Testing\nTo start a hypothesis test, you will need to define a status-quo null hypothesis \\(H_0\\) and an alternative hypothesis \\(H_1\\).\nThen, you should take a sample from the population, and compute a sample estimate \\(\\hat\\theta\\) with your estimator. You can then derive a standard error (square root of the variance of the sampling distribution \\(\\V \\hat\\theta_n\\)). This standard error formula will differ depending on the statistical model, and we will introduce several of these for regression.\nThen, you can compute a test statistic. The name of the test-statistic will differ based on the statistical model, but the formula is always the same:\n\\[\n\\text{test statistic} = \\frac{\\hat\\theta - H_0}{\\text{standard error of } \\hat\\theta}\n\\]\nThis formula might look familiar - we are essentially turning our normal distribution of the estimator into the standard normal distribution. The test statistic is essentially measuring how many standard deviations away from the null hypothesis our sample estimate \\(\\hat\\theta\\) is.\n\n\n\n\n\nNow, go onto the x-axis of our standard normal distribution, and start at the mean (at 0). Now, move in both directions by the distance specified by the test statistic. Mark these two points on either side of the mean. Highlight the are under the sampling distribution beyond these points. The figure below shows a test statistic of 2.228:\n\n\n\n\n\nThe highlighted area is the probability (p-value) of getting a sample estimate \\(\\hat\\theta\\) equal or more extreme than the one we got, assuming the null hypothesis is true. If the p-value is below 0.05 (5%), we have sufficient evidence to reject the null hypothesis. If the p-value is above 0.05 (5%), we cannot reject the null hypothesis.\n\n\n\n\n\n\nNote on Distributions\n\n\n\n\n\nHere, we have stated you use the standard normal distribution.\nHowever, with different tests, you might actually use some different distribution. Most often, you will use the t-distribution.\nThe reason we use the t-distribution is because we often cannot actually calculate the variance of our sampling distribution. Instead, we estimate it with some uncertainty, so the t-distribution accounts for this uncertainty.\n\n\n\n\n\n\nConfidence Intervals\nSometimes, we are not just interested in proving a null hypothesis wrong. We often are interested in the actual parameter value of \\(\\theta\\). As we know already, there is variation in our estimates \\(\\hat\\theta_n\\) between samples.\nThus to account for this uncertainty, we want to create some range around our sample estimate \\(\\hat\\theta\\) that is likely to contain the true value of \\(\\theta\\). This is called a confidence interval.\nAccording the the central limit theorem, we know that any sample estimate \\(\\hat\\theta_n\\) is 95% likely to be within 2 standard deviations of the true population value \\(\\theta\\).\nThus, we can construct an interval of 2 standard errors (square deviations) on both sides of our sample estimate \\(\\hat\\theta_n\\). This means that 95% of the time, our interval will include the true population value of \\(\\theta\\). Thus, our 95% confidence interval is:\n\\[\n(\\hat\\theta - 1.96se(\\hat\\theta), \\ \\ \\hat\\theta+1.96se(\\hat\\theta)\n\\]\n\nWhy 1.96? Because exactly 95% of a normal distribution is within 1.96 standard deviations of the mean.\n\nThis interval means that under repeated sampling and estimating \\(\\hat\\theta_n\\), 95% of the confidence intervals we construct will include the true population \\(\\theta\\) value.\n\n\n\nNonparametric Bootstrap\nMost traditional statistical tests rely on asymptotic normality established by the central limit theorem. However, asymptotic normality can only be satisfied if we have a large enough sample size. When we are dealing with small samples, we cannot invoke central limit theorem.\nNonparametric Bootstrap, instead of assuming some sampling distribution, is a method to simulate the sampling distribution. This is done by re-sampling from the sample with replacement. The procedure is as follows:\n\nYou take the sample you observe (with sample size \\(n\\)), and randomly re-sample \\(n\\) observations from that sample with replacement (so allowing observations to repeat in our re-sample).\nContinue to do this over and over again to get \\(B\\) number of re-samples.\nFor each re-sample \\(b\\), you should calculate the \\(\\widehat{\\theta_b}\\). Plot all of the sample \\(\\widehat{\\theta_b}\\) in a distribution.\n\nYou can also estimate the standard error of \\(\\hat\\theta\\) using the standard deviation of the distribution. However, do not use these standard errors for confidence intervals or tests unless you are confident the sampling distribution is approximately normal.\nNonparametric Bootstrap is also used in some more complex estimators where it is very difficult to calculate or estimate the standard errors.\n\n\n\n\n\n\nCorrelations Between Variables\n\nCorrelated Variables\nIn political science and most social sciences, our primary concern is relationships between variables. How does education correlate with voter turnout? Are less educated citizens more anti-immigrant? How do inflation levels correlate with incumbent popularity?\nWe describe a relationship between two variables \\(X_i\\) and \\(Y_i\\) as a correlation (from now on, all random variables are denoted with a subscript).\n\nIf we are more likely to observe higher values of \\(Y_i\\) when we also observe higher values of \\(X_i\\), that means we have a positive correlation.\nIf we are more likely to observe lower values of \\(Y_i\\) when we also observe higher values of \\(X_i\\), that means we have a negative correlation.\nIf the value of \\(Y_i\\) does not change no matter the value of \\(X\\), then the two variables are uncorrelated.\n\nImportant Note: correlation is not causation! We can view correlations between variables graphically:\n\n\n\n\n\n\n\nQuantifying Correlations\nThere are two primary ways to quantify correlations between variables. The first is called covariance:\n\\[\ncov(X_i,Y_i)  = \\E((X_i - \\E X_i)(Y_i - \\E Y_i)) \\\\\n\\]\nIf covariance is negative, that means we have a negative correlation. If covariance is positive, we have a positive correlation. If covariance is 0, we have no correlation.\nHowever, we cannot interpret the actual number of covariance, only the sign. This is because covariance is sensitive to measurement scale: if we change something measured in feet to inches, covariance increases.\nObviously, we do not want a measure that is affected by measurement scale. We want some measure of correlation that can be compared across different scales. This is where the correlation coefficient comes in:\n\\[\n\\rho = \\frac{cov(X_i, Y_i)}{\\sqrt{\\V X \\V Y}}\n\\]\nEssentially, we are “normalising” the covariance metric by dividing by the variances, which gets rid of the impact of scale.\nCorrelation coefficients are always between -1 and 1. Values closer to -1 and 1 are stronger correlations, and values closer to 0 are weaker correlations.\n\n\n\n\n\nWe can see here that stronger correlations have points that fit closer to a straight line.\n\n\n\nBest-Fit Lines and Magnitude\nCorrelation coefficients have a huge weakness: they measure the “strength” of correlation, but not the magnitude.\n\n\n\n\n\nWe can see above that these two relationships have the same strength of correlation. However, the slope of the left graph is much higher. Why is the slope important? Well, the slope is essentially how much \\(Y_i\\) changes when \\(X_i\\) increases by 1. It is the magnitude of our relationship.\nThus, we might wish to instead look at relationships with a best-fit straight line. Take this figure below:\n\n\n\n\n\nWe can express this line mathematically as \\(y = mx + b\\), where \\(m\\) is the slope - the amount \\(y\\) change for every one unit increase in \\(x\\). Thus, the slope \\(m\\) is a measurement of the magnitude of correlation between \\(x\\) and \\(y\\).\n\n\n\nSimple Linear Regression\nSimple Linear Regression is a mathematical model of a best fit line. It takes the following form:\n\\[\nY_i = \\beta_0 + \\beta_1X_i + \\eps_i\n\\]\n\nWhere \\(X_i\\) and \\(Y_i\\) are random variables.\nWhere \\(\\beta_1\\) is the slope - the measurement of the relationship between \\(x\\) and \\(y\\). \\(\\beta_0\\) is the intercept (y-intercept).\n\\(\\eps_i\\) is the error term: the vertical distance between the actual points and the best-fit line.\n\nWe have some special terminology for the parts of the simple linear regression. The independent variable, also called the explanatory variable or the regressor, is labelled \\(X_i\\). This is the variable multiplied to the slope parameter \\(\\beta_1\\). The dependent variable, also called the response or outcome variable, is labelled \\(Y_i\\).\nWe can also write regression as the conditional expectation \\(\\E(Y_i | X_i)\\) of the conditional distribution.\n\\[\n\\E(Y_i|X_i) = \\beta_0 + \\beta_1 X_i\n\\]\nThroughout this chapter, we have talked about statistical inference and estimators. This is no different in linear regression. The above equation is what we call our population model (also called the data generating process):\n\\[\nY_i = \\beta_0 + \\beta_1X_i + \\eps_i\n\\]\nHowever, once again, we cannot know the true population values of our intercept and slope. So, we will have a sample model of the following:\n\\[\nY_i = \\hat\\beta_0 + \\hat\\beta_1X_i + \\hat\\eps_i\n\\]\nWe will discuss the details of this estimation process of \\(\\hat\\beta_0, \\hat\\beta_1, \\hat\\eps_i\\) in the next chapter, and how we can conduct statistical inference and tests. The points on our predicted best-fit line (so excluding error \\(\\widehat{u_i}\\)) are called our fitted values:\n\\[\n\\hat Y_i = \\hat\\beta_0 + \\hat\\beta_1X_i\n\\]\nThe simple linear regression model is very simple. In the real world, rarely does only one thing cause another. For example, changes in income could be caused by age, but also education. Instead of just one explanatory variable \\(X_i\\), we can create regression models with multiple explanatory variables \\(X_{i1}, X_{i2}, \\dots, X_{ip}\\). This is the multiple linear regression that we will introduce in the quantitative methods section.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Background Statistics"
    ]
  },
  {
    "objectID": "z1.html",
    "href": "z1.html",
    "title": "Classical Least Squares Theory",
    "section": "",
    "text": "Last chapter, we discussed the multiple linear regression model, and how it can help us measure relationships between explanatory and outcome variables.\nThis chapter introduces some key theory regarding the ordinary least squares estimator behind linear regression. Topics covered includes properties of estimators, the OLS estimator, and the Method of Moments estimator.\nUse the right sidebar for quick navigation. This chapter is heavy on linear algebra, so consulting the linear algebra reference is useful.\n\n\nWeighted Least Squares\nLet us say that the heteroscedastic variance of the errors takes the form \\(\\V(\\eps_i|X_i) = c \\sigma^2_i\\), where \\(c\\) is some unknown constant, and \\(\\sigma^2_i = h(X_i)\\) is some known function of the regressors that determines the variance of the error. Using a lemma \\(\\V(a(x) \\eps | x) = a(x)^2 \\V(\\eps|x)\\), we can determine that:\n\\[\n\\V \\left( \\frac{\\eps_i}{\\sqrt{\\sigma^2_i}} | X_i \\right) = \\left( \\frac{1}{\\sqrt{\\sigma^2_i}}\\right)^2 \\V (\\eps_i|X_i ) =c\n\\]\nSo this tells us that if we create a new error term \\(\\eps_i^* = \\eps_i / \\sqrt{\\sigma_i^2}\\), our variance of the new error term \\(\\eps_i^*\\) will be homoscedastic (constant at \\(c\\) no matter the value of \\(X_i\\)). What this implies is that if we divide all terms of our regression by the heteroscedastic errors \\(\\sqrt{\\sigma_i^2} = \\sigma_i\\), we can get a new regression with homoscedasticity met. Let us define \\(\\Omega^{-1/2}\\) as the inverse of the square root of the heteroscedastic covariance matrix of errors.\n\\[\n\\Omega^{-1/2} = \\begin{pmatrix}\n1/\\sigma_1 & 0 & \\dots & 0 \\\\\n0 & 1/\\sigma_2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots  \\\\\n0 & 0 & \\dots & 1/\\sigma_n\n\\end{pmatrix}\n\\] Using this formula, we can write the transformed regression equation as:\n\\[\n\\underbrace{\\Omega^{-1/2}y}_{y^*}  = \\underbrace{\\Omega^{-1/2}X}_{X^*} \\beta+ \\underbrace{\\Omega^{-1/2}\\eps}_{\\eps_i^*}\n\\]\nAnd since this is a homoscedastic regression, we can use OLS on this new regression and have a BLUE estimator:\n\\[\n\\begin{align}\n\\hat\\beta & = (X^{*\\top} X^*)^{-1}X^{*\\top}y^* && \\text{(OLS estimator solution)}  \\\\\n& = \\left[ (\\Omega^{-1/2}X)^\\top (\\Omega^{-1/2}X)\\right]^{-1} (\\Omega^{-1/2}X)^\\top (\\Omega^{-1/2}y) && (\\text{plug in }X^*, y^*) \\\\\n& = [X^\\top \\Omega^{-1/2}\\Omega^{-1/2}X]^{-1}X^\\top\\Omega^{-1/2}\\Omega^{-1/2}y && \\text{(matrix transposes)}\\\\\n& =(X^\\top \\Omega^{-1} X)^{-1}X^\\top \\Omega^{-1}y && (\\because \\Omega^{-1/2} \\Omega^{-1/2} = \\Omega^{-1})\n\\end{align}\n\\]\nThis weighted least squares estimator is BLUE. This estimator, under the name of the generalised least squares (GLS) estimator, can also be applied to autocorrelated \\(\\Omega\\) and is BLUE in that case. Essentially, the estimator is weighting observations with lower \\(\\sigma_i\\) higher, and observations with higher \\(\\sigma_i\\) lower.\n\n\n\nFeasible Generalised Least Squares\nWe saw that our weighted least squares estimator (and GLS estimator) requires us to know \\(\\Omega^{-1/2}\\), which implies we need to know the covariance-variance matrix of the error terms \\(\\Omega\\). If you already know the form \\(\\Omega\\) for whatever reason, the task becomes easy.\nHowever, if you do not know \\(\\Omega\\), you will need to estimate \\(\\hat\\Omega\\). This also means our actual “feasible” estimator will not be BLUE. Feasible weighted least squares (with only heteroscedasticity) is generally still more efficient than OLS, however for autocorrelation GLS, this is not the case.\nWe typically do a two stage approach of estimating \\(\\hat\\Omega\\). First, you run a normal OLS regression (which remains unbiased). This OLS regression allows you to obtain \\(\\hat\\eps_i^2 \\approx \\sigma^2_i\\), which you can square root to get our estimate of \\(\\sigma_i\\) for creating \\(1/\\sigma_i\\).\nAnother alternative is to use \\(1/\\sqrt{|\\hat\\eps_i|}\\), as an correlated value to \\(1/\\sigma_i\\). The reason you might want to do this is that taking the original variance estimate involves squaring the OLS residuals \\(\\hat\\eps_i\\), which can increase the power of outliers. Even though \\(\\sqrt{|\\hat\\eps_i|} ≠ \\sigma_i\\), they are correlated and can get a similar result.\nWhen doing two stage estimation, you should check that your first stage OLS \\(\\hat\\beta\\) and your second stage weighted least squares \\(\\hat\\beta\\) are similar. If they are not, this could mean the presence of outliers/issues in your estimation of \\(\\Omega\\).\nThe complicated nature of weighted least squares in practice makes it less popular than just using OLS with robust standard errors. However, if done correctly, feasible weighted least squares is unbiased and more efficient than OLS, even when taking into the uncertainty and efficiency loss due to \\(\\hat\\Omega\\).\nFor autocorrelation \\(\\Omega\\) and feasible GLS, things become less beneficial. There is little evidence to suggest that feasible GLS with autocorrelation is as efficient as OLS, due to difficulties in estimating \\(\\hat\\Omega\\).\nWhile weighted least squares and GLS is not used super frequently, it is still a valuable method to understand, as we will return to weighted estimators in the causal inference section.\n\n\nOrdinary Least Squares Estimator\n\nDeriving the Estimator\nOur linear regression model, and the fitted values \\(\\hat y\\), take the following form:\n\\[\ny = X\\beta + \\eps, \\quad \\hat y = X\\hat\\beta\n\\]\nOLS minimises the sum of squared residuals \\(S(\\hat{\\boldsymbol\\beta})\\) - the differences between the actual \\(\\mathbf y\\) and our predicted \\(\\hat{\\mathbf y}\\):\n\\[\n\\begin{align}\nS(\\hat\\beta) &  = (y - \\hat y)^\\top (y - \\hat y)\\\\\n& = (y - \\color{blue}{X \\hat\\beta}\\color{black} )^\\top (y - \\color{blue}{X \\hat\\beta}\\color{black})  && (\\because \\color{blue}{\\hat y = X \\hat\\beta}\\color{black})\\\\\n& = y^\\top y - \\hat\\beta^\\top X^\\top y - y^\\top X \\hat\\beta +  \\hat\\beta^\\top X^\\top X \\hat\\beta && (\\text{distribute out}) \\\\\n& = y^\\top y \\ \\color{blue}{-  2 \\hat\\beta^\\top X^\\top y} \\color{black}  +  \\underbrace{\\hat\\beta^\\top X^\\top X \\hat\\beta}_{\\text{quadratic}} && (\\because \\color{blue}{- \\hat\\beta^\\top X^\\top y - y^\\top X \\hat\\beta = - 2 \\hat\\beta^\\top X^\\top y} \\color{black})\n\\end{align}\n\\]\nNow, let us take the gradient to find the first order condition:\n\\[\n\\frac{\\partial S(\\hat\\beta)}{\\partial \\hat\\beta} = -2 X^\\top y + 2 X^\\top X \\hat\\beta = 0\n\\]\nWhen assuming \\(X^\\top X\\) is invertable (which is true if \\(X\\) is full rank), we can isolate \\(\\hat\\beta\\) to find the solution to OLS:\n\\[\n\\begin{align}\n-2 X^\\top y + 2 X^\\top X \\hat\\beta & = 0 \\\\\n2 X^\\top X \\hat\\beta & = 2 X^\\top y && ( + 2X^\\top y \\text{ to both sides}) \\\\\n\\hat\\beta & = (2X^\\top X)^{-1} -2 X^\\top y && (\\times (2X^\\top X)^{-1} \\text{ to both sides}) \\\\\n\\hat\\beta & = (X^\\top X)^{-1} X^\\top y && (2^{-1}, 2 \\text{ cancel out})\n\\end{align}\n\\tag{1}\\]\nThose are our coefficient solutions to OLS.\n\n\n\n\n\n\nAlternative Derivation for Simple Linear Regression\n\n\n\n\n\nCurrently, we are deriving the first order conditions for multiple linear regression using linear algebra.\nFor simple linear regression (with one explanatory variable), we can use summation notation. Recall our sum of squared residuals in summation form:\n\\[\nSSR = S(\\hat\\beta_0, \\hat\\beta_1)= \\sum\\limits_{i=1}^n(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i)^2\n\\]\nWe want to minimise the SSR in respect to both \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\). We can do this by finding our first order conditions:\n\\[\n\\begin{align}\n\\frac{\\partial S(\\hat\\beta_0, \\hat\\beta_1)}{\\partial \\hat\\beta_0} & = \\sum\\limits_{i=1}^n(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i) = 0 \\\\\n\\frac{\\partial S(\\hat\\beta_0, \\hat\\beta_1)}{\\partial \\hat\\beta_1} & = \\sum\\limits_{i=1}^n X_i(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i) = 0 \\\\\n\\end{align}\n\\]\nThese conditions create a system of equations, which you can solve for the OLS solutions of \\(\\widehat{\\beta_0}\\) and \\(\\widehat{\\beta_1}\\). I will not show it step by step, as it is tedious (and not that important). The OLS solutions are\n\\[\n\\begin{align}\n\\hat\\beta_0 & = \\bar Y - \\widehat{\\beta_1} \\bar X \\\\\n\\hat\\beta_1 & = \\frac{\\sum_{i=1}^n(X_i - \\bar X)(Y_i - \\bar Y)}{\\sum_{i=1}^n(X_i - \\bar X)^2} = \\frac{Cov(X_i, Y_i)}{\\V Y_i}\n\\end{align}\n\\]\n\n\n\n\n\n\nProjection and Residual Maker\nWe can use the OLS solution from Equation 1 to get our fitted values \\(\\hat{y}\\):\n\\[\n\\begin{align}\n\\hat y & = X\\hat\\beta \\\\\n& = X \\color{blue}{(X^\\top X)^{-1}X^\\top y} && \\color{black}(\\because \\color{blue}{\\hat\\beta = (X^\\top X)^{-1}X^\\top y} \\color{black}) \\\\\n& = \\color{red}{P}\\color{black}y && (\\because \\color{red}{P:= X(X^\\top X)^{-1}X^\\top})\n\\end{align}\n\\tag{2}\\]\nMatrix \\(\\color{red}{P}\\), called the projection matrix, is a matrix operator that performs the linear mapping \\(y \\rightarrow \\hat{ y}\\).\nWe can also use the OLS solution from Equation 1 to get our residuals \\(\\hat{\\eps}\\) (note \\(I\\) is the identity matrix):\n\\[\n\\begin{align}\n\\hat\\eps & = y - \\hat y  \\\\\n& = y - \\color{blue}{Py} && \\color{black}( \\because \\color{blue}{\\hat y = Py}\\color{black}) \\\\\n& = (I-P)y && (\\text{factor out y}) \\\\\n& = \\color{purple}{M}\\color{black}y && (\\because \\color{purple}{M:= I - P}\\color{black})\n\\end{align}\n\\]\nMatrix \\(\\color{purple}{M}\\), called the residual maker, is a matrix operator that performs the linear mapping \\(y \\rightarrow \\hat{\\eps}\\).\nBoth \\(\\color{red}{P}\\) and \\(\\color{purple}{M}\\) are symmetric matrices: \\(P^\\top = P, \\ M^\\top = M\\). They are also both idempotent matrices: \\(PP = P, \\ MM = M\\). We can prove this second statement using the first (I will only do it for \\(P\\), but the same applies for \\(M\\):\n\\[\n\\begin{align}\nPP & = X(X^\\top X)^{-1} \\underbrace{X^\\top X(X^\\top X)^{-1}}_{= I} X^\\top \\\\\n& = X(X^\\top X)^{-1} X^\\top = P\n\\end{align}\n\\tag{3}\\]\n\\(\\color{red}{ P}\\) and \\(\\color{purple}{ M}\\) are also orthogonal to each other - i.e. \\(P^\\top M = 0\\):\n\\[\n\\begin{align}\nP^\\top M & = \\color{blue}{P}\\color{black}M && (\\because \\color{blue}{P^\\top = P}\\color{black}) \\\\\n& = P(\\color{blue}{I-P}\\color{black}) && (\\because \\color{blue}{M:= I - P}\\color{black}) \\\\\n& = P - PP && \\text{(distribute out)} \\\\\n& = P - \\color{blue}{P} && \\color{black}(\\because \\color{blue}{PP = P}\\color{black}) \\\\\n& = 0\n\\end{align}\n\\]\n\n\n\nOrthogonal Projection of OLS\nWe know that our fitted values \\(\\hat{y}\\) are created as a linear combination of our explanatory variables \\(X\\):\n\\[\n\\hat Y_i = \\hat\\beta_0 + \\hat\\beta_1X_{i1} + \\dots + \\hat\\beta_pX_{ip}\n\\]\nThat means, by the definition of vector spaces, that our explanatory variable vectors \\(x_1, x_2, \\dots, x_p\\) span a space that includes our fitted values vector \\(\\hat{y}\\). So, what \\(\\color{red}{ P}\\) is doing is taking our original data vector \\(y\\), and projecting it into the space spanned by our explanatory variables \\(X\\) (called the column space).\nWe can see in the figure below, our observed \\(y\\) vector is being projected onto the blue plane spanned by \\(X\\) to create our fitted values vector \\(\\hat{y}\\).\n\n\n\n\n\nResidual maker matrix \\(\\color{purple}{M}\\) projects \\(y\\) onto the space orthogonal to the column space of \\(X\\) to get our residuals \\(\\hat{\\eps}\\). We can see this in the figure above, where the residuals vector (notated \\(\\mathbf e\\) in the figure) is orthogonal/perpendicular to the space of \\(\\mathbf X\\).\n\n\n\nError Covariance Matrix\nAside from the population parameters \\(\\beta\\), there is another part of the linear model that needs to be estimated: the population covariance matrix of error terms \\(\\eps_1, \\dots, \\eps_n\\):\n\\[\n\\underbrace{\\V(\\eps|X)}_{\\mathrm{cov. \\ matrix}} = \\begin{pmatrix}\n\\V\\eps_1 & cov(\\eps_1, \\eps_2) & cov(\\eps_1, \\eps_3) & \\dots \\\\\ncov(\\eps_2, \\eps_1) & \\V\\eps_2 & cov(\\eps_2, \\eps_3) & \\dots \\\\\ncov(\\eps_3, \\eps_1) & cov(\\eps_3, u_2) & \\V\\eps_3 & \\vdots \\\\\n\\vdots & \\vdots & \\dots & \\ddots\n\\end{pmatrix}\n\\]\nUnder the assumption of independence of observations (a key assumption of the linear model), the covariance elements should all equal 0, i.e. \\(cov(\\eps_i, \\eps_k) = 0, \\ \\forall \\ i, k\\). This assumption is also called no autocorrelation.\nThus, under this assumption, we have a diagonal matrix.\n\\[\n\\underbrace{\\V(\\eps| X)}_{\\mathrm{cov. \\ matrix}} = \\begin{pmatrix}\n\\V\\eps_1 & 0 & 0 & \\dots \\\\\n0 & \\V\\eps_2 & 0 & \\dots \\\\\n0 & 0 & \\V \\eps_3 & \\vdots \\\\\n\\vdots & \\vdots & \\dots & \\ddots\n\\end{pmatrix}\n\\]\nWe are not really going to discuss what happens when autocorrelation is present, as generally for many of our purposes, ruling out autocorrelation is okay. However, if you are interested in time series (common in economics), or spatial statistics, these are types of data that frequently have autocorrelation issues, and this creates further complications.\n\n\n\nHomoscedasticity and Heteroscedasticity\nNow, there are two possible forms of our covariance matrix of errors. Homoscedasticity assumes that every single unit \\(i\\) has the same variance in error \\(\\V \\eps_i = \\sigma^2\\). Or in other words, the error of \\(\\eps_i\\) does is independent of which unit \\(i\\) (which also means \\(\\eps_i \\perp\\!\\!\\!\\perp X\\)). This assumption (with no autocorrelation) is also called Spherical Errors.\n\\[\n\\V(\\eps | X) = \\sigma^2 I_n = \\begin{pmatrix}\n\\sigma^2 & 0 & \\dots & 0 \\\\\n0 & \\sigma^2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & 0 \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{pmatrix}\n\\tag{4}\\]\n\n\n\n\n\n\nVisualisation of \\(\\sigma^2\\)\n\n\n\n\n\nBelow is a figure illustrating different residual standard deviations, with the same best-fit line.\n\n\n\n\n\n\n\n\n\\(\\sigma^2\\) is a population parameter. We can estimate it with the following unbiased estimator \\(s^2 = \\frac{\\hat{\\eps}^\\top \\hat{\\eps}}{n-k-1}\\). This is the reason we use a t-distribution in hypotheses tests - to account for the uncertainty of this estimator. We will show this later when deriving variance.\nHeteroscedasticity is when we do not believe the assumption of a constant variance for all units. Instead, we assume each unit \\(i = 1, \\dots, n\\) has their own variance \\(\\sigma^2_1, \\dots, \\sigma^2_n\\), which also implies that \\(\\eps_i\\) is dependent on \\(X\\) values for unit \\(i\\).\n\\[\n\\V(\\eps| X) = \\begin{pmatrix}\n\\sigma^2_1 & 0 & \\dots & 0 \\\\\n0 & \\sigma^2_2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & 0 \\\\\n0 & 0 & \\dots & \\sigma^2_n\n\\end{pmatrix}\n\\tag{5}\\]\nWe can estimate each individual population parameter \\(\\sigma^2_i\\) with \\(s^2_i = \\hat\\eps_i^2\\).\n\n\n\n\n\n\nVisualisation of Homoscedasticity\n\n\n\n\n\nAn easy way to identify homoscedasticity is to look at a residual plot (just the plot of all \\(\\hat\\eps_i\\)):\n\n\n\n\n\nNotice how the homoscedasticity residuals seem to have the same up-down variance, no matter the value of \\(X_i\\).\nThe heteroscedasticity residuals have a clear pattern - the up-down variance is smaller when \\(X_i\\) is smaller, and the up-down variance is larger when \\(X_i\\) is larger.\nEssentially, if you see a pattern in the residual plot, it is likely heteroscedasticity.\n\n\n\n\n\n\n\n\n\nSample Properties of OLS\n\nOLS as an Unbiased Estimator\nOLS is an unbiased estimator of the relationship between any \\(X_{ij}\\) and \\(Y_i\\) under 4 conditions:\n\nLinearity in parameters: the population model can be modelled as \\(y = X\\beta + \\eps\\).\nRandom Sampling: our sample is randomly sampled (implying independence of observations).\nNo Perfect Multicolinearity: There is no exact linear relationships between the regressors. This ensures that \\(X^\\top X\\) is invertible, which is required for the derivation of OLS.\nZero Conditional Mean: \\(\\E(\\eps| X) = 0\\). This implies that no \\(X_{ij}\\) is correlated with \\(\\eps\\) (exogeneity), and no function of multiple regressors is correlated with \\(\\eps\\).\n\nLet us prove OLS is unbiased - i.e. \\(\\E\\hat\\beta = \\beta\\). Let us manipulate our OLS solution:\n\\[\n\\begin{align}\n\\hat\\beta & = (X^\\top X)^{-1} X^\\top y \\\\\n& = (X^\\top X)^{-1} X^\\top(\\color{blue}{X\\beta + \\eps}\\color{black}) && (\\because \\color{blue}{y = X\\beta + \\eps}\\color{black}) \\\\\n& = \\underbrace{(X^\\top X)^{-1} X^\\top X}_{= \\ I}\\beta + (X^\\top X)^{-1}X^\\top \\eps && \\text{(multiply out)} \\\\\n& = \\beta + (X^\\top X)^{-1}X^\\top \\eps\n\\end{align}\n\\tag{6}\\]\nNow, let us take the expectation of \\(\\hat\\beta\\) conditional on \\(X\\). Remember condition 4, \\(\\E(\\eps | X) = 0\\):\n\\[\n\\E(\\hat\\beta | X) = \\beta + (X^\\top X)^{-1} \\underbrace{\\E(\\eps | X)}_{= \\ 0} \\  = \\ \\beta\n\\]\nNow, we can use the law of iterated expectations (LIE) to conclude this proof:\n\\[\n\\begin{align}\n\\E \\hat\\beta & = \\E(\\E(\\hat\\beta|X)) && (\\because \\mathrm{LIE}) \\\\\n& = \\E(\\color{blue}{\\beta}\\color{black}) && (\\because \\color{blue}{\\E(\\hat\\beta | X = \\beta)}\\color{black}) \\\\\n& = \\beta && \\text{(expectation of a constant)}\n\\end{align}\n\\]\nThus, OLS is unbiased under the 4 conditions above.\n\n\n\nDeriving Variance\nWe want to find the variance of our estimator, \\(\\V(\\hat\\beta | X)\\). First, let us start off where we left off in Equation 6 .\n\\[\n\\begin{align}\n& \\hat\\beta = \\beta + (X^\\top X)^{-1} X^\\top \\eps \\\\\n& \\V(\\hat\\beta | X) = \\V(\\beta + (X^\\top X)^{-1} X^\\top \\eps)\n\\end{align}\n\\]\n\n\n\n\n\n\nLemma: Variance\n\n\n\n\n\nIf \\(\\eps\\) is an \\(n\\) dimensional vector of random variables, \\(c\\) is an \\(m\\) dimensional vector, and \\(B\\) is an \\(n \\times m\\) dimensional matrix with fixed constants, then the following is true:\n\\[\n\\V(c + B\\eps) =  B \\V(\\eps) B^\\top\n\\tag{7}\\]\nI will not prove this lemma here, but it is provable.\n\n\n\n\\(\\beta\\) is a vector of fixed constants. \\((X^\\top X)^{-1} X^\\top \\eps\\) can be imagined as a matrix of fixed constants, since we are conditioning the variance on \\(X\\) (so for each \\(X\\), it is fixed). With the Lemma above:\n\\[\n\\begin{align}\n\\V (\\hat\\beta | X) & = (X^\\top X)^{-1}X^\\top \\V(\\eps|X) [(X^\\top X)^{-1}X^\\top]^{-1} && \\text{(lemma)} \\\\\n& = (X^\\top X)^{-1}X^\\top \\V(\\eps|X) \\color{blue}{X(X^\\top X)^{-1}} && \\color{black}(\\because \\color{blue}{[(X^\\top X)^{-1}X^\\top]^{-1} = X(X^\\top X)^{-1}} \\color{black})\n\\end{align}\n\\]\nFrom here on, homoscedasticity and heteroscedasticity matter. Let us first start by deriving variance with homoscedasticity, using the definition given by Equation 4 :\n\\[\n\\begin{align}\n\\V (\\hat\\beta | X) & = (X^\\top X)^{-1}X^\\top \\color{blue}{\\sigma^2I_n}\\color{black}{X} (X^\\top X)^{-1} && (\\because \\color{blue}{\\V(\\eps|X) = \\sigma^2 I_n}\\color{black}) \\\\\n& = \\color{blue}{\\sigma^2}\\color{black}{\\underbrace{(X^\\top X)^{-1}X^\\top X}_{= \\ I}(X^\\top X)^{-1}} && \\text{(rearrange and simplify)} \\\\\n& = \\sigma^2 (X^\\top X)^{-1}\n\\end{align}\n\\]\nNow, let us calculate the variance for when heteroscedasticity is present, as defined by Equation 5 :\n\\[\n\\V(\\hat{\\beta}| X)  = (X^\\top X)^{-1} X^\\top \\color{blue}{\\begin{pmatrix}\n\\sigma^2_1 & 0 & \\dots & 0 \\\\\n0 & \\sigma^2_2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & 0 \\\\\n0 & 0 & \\dots & \\sigma^2_n\n\\end{pmatrix}}\\color{black} X ( X^\\top X)^{-1}\n\\]\nTo calculate both sets of standard errors (normal and robust), we estimate \\(\\sigma^2\\) and \\(\\sigma_i^2\\) as discussed previously. The standard errors are then the square root of our estimated variances.\n\n\n\nGauss-Markov Theorem\nThe Gauss-Markov Theorem states that the OLS estimator is the best linear unbiased estimator (BLUE) - the unbiased linear estimator with the lowest variance, under 5 conditions: linearity, random sampling, no perfect multicollinearity, zero-conditional mean, and homoscedasticity.\nAny linear estimator takes the form \\(\\tilde{\\beta} = Cy\\). For example, the OLS estimator is of the form \\(\\hat\\beta = (X^\\top X)^{-1} X^\\top y\\), which is the same as form as \\(Cy\\) if you define \\(C:= ( X^\\top X)^{-1} X^\\top\\). For any linear estimator \\(\\tilde{\\beta} = Cy\\) to be unbiased, we need to assume \\(\\color{red}{CX = I}\\).\n\n\n\n\n\n\nProof \\(CX = I\\) For a Unbiased Linear Estimator\n\n\n\n\n\nFor any linear estimator \\(\\tilde{\\beta} = Cy\\) to be unbiased, we need to assume \\(\\color{red}{CX = I}\\). The proof of this is as follows:\n\\[\n\\begin{align}\n\\tilde\\beta =  C & (\\color{blue}{C\\beta + \\eps}\\color{black}) && (\\because \\color{blue}{y = X\\beta + \\eps}\\color{black}) \\\\\n=  C & X\\beta + C\\eps && \\text{(multiply out)} \\\\\n\\E(\\tilde\\beta | X) & = \\E(C X\\beta + C\\eps) \\\\\n& = CX\\beta + C \\underbrace{\\E(\\eps | X)}_{= \\ 0} && \\text{(take constants out of exp.)} \\\\\n& = CX\\beta \\\\\n& = \\color{red}{I}\\color{black}\\beta = \\beta && (\\because \\color{red}{CX = I}\\color{black}) \\\\\n\\E \\tilde\\beta & = \\E( \\E(\\tilde\\beta|X)) && \\text{(law of iterated expect.)} \\\\\n& = \\E(\\color{blue}{\\beta}\\color{black}) && (\\because \\color{blue}{\\E(\\tilde\\beta|X) = \\beta}\\color{black}) \\\\\n& = \\beta && \\text{(expect. of a constant)}\n\\end{align}\n\\]\nThus, we have shown \\(\\color{red}{CX = I}\\) is a necessary condition for any linear estimator \\(\\tilde{\\beta} = Cy\\) to be unbiased.\n\n\n\nNow, let us calculate the variance of \\(\\tilde{\\beta}\\), taking into consideration the lemma (Equation 7) used in the OLS variance:\n\\[\n\\begin{align}\n\\V(\\tilde\\beta | X) & = \\V(Cy|X) \\\\\n& = \\V(C(\\color{blue}{X\\beta + \\eps}\\color{black})|X) && (\\because \\color{blue}{y = X\\beta + \\eps}\\color{black}) \\\\\n& = \\V(\\underbrace{CX}_{= I}\\beta + C\\eps | X) && \\text{(multiply out)} \\\\\n& = \\V(\\beta + C\\eps | X) \\\\\n& = C \\V(\\eps | X) C^\\top && \\text{(using lemma)} \\\\\n& = C \\color{blue}{\\sigma^2 I_n} \\color{black} C^\\top && (\\mathrm{homoscedasticity} \\ \\color{blue}{\\V(\\eps|X) = \\sigma^2 I_n}\\color{black}) \\\\\n& = \\sigma^2 CC^\\top && \\text{(rearrange and simplify)}\n\\end{align}\n\\]\nNow, we want to show that the variance of the OLS estimator \\(\\hat{\\beta}\\) (under homoscedasticity) is smaller than any linear estimator \\(\\tilde{\\beta}\\). Let us find the difference between the variances of estimator \\(\\tilde{\\beta}\\) and \\(\\hat{\\beta}\\). Note: since \\(\\color{red}{CX = I}\\), the following is also true: \\(\\color{red}{ X^\\top C^\\top = (CX)^\\top = I}\\).\n\\[\n\\begin{align}\n\\V(\\tilde\\beta | X)  - \\V(\\hat\\beta|X) & = \\sigma^2 CC^\\top - \\sigma^2 (X^\\top X)^{-1} \\\\\n& = \\sigma^2(CC^\\top - (X^\\top X)^{-1}) && (\\text{factor out }\\sigma^2) \\\\\n& = \\sigma^2(CC^\\top - \\color{red}{CX}\\color{black}(X^\\top X)^{-1} \\color{red}{X^\\top C^\\top}\\color{black}) && (\\because \\color{red}{X^\\top C^\\top = CX = I}\\color{black}) \\\\\n& = \\sigma^2 C(I - X(X^\\top X)^{-1} X^\\top) C^\\top && (\\text{factor out }C, C^\\top) \\\\\n& = \\sigma^2 C \\color{blue}{M}\\color{black}C^\\top && (\\text{residual maker matrix } \\color{blue}{M}\\color{black})\n\\end{align}\n\\]\nSince \\(\\sigma^2 CM C^\\top\\) is positive semi-definite (I will not prove this, but it is provable with the properties of \\(M\\) introduced earlier), we know that \\(V(\\tilde{\\beta}| X) &gt; V(\\hat{\\beta}| X)\\). Thus, OLS is BLUE under the Gauss-Markov Theorem.\n\n\n\nAsymptotic Consistency of OLS\nOLS is an asymptotically consistent estimator of the relationship between any \\(x_j\\) and \\(y\\) under 4 conditions. These conditions are identical to the unbiasedness conditions EXCEPT condition 4, which is weakened from the original unbiasedness condition.\n\nLinearity (see unbiasedness)\nRandom Sampling (…)\nNo Perfect Multicolinearity (…)\nZero Mean and Exogeneity: \\(\\E(\\eps_i) = 0\\), and \\(Cov(x_i, \\eps_i) = 0\\), which implies \\(E(X_i \\eps_i) = 0\\). This means that no regressor should be correlated with \\(\\mathbf u\\). This is weaker than Zero-Conditional mean, since it means a function of regressors can be correlated with \\(\\eps_i\\).\n\n\n\n\n\n\n\nLemma: Vector Notation\n\n\n\n\n\nThe following statements are true (with \\(x_i\\) being a vector and \\(\\eps_i\\) being a scalar):\n\\[\n\\begin{split}\n& X^\\top X = \\sum\\limits_{i=1}^n x_i x_i^\\top\\\\\n&  X^\\top \\mathbf \\eps = \\sum\\limits_{i=1}^n x_i \\eps_i\n\\end{split}\n\\]\n\n\n\nLet us start of where we left of from Equation 6. Using vector notation, law of large numbers, and zero-mean and exogeneity condition:\n\\[\n\\begin{align}\n\\hat\\beta & = \\beta + (X^\\top X)^{-1} X^\\top \\eps \\\\\n& = \\beta \\left( \\sum\\limits_{i=1}^n x_i x_i^\\top \\right)^{-1} \\left( \\sum\\limits_{i=1}^n x_i \\eps_i \\right) && \\text{(vector notation)} \\\\\n& = \\beta + \\left( \\frac{1}{n}\\sum\\limits_{i=1}^n x_i x_i^\\top \\right)^{-1} \\left( \\frac{1}{n} \\sum\\limits_{i=1}^n x_i \\eps_i \\right) && (\\left(\\frac{1}{n}\\right)^{-1}, \\frac{1}{n} \\text{ cancel out})\n\\end{align}\n\\]\n\\[\n\\begin{align}\n\\mathrm{plim}\\hat\\beta & = \\beta + \\left( \\mathrm{plim} \\frac{1}{n}\\sum\\limits_{i=1}^n x_i x_i^\\top \\right)^{-1} \\left( \\mathrm{plim}\\frac{1}{n} \\sum\\limits_{i=1}^n x_i \\eps_i \\right) \\\\\n& = \\beta + (\\E(x_i x_i^\\top))^{-1} \\underbrace{\\E(x_i \\eps_i)}_{= 0} = \\beta && \\text{(law of large numbers)}\n\\end{align}\n\\]\nThus, OLS is asymptotically consistent under the 4 conditions above.\n\n\n\n\n\n\nRegression Anatomy and Specification\n\nPartitioned Regression Model\nWe can split up matrix \\(X\\) into two matrices - \\(X_1\\) containing the regressors we care about, and \\(X_2\\) containing regressors we do not care about. Vector \\(\\beta\\) will be split in the same way. Our partitioned model is:\n\\[\ny = X_1 \\beta_1 + X_2 \\beta_2 + \\eps\n\\]\nRecall our “residual maker” matrix \\(M\\). First, note a unique property: \\(\\color{red}{MX = 0}\\). Now, let us define the residual making matrix for the second part of the regression \\(M_2\\):\n\\[\nM_2 = I - X_2 (X_2^\\top X_2)^{-1}X_2^\\top\n\\]\nNow, let us multiply both sides of our above partitioned model by \\(M_2\\):\n\\[\n\\begin{align}\nM_2 y & = M2(X_1\\beta_1 + X_2\\beta_2 + \\eps) \\\\\nM_2 y & = M_2X_1 \\beta_1 + M_2 X_2 \\beta_2 + M_2 \\eps && \\text{(multiply out)} \\\\\nM_2 y & = M_2 X_1 \\beta_1 + M_2 \\eps && (\\because M_2X_2 = 0, \\ \\because \\color{red}{MX = 0}\\color{black})\n\\end{align}\n\\]\nNow, let us denote \\(\\tilde{y} := M_2 y\\), \\(\\tilde{X}_1: = M_2 X_1\\), and error \\(\\tilde\\eps := M_2 \\eps\\). Then we get the following regression equation and OLS coefficient estimates:\n\\[\n\\tilde y = \\tilde X_1 \\beta_1 + \\tilde\\eps\n\\]\n\\[\n\\hat\\beta_1 = (\\tilde X_1^\\top \\tilde X_1)^{-1}\\tilde X_1 ^\\top \\tilde y\n\\]\nRemember that vector \\(\\hat{\\beta}_1\\) is our coefficient estimates for \\(X_1\\), the portion of \\(X\\) we are interested in. This is equivalent to the coefficient estimates had we not partitioned the model.\nNotice how in the formula, we have \\(\\tilde{X}_1\\). What is \\(\\tilde{X}_1 := M_2 X_1\\)? Well, we know that $ M_2 X_2 = 0$. That tells us that any part of \\(X_1\\) that was correlated to \\(X_2\\) also became 0. Thus, \\(\\tilde{X}_1\\) is the part of \\(X_1\\) that is uncorrelated with \\(X_2\\).\nGeneralised, What this essentially means is that the coefficient estimates of OLS \\(\\hat\\beta_j\\) actually measure the effect on \\(Y_i\\) of the part of \\(X_{ij}\\) uncorrelated with the other explanatory variables \\(X_{i1}, \\dots, x_{ip}\\). Essentially, we are partialling out the effect of other variables. This is why we can “control” for other variables when focusing on the coefficient of one (or a few) variables.\n\n\n\nOmitted Variable Bias\nFrom the regression anatomy theorem, we know that \\(\\hat\\beta_j\\) is the relationship of \\(Y_i\\) and the part of \\(X_{ij}\\) that is uncorrelated with all the other explanatory variables. That implies that if we omit a variable that is correlated with both \\(X_{ij}\\) and \\(Y_i\\), that we will get a different (biased) coefficient estimate. This is called omitted variable bias.\nSuppose there is some variable \\(Z_i\\) that we have not included in a “short” regression. The actual, “true” regression of the population, would include this confounder \\(Z_i\\)\n\\[\n\\underbrace{y = X\\beta + \\eps}_{\\text{short regression}}\n\\qquad \\underbrace{y = X\\beta + z\\delta + \\eps}_{\\text{true regression with z} }\n\\]\nThe OLS estimate of the “short regression” excluding confounder \\(Z_i\\) is:\n\\[\n\\begin{align}\n\\hat\\beta & = (X^\\top X)^{-1}X^\\top y \\\\\n& = (X^\\top X)^{-1}X^\\top(\\color{blue}{X\\beta + z\\delta + \\eps}\\color{black}) && (\\because \\color{blue}{y = X\\beta + z\\delta + \\eps}\\color{black}) \\\\\n& = \\underbrace{(X^\\top X)^{-1}X^\\top X}_{= \\ I}\\beta + (X^\\top X)^{-1}X^\\top z\\delta + (X^\\top X)^{-1} X^\\top \\eps && \\text{(multiply out)} \\\\\n& = \\beta + (X^\\top X)^{-1}X^\\top z\\delta + (X^\\top X)^{-1} X^\\top \\eps\n\\end{align}\n\\]\nNow, let us find the expected value of \\(\\hat\\beta\\), which is conditional on \\(X, z\\), and simplify (using zero conditional mean):\n\\[\n\\begin{align}\n\\E(\\hat\\beta | X, z) & = \\beta + (X^\\top X)^{-1}X^\\top z \\delta + (X^\\top X)^{-1} X^\\top \\underbrace{\\E(\\eps | X, z)}_{= 0} \\\\\n& = \\beta + (X^\\top X)^{-1}X^\\top z \\delta\n\\end{align}\n\\]\nNow, what if we had a regression of outcome variable being the confounder \\(z\\), on the explanatory variables \\(X\\), such that \\(z = X\\eta + u\\). Our OLS estimate would have the solution:\n\\[\n\\hat\\eta = (X^\\top X)^{-1} X^\\top z\n\\]\nNow, we can plug \\(\\hat\\eta\\) into our expected value of \\(\\hat\\beta\\). Assume our estimator \\(\\hat{\\eta}\\) is unbiased:\n\\[\n\\begin{align}\n\\E(\\hat\\beta | X, z) & = \\beta + (X^\\top X)^{-1}X^\\top z \\delta\\\\\n& = \\beta + \\color{blue}{\\hat\\eta}\\color{black}\\delta && (\\because \\color{blue}{\\hat\\eta = (X^\\top X)^{-1} X^\\top z }\\color{black}) \\\\\n\\E\\hat\\beta & = \\E(\\E(\\hat\\beta|X, z)) && \\text{(law of iterated expect.)} \\\\\n& = E(\\color{blue}{\\beta + \\hat\\eta \\delta}\\color{black}) && (\\because \\color{blue}{\\E(\\hat\\beta|X, z) = \\beta + \\hat\\eta\\delta} \\color{black}) \\\\\n& = \\beta + \\E\\hat\\eta \\ \\delta && \\text{(take out constants from exp.)} \\\\\n& = \\beta + \\eta\\delta && (\\text{unbiased estimator } \\E\\hat\\eta = \\eta)\n\\end{align}\n\\]\nThus, we can see by not including confounder \\(z\\) in our “short regression”, the estimator is now biased by \\(\\hat\\eta \\delta\\). In the next chapter when we start discussing causality, we will see omitted confounders as a huge issue in our estimation.\n\n\n\n\n\n\nMethod of Moments Estimator\n\nMethod of Moments\nThe Method of Moments Estimator is another estimator of the true value of populations in the parameter. The estimator defines key population moments of interest - which are the population parameters written in terms of expected value functions set equal to 0.\nThen, the Method of Moments uses the sample equivalents of the population moments to estimate the population parameter. For example, to estimate the population mean, the Method of Moments uses the sample mean.\nIn order to define a method of moments for a set of parameters \\(\\theta_1, \\dots, \\theta_p\\), we need to specify at least one population moment per parameter. Or in other words, we must have more than \\(p\\) population moments.\nOur population moments can be defined as the expected value of some function \\(m(\\theta; Y_i)\\) that consists of both the variable \\(Y_i\\) and our unknown parameter \\(\\theta\\). The expectation of the function \\(m(\\theta; Y_i)\\) should equal 0.\n\\[\n\\E(m(\\theta; Y_i)) = 0\n\\]\nOur sample moments will be the sample analogues of \\(\\theta\\) and \\(Y_i\\), which are \\(\\hat\\theta\\) and \\(Y_i\\):\n\\[\n\\frac{1}{n}\\sum\\limits_{i=1}^n m(\\hat\\theta; Y_i) = 0\n\\]\nMethod of moments estimators are asymptotically consistent, because of the law of large numbers.\n\n\n\nPopulation Mean Estimator\nLet us say that we have some random variable \\(y\\), with a true population mean \\(\\mu\\). We want to estimate \\(\\mu\\), but we only have a sample of the population. How can we define \\(\\mu\\) in a moment of the form: \\(\\E(m(\\mu, y)) = 0\\)? Well, we know \\(\\mu\\) is the expectation of \\(Y_i\\), so \\(\\mu = \\E(Y_i)\\). Since they are equal, \\(\\mu - E(y) = 0\\). Thus, we can define the mean as a moment of the following condition:\n\\[\n\\E(Y_i - \\mu) = 0\n\\]\nThe method of moments estimator uses the sample equivalent of the population moment. The sample equivalent of \\(\\mu\\), is the sample mean \\(\\bar y\\):\n\\[\n\\E(Y_i - \\hat\\mu) = \\frac{1}{n}\\sum\\limits_{i=1}^n (Y_i - \\hat\\mu) = 0\n\\]\nWith this equation, we can then solve for \\(\\hat\\mu\\):\n\\[\n\\begin{align}\n0 & = \\frac{1}{n}\\sum\\limits_{i=1}^n (Y_i - \\hat\\mu) \\\\\n0 & = \\frac{1}{n}\\sum\\limits_{i=1}^nY_i - \\frac{1}{n}\\sum\\limits_{i=1}^n \\hat\\mu  && (\\text{multiply out})\\\\\n0 & = \\frac{1}{n}\\sum\\limits_{i=1}^nY_i - \\frac{1}{n} n \\hat\\mu &&(\\text{summation property of constant } \\hat\\mu)\\\\\n0 & = \\bar Y - \\hat \\mu && (\\text{definition of mean }\\frac{1}{n}\\sum\\limits_{i=1}^nY_i = \\bar Y)\\\\\n\\hat\\mu & = \\bar Y && (+\\hat\\mu\\text{ to both sides})\n\\end{align}\n\\]\nSo, we see the method of moments estimates our true population mean \\(\\mu\\), with the sample mean \\(\\bar Y\\). As a method of moments estimator, it is also asymptotically consistent.\n\n\n\nOLS as a Method of Moments\nOLS is a special case of the Method of Moments Estimator. Consider the bivariate regression model. The OLS estimator can be derived as a method of moments estimator, with 2 moments (expectation functions set equal to 0), one for each parameter (\\(\\beta_0, \\beta_1\\)):\n\\[\n\\begin{split}\n& \\E(Y_i-\\beta_0 -\\beta_1X_i) = E(\\eps_i) = 0 \\\\\n& \\E(X_i(Y_i - \\beta_0 - \\beta_1 X_i)) = E(x_i \\eps_i) = 0\n\\end{split}\n\\]\nThe estimates of these moments would use the sample equivalents: \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\).\n\\[\n\\begin{split}\n& \\E(Y_i-\\hat\\beta_0 -\\hat\\beta_1X_i) = 0 \\\\\n& \\E(X_i(Y_i - \\hat\\beta_0 - \\hat\\beta_1 X_i)) = 0\n\\end{split}\n\\]\nNow, recall our OLS minimisation conditions (simple linear regression, presented in the extra info box above). Since by definition, average/expectation is \\(\\E(x) = \\frac{1}{n} \\sum x_i\\), we can rewrite as:\n\\[\n\\begin{split}\n& \\sum\\limits_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i) = \\ n \\times \\E(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i) = 0 \\\\\n& \\sum\\limits_{i=1}^n X_i (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i) = \\  n \\times \\E(X_i(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i)) = 0\n\\end{split}\n\\]\nAnd since anything multiplied to a zero turns into zero, we can ignore the \\(n\\) in the first order condition. Which as we can see, are the exact same minimisation conditions as the method of moments estimator.\nThus, the OLS estimator is a special case of the Method of Moments estimator, and they produce the same coefficients. This is an important property for the instrumental variables method that will be covered later.\n\n\n\n\n\n\nMaximum Likelihood Estimator\n\nLikelihood Functions\nWe have a set of population parameters \\(\\boldsymbol\\theta\\) we want to estimate. This set of parameters determines our population distribution of \\(y\\), which we can describe with some probability density function \\(\\varphi(y, \\boldsymbol\\theta)\\). For example, in linear regression, our population \\(y\\) is determined by the population parameters \\(\\beta_0, \\dots, \\beta_k\\).\nWhen we are estimating parameters, we will have sample data with \\(n\\) number of observations, with each observation \\(i\\) having its own \\(y_i\\) value. Thus, our sample looks something like \\((y_1, \\dots, y_n)\\). Based on the probability density function of the population \\(y\\), the probability of selecting point \\(y_1\\) from the population data is \\(\\varphi(y_1, \\boldsymbol\\theta)\\), and the probability of selection point \\(y_i\\) is \\(\\varphi(y_i, \\boldsymbol\\theta)\\).\nWe know by the rules of probability, that the probability of multiple independent events is the product of their probabilities. Thus, the probability that we get a specific sample with \\(y\\) values \\((y_1, \\dots ,y_n)\\), based on the value of our population parameters \\(\\boldsymbol\\theta\\) is given by the likelihood function \\(L\\):\n\\[\n\\begin{align}\nL(\\boldsymbol\\theta, y_1, \\dots y_n) & = \\varphi(y_1;\\boldsymbol\\theta) \\times \\varphi(y_2;\\boldsymbol\\theta) \\times \\dots \\times \\varphi(y_n;\\boldsymbol\\theta) \\\\\n& = \\prod\\limits_{i=1}^n \\varphi(y_i, \\boldsymbol\\theta) \\\\\n\\end{align}\n\\]\nWe want to find some values of \\(\\boldsymbol\\theta\\) that make it the highest probability we observe our sample \\(y_1, \\dots ,y_n\\). This is done by maximising the likelihood function \\(L\\). However, maximising the actual likelihood function is very difficult, because of the product notation. Luckily, we can use the log of the likelihood function, which retains the same maximum/minimum points. Using the properties of logarithms, we can also rewrite our log-likelihood function in terms of summation notation, making maximisation far easier.\n\\[\n\\begin{align}\n\\log L(\\boldsymbol\\theta, y_1, \\dots y_n) & = \\log \\left(\\prod_{i=1}^n \\varphi(y_i; \\theta) \\right) \\\\\n& = \\log[\\varphi(y_1, \\boldsymbol\\theta) \\times \\varphi(y_2, \\boldsymbol\\theta) \\times \\dots \\times \\varphi(y_n, \\boldsymbol\\theta)] && \\text{(expand product notation)} \\\\\n& = \\log[\\varphi(y_1, \\boldsymbol\\theta)] + \\log [\\varphi(y_2, \\boldsymbol\\theta)] + \\dots + \\log[\\varphi(y_n, \\boldsymbol\\theta)] && \\text{(property of logs)} \\\\\n& = \\sum\\limits_{i=1}^n \\log[\\varphi(y_i, \\boldsymbol\\theta)] && \\text{(condense into sum)}\n\\end{align}\n\\tag{8}\\]\nWe then maximise this log-likelihood function to obtain our estimates \\(\\hat{\\boldsymbol\\theta}\\). Sometimes, maximisation is not possible with mathematics, so we need algorithms to do this.\n\n\n\n\n\n\nGradient Descent Algorithms\n\n\n\n\n\nIn some more complex models, we cannot mathematically find the minimum of the log-likelihood function. So instead, we resort to a series of computer algorithms called gradient descent. The algorithm takes the following form:\n\nWe randomly choose values of \\(\\boldsymbol\\theta\\) to start (let us notate the chosen as \\(\\boldsymbol\\theta^*\\)), and calculate the likelihood \\(L\\) with those chosen at \\(\\boldsymbol\\theta^*\\).\nWe then slightly shift the values of \\(\\boldsymbol\\theta^*\\) upwards and downwards, calculating all the likelihoods. We see in which shift-direction does the likelihood \\(L\\) increase the most.\nOnce we determine the direction that \\(L\\) increases the most, we shift int hat direction to a new \\(\\boldsymbol\\theta'\\) value. Once again, we slightly shift values of \\(\\boldsymbol\\theta'\\) upwards and downwards, and see which shift-direction does the likelihood \\(L\\) increase the most.\nWe keep repeating this process of moving in the direction that increases \\(L\\) the most, shifting around in all directions at that point, and once again moving in the direction that increases \\(L\\) the most.\nWe stop when we are at some point \\(\\boldsymbol\\theta^!\\) where all directions of shits decrease \\(L\\). We are “at the top of the mountain”, and that becomes our estimate.\n\nThis is a very simple gradient descent algorithm. You might point out that this algorithm only works if there is one global maximum, and no local maxima (since we would stop the algorithm at a local extrema if this were the case). This usually is not an issue since in common regression models (linear, logistic, poisson), their is only one global maximum. The figure below shows this:\n\n\n\n\n\nA solution for this problem of local maxima (which becomes more an issue with machine learning models) is to basically do the estimation algorithm multiple times, each time starting at some random \\(\\boldsymbol\\theta^*\\). Then, we find the time with the largest \\(L\\). This will in theory help us determine which are local and global maxima.\n\n\n\n\n\n\nProperties and Information Criterion\nThe Maximum Likelihood Estimator has a few very desirable properties for us.\n\nThe maximum likelihood estimator is asymptotically consistent \\(plim(\\hat\\theta_{MLE}) = \\theta\\).\nThe maximum likelihood estimator is asymptotically normal, which allows for statistical inference.\nThe maximum likelihood estimator also has the smallest asymptotic variance in the general class of estimators. Thus in large sample sizes, the MLE is asymptotically efficient.\n\nAnother reason the maximum likelihood estimator is so useful is because it is relatively intuitive. The MLE is simply maximising the likelihood we observe our sample given the population parameters. It is not imposing some strange criteria of maximisation.\nFinally, maximum likelihood estimation allows us to use Information Criterion statistics, which can tell us how good our model is. Recall that the likelihood function \\(L\\) is essentially the probability of observing our sample data given our model and parameters. Thus, we should expect better specified models to have a higher likelihood \\(L\\).\nYou could compare the likelihood \\(L\\) between models without any modification. In fact, the likelihood ratio test (similar to f-test but for MLE) does exactly this, and is commonly used in non-OLS estimated models (logistic, poisson, factor anlaysis, etc.).\nHowever, statisticians don’t just want the best fitting model - they also want a model that balances complexity. This implies some punishment for adding a ton of extra variables. The most popular is Akaike’s Information Criterion (AIC), which is calculated as follows:\n\\[\nAIC = -2 \\log L + 2k\n\\]\n\nWhere \\(L\\) is the likelihood of the model, and \\(k\\) is the number of parameters.\n\nThe lower the AIC, the better the model is considered. There are also alternatives, including Bayesian Information Criterion (BIC), but these are less commonly used.\n\n\n\nOLS as a Maximum Likelihood Estimator\nOne common question of students regarding OLS is - why least squares (SSR), and why not minimising the absolute values of errors (or the quartic of errors)? Well the answer is that OLS is a maximum likelihood estimator when assuming homoscedasticity and error term \\(u \\sim \\mathcal N (0, \\sigma^2)\\).\nWe know in a simple linear regression, \\(\\mu = E(y|x) = \\beta_0 - \\beta_1 x_i\\). Let us plug that into the probability density function of a normal distribution to get the PDF of \\(y\\) in a simple linear regression:\n\\[\n\\varphi(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{\\left( -\\frac{(y- \\mu)^2}{2 \\sigma^2}\\right)}, \\quad \\varphi(y_i|\\beta_0, \\beta_1, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{\\left( -\\frac{1}{2 \\sigma^2}(y_i - \\beta_0 - \\beta_1 x_i)^2\\right)}\n\\]\nBy Equation 8, the log-likelihood function of our sample for linear regression is:\n\\[\n\\begin{align}\n\\log L(y_i | \\beta_0, \\beta_1 \\sigma^2) & = \\log \\left(\\prod\\limits_{i=1}^n \\varphi(y_i | \\beta_0, \\beta_1, \\sigma^2) \\right) \\\\\n& = \\sum\\limits_{i=1}^n \\log \\varphi(y_i, \\beta_0, \\beta_1, \\sigma^2) && (\\text{from eq. (8)}) \\\\\n& = \\sum\\limits_{i=1}^n \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{\\left( -\\frac{1}{2 \\sigma^2}(y_i - \\beta_0 - \\beta_1 x_i)^2\\right)} \\right) \\\\\n& = \\sum\\limits_{i=1}^n \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) + \\log\\left( e^{\\left( -\\frac{1}{2 \\sigma^2}(y_i - \\beta_0 - \\beta_1 x_i)^2\\right)}\\right) && \\text{(prop. of logs)} \\\\\n& = \\sum\\limits_{i=1}^n \\log (1) - \\log (\\sqrt{2\\pi\\sigma^2})  + \\log\\left( e^{\\left( -\\frac{1}{2 \\sigma^2}(y_i - \\beta_0 - \\beta_1 x_i)^2\\right)}\\right) && \\text{(prop. of logs)} \\\\\n& = \\sum\\limits_{i=1}^n \\log (1) - \\log (\\sqrt{2\\pi})  - \\log(\\sqrt{\\sigma^2}) + \\log\\left( e^{\\left( -\\frac{1}{2 \\sigma^2}(y_i - \\beta_0 - \\beta_1 x_i)^2\\right)}\\right) && \\text{(prop. of roots)} \\\\\n& = \\sum\\limits_{i=1}^n 0 - \\frac{1}{2}\\log ({2\\pi})  - \\frac{1}{2}\\log(\\sqrt{\\sigma^2}) + \\log\\left( e^{\\left( -\\frac{1}{2 \\sigma^2}(y_i - \\beta_0 - \\beta_1 x_i)^2\\right)}\\right) && \\text{(prop. of logs)} \\\\\n& = \\sum\\limits_{i=1}^n -\\frac{1}{2} \\log (2\\pi) - \\frac{1}{2} \\log (\\sigma^2) + \\left( -\\frac{1}{2 \\sigma^2}(y_i - \\beta_0 - \\beta_1 x_i)^2\\right) \\\\\n& = -\\frac{n}{2} \\log (2\\pi) - \\frac{n}{2} \\log (\\sigma^2)  -\\frac{1}{2 \\sigma^2}\\sum\\limits_{i=1}^n(y_i - \\beta_0 - \\beta_1 x_i)^2 && \\text{(prop. of sums)}\n\\end{align}\n\\]\nNow, let us find the first-order conditions of this equation in respect to parameters \\(\\beta_0, \\beta_1, \\sigma^2\\) to maximise:\n\\[\n\\begin{align}\n& \\frac{\\partial \\log L}{\\partial \\hat\\beta_0} = \\frac{1}{\\sigma^2}\\sum\\limits_{i=1}^n (y_i - \\hat\\beta_0 - \\hat\\beta_1x_i) = 0\\\\\n& \\frac{\\partial \\log L}{\\partial \\hat\\beta_1} = \\frac{1}{\\sigma^2}\\sum\\limits_{i=1}^n x_i(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i) = 0\\\\\n& \\frac{\\partial \\log L}{\\partial \\hat\\sigma^2} = -\\frac{n}{2 \\hat\\sigma^2} + \\frac{1}{2\\hat\\sigma^4} \\sum\\limits_{i=1}^n (y_i - \\hat\\beta_0 - \\hat\\beta_1x_i) = 0\\\\\n\\end{align}\n\\]\nWe can ignore the third condition. If we look at the first two conditions, since anything times 0 equals 0, we can ignore the \\(\\frac{1}{\\sigma^2}\\). Ignoring those, we see we have the exact same first order conditions as OLS, which will yield the same estimates. This illustrates that OLS is a case of MLE, and the same intuitive reasoning of MLE can be applied to OLS.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "z2.html",
    "href": "z2.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "In the last chapter, we discussed the basics of statistics, and briefly introduced regression as a way to find correlations. This chapter dives deep into multiple linear regression, the foundational model for all of statistics. We cover the specification of the model, estimation and statistical inference, as well as extensions.\nUse the right sidebar for quick navigation. R-code is provided at the bottom.\n\n\nBasics of the Model\n\nModel Specification\nThere is some random outcome variable \\(Y_i\\), and several random explanatory variables \\(X_{i1}, X_{i2}, \\dots, X_{ip}\\). There are \\(n\\) number of observations \\(i = 1, \\dots n\\). The population linear model is specified as:\n\\[\nY_i = \\beta_0 + \\beta_1X_{i1} + \\dots + \\beta_pX_{ip} + \\eps_i \\ = \\  \\beta_0 + \\sum\\limits_{j=1}^p \\beta_jX_{ij} + \\eps_i\n\\]\nWhere \\(\\beta_0, \\dots, \\beta_p\\) are population parameters to be estimated, and \\(\\eps_i\\) is the error term.\n\n\n\n\n\n\nMore Info on the Error Term \\(\\varepsilon_i\\)\n\n\n\n\n\nThe \\(\\varepsilon_i\\) is called the error term. This indicates that not every value of \\(Y_i\\) in our data will be exactly on the linear best-fit line. Graphically, it is the highlighted part:\n\n\n\n\n\nIn social science terms, the \\(\\eps_i\\) is the effect of any other explanatory variable not included in our model.\n\n\n\n\n\n\n\n\n\nVisualising the Geometry of Regression\n\n\n\n\n\nOur linear model is essentially a hyperplane space \\(\\mathbb R^k\\). The figure below shows this:\n\n\n\n\n\n\n\n\nThis implies for each observation \\(i=1,\\dots ,n\\) with values \\((y_1, x_{11} \\dots, x_{1p})\\) has a regression equation:\n\\[\n\\begin{align}\ny_1 = & \\ \\beta_0 + \\beta_1x_{11} + \\dots + \\beta_px_{1p} + \\eps_1 \\\\\ny_2 = & \\ \\beta_0 + \\beta_1x_{21} + \\dots + \\beta_px_{2p} + \\eps_2 \\\\\n& \\qquad \\vdots \\qquad \\qquad \\vdots \\\\\ny_n = & \\ \\beta_0 + \\beta_1x_{n1} + \\dots + \\beta_px_{np} + \\eps_n\n\\end{align}\n\\]\nWe can write this system of regression equations in linear algebra form:\n\\[\ny = X\\beta + \\eps \\quad \\iff \\quad \\begin{pmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\\end{pmatrix}  =\n\\begin{pmatrix}1 & x_{11} & \\dots & x_{1p} \\\\1 & x_{21} & \\dots & x_{2p} \\\\\\vdots & \\vdots & \\vdots & \\vdots \\\\1 & x_{n1} & \\dots & x_{np}\\end{pmatrix}\n\\begin{pmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p\\end{pmatrix}\n+ \\begin{pmatrix}\\eps_1 \\\\ \\eps_2 \\\\ \\vdots \\\\ \\eps_n\\end{pmatrix}\n\\]\nNotation note: I generally indicate vectors with a lowercase letter (ex. \\(y\\)), and matrices with an uppercase vector (ex. \\(X\\)). Random variables will typically have subscript \\(i\\), (ex. \\(Y_i, X_i\\)), while realisations of random variables (and also scalars) will be lowercase (ex. \\(y_1, x_1\\)).\n\n\n\nEstimation Process\nTo estimate the population parameters \\(\\beta_0, \\dots, \\beta_p\\), we use our sample data, and try to find the values \\(\\hat\\beta_0, \\dots, \\hat\\beta_p\\) that minimise the square sum of residuals (SSR). The residuals are the difference from our predicted best-fit line result \\(\\hat Y_i\\), and the actual value of \\(Y_i\\) in the data. Below highlighted in red are the residuals.\n\n\n\n\n\nAfter we have the residual values, we simply square each of them, then sum all of them together. That is the sum of squared residuals (SSR). We will define the SSR as function \\(S\\). In the linear algebra representation (where \\(\\hat\\beta\\) is the vector of estimated parameters \\(\\hat\\beta_0, \\dots, \\hat\\beta_p\\)):\n\\[\n\\begin{align}\nS(\\hat\\beta) & = (y - \\hat y)^\\top (y - \\hat y) \\\\\n& = (y - \\color{blue}{X\\hat\\beta}\\color{black})^\\top (y - \\color{blue}{X\\hat\\beta}\\color{black}) && (\\because \\color{blue}{\\hat y = X\\hat\\beta} \\color{black})\n\\end{align}\n\\tag{1}\\]\nThis estimation is called the ordinary least squares (OLS) estimator. The solutions to the OLS estimator can be derived mathematically.\n\n\n\nDeriving OLS Estimates\nOLS wants to minimise the sum of squared residuals \\(S(\\hat{\\boldsymbol\\beta})\\) shown in Equation 1.\n\\[\n\\begin{align}\nS(\\hat\\beta) &  = (y - X \\hat\\beta )^\\top (y - X \\hat \\beta) \\\\\n& = y^\\top y - \\hat\\beta^\\top X^\\top y - y^\\top X \\hat\\beta +  \\hat\\beta^\\top X^\\top X \\hat\\beta && (\\text{distribute out}) \\\\\n& = y^\\top y \\ \\color{blue}{-  2 \\hat\\beta^\\top X^\\top y} \\color{black}  +  \\underbrace{\\hat\\beta^\\top X^\\top X \\hat\\beta}_{\\text{quadratic}} && (\\because \\color{blue}{- \\hat\\beta^\\top X^\\top y - y^\\top X \\hat\\beta = - 2 \\hat\\beta^\\top X^\\top y} \\color{black})\n\\end{align}\n\\]\nNow, let us take the gradient to find the first order condition:\n\\[\n\\frac{\\partial S(\\hat\\beta)}{\\partial \\hat\\beta} = -2 X^\\top y + 2 X^\\top X \\hat\\beta = 0\n\\]\n\n\n\n\n\n\nFirst Order Conditions for Simple Linear Regression\n\n\n\n\n\nCurrently, we are deriving the first order conditions for multiple linear regression using linear algebra. For simple linear regression (with one explanatory variable), we can use summation notation. Our sum of squared residuals in summation form is:\n\\[\nSSR = S(\\hat\\beta_0, \\hat\\beta_1)= \\sum\\limits_{i=1}^n(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i)^2\n\\]\nWe want to minimise the SSR in respect to both \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\). We can do this by finding our first order conditions:\n\\[\n\\begin{align}\n\\frac{\\partial S(\\hat\\beta_0, \\hat\\beta_1)}{\\partial \\hat\\beta_0} & = \\sum\\limits_{i=1}^n(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i) = 0 \\\\\n\\frac{\\partial S(\\hat\\beta_0, \\hat\\beta_1)}{\\partial \\hat\\beta_1} & = \\sum\\limits_{i=1}^n X_i(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i) = 0 \\\\\n\\end{align}\n\\tag{2}\\]\nThese conditions create a system of equations, which you can solve for the OLS solutions of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\). I will not do that here, since this method is more tedious than the linear algebra method, and can only apply to simple linear regression.\nHowever, while we are not going to solve for our OLS solutions with summations, it is still useful to know these first order conditions, since we will use them in many proofs that show other estimators are the same as OLS because they have the same first order conditions.\n\n\n\nWhen assuming \\(X^\\top X\\) is invertable (which is true if \\(X\\) is full rank), we can isolate \\(\\hat\\beta\\) to find the solution to OLS:\n\\[\n\\begin{align}\n-2 X^\\top y + 2 X^\\top X \\hat\\beta & = 0 \\\\\n2 X^\\top X \\hat\\beta & = 2 X^\\top y && ( + 2X^\\top y \\text{ to both sides}) \\\\\n\\hat\\beta & = (2X^\\top X)^{-1} -2 X^\\top y && (\\times (2X^\\top X)^{-1} \\text{ to both sides}) \\\\\n\\hat\\beta & = (X^\\top X)^{-1} X^\\top y && (2^{-1}, 2 \\text{ cancel out})\n\\end{align}\n\\]\nThose are our coefficient solutions to OLS. With the estimated parameters \\(\\hat\\beta_0, \\dots, \\hat\\beta_p\\), we now have a best-fit line/plane, called the fitted values \\(\\hat y\\). We can also determine our OLS residuals \\(\\hat\\eps\\):\n\\[\n\\begin{align}\n\\hat y & = X \\hat\\beta = X(X^\\top X)^{-1}X^\\top y \\\\\ny & = X \\hat\\beta + \\hat\\eps\n\\end{align}\n\\tag{3}\\]\nWe will discuss the OLS estimator in far more detail in the next chapter, discussing its properties, and its strengths/weaknesses.\n\n\n\nConditional Expectation Function\nPreviously, we wrote the linear regression model in respect to \\(Y_i\\). However, we can also write the linear regression model as the best linear approximation of a conditional expectation function (CEF) \\(\\E(Y_i |X_i)\\):\n\\[\n\\E(Y_i|X_i) = \\beta_0 + \\beta_1 X_{i1} + \\dots + \\beta_p X_{ip}\n\\]\nBest-approximation is defined by the lowest mean-squared error (MSE). Let us prove OLS on \\(Y_i\\) gets the same \\(\\beta_0, \\dots, \\beta_p\\) as the best linear approximation of \\(\\E(Y_i|X_i)\\). Take this very simple CEF and its MSE:\n\\[\n\\begin{align}\n\\E(Y_i|X_i) & = b_0 + b_1X_i \\\\\nMSE & = \\E(Y_i - \\E(Y_i|X_i))^2 \\\\\n& =  \\E(Y_i - (\\color{blue}{b_0 + b_1X_i}\\color{black}))^2  && (\\because \\color{blue}{\\E(Y_i|X_i) = b_0 + b_1X_i}\\color{black})\\\\\n& = \\E(Y_i - b_0 - b_1 X_i) && \\text{(distribute negative sign)}\n\\end{align}\n\\]\nThe first order conditions are (using chain rule and partial derivatives):\n\\[\n\\begin{split}\n& \\E(Y_i - b_0 - b_1X_i) = 0 \\\\\n& \\E(X_i(Y_i - b_0 - b_1X_i) = 0\n\\end{split}\n\\]\nNow, recall our OLS minimisation conditions (simple linear regression, presented in the extra info box above). Since by definition, average/expectation is \\(\\E(x) = \\frac{1}{n} \\sum x_i\\), we can rewrite as:\n\\[\n\\begin{split}\n& \\sum\\limits_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i) = \\ n \\times \\E(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i) = 0 \\\\\n& \\sum\\limits_{i=1}^n X_i (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i) = \\  n \\times \\E(X_i(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i)) = 0\n\\end{split}\n\\]\nAnd since anything multiplied to a zero turns into zero, we can ignore the \\(n\\) in the first order condition. Thus, we can see, are the exact same minimisation conditions as the conditional expectation function. Thus, OLS is the best approximation of the conditional expectation function.\nThis property is very useful for interpreting our regression results. You will hear “expected changes in \\(Y_i\\) given an increase in \\(X_i\\)” very frequently when it comes to regression and its applications.\n\n\n\n\n\n\nInterpretation\n\nInterpretation of Parameters\nI define \\(\\hat\\beta_j \\in \\{\\hat\\beta_1, \\dots, \\hat\\beta_p\\}\\), multiplied to \\(X_{ij} \\in \\{X_{i1}, \\dots, X_{ip}\\}\\). \\(\\hat\\beta_0\\) is the intercept. We assume a continuous \\(Y_i\\) variable - for a binary \\(Y_i\\), see the below section on the linear probability model.\n\n\n\n\n\n\n\n\n\nContinuous \\(X_{ij}\\)\nBinary \\(X_{ij}\\)\n\n\n\\(\\hat\\beta_j\\)\nFor every one unit increase in \\(X_{ij}\\), there is an expected \\(\\hat\\beta_j\\) unit change in \\(Y_i\\), holding all other explanatory variables constant.\nThere is a \\(\\hat\\beta_j\\) unit difference in \\(Y_i\\) between category \\(X_{ij} = 1\\) and category \\(X_{ij} = 0\\), holding all other explanatory variables constant.\n\n\n\\(\\hat\\beta_0\\)\nWhen all explanatory variables equal 0, the expected value of \\(Y_i\\) is \\(\\hat\\beta_0\\).\nFor category \\(X_{ij} = 0\\), the expected value of \\(Y_i\\) is \\(\\hat\\beta_0\\) (when all other explanatory variables equal 0).\n\n\n\nSometimes, these interpretations are not useful. For example, if \\(Y_i\\) is democracy, what does a 5 unit increase in democracy actually mean? Instead, w can express the change of \\(Y_i\\) and \\(X_i\\) in terms of standard deviations. Or in other words, we want to find the change in \\(\\frac{\\hat Y_i}{\\sigma_Y}\\) for every one standard deviation \\(\\sigma_X\\) increase in \\(X_i\\). For simplicity, let us use a simple linear regression \\(\\E(Y_i|X_i) = \\beta_0 + \\beta_1 X_i\\):\n\\[\n\\begin{align}\n& \\E \\left(\\frac{Y_i}{\\sigma_Y} | X_i = x + \\sigma_X \\right ) - \\E \\left(\\frac{Y_i}{\\sigma_Y} | X_i = x \\right ) \\\\\n& = \\frac{\\E(Y_i|X_i = x+ \\sigma_X)}{\\sigma_Y} - \\frac{\\E(Y_i|X_i = x)}{\\sigma_Y} &&(\\text{property of expectation}) \\\\\n& = \\frac{\\E(Y_i|X_i = x+ \\sigma_X) - \\E(Y_i|X_i = x)}{\\sigma_Y} && (\\text{combine into 1 fraction})\\\\\n& = \\frac{\\beta_0 + \\beta_1(x+\\sigma_X) - [\\beta_0 + \\beta_1(x)]}{\\sigma_Y} && (\\text{plug in regression models})\\\\\n& = \\frac{\\beta_1\\sigma_X}{\\sigma_Y} && (\\text{cancel and simplify})\n\\end{align}\n\\]\nThus, for a one standard deviation \\(\\sigma_X\\) increase in \\(X_{ij}\\), there is an expected \\(\\frac{\\beta_j\\sigma_X}{\\sigma_Y}\\)-standard deviation change in \\(Y_i\\).\n\n\n\nR-Squared and Other Fit Metrics\nOur fitted values equation, shown in Equation 3, can be rewritten as:\n\\[\n\\begin{align}\n\\hat y = X(X^\\top X)^{-1} X^\\top y \\ = \\ \\color{blue}{P}\\color{black}y && (\\because \\color{blue}{P := X(X^\\top X)^{-1} X^\\top})\n\\end{align}\n\\]\nWe can see that \\(P\\) is a matrix that turns \\(y \\rightarrow \\hat y\\). Matrix \\(P\\) is our linear model that projects the true values \\(y\\) into a space of our regressors \\(X\\). We will not go too into depth here on this, but see the next chapter for more details.\nOne thing we might be interested in is how well our model \\(Py\\) explains the actual \\(y\\). One way we can do this is the scalar product: the scalar product \\(y^\\top Py\\) describes the shadow the actual \\(y\\) casts on our projected model. However, this value will change based on the scale of our \\(y\\) variable. Thus, we will divide it by \\(y^\\top y\\), which is the “maximum” shadow possible (perfect shadow). This ratio is called \\(R^2\\).\n\\[\nR^2 = \\frac{y^\\top Py}{y^\\top y}\n\\]\nWe can also reason about \\(R^2\\) in a another way. The total amount of variation in \\(y\\) is called the total sum of squares (SST). The part of \\(y\\) we cannot explain is the Sum of Squared Residuals (SSR) that we minimised for OLS esimtation. That implies that the remaining part SST-SSR is the variation in \\(y\\) that our model explains, called the sum of explained squares (SSE). \\(R^2\\) can be though of the ratio of explained variation in \\(y\\) by our model to the total variation in \\(y\\):\n\\[\nR^2 = \\frac{SSE}{SST} = \\frac{SST - SSR}{SST} = 1 - \\frac{SSR}{SST} = 1 - \\frac{\\sum (Y_i - \\hat Y_i)^2}{\\sum(Y_i - \\bar Y)^2}\n\\]\nR-Squared (\\(R^2\\)) measures the proportion of variation in \\(y\\) that is explained by our explanatory variables. R-Squared is always between 0 and 1 (0%-100%). Higher values indicate our model better explains the variation in \\(y\\).\nThere are a few other fit metrics, based around the idea of maximum likelihood estimation (explained in the next chapter). These metrics include Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). Lower values indiciate a better model. However, unlike \\(R^2\\), neither has a nice substantive interpretation.\n\n\n\n\n\n\nStatistical Inference\n\nT-Tests\nIn the last chapter, we discussed the basics of hypothesis testing. In regression, our typical hypotheses are:\n\n\\(H_0 : \\beta_j = 0\\) (i.e. there is no relationship between \\(X_{ij}\\) and \\(Y_i\\)).\n\\(H_1:\\beta_j ≠ 0\\) (i.e. there is a relationship between \\(X_{ij}\\) and \\(Y_i\\)).\n\nUsing the robust standard errors, we calculate the \\(t\\)-statistic, and using the \\(t\\)-statistic and a t-distribution, we calculate a p-value. Why a t-distribution and not a standard normal distribution? The reason will be explained in the next chapter, but has to deal with the fact we cannot observe the variance of the error term \\(\\eps_i\\), so we have to estimate it (with some uncertainty).\n\n\n\n\n\n\nDetails of Running a Hypothesis Test\n\n\n\n\n\nFirst, we calculate the t-test statistic:\n\\[\nt = \\frac{\\hat\\beta_j - H_0}{\\widehat{se}(\\hat\\beta_j)}\n\\]\n\nWhere \\(H_0\\) is typically 0, but if you do decide to alter the null hypothesis, you would plug it in.\n\nNow, we consult a t-distribution of \\(n-k-1\\) degrees of freedom. We use a t-distribution because the standard error calculation used in OLS is slightly imprecise.\n\nNote: we can only do this step if we believe the central limit theorem is met (that our errors are asymptotically normal). We need a large enough sample size.\n\nWe start from the middle of the t-distribution, and move t-test-statstic number of standard deviations from both sides of the middle.\nThen, we find the probability of getting a t-test statistic even further from the middle than the one we got. The area highlighted in the figure below showcases this. In the figure, the t-test statistic is 2.228.\n\n\n\n\n\nThe area highlighted, divided by the entire area under the curve, is the p-value.\n\n\n\n\n\n\n\n\n\nRobust Standard Errors\n\n\n\n\n\nIn modern econometrics, we tend to use robust standard errors, not normal standard errors. To understand why, we need to look at homoscedasticity.\nHomoscedasticity is the idea that no matter the values of any explanatory variable, the error term variance is constant. If this is false, then we have heteroscedasticity. An easy way to identify homoscedasticity is to look at a residual plot (just the plot of all \\(\\widehat{\\eps_i}\\)):\n\n\n\n\n\nNotice how the homoscedasticity residuals seem to have the same up-down variance, no matter the value of \\(X_i\\). The heteroscedasticity residuals have a clear pattern - the up-down variance is smaller when \\(X_i\\) is smaller, and the up-down variance is larger when \\(X_i\\) is larger.\nEssentially, if you see a pattern in the residual plot, it is likely heteroscedasticity.\nIf you have homoscedasticity, you should use normal OLS standard errors. However, it is often hard to prove your data is homoscedastic. Thus, we generally default to heteroscedasticity-robust standard errors, unless we can prove we have homoscedasticity.\nWe will discuss homoscedasticity and heteroscedasticity in more detail in the next chapter, as well as derive the standard errors.\n\n\n\nThe p-value we get is the probability of getting a test statistic equally or more extreme than the one we got, given the null hypothesis is true.\n\nIf \\(p&lt;0.05\\), we believe the probability of a null hypothesis is low enough, such that we reject the null hypothesis (that there is no relationship between \\(X_{ij}\\) and \\(Y_i\\)), and conclude our alternate hypothesis (that there is a relationship between \\(X_{ij}\\) and \\(Y_i\\)).\nIf \\(p &gt; 0.05\\), we cannot reject the null hypothesis, and cannot reject there is no relationship between \\(X_{ij}\\) and \\(Y_i\\).\n\nNOTE: this is not causality - we are only looking at the relationship. Causality needs to be established with an adequate research design, which we will explore in later chapters.\n\n\n\nConfidence Intervals\nIn the last chapter, we discussed the idea of confidence intervals. The 95% confidence intervals of coefficients have the following bounds:\n\\[\n(\\hat\\beta_j - 1.96 \\widehat{se}(\\hat\\beta_j), \\ \\ \\hat\\beta_j + 1.96 \\widehat{se}(\\hat\\beta_j))\n\\]\n\nThe 1.96 is an approximation assuming a normal distribution. The actual confidence intervals (calculated by computers) will use a t-distribution of \\(n-k-1\\), which will result in a slightly different multiplicative factor.\n\nThe confidence interval means that under repeated sampling and estimating \\(\\hat\\beta_j\\), 95% of the confidence intervals that we construct will include the true \\(\\beta_j\\) value in the population.\nIf the confidence interval contains 0, we cannot conclude a relationship between \\(X_{ij}\\) and \\(Y_i\\), as 0 is a plausible value of \\(\\beta_j\\). These results will always match those of the t-test.\n\n\n\nF-Tests\nF-tests are used to test more than one coefficient at a time. For example, you might want to test if two variables or coefficients are jointly significant (this will become more clear in the extensions of regression).\nOur hypotheses will be:\n\n\\(M_0 : Y_i = \\beta_0 + \\sum\\limits_{j=1}^g \\beta_{j} X_{ij} + \\eps_i\\) (the smaller null model with \\(g\\) variables).\n\\(M_a : Y_i = \\beta_0 + \\sum\\limits_{j=1}^g \\beta_{j} X_{ij} + \\sum\\limits_{j=g+1}^p \\beta_{j} X_{ij} + \\eps_i\\) (the bigger model with the original \\(g\\) variables + additional variables up to \\(p\\)).\n\n\n\n\n\n\n\nDetails of the F-test\n\n\n\n\n\nF-tests compare the \\(R^2\\) of the two models through the F-statistic:\n\\[\nF = \\frac{(SSR_0 - SSR_a) / (k_a - k_0)}{SSR_a /(n - k_a - 1)}\n\\]\nWe then consult a F-distribution with \\(k_a - k_0\\) and \\(n-k_a - 1\\) degrees of freedom, obtaining a p-value (in the same way as the t-test).\n\n\n\nThe p-value we get is the probability of getting a test statistic equally or more extreme than the one we got, given the null hypothesis is true.\n\nIf \\(p&lt;0.05\\), the we believe the probability of the null hypothesis is low enough, such that we reject the null hypothesis (that \\(M_0\\) is the better model), and conclude our alternate hypothesis (that \\(M_a\\) is a better model). This also means the extra coefficients in \\(M_a\\) are jointly statistically significant.\nIf \\(p &gt; 0.05\\), we cannot reject the null hypothesis, and cannot reject that \\(M_0\\) is a better model. Thus, the extra coefficients in \\(M_a\\) are jointly not statistically significant.\n\n\n\n\n\n\n\nExtension: Different Variables\n\nLinear Probability Model\nThe standard linear model assumes a continuous \\(Y_i\\) variable. However, we can adapt the linear model to fit binary \\(Y_i\\) variables. When \\(Y_i\\) is binary and only has values \\(Y_i \\in \\{0, 1\\}\\), our linear model is actually no longer a predictor of \\(Y_i\\), since our regression will output values that are not 0 and 1.\nInstead, our linear model will now predict the probability of unit \\(i\\) having \\(Y_i = 1\\). The is due to the conditional expectation interpretation of regression, and the expectation of the bernoulli distribution:\n\\[\n\\begin{align}\n\\E(Y_i|X_i) & = \\underbrace{0 \\times \\P(Y_i = 0|X_i) \\ + \\ \\P(Y_i = 1|X_i)}_{\\text{a weighted avg. formula}} \\\\\n& = \\P(Y_i=1|X_i)\n\\end{align}\n\\]\nThus, we can rewrite our linear model with the primary outcome being \\(\\P(Y_i = 1|X_i)\\). This model is called the linear probability model:\n\\[\n\\P(Y_i = 1|X_i) = \\beta_0 + \\beta_1X_{i1} + \\dots + \\beta_pX_{ip} + \\eps_i\n\\]\nOur interpretations of coefficients also slightly change.\n\n\n\n\n\n\n\n\n\nContinuous \\(X_{ij}\\)\nBinary \\(X_{ij}\\)\n\n\n\\(\\hat\\beta_j\\)\nFor every one unit increase in \\(X_{ij}\\), there is an expected \\(\\hat\\beta_j \\times 100\\) percentage point change in the probability of a unit being in category \\(Y_i=1\\), holding all other explanatory variables constant.\nThere is a \\(\\hat\\beta_j\\times 100\\) percentage point difference in the probability of a unit being in category \\(Y_i=1\\) between category \\(X_{ij} = 1\\) and category \\(X_{ij} = 0\\), holding all other explanatory variables constant.\n\n\n\\(\\widehat{\\beta_0}\\)\nWhen all explanatory variables equal 0, the expected probability of a unit being in category \\(Y_i=1\\) is \\(\\hat\\beta_0 \\times 100\\)\nFor category \\(X_{ij} = 0\\), the expected probability of a unit being in category \\(Y_i=1\\) is \\(\\hat\\beta_j \\times 100\\) (when all other explanatory variables equal 0).\n\n\n\nNote: we cannot do linear regression with a categorical (more than 2 categories and undordered) \\(Y_i\\). We will need to use a multinomial logistic regression. We can do a linear regression with an ordinal \\(Y_i\\) (more than 2 categories with an order) by pretending that the ordinal \\(Y_i\\) is continuous. We can also is a ordinal logistic regression for that.\n\n\n\nCategorical Explanatory Variables\nTake an explanatory variable \\(X_i\\), which has \\(g\\) number of categories \\(1, \\dots, g\\). To include \\(X_i\\) in our regression, we would create \\(g-1\\) dummy (binary) variables, to create the following regression model:\n\\[\n\\E(Y_i|X_i) = \\beta_0 + \\sum\\limits_{j=1}^{g-1} \\beta_j X_{ij}\n\\]\n\nCategories \\(1, \\dots, g-1\\) get there own binary variable \\(X_{i1}, \\dots, X_{ig-1}\\).\nCategory \\(g\\) (the reference category) does not get its own variable. We can change which category we wish to be the reference.\n\nInterpretation is as follows (category \\(j\\) is any one of category \\(1, \\dots, g-1\\)).\n\n\\(\\beta_j\\) is the difference in expected \\(Y_i\\) between category \\(j\\) and the reference category \\(g\\).\n\\(\\beta_0\\) is the expected \\(Y_i\\) of the reference category \\(g\\).\nThus, category \\(j\\) has an expected \\(Y_i\\) of \\(\\beta_0 + \\beta_j\\).\n\n\n\n\n\n\n\nExample of a Categorical Explanatory Variable\n\n\n\n\n\nLet us say that \\(X_i\\) is the variable development level of a country, with 3 categories: low (L), medium (M), and high (H). \\(Y_i\\) will be the crime rate of the country.\nLet us set low development (L) as our reference category. Our regression will be:\n\\[\nE(Y_i|X_i) = \\beta_0 + \\beta_1X_{iM} + \\beta_2 X_{iH}\n\\]\nNow let us interpret the coefficients:\n\n\\(\\beta_0\\) is the expected crime rate for a country of low (L) development.\n\\(\\beta_1\\) is the difference in expected crime rate between a medium (M) developed country and a low (L) developed country (since low is the reference category).\n\\(\\beta_2\\) is the difference in expected crime rate between a high (H) developed country and a low (L) developed country (since low is the reference category).\n\nThe expected/predicted \\(Y_i\\) (crime rate) for each category is:\n\nLow (L): \\(\\beta_0\\)\nMedium (M): \\(\\beta_0 + \\beta_1\\)\nHigh (H): \\(\\beta_0 + \\beta_2\\).\n\n\n\n\nEach coefficient \\(\\hat\\beta_j\\)’s statistical significance is a difference-in-means significance test, not the significance of the categorical variable as a whole. To find if the entire categorical variable is significant, you should use a F-test.\n\n\n\nInteraction Effects\nAn interaction between two variables means they are multiplied in the regression equation:\n\\[\nY_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\color{red}{\\beta_3 X_{i1} X_{i2}}\n\\]\nThe red is the interaction term. Interpretation of the relationship between \\(X_{i1}\\) and \\(Y_i\\) is:\n\n\n\n\n\n\n\n\n\nBinary \\(X_{i2}\\)\nContinuous \\(X_{i2}\\)\n\n\nBinary \\(X_{i1}\\)\nWhen \\(X_{i2} = 0\\), the effect of \\(X_{i1}\\) (going from 0 to 1) on \\(Y_i\\) is \\(\\hat\\beta_1\\).\nWhen \\(X_{i2} = 1\\), the effect of \\(X_{i1}\\) (going from 0 to 1) on \\(Y_i\\) is \\(\\hat\\beta_1 + \\hat\\beta_3\\).\nThe effect of \\(X_{i1}\\) (going from 0 to 1) on \\(Y_i\\) is \\(\\hat\\beta_1 + \\hat\\beta_3 X_{i2}\\).\n\n\nContinuous \\(X_{i1}\\)\nWhen \\(X_{i2} = 0\\), for every increase in one unit of \\(X_{i1}\\), there is an expected \\(\\widehat{\\beta_1}\\) unit change in \\(Y_i\\).\nWhen \\(X_{i2} = 1\\), for every increase in one unit of \\(X_{i1}\\), there is an expected \\(\\hat\\beta_1+ \\hat\\beta_3\\) change in \\(Y_i\\).\nFor every increase of one unit in \\(X_{i1}\\), there is an expected \\(\\hat\\beta_1 + \\hat\\beta_3 X_{i2}\\) change in \\(Y_i\\).\n\n\n\n\n\n\n\n\n\nProof of Interpretations of Interactions\n\n\n\n\n\nWe can solve for the change of \\(X_{i1}\\) on \\(Y_i\\) using a partial derivative of \\(Y_i\\) in respect to \\(X_{i1}\\):\n\\[\n\\begin{split}\n\\frac{\\partial \\hat Y_i}{\\partial X_{i1}} & = \\frac{\\partial}{\\partial X_{i1}} \\left[ \\widehat{\\beta_0} + \\widehat{\\beta_1}X_{i1} + \\widehat{\\beta_2}X_{i2} + \\widehat{\\beta_3}X_{i1}X_{i2}\\right] \\\\\n\\frac{\\partial \\hat Y_i}{\\partial X_{i1}} & = \\hat\\beta_1 + \\hat\\beta_3X_{i2}\n\\end{split}\n\\]\nThis gives us the effect of \\(X_{i1}\\) on \\(Y_i\\).\n\n\n\n\\(\\hat\\beta_0\\) is still the expected \\(Y_i\\) when all explanatory variables equal 0.\nThe coefficient of the interaction \\(\\hat\\beta_3\\), when statistically significant, indicates a statistically significant interaction effect. If it is not statistically significant, then the interaction effect is not statistically significant (and can be dropped).\n\n\n\nPolynomial Transformations\nSometimes the relationship between two variables is not a straight line - we can add more flexibility with polynomials. The most common form of polynomial transformation is the quadratic transformation:\n\\[\nY_i = \\beta_0 + \\beta_1X_i + \\beta_2 X_i^2 + \\eps_i\n\\]\nOur estimated \\(\\hat\\beta_0\\) remains the expected value of \\(Y_i\\) when all explanatory variables equal 0.\nUnfortunately, the \\(\\hat\\beta_1\\) and \\(\\hat\\beta_2\\) coefficients are not directly interpretable.\n\n\\(\\hat\\beta_2\\)’s sign can tell us if the best-fit parabola opens upward or downward.\nThe significance of \\(\\hat\\beta_2\\) also indicates if the quadratic term is statistically significant. If it is not, we can remove the transformation.\n\nWe can interpret two things about the quadratic transformation:\n\nFor every one unit increase in \\(X_i\\), there is an expected \\(\\hat\\beta_1 + 2 \\hat\\beta_2X_i\\) unit increase in \\(Y_i\\).\nThe minimum/maximum point in the best-fit parabola occurs at \\(X_i = - \\hat\\beta_1/2 \\hat\\beta_2\\)\n\n\n\n\n\n\n\nProof of Polynomial Interpretations\n\n\n\n\n\nWe can derive the change in \\(Y_i\\) given a one unit increase in \\(X_i\\) by finding the partial derivative of \\(Y_i\\) in respect to \\(X_i\\):\n\\[\n\\begin{split}\n\\frac{\\partial \\hat Y_i}{\\partial X_i} & = \\frac{\\partial}{\\partial X_i} \\left[ \\hat\\beta_0 + \\hat\\beta_1X_i + \\hat\\beta_2X_i^2 \\right] \\\\\n\\frac{\\partial \\hat Y_i}{\\partial X_i} & = \\hat\\beta_1 + 2 \\hat\\beta_2X_i\n\\end{split}\n\\]\nWe can also solve for the \\(X_i\\) that results in the minimum/maximum of the best-fit parabola by setting the partial derivative equal to 0:\n\\[\n\\begin{split}\n0 & = \\hat\\beta_1 + 2 \\hat\\beta_2X_i \\\\\nX_i & = - \\hat\\beta_1/2 \\hat\\beta_2\n\\end{split}\n\\]\n\n\n\nWe can go beyond quadratic - as long as we always include lower degree terms in our model.\n\n\n\nLogarithmic Transformations\nLogarithmic transformations are often used to change skewed variables into normally distributed variables. These are not as common in political science as compared to economics, but can be useful in certain situations.\n\n\n\n\n\n\nLogging a Skewed Variable\n\n\n\n\n\nMany monetary variables are heavily skewed. Natural logging these variables can turn them into normal distributions. This is useful, since skewed variables tend to have heteroscedasticity, and by making them normal, we can use the smaller normal standard errors.\nFor example, take this variable called expenses with a significant right skew:\n\n\n\n\n\nIf we take the log of this variable, we get the following distribution that is almost normal:\n\n\n\n\n\n\n\n\nWe have 3 types of logarithmic transformations:\n\n\n\n\n\n\n\n\n\n\\(X_i\\)\n\\(\\log (X_i)\\)\n\n\n\\(Y_i\\)\nLinear Model:\n\\(Y_i = \\beta_0 + \\beta_1 X_i + \\eps_i\\)\nLinear-Log Model:\n\\(Y_i = \\beta_0 + \\beta_1 \\log X_i + \\eps_i\\)\n\n\n\\(\\log (Y_i)\\)\nLog-Linear Model:\n\\(\\log Y_i = \\beta_0 + \\beta_1 X_i + \\eps_i\\)\nLog-Log Model:\n\\(\\log Y_i = \\beta_0 + \\beta_1 \\log X_i + \\eps_i\\)\n\n\n\n\nInterpreting the models:\n\n\n\n\n\n\n\n\n\n\\(X_i\\)\n\\(\\log (X_i)\\)\n\n\n\\(Y_i\\)\nLinear Model:\nWhen \\(X_i\\) increases by one unit, there is an expected \\(\\hat\\beta_1\\) unit change in \\(Y_i\\).\nLinear-Log Model:\nWhen \\(x\\) increases by 10%, there is an expected \\(0.096 \\hat\\beta_1\\) unit change in \\(Y_i\\).\n\n\n\\(\\log (Y_i)\\)\nLog-Linear Model:\nFor every one unit increase in \\(X_i\\), the expected \\(Y_i\\) is multiplied by \\(e^{\\hat\\beta_1}\\).\nLog-Log Model:\nMultiplying \\(X_i\\) by \\(e\\) will multiply the expected value of \\(Y_i\\) by \\(e^{\\hat\\beta_1}\\).\n\n\n\n\n\n\n\n\n\nProof of Interpretations for Log Transformations\n\n\n\n\n\nProof of Linear-Log Model:\n\\[\n\\begin{split}\n& \\E(Y_i|X_i = x) = \\beta_0 + \\beta_1 \\log x \\\\\n& \\E(Y_i | X_i = e^A x) = \\beta_0 + \\beta_1 \\log(e^A x) \\\\\n& = \\beta_0 + \\beta_1 (\\log(e^A) + \\log x) \\\\\n& = \\beta_0 + \\beta_1 (A + \\log x) \\\\\n& = \\beta_0 + \\beta_1A + \\beta_1 \\log x\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\E(Y_i|X_i = \\alpha x) - \\E(Y_i|X_i = x) & = \\beta_0 + \\beta_1 A + \\beta_1 \\log (x) - (\\beta_0 + \\beta_1 \\log x) \\\\\n& = \\beta_1 A\n\\end{split}\n\\]\n\nWhen \\(A = 0.095\\), then \\(e^A = 1.1\\). Thus, a 1.1 times increase of \\(X_i\\) results in a \\(0.095 \\widehat{\\beta_1}\\) change in \\(Y_i\\).\n\n\nProof of Log-Linear Model:\n\\[\n\\begin{split}\n\\E(\\log Y_i | X_i = x) =  \\log Y_i & = \\beta_0 + \\beta_1 x \\\\\nY_i & = e^{\\beta_0 + \\beta_1 x} \\\\\nY_i & = e^{\\beta_0}e^{\\beta_1 x} \\\\\n\\E(\\log Y_i|X_i = x+1) = \\log Y_i & = \\beta_0 + \\beta_1(x+1) \\\\\nY_i & = e^{\\beta_0 + \\beta_1 + \\beta_1 x} \\\\\nY_i & = e^{\\beta_0}e^{\\beta_1}e^{\\beta_1x}\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\frac{\\E(\\log Y_i|X_i = x+1)}{\\E(\\log Y_i | X_i = x)} & = \\frac{e^{\\beta_0}e^{\\beta_1}e^{\\beta_1x}}{e^{\\beta_0}e^{\\beta_1x}} \\\\\n& = e^{\\beta_1}\n\\end{split}\n\\]\n\nThus, when \\(X_i\\) increases by one, there is a multiplicative increase of \\(e^{\\beta_1}\\) for \\(Y_i\\).\n\n\nProof of Log-Log model:\n\\[\n\\begin{split}\n\\E(\\log Y_i | X_i = x) =  \\log Y_i & = \\beta_0 + \\beta_1 \\log x \\\\\nY_i & = e^{\\beta_0 + \\beta_1 \\log x} \\\\\nY_i & = e^{\\beta_0}e^{\\beta_1 \\log x} \\\\\n\\E(\\log Y_i|X_i = ex) = \\log Y_i & = \\beta_0 + \\beta_1 \\log (ex) \\\\\nY_i & = e^{\\beta_0 + \\beta_1 \\log e + \\beta_1 \\log x} \\\\\nY_i & = e^{\\beta_0}e^{\\beta_1}e^{\\beta_1 \\log x}\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\frac{\\E(\\log Y_i|X_i = ex)}{\\E(\\log Y_i | X_i = x)} & = \\frac{e^{\\beta_0}e^{\\beta_1}e^{\\beta_1 \\log x}}{e^{\\beta_0}e^{\\beta_1 \\log x}} \\\\\n& = e^{\\beta_1}\n\\end{split}\n\\]\n\nThus, when \\(X_i\\) is multiplied by \\(e\\), there is a multiplicative increase of \\(e^{\\beta_1}\\) in \\(Y_i\\).\n\n\n\n\n\n\n\n\n\n\nExtension: Hiearchical Data\n\nClustered and Panel Data\nHierarchical data is data where the basic units of analysis \\(i\\) are clustered, grouped, or nested into clusters.\nFor example, let us say we are measuring how income affects voter turnout in european countries. We have observations from France, Switzerland, Germany, and many other countries. However, these observations can be grouped by the country they came from.\nWhy is this grouping important? This is because there may be something in common between observations within the same cluster. For example, Switzerland might just have higher voter turnout in general due to something about Swiss institutions or culture.\nThis means that observations aren’t random - i.e. we know that if we select from switzerland, it is likely to have higher turnout - observations from the same country are correlated. Thus, we need some way to account for this clustering of observations. We will explore this below.\nPanel data is data that can be clustered in two ways - by unit, and by time. For example, let us say we have a dataset on all countries and their GDP between 1960-2020.\nWe will have clusters based on country: Germany will have an observation in 1960, in 1961, …, to 2020. Same for every other country. These observations are grouped by the unit (country in this case).\nWe will also have clusters based on time: We will have all GDP observations for all countries in 1960, in 1961, etc. These observations are grouped by the time (year in this case).\n\n\n\nFixed Effects\nWhen we have hierarchical or panel data, we need to control for differences between clusters. We essentially include the cluster variable as a categorical variable in our regression. Let us say we have \\(m\\) number of clusters \\(i = 1, \\dots, m\\). Within each cluster, we will have units \\(t = 1, \\dots, n\\). Our model takes the form:\n\\[\n\\begin{split}\nY_{it} & = \\alpha_i + \\sum\\limits_{j=1}^p\\beta_jX_{itj} + \\eps_{it} \\\\\n& \\text{where } \\alpha_i = \\beta_{00} + \\underbrace{\\beta_{02}D_{i2} + \\beta_{03}D_{i3} + \\dots + \\beta_{0m}D_{im}}_{\\text{unique intercepts for each cluster}}\n\\end{split}\n\\]\n\nWhere \\(Y_{it}\\) is the value for the \\(i\\)th observation within the \\(t\\)th cluster.\nWhere \\(D_{i2}, D_{i3}, \\dots, D_{im}\\) are dummy variables for clusters \\(2, \\dots, m\\). Cluster 1 is the reference category.\n\nWe essentially add a unique intercept term for every cluster, accounting for the average differences in \\(Y_{it}\\) between each category. \\(\\beta_{00}\\) is the intercept for the reference category 1. \\(\\beta_{00} + \\beta_{0i}\\) is the intercept for the \\(i\\)th category.\nFor panel data, we use two-way fixed effects, which is basically just two fixed effects for different clustering. Let us say we have \\(i = 1, \\dots, m\\) units with \\(t = 1, \\dots, n\\) different numbers of time periods. Our two way fixed effects model takes the form:\n\\[\n\\begin{split}\nY_{it} & = \\alpha_i + \\gamma_t + \\sum\\limits_{j=1}^p\\beta_jX_{itj} + \\eps_{it} \\\\\n& \\text{where } \\alpha_i =  \\alpha_{00} + \\underbrace{\\alpha_{02}D_{2t} + \\alpha_{03}D_{3t} + \\dots + \\alpha_{0m}D_{mt}}_{\\text{unique intercepts for each unit}}, \\quad \\gamma_t =  \\gamma_{00} + \\underbrace{\\gamma_{02}T_{i2} + \\dots + \\gamma_{0n}T_{in}}_{\\text{unique intercepts for each time}} \\\\\n\\end{split}\n\\]\n\nWhere \\(Y_{it}\\) is the value of the \\(i\\)th unit at time period \\(t\\).\nWhere \\(D_{i2}, D_{i3}, \\dots, D_{im}\\) are dummy variables for units \\(2, \\dots, m\\)., and \\(T_{i2}, T_{i3}, \\dots, T_{in}\\) are dummy variables for time periods \\(2, \\dots, n\\).\n\nFor two-way fixed effects, we add a unique intercept term for every year and country, accounting for the average differences in \\(Y_{it}\\) between each country, and the average differences in \\(Y_{it}\\) between each year.\nNote: when conducting statistical inference with fixed-effects models, we must adjust our standard errors to clustered-standard errors. This is because observatios within a cluster are not independent of each other.\n\n\n\n\n\n\nExtension: Spatial Data\n\nSpatial Data and Spatial Weights\nSpatial data is data that has some geographic-spatial component. If we have spatial data, a region’s neighbours may have some effect on a region’s own outcomes:\n\nLag y: The \\(Y_i\\) of a region might be affected by a neighbouring region’s \\(Y_i\\) values. For example, one state having a high crime rate may affect another nearby state’s crime rate as criminals can move across the border.\nLag X: The \\(Y_i\\) of a region might be affected by a neighbour’s \\(X_i\\) values. For example, one state’s low vaccination rate could affect the prevalence of a disease in a nearby state.\nSpatial Autocorrelation: The residuals \\(\\eps_i\\) of a region might be affected by the residuals in a neigbouring region. For example, perhaps if we are studying how minimum wage affects unemployment rates, there could be some common factor with nearby states that affects unemployment rates (for example, prevalence of a certain industry) which is not included in our model.\n\nTo include spatial considerations, we include a spatial weights matrix \\(\\mathbf W\\). This defines what regions are “neighbours”. For example, imagine 3 regions A, B, C, with A on the left, B in the middle, and C on the right. The spatial weights matrix would be a matrix with rows \\(i = A, B ,C\\) and columns \\(j = A, B, C\\).Element \\(w_{ij}=1\\) if regions \\(i\\) and \\(j\\) border each other, and \\(w_{ij} = 0\\) if regions \\(i\\) and \\(j\\) do not touch each other:\n\\[\nW_{3 \\times 3} = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{pmatrix}\n\\]\nWe often “normalise/standardise” our weights matrix so that each row adds up to 1. Essentially, we divide an element by the number of 1’s in each row:\n\\[\nW_{3 \\times 3} = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0.5 & 0 & 0,5 \\\\\n0 & 1 & 0\n\\end{pmatrix}\n\\]\nThere are other ways to define neighbours \\(w_{ij} = 1\\). For example, we could consider any region who’s centroid is within \\(x\\) miles of another state’s centroid as a neighbour. This is up to the modelers discretion.\n\n\n\nSpatial Regression\nWe define neighbours with a weights matrix \\(\\mathbf W\\). The largest spatial model is the Manski Model, which incorporates lag y, lag X, and spatial autocorrelation.\n\\[\ny = \\underbrace{\\rho Wy}_{\\mathrm{lag \\ y}} + X\\beta + \\underbrace{WX\\theta}_{\\mathrm{lag \\ x}} + \\underbrace{\\lambda Wu}_{\\mathrm{autocor.}} + \\epsilon\n\\]\n\n\\(\\rho\\) (scalar) is the lag y coefficient, and measures how an average of neighbouring region’s \\(Y_i\\) values are associated with a region’s own \\(Y_i\\) values. For every one unit increase in the average value of \\(Y_i\\) in neighbouring regions, there is an expected \\(\\rho\\) unit change in \\(Y_i\\) in the region, holding all else constant.\n\\(\\beta\\) (vector) is our normal OLS coefficients \\(\\beta_0, \\dots, \\beta_p\\), and would be interpreted in the same way.\n\\(\\theta\\) (vector) is the lag X coefficients \\(\\theta_0, \\dots, \\theta_p\\), and measures how an average of neighbouring region’s \\(X_i\\) values are associated with a region’s own \\(Y_i\\) values. For every one unit increase in the average \\(X_{ij}\\) of neighbouring regions, there is an expected \\(\\theta_j\\) unit change in \\(Y_i\\) in the region, holding all else constant.\n\\(\\lambda\\) (scalar) is spatial autocorrelation coefficient, and measures how an average of neighbouring regions’ error terms are associated with a region’s own error term. Unobserved factors shared by neighbouring regions contribute to a \\(\\lambda\\)-unit change in \\(Y_i\\), holding all else constant.\n\nHowever, in reality, we often cannot estimate this entire model due to the large amount of parameters. Thus, multiple variations exist with only parts of the full model:\n\nKeleijian-Prucha Model: \\(y = \\rho W y + X\\beta + \\lambda W u + \\epsilon\\).\nSpatial Durbin Model: \\(y = \\rho W y + X\\beta + WX\\theta + \\epsilon\\).\nSpatially Lagged X Model: \\(y = X\\beta + WX\\theta + \\epsilon\\).\nSpatial Lag Model (Spatial Autoregressive): \\(y = \\rho W y + X\\beta + \\epsilon\\).\nSpatial Error Model: \\(y = X\\beta + \\lambda W u + \\epsilon\\).\n\nThe models are estimated with Maximum Likelihood Estimation (see next chapter), and model selection can be done with a combination of theory regarding the topic, eliminating insignificant portions, and AIC and BIC metrics.\n\n\n\n\n\n\nImplementation in R\nYou will need package fixest and estimatr.\n\nlibrary(fixest)\nlibrary(estimatr)\n\nRegression with normal standard errors can be done with the lm() function:\n\nmodel &lt;- lm(y ~ x1 + x2 + x3, data = mydata)\nsummary(model)\n\nRegression with robust standard errors can be done with the feols() function or lm_robust() function:\n\n# feols\nmodel &lt;- feols(y ~ x1 + x2 + x3, data = mydata, se = \"hetero\")\nsummary(model)\n\n# lm robust\nmodel &lt;- lm_robust(y ~ x1 + x2 + x3, data = mydata)\n\nOutput will include coefficients, standard errors, p-values, and more.\n\n\n\n\n\n\nBinary and Categorical Variables\n\n\n\n\n\nYou can include binary and categorical variables by using the as.factor() function:\n\nfeols(y ~ x1 + as.factor(x2) + x3, data = mydata, se = \"hetero\")\n\nYou can do the same for \\(y\\) or \\(x\\). Just remember, \\(y\\) cannot be a categorical variable (use multinomial logsitic regression instead).\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n\n\n\n\nYou can include one-way fixed effects by adding a | after your regression formula in feols():\n\nmodel &lt;- feols(y ~ x1 + x2 + x3 | cluster,\n               data = mydata, se = \"hetero\")\nsummary(model)\n\nYou can add two-way fixed effects as follows:\n\nmodel &lt;- feols(y ~ x1 + x2 + x3 | unit + year,\n               data = mydata, se = \"hetero\")\nsummary(model)\n\n\n\n\n\n\n\n\n\n\nInteraction Effects\n\n\n\n\n\nTwo interact two variables, use * between them. This will automatically include both the interaction term, and the two variables by themselves.\n\nfeols(y ~ x1 + x2*x3, data = mydata, se = \"hetero\")\n\nIf for some reason, you only want the interaction term, but not the variables by themselves, you can use a colon : between the two variables:\n\nfeols(y ~ x1 + x2:x3, data = mydata, se = \"hetero\")\n\n\n\n\n\n\n\n\n\n\nPolynomial Transformations\n\n\n\n\n\nTo conduct a polynomial transformation, you can use the I() function. The second argument is the degree of the polynomial:\n\nfeols(y ~ x1 + I(x2, 3), data = mydata, se = \"hetero\") #cubic for x2\n\n\n\n\n\n\n\n\n\n\nLogarithmic Transformations\n\n\n\n\n\nThe best way to do a logarithmic transformation is to create a new variable that is the log of the variable you want to transform using the log() function, before you even start the regression:\n\nmydata$x1_log &lt;- log(mydata$x1)\n\n\n\n\n\n\n\n\n\n\nConfidence Intervals\n\n\n\n\n\nTo find the confidence intervals for coefficients, first estimate the model with lm() or feols() as shown previously, then use the confint() command:\n\nconfint(model)\n\n\n\n\n\n\n\n\n\n\nF-Tests\n\n\n\n\n\nTo run a f-test, use the anova() command, and input your two different models, with the null model going first.\n\nanova(model1, model2)\n\nNote: F-tests only work with models that are run with homoscedastic standard errors. Robust standard errors will not work.\n\n\n\n\n\n\n\n\n\nLaTeX Regression Tables\n\n\n\n\n\nYou can use the texreg package to make nice regression tables automatically.\n\nlibrary(texreg)\n\nThe syntax for texreg() is as follows:\n\ntexreg(l = list(model1, model2, model3),\n       custom.model.names = c(\"model 1\", \"model 2\", \"model 3\"),\n       custom.coef.names = c(\"intercept\", \"x1\", \"x2\"),\n       digits = 3)\n\nYou can replace texreg() with screenreg() if you want a nicer regression table in the R-console.\nNote: you must have the same amount of model names as total models in your texreg, and you must have the same amount of coeficient names as the total amount of coefficients in all of your models.\n\n\n\n\n\n\n\n\n\nPrediction\n\n\n\n\n\nWe can use the predict() function to generate fitted value predictions in R:\n\nmy_predictions &lt;- predict(model, newdata = my_new_data)\n\nmy_new_data is a dataframe with a bunch of explanatory variable values (for every explanatory variable) for a collection of observations, that you wish to predict \\(\\hat y\\) for.\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "quant1.html#details-of-the-f-test",
    "href": "quant1.html#details-of-the-f-test",
    "title": "Classical Linear Model",
    "section": "Details of the F-test",
    "text": "Details of the F-test\nF-tests compare the \\(R^2\\) of the two models through the F-statistic:\n\\[\nF = \\frac{(SSR_0 - SSR_a) / (k_a - k_0)}{SSR_a /(n - k_a - 1)}\n\\]\nWe then consult a F-distribution with \\(k_a - k_0\\) and \\(n-k_a - 1\\) degrees of freedom, obtaining a p-value (in the same way as the t-test).\nThe p-value we get is the probability of getting a test statistic equally or more extreme than the one we got, given the null hypothesis is true.\n\nIf \\(p&lt;0.05\\), the we believe the probability of the null hypothesis is low enough, such that we reject the null hypothesis (that \\(M_0\\) is the better model), and conclude our alternate hypothesis (that \\(M_a\\) is a better model). This also means the extra coefficients in \\(M_a\\) are jointly statistically significant.\nIf \\(p &gt; 0.05\\), we cannot reject the null hypothesis, and cannot reject that \\(M_0\\) is a better model. Thus, the extra coefficients in \\(M_a\\) are jointly not statistically significant.",
    "crumbs": [
      "1 Classical Linear Model"
    ]
  },
  {
    "objectID": "clm.html",
    "href": "clm.html",
    "title": "The Classical Linear Model",
    "section": "",
    "text": "Before we start with causal inference, we need to understand relationships between variables (correlations). This chapter inroduces the classical linear model, its mechanics, and how we can achieve unbiased estimates of relationships. This chapter discusses the classical linear models, the properties of the ordinary least squares estimator, statistical inference in a regression setting, and model selection.\nUse the right sidebar for quick navigation. R-code is provided at the bottom.\n\n\nThe Classical Model\n\nModel Specification\nThere is some random outcome variable \\(Y_i\\), and \\(p\\) explanatory variables \\(X_{i1}, X_{i2}, \\dots, X_{ip}\\). The population linear model is specified as a conditional expectation function:\n\\[\n\\E(Y_i|X_i) = \\beta_0 + \\beta_1X_{i1} + \\dots + \\beta_p X_{ip} \\ \\equiv \\ \\beta_0 + \\sum\\limits_{j=1}^p \\beta_jX_{ij}\n\\]\nWhere vector \\(\\beta\\) are population parameters to be estimated. We can rewrite this with vectors \\(x_i\\) and \\(\\beta\\):\n\\[\n\\E(Y_i|X_i) = x_i^\\top\\beta, \\quad x_i = \\begin{pmatrix} 1 \\\\ X_{i1} \\\\ \\vdots \\\\ X_{ip}\\end{pmatrix}, \\ \\beta = \\begin{pmatrix} \\beta_0 \\\\ \\beta_{1} \\\\ \\vdots \\\\ \\beta_{p}\\end{pmatrix}\n\\]\nInstead of \\(\\E(Y_i|X_i)\\), We can also specify for each observation of \\(Y_i\\):\n\\[\n\\begin{align}\nY_i = \\beta_0 + \\sum\\limits_{j=1}^p \\beta_jX_{ij} + \\eps_i \\quad  \\iff \\quad Y_i =  x_i^\\top \\beta + \\eps_i\n\\end{align}\n\\]\nWhere \\(\\eps_i\\) is the error term representing the variation in \\(Y_i\\) that is not explained by the \\(p\\) explanatory variables: either a missing variable in our model, or some random noise. This model implies for each observation/individual \\(i=1,\\dots ,n\\) in the population, with values \\((y_i, x_{i1} \\dots, x_{ip})\\) has their own regression equation. We can write this system of \\(n\\) regression equations in linear algebra form:\n\\[\ny = X\\beta + \\eps \\quad \\iff \\quad \\begin{pmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\\end{pmatrix}  =\n\\begin{pmatrix}1 & x_{11} & \\dots & x_{1p} \\\\1 & x_{21} & \\dots & x_{2p} \\\\\\vdots & \\vdots & \\vdots & \\vdots \\\\1 & x_{n1} & \\dots & x_{np}\\end{pmatrix}\n\\begin{pmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p\\end{pmatrix}\n+ \\begin{pmatrix}\\eps_1 \\\\ \\eps_2 \\\\ \\vdots \\\\ \\eps_n\\end{pmatrix}\n\\]\nOur goal is to estimate \\(\\beta\\) to get our estimates \\(\\hat\\beta\\), which allows us to interpret the relationships between \\(X\\) and \\(y\\), as well as create predictions of \\(y\\) with fitted values \\(\\hat y = X\\hat\\beta\\).\n\n\n\nThe Classical Assumptions\nThe classical model above has several “classical” assumptions that are normally assumed to be met:\n1) Linearity in Parameters. The linear model must be able to be written in the form \\(y = X\\beta + \\eps\\). This does not mean the best-fit line must be linear (as we will explore later).\n2) Independent and Identically Distributed (i.i.d.). Any two observations \\(i\\) and \\(j\\) are sampled from the same random variable distribution with the same probabilities.\n3) No Perfect Multicolinearity. No explanatory variables \\(X_{i1}, \\dots, X_{ip}\\) are perfectly correlated, i.e. can be written as an exact linear combination of other explanatory variables in the model.\n4) Zero Conditional Mean \\(\\E(\\eps|X) = 0\\). This can be broken down into two parts. First, \\(\\E(\\eps) = 0\\): This is always met since if it is not 0, you can adjust \\(\\beta_0\\) until it is 0. Second, \\(\\E(X^\\top \\eps) = 0\\). This means that all regressors \\(X_{ij}\\), and any combination of regressors, should be uncorrelated with the error term \\(\\eps\\).\n5) Spherical Errors. This is an assumption made on the variance-covariance matrix of the error terms \\(\\eps_i\\):\n\\[\n\\underbrace{\\V(\\eps|X)}_{\\mathrm{cov. \\ matrix}} = \\begin{pmatrix}\n\\V\\eps_1 & cov(\\eps_1, \\eps_2) & cov(\\eps_1, \\eps_3) & \\dots \\\\\ncov(\\eps_2, \\eps_1) & \\V\\eps_2 & cov(\\eps_2, \\eps_3) & \\dots \\\\\ncov(\\eps_3, \\eps_1) & cov(\\eps_3, \\eps_2) & \\V\\eps_3 & \\vdots \\\\\n\\vdots & \\vdots & \\dots & \\ddots\n\\end{pmatrix} = \\sigma^2 I_n = \\begin{pmatrix}\n\\sigma^2 & 0 & \\dots & 0 \\\\\n0 & \\sigma^2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & 0 \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{pmatrix}\n\\tag{1}\\]\nThis assumption can also be broken into two parts. No Autocorrelation means all error terms are uncorrelated \\(cov(\\eps_i, \\eps_j) = 0\\), and is a result of the earlier i.d.d. assumption. Second, Homoscedasticity says that all the variances of each error term \\(\\V(\\eps_i)\\) is constant at some value \\(\\sigma^2\\), which implies \\(X\\) has no impact on the variance of \\(\\eps_i\\). If homoscedasticity is violated, we have heteroscedasticity.\n\n\n\n\n\n\nVisualising Homoscedasticity\n\n\n\n\n\nAn easy way to identify homoscedasticity is to look at a residual plot (just the plot of all \\(\\hat\\eps_i\\)):\n\n\n\n\n\nNotice how the homoscedasticity residuals seem to have the same up-down variance, no matter the value of \\(X_i\\).\nThe heteroscedasticity (when homoscedasticity is violated) residuals have a clear pattern - the up-down variance is smaller when \\(X_i\\) is smaller, and the up-down variance is larger when \\(X_i\\) is larger. Essentially, if you see a pattern in the residual plot, it is likely heteroscedasticity.\n\n\n\n\n\n\nOrdinary Least Squares Estimator\nTo estimate the population parameters \\(\\beta_0, \\dots, \\beta_p\\), we use our sample data and try to find the values \\(\\hat\\beta_0, \\dots, \\hat\\beta_p\\) that minimise the square sum of residuals (SSR): \\(\\sum(Y_i - \\hat Y_i)^2\\) .We will define the SSR as function \\(S(\\hat\\beta)\\):\n\\[\n\\begin{align}\nS(\\hat\\beta) &  = (y - \\hat y)^\\top (y - \\hat y) && (\\Sigma(Y - \\hat Y_i)^2 \\text{ in linear algebra})\\\\\n& = (y - \\color{blue}{X \\hat\\beta}\\color{black} )^\\top (y - \\color{blue}{X \\hat\\beta}\\color{black})  && (\\because \\color{blue}{\\hat y = X \\hat\\beta}\\color{black})\\\\\n& = y^\\top y - \\hat\\beta^\\top X^\\top y - y^\\top X \\hat\\beta +  \\hat\\beta^\\top X^\\top X \\hat\\beta && (\\text{distribute out}) \\\\\n& = y^\\top y \\ \\color{blue}{-  2 \\hat\\beta^\\top X^\\top y} \\color{black}  +  \\underbrace{\\hat\\beta^\\top X^\\top X \\hat\\beta}_{\\text{quadratic}} && (\\because \\color{blue}{- \\hat\\beta^\\top X^\\top y - y^\\top X \\hat\\beta = - 2 \\hat\\beta^\\top X^\\top y} \\color{black})\n\\end{align}\n\\]\nNow, let us take the gradient to find the first order condition:\n\\[\n\\frac{\\partial S(\\hat\\beta)}{\\partial \\hat\\beta} = -2 X^\\top y + 2 X^\\top X \\hat\\beta = 0\n\\]\nWhen assuming \\(X^\\top X\\) is invertable (which is true if the no perfect multicollinearity classical assumption is met), we can isolate \\(\\hat\\beta\\) to find the solution to OLS:\n\\[\n\\begin{align}\n-2 X^\\top y + 2 X^\\top X \\hat\\beta & = 0 \\\\\n2 X^\\top X \\hat\\beta & = 2 X^\\top y && ( + 2X^\\top y \\text{ to both sides}) \\\\\n\\hat\\beta & = (2X^\\top X)^{-1} -2 X^\\top y && (\\times (2X^\\top X)^{-1} \\text{ to both sides}) \\\\\n\\hat\\beta & = (X^\\top X)^{-1} X^\\top y && (2^{-1}, 2 \\text{ cancel out})\n\\end{align}\n\\tag{2}\\]\nVector \\(\\hat\\beta\\) is our parameter estimates. With \\(\\hat\\beta\\), we now have our “fitted” model:\n\\[\n\\hat y = X\\hat\\beta  \\ \\equiv \\ X(X^\\top X)^{-1}X^\\top y\n\\]\n\n\n\n\n\n\nAlternative Derivation for Simple Linear Regression\n\n\n\n\n\nFor simple linear regression (with one explanatory variable), we can use summation notation instead of linear regression. Our SSR in summation form is:\n\\[\nSSR = S(\\hat\\beta_0, \\hat\\beta_1)= \\sum\\limits_{i=1}^n(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i)^2\n\\]\nWe want to minimise the SSR in respect to both \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\). We can do this by finding our first order conditions:\n\\[\n\\begin{align}\n\\frac{\\partial S(\\hat\\beta_0, \\hat\\beta_1)}{\\partial \\hat\\beta_0} & = \\sum\\limits_{i=1}^n(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i) = 0 \\\\\n\\frac{\\partial S(\\hat\\beta_0, \\hat\\beta_1)}{\\partial \\hat\\beta_1} & = \\sum\\limits_{i=1}^n X_i(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i) = 0 \\\\\n\\end{align}\n\\]\nThese conditions create a system of equations, which you can solve for the OLS solutions of \\(\\widehat{\\beta_0}\\) and \\(\\widehat{\\beta_1}\\). I will not show it step by step, as it is tedious (and not that important). The OLS solutions are\n\\[\n\\begin{align}\n\\hat\\beta_0 & = \\bar Y - \\widehat{\\beta_1} \\bar X \\\\\n\\hat\\beta_1 & = \\frac{\\sum_{i=1}^n(X_i - \\bar X)(Y_i - \\bar Y)}{\\sum_{i=1}^n(X_i - \\bar X)^2} = \\frac{Cov(X_i, Y_i)}{\\V Y_i}\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nProof OLS Estimates are Equal with \\(Y_i\\) and \\(\\E(Y_i|X_i)\\)\n\n\n\n\n\nBefore, we specified the linear model in terms of both \\(Y_i\\) and the conditional expectation function \\(\\E(Y_i|X_i)\\). However, I only derived OLS estimates in respect to \\(Y_i\\).\nBest-approximation of a conditional expectation function is defined by the lowest mean-squared error (MSE). Let us prove OLS on \\(Y_i\\) gets the same \\(\\beta_0, \\dots, \\beta_p\\) as the best linear approximation of \\(\\E(Y_i|X_i)\\). Take this very simple CEF and its MSE:\n\\[\n\\begin{align}\n\\E(Y_i|X_i) & = b_0 + b_1X_i \\\\\nMSE & = \\E(Y_i - \\E(Y_i|X_i))^2 \\\\\n& =  \\E(Y_i - (\\color{blue}{b_0 + b_1X_i}\\color{black}))^2  && (\\because \\color{blue}{\\E(Y_i|X_i) = b_0 + b_1X_i}\\color{black})\\\\\n& = \\E(Y_i - b_0 - b_1 X_i) && \\text{(distribute negative sign)}\n\\end{align}\n\\]\nThe first order conditions are (using chain rule and partial derivatives):\n\\[\n\\begin{split}\n& \\E(Y_i - b_0 - b_1X_i) = 0 \\\\\n& \\E(X_i(Y_i - b_0 - b_1X_i) = 0\n\\end{split}\n\\]\nNow, recall our OLS minimisation conditions (simple linear regression, presented in the extra info box above). Since by definition, average/expectation is \\(\\E(x) = \\frac{1}{n} \\sum x_i\\), we can rewrite as:\n\\[\n\\begin{split}\n& \\sum\\limits_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i) = \\ n \\times \\E(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i) = 0 \\\\\n& \\sum\\limits_{i=1}^n X_i (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i) = \\  n \\times \\E(X_i(Y_i - \\hat\\beta_0 - \\hat\\beta_1X_i)) = 0\n\\end{split}\n\\]\nAnd since anything multiplied to a zero turns into zero, we can ignore the \\(n\\) in the first order condition. Thus, we can see, are the exact same minimisation conditions as the conditional expectation function.\n\n\n\n\n\n\nOrthogonal Projection of OLS\nWe can use the OLS solution from Equation 2 to get our fitted model \\(\\hat{y}\\) and residuals \\(\\hat\\eps\\):\n\\[\n\\begin{align}\n\\hat y &  = X (X^\\top X)^{-1}X^\\top y  = \\color{red}{P}\\color{black}y && (\\because \\color{red}{P:= X(X^\\top X)^{-1}X^\\top}) \\\\\n& \\\\\n\\hat\\eps & = y - \\hat y  = y - \\color{blue}{Py} && \\color{black}( \\because \\color{blue}{\\hat y = Py}\\color{black}) \\\\\n& = (I-P)y && (\\text{factor out y}) \\\\\n& = \\color{purple}{M}\\color{black}y && (\\because \\color{purple}{M:= I - P}\\color{black})\n\\end{align}\n\\]\nMatrix \\(\\color{red}{P}\\), called the projection matrix, is a matrix operator that performs the linear mapping \\(y \\rightarrow \\hat{ y}\\). Matrix \\(\\color{purple}{M}\\), called the residual maker, is a matrix operator that performs the linear mapping \\(y \\rightarrow \\hat{\\eps}\\). Both matrices are symmetric and idempotent (explanation in info box).\n\n\n\n\n\n\nMatrix Properties of \\(P\\) and \\(M\\)\n\n\n\n\n\nBoth \\(\\color{red}{P}\\) and \\(\\color{purple}{M}\\) are symmetric matrices: \\(P^\\top = P, \\ M^\\top = M\\). They are also both idempotent matrices: \\(PP = P, \\ MM = M\\). We can prove this second statement using the first (I will only do it for \\(P\\), but the same applies for \\(M\\):\n\\[\n\\begin{align}\nPP & = X(X^\\top X)^{-1} \\underbrace{X^\\top X(X^\\top X)^{-1}}_{= I} X^\\top \\\\\n& = X(X^\\top X)^{-1} X^\\top = P\n\\end{align}\n\\]\n\\(\\color{red}{ P}\\) and \\(\\color{purple}{ M}\\) are also orthogonal to each other - i.e. \\(P^\\top M = 0\\):\n\\[\n\\begin{align}\nP^\\top M & = \\color{blue}{P}\\color{black}M && (\\because \\color{blue}{P^\\top = P}\\color{black}) \\\\\n& = P(\\color{blue}{I-P}\\color{black}) && (\\because \\color{blue}{M:= I - P}\\color{black}) \\\\\n& = P - PP && \\text{(distribute out)} \\\\\n& = P - \\color{blue}{P} && \\color{black}(\\because \\color{blue}{PP = P}\\color{black}) \\\\\n& = 0\n\\end{align}\n\\]\n\n\n\nFitted values \\(\\hat{y}\\) are a linear combination of explanatory variable vectors \\(x_1, x_2, \\dots, x_p\\), and thus \\(\\hat y\\) is in the vector space spanned by \\(X\\). Projection matrix \\(\\color{red}{ P}\\) projects \\(y\\) into \\(\\hat y\\) which is in the space spanned by our \\(X\\) (called the column space):\n\n\n\n\n\nResidual maker \\(\\color{purple}{M}\\) projects \\(y\\) onto the space orthogonal to the column space of \\(X\\) to get our residuals \\(\\hat{\\eps}\\). We can see in the figure the residuals vector (notated \\(\\mathbf e\\) in the figure) is orthogonal/perpendicular to the space of \\(\\mathbf X\\).\n\n\n\nGoodness of Fit with R-Squared\nRecall our fitted values equation, shown previously, can be rewritten with the projection matrix \\(P\\):\n\\[\n\\begin{align}\n\\hat y = X(X^\\top X)^{-1} X^\\top y \\ = \\ \\color{blue}{P}\\color{black}y && (\\because \\color{blue}{P := X(X^\\top X)^{-1} X^\\top})\n\\end{align}\n\\]\nWe are interested in is how well our model \\(Py\\) explains the actual \\(y\\). The scalar product \\(y^\\top Py\\) describes the shadow the actual \\(y\\) casts on our projected model. We can divide it by \\(y^\\top y\\), which is the “maximum” shadow possible (perfect shadow) to create a value between 0 and 1. This ratio is \\(R^2\\).\n\\[\nR^2 = \\frac{y^\\top Py}{y^\\top y}\n\\]\nThe total amount of variation in \\(y\\) is called the total sum of squares (SST). The part of \\(y\\) we cannot explain is the SSR that we minimised for OLS esimtation. That implies that the remaining part SST-SSR is the variation in \\(y\\) that our model explains, called the sum of explained squares (SSE). \\(R^2\\) can be though of the ratio of explained variation in \\(y\\) by our model to the total variation in \\(y\\):\n\\[\nR^2 = \\frac{SSE}{SST} = \\frac{SST - SSR}{SST} = 1 - \\frac{SSR}{SST} = 1 - \\frac{\\sum (Y_i - \\hat Y_i)^2}{\\sum(Y_i - \\bar Y)^2}\n\\]\nR-Squared (\\(R^2\\)) measures the proportion of variation in \\(y\\) that is explained by our explanatory variables. R-Squared is always between 0 and 1 (0%-100%). Higher values indicate our model better explains the variation in \\(y\\).\n\n\n\nRegression Anatomy\nWe can split up matrix \\(X\\) into two matrices - \\(X_1\\) containing the regressors we care about, and \\(X_2\\) containing regressors we do not care about. Vector \\(\\beta\\) will be split in the same way. Our partitioned model is:\n\\[\ny = X_1 \\beta_1 + X_2 \\beta_2 + \\eps\n\\]\nRecall “residual maker” matrix \\(M\\). First, note a unique property: \\(\\color{red}{MX = 0}\\). Now, let us define the residual making matrix for the second part of the regression \\(M_2\\):\n\\[\nM_2 = I - X_2 (X_2^\\top X_2)^{-1}X_2^\\top\n\\]\nNow, let us multiply both sides of our above partitioned model by \\(M_2\\):\n\\[\n\\begin{align}\nM_2 y & = M2(X_1\\beta_1 + X_2\\beta_2 + \\eps) \\\\\nM_2 y & = M_2X_1 \\beta_1 + M_2 X_2 \\beta_2 + M_2 \\eps && \\text{(multiply out)} \\\\\nM_2 y & = M_2 X_1 \\beta_1 + M_2 \\eps && (\\because M_2X_2 = 0, \\ \\because \\color{red}{MX = 0}\\color{black})\n\\end{align}\n\\]\nNow, let us define \\(\\tilde{y} := M_2 y\\), \\(\\tilde{X}_1: = M_2 X_1\\), and error \\(\\tilde\\eps := M_2 \\eps\\). Then we get the following regression equation and OLS coefficient estimates:\n\\[\n\\tilde y = \\tilde X_1 \\beta_1 + \\tilde\\eps\n\\]\n\\[\n\\hat\\beta_1 = (\\tilde X_1^\\top \\tilde X_1)^{-1}\\tilde X_1 ^\\top \\tilde y\n\\]\nRemember that vector \\(\\hat{\\beta}_1\\) is our coefficient estimates for \\(X_1\\), the portion of \\(X\\) we are interested in. This is equivalent to the coefficient estimates had we not partitioned the model.\nNotice how in the formula, we have \\(\\tilde{X}_1 := M_2 X_1\\). We know that \\(M_2 X_2 = 0\\). That tells us that any part of \\(X_1\\) that was correlated to \\(X_2\\) also became 0. Thus, \\(\\tilde{X}_1\\) is the part of \\(X_1\\) that is uncorrelated with \\(X_2\\). Essentially, we are partialling out the effect of other variables. This is why we can “control” for other variables when conducting multiple regression.\n\n\n\nInterpretation of Coefficients\nAbove, we showed OLS coefficients partial out (control) for the other control variables in the regression. Here, we formalise the interpretations. I define \\(\\hat\\beta_j \\in \\{\\hat\\beta_1, \\dots, \\hat\\beta_p\\}\\), multiplied to \\(X_{ij} \\in \\{X_{i1}, \\dots, X_{ip}\\}\\). \\(\\hat\\beta_0\\) is the intercept.\nI assume here that \\(Y_i\\) is continuous. For interpretations for non-linear \\(Y_i\\), see the linear probability model section below. For categorical \\(X_{ij}\\), see the categorical explanatory variables section.\n\n\n\n\n\n\n\n\n\nContinuous \\(X_{ij}\\)\nBinary \\(X_{ij}\\)\n\n\n\\(\\hat\\beta_j\\)\nFor every one unit increase in \\(X_{ij}\\), there is an expected \\(\\hat\\beta_j\\) unit change in \\(Y_i\\), holding all other explanatory variables constant.\nThere is a \\(\\hat\\beta_j\\) unit difference in \\(Y_i\\) between category \\(X_{ij} = 1\\) and category \\(X_{ij} = 0\\), holding all other explanatory variables constant.\n\n\n\\(\\hat\\beta_0\\)\nWhen all explanatory variables equal 0, the expected value of \\(Y_i\\) is \\(\\hat\\beta_0\\).\nFor category \\(X_{ij} = 0\\), the expected value of \\(Y_i\\) is \\(\\hat\\beta_0\\) (when all other explanatory variables equal 0).\n\n\n\nNote: these interpretations are not causal effects, just correlations. We also have not discussed if the actual estimates of \\(\\hat\\beta_j\\) are reliable (which will be covered in the classical least squares theory below).\n\n\n\n\n\n\nStandardised Interpretations\n\n\n\n\n\nSometimes, unit change is not very useful - as it depends on how the variable is measured. For example, what does a 5 unit change in democracy mean? Is that big, small? It is hard to tell.\nInstead, we can look at the change in standard deviations. For a one standard deviation \\(\\sigma_X\\) increase in \\(X_{ij}\\), there is an expected \\(\\frac{\\beta_j\\sigma_X}{\\sigma_Y}\\)-standard deviation change in \\(Y_i\\). The proof is provided below.\nProof: For simplicity, let us use a simple linear regression \\(\\E(Y_i|X_i) = \\beta_0 + \\beta_1 X_i\\):\n\\[\n\\begin{align}\n& \\E \\left(\\frac{Y_i}{\\sigma_Y} | X_i = x + \\sigma_X \\right ) - \\E \\left(\\frac{Y_i}{\\sigma_Y} | X_i = x \\right ) \\\\\n& = \\frac{\\E(Y_i|X_i = x+ \\sigma_X)}{\\sigma_Y} - \\frac{\\E(Y_i|X_i = x)}{\\sigma_Y} &&\\text{(property of expectation)} \\\\\n& = \\frac{\\E(Y_i|X_i = x+ \\sigma_X) - \\E(Y_i|X_i = x)}{\\sigma_Y} && \\text{(combine into 1 fraction)}\\\\\n& = \\frac{\\beta_0 + \\beta_1(x+\\sigma_X) - [\\beta_0 + \\beta_1(x)]}{\\sigma_Y} && \\text{(plug in regression models)}\\\\\n& = \\frac{\\beta_1\\sigma_X}{\\sigma_Y} && \\text{(cancel and simplify)}\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nLeast Squares Theory\n\nUnbiasedness of OLS\nOLS is an unbiased estimator of the relationship between any \\(X_{ij}\\) and \\(Y_i\\) under the first 4 classical assumptions: linearity, i.i.d., no perfect multicollinearity, and zero-conditional mean.\n\n\n\n\n\n\nUnbiased Estimators\n\n\n\n\n\nWe have some true population parameter \\(\\theta\\) that we are estimating with a sample. Our sample will produce some estimate \\(\\hat\\theta_n\\).\nImagine we take another sample from the population: we will get a slightly different estimate. Taking \\(N\\) samples, we get estimates \\(\\hat\\theta_1, \\hat\\theta_2, \\dots, \\hat\\theta_N\\). Thus, our single sample estimate \\(\\hat\\theta_n\\) is actually a realisation of a random variable, drawn from the sampling distribution \\(\\hat\\theta_1, \\dots, \\hat\\theta_N\\). The best guess of the value of \\(\\hat\\theta_n\\) is the expected value \\(\\E \\hat\\theta_n\\) of the distribution.\nAn unbiased estimator is when \\(\\E\\hat\\theta_n = \\theta\\), or in other words, the expected sample estimate when drawn from the sampling distribution is equal to the true population value \\(\\theta\\). We want an unbiased estimator, because that means any sample estimate \\(\\hat\\theta_n\\) is expected to equal the true population parameter.\n\n\n\nLet us prove OLS is unbiased \\(\\E\\hat\\beta = \\beta\\) under the 4 classical assumptions. Let us manipulate our OLS solution:\n\\[\n\\begin{align}\n\\hat\\beta & = (X^\\top X)^{-1} X^\\top y \\\\\n& = (X^\\top X)^{-1} X^\\top(\\color{blue}{X\\beta + \\eps}\\color{black}) && (\\because \\color{blue}{y = X\\beta + \\eps}\\color{black}) \\\\\n& = \\underbrace{(X^\\top X)^{-1} X^\\top X}_{= \\ I}\\beta + (X^\\top X)^{-1}X^\\top \\eps && \\text{(multiply out)} \\\\\n& = \\beta + (X^\\top X)^{-1}X^\\top \\eps\n\\end{align}\n\\tag{3}\\]\nNow, let us take the expectation of \\(\\hat\\beta\\) conditional on \\(X\\). Remember condition 4, \\(\\E(\\eps | X) = 0\\):\n\\[\n\\E(\\hat\\beta | X) = \\beta + (X^\\top X)^{-1} \\underbrace{\\E(\\eps | X)}_{= \\ 0} \\  = \\ \\beta\n\\]\nNow, we can use the law of iterated expectations (LIE) to conclude this proof:\n\\[\n\\begin{align}\n\\E \\hat\\beta & = \\E(\\E(\\hat\\beta|X)) && (\\because \\mathrm{LIE}) \\\\\n& = \\E(\\color{blue}{\\beta}\\color{black}) && (\\because \\color{blue}{\\E(\\hat\\beta | X = \\beta)}\\color{black}) \\\\\n& = \\beta && \\text{(expectation of a constant)}\n\\end{align}\n\\]\nThus, OLS is unbiased under the 4 conditions above. This is extremely desirable, as in causal inference (which is a major goal in the social sciences), we will want unbiased estimators that can accurately find the causal effects of one variable on another. In fact, most of the methods we cover in later chapters will be about trying to satisfy these conditions.\n\n\n\nVariance of the OLS Estimator\nWe want to find the variance of our estimator’s sampling distribution, \\(\\V(\\hat\\beta | X)\\), under all 5 of the classical assumptions.\n\n\n\n\n\n\nVariance of Estimators\n\n\n\n\n\nWe have some true population parameter \\(\\theta\\) that we are estimating with a sample. Our sample will produce some estimate \\(\\hat\\theta_n\\).\nImagine we take another sample from the population: we will get a slightly different estimate. Taking \\(N\\) samples, we get estimates \\(\\hat\\theta_1, \\hat\\theta_2, \\dots, \\hat\\theta_N\\). Thus, our single sample estimate \\(\\hat\\theta_n\\) is actually a realisation of a random variable, drawn from the sampling distribution \\(\\hat\\theta_1, \\dots, \\hat\\theta_N\\). The best guess of the value of \\(\\hat\\theta_n\\) is the expected value \\(\\E \\hat\\theta_n\\) of the distribution.\nHowever, our sampling distribution of \\(\\hat\\theta_1, \\dots, \\hat\\theta_N\\) can also be described by the variance - how spread out the distribution is. Even if our estimator is unbiased, if it has a very high variance (spread), that means any specific realisation \\(\\hat\\theta_n\\) might be far away from the expected value. Thus, we want a low-variance estimator.\n\n\n\nFirst, let us start off where we left off in Equation 3:\n\\[\n\\begin{align}\n& \\hat\\beta = \\beta + (X^\\top X)^{-1} X^\\top \\eps \\\\\n& \\V(\\hat\\beta | X) = \\V(\\beta + (X^\\top X)^{-1} X^\\top \\eps)\n\\end{align}\n\\]\n\n\n\n\n\n\nLemma: Property of Variance\n\n\n\n\n\nLemma: If \\(\\eps\\) is an \\(n\\) dimensional vector of random variables, \\(c\\) is an \\(m\\) dimensional vector, and \\(B\\) is an \\(n \\times m\\) dimensional matrix with fixed constants, then the following is true (I will not prove this lemma here, but it is provable):\n\\[\n\\V(c + B\\eps) =  B \\V(\\eps) B^\\top\n\\tag{4}\\]\n\n\n\n\\(\\beta\\) is a vector of fixed constants. \\((X^\\top X)^{-1} X^\\top \\eps\\) can be imagined as a matrix of fixed constants, since we are conditioning the variance on \\(X\\) (so for each \\(X\\), it is fixed). With the Lemma above:\n\\[\n\\begin{align}\n\\V (\\hat\\beta | X) & = (X^\\top X)^{-1}X^\\top \\V(\\eps|X) [(X^\\top X)^{-1}X^\\top]^{-1} && \\text{(lemma)} \\\\\n& = (X^\\top X)^{-1}X^\\top \\V(\\eps|X) \\color{blue}{X(X^\\top X)^{-1}} && \\color{black}(\\because \\color{blue}{[(X^\\top X)^{-1}X^\\top]^{-1} = X(X^\\top X)^{-1}} \\color{black})\n\\end{align}\n\\tag{5}\\]\nNow, assuming spherical errors in Equation 1, we can conclude the derivation.\n\\[\n\\begin{align}\n\\V (\\hat\\beta | X) & = (X^\\top X)^{-1}X^\\top \\color{blue}{\\sigma^2I_n}\\color{black}{X} (X^\\top X)^{-1} && (\\because \\color{blue}{\\V(\\eps|X) = \\sigma^2 I_n}\\color{black}) \\\\\n& = \\color{blue}{\\sigma^2}\\color{black}{\\underbrace{(X^\\top X)^{-1}X^\\top X}_{= \\ I}(X^\\top X)^{-1}} && \\text{(rearrange and simplify)} \\\\\n& = \\sigma^2 (X^\\top X)^{-1}\n\\end{align}\n\\]\nThis is the variance of the sampling distribution of \\(\\hat\\beta\\). We will use this for the Gauss-Markov theorem (below), and also statistical inference later.\n\n\n\nGauss-Markov Theorem\nThe Gauss-Markov Theorem states that if all 5 classical assumptions are met, the OLS estimator is the best linear unbiased estimator (BLUE) - the unbiased linear estimator with the lowest variance. Any linear estimator takes the form \\(\\tilde{\\beta} = Cy\\), including OLS. For any linear estimator \\(\\tilde{\\beta} = Cy\\) to be unbiased, we need to assume \\(\\color{red}{CX = I}\\).\n\n\n\n\n\n\nProof \\(CX = I\\) For a Unbiased Linear Estimator\n\n\n\n\n\nFor any linear estimator \\(\\tilde{\\beta} = Cy\\) to be unbiased, we need to assume \\(\\color{red}{CX = I}\\). The proof of this is as follows:\n\\[\n\\begin{align}\n\\tilde\\beta =  C & (\\color{blue}{C\\beta + \\eps}\\color{black}) && (\\because \\color{blue}{y = X\\beta + \\eps}\\color{black}) \\\\\n=  C & X\\beta + C\\eps && \\text{(multiply out)} \\\\\n\\E(\\tilde\\beta | X) & = \\E(C X\\beta + C\\eps) \\\\\n& = CX\\beta + C \\underbrace{\\E(\\eps | X)}_{= \\ 0} && \\text{(take constants out of exp.)} \\\\\n& = CX\\beta \\\\\n& = \\color{red}{I}\\color{black}\\beta = \\beta && (\\because \\color{red}{CX = I}\\color{black}) \\\\\n\\E \\tilde\\beta & = \\E( \\E(\\tilde\\beta|X)) && \\text{(law of iterated expect.)} \\\\\n& = \\E(\\color{blue}{\\beta}\\color{black}) && (\\because \\color{blue}{\\E(\\tilde\\beta|X) = \\beta}\\color{black}) \\\\\n& = \\beta && \\text{(expect. of a constant)}\n\\end{align}\n\\]\nThus, we have shown \\(\\color{red}{CX = I}\\) is a necessary condition for any linear estimator \\(\\tilde{\\beta} = Cy\\) to be unbiased.\n\n\n\nNow, let us calculate the variance of \\(\\tilde{\\beta}\\), taking into consideration the lemma (Equation 4) used in the OLS variance:\n\\[\n\\begin{align}\n\\V(\\tilde\\beta | X) & = \\V(Cy|X) \\\\\n& = \\V(C(\\color{blue}{X\\beta + \\eps}\\color{black})|X) && (\\because \\color{blue}{y = X\\beta + \\eps}\\color{black}) \\\\\n& = \\V(CX\\beta + C\\eps | X) && \\text{(multiply out)} \\\\\n& = \\V(\\beta + C\\eps | X)  && (\\because CX = I)\\\\\n& = C \\V(\\eps | X) C^\\top && \\text{(using lemma)} \\\\\n& = C \\color{blue}{\\sigma^2 I_n} \\color{black} C^\\top && (\\mathrm{homoscedasticity} \\ \\color{blue}{\\V(\\eps|X) = \\sigma^2 I_n}\\color{black}) \\\\\n& = \\sigma^2 CC^\\top && \\text{(rearrange and simplify)}\n\\end{align}\n\\]\nNow, we want to show that the variance of the OLS estimator \\(\\hat{\\beta}\\) (under homoscedasticity) is smaller than any linear estimator \\(\\tilde{\\beta}\\). Let us find the difference between the variances of estimator \\(\\tilde{\\beta}\\) and \\(\\hat{\\beta}\\). Note: since \\(\\color{red}{CX = I}\\), the following is also true: \\(\\color{red}{ X^\\top C^\\top = (CX)^\\top = I}\\).\n\\[\n\\begin{align}\n\\V(\\tilde\\beta | X)  - \\V(\\hat\\beta|X) & = \\sigma^2 CC^\\top - \\sigma^2 (X^\\top X)^{-1} \\\\\n& = \\sigma^2(CC^\\top - (X^\\top X)^{-1}) && (\\text{factor out }\\sigma^2) \\\\\n& = \\sigma^2(CC^\\top - \\color{red}{CX}\\color{black}(X^\\top X)^{-1} \\color{red}{X^\\top C^\\top}\\color{black}) && (\\because \\color{red}{X^\\top C^\\top = CX = I}\\color{black}) \\\\\n& = \\sigma^2 C(I - X(X^\\top X)^{-1} X^\\top) C^\\top && (\\text{factor out }C, C^\\top) \\\\\n& = \\sigma^2 C \\color{blue}{M}\\color{black}C^\\top && (\\text{residual maker matrix } \\color{blue}{M}\\color{black})\n\\end{align}\n\\]\nWe know \\(\\sigma^2&gt;0\\) (variance of error term). Let us show \\(CMC^\\top\\) is positive semi-definite. We know that \\(M\\) is positive semi-definite if \\(z^\\top Mz ≥ 0\\) for every vector \\(z\\), and \\(M\\) is symmetric and idempotent\n\\[\n\\begin{align}\nz^\\top CMC^\\top z = \\underbrace{z^\\top CM}_{w^\\top} \\underbrace{M^\\top C^\\top z}_{w} = w^\\top w = \\sum\\limits_{i=1}^n w_i^2 ≥0\n\\end{align}\n\\]\nThus, \\(\\sigma^2 CMC^\\top\\) is positive, thus \\(\\V(\\tilde{\\beta}| X) &gt; \\V(\\hat{\\beta}| X)\\), thus OLS is BLUE.\n\n\n\nAsymptotic Consistency of OLS\nWe know OLS is unbiased under the first 4 classical assumptions: linearity, i.i.d., no perfect multicollinearity, and zero-conditional mean. OLS is also an asymptotically consistent estimator of \\(\\beta_j\\) under the first 3 classical assumptions, and one weakened version of the zero-conditional mean.\n\n\n\n\n\n\nAsymptotic Consistency of Estimators\n\n\n\n\n\nAn estimator is asymptotically consistency, if as we increase the sample size \\(n\\) towards \\(∞\\), the sampling distribution will become more and more concentrated around the true population \\(\\theta\\). At \\(n = ∞\\), our sampling distribution collapses to just the true population value \\(\\theta\\). Mathematically:\n\\[\n\\P(|\\hat\\theta_n - \\theta) &gt; \\epsilon) \\rightarrow 0, \\ \\mathrm{as} \\ n \\rightarrow ∞\n\\]\nThis essentially means that the probability that the difference between our sample estimate \\(\\hat\\theta_n\\) and \\(\\theta\\) is greater than some arbitrarily small value \\(\\epsilon\\) becomes 0, as our sample size approaches infinity.\nNote: estimators can be biased but consistent. This means they are biased at small sample sizes, but as sample size increases, they are consistent.\n\n\n\nFor asymptotic consistency, we replace zero-conditional mean with zero-mean and exogeneity: \\(\\E(\\eps_i) = 0\\), and \\(Cov(x_i, \\eps_i) = 0\\), which implies \\(E(X_i \\eps_i) = 0\\). This means that no regressor should be correlated with \\(\\eps\\). This is weaker than Zero-Conditional mean, since it means a function of regressors can be correlated with \\(\\eps_i\\).\n\n\n\n\n\n\nLemma: Vector Notation\n\n\n\n\n\nThe following statements are true (with \\(x_i\\) being a vector and \\(\\eps_i\\) being a scalar):\n\\[\n\\begin{split}\n& X^\\top X = \\sum\\limits_{i=1}^n x_i x_i^\\top\\\\\n&  X^\\top \\mathbf \\eps = \\sum\\limits_{i=1}^n x_i \\eps_i\n\\end{split}\n\\]\n\n\n\nLet us start of where we left of from Equation 3. Using vector notation, we can simplify:\n\\[\n\\begin{align}\n\\hat\\beta & = \\beta + (X^\\top X)^{-1} X^\\top \\eps \\\\\n& = \\beta \\left( \\sum\\limits_{i=1}^n x_i x_i^\\top \\right)^{-1} \\left( \\sum\\limits_{i=1}^n x_i \\eps_i \\right) && \\text{(vector notation)} \\\\\n& = \\beta + \\left( \\frac{1}{n}\\sum\\limits_{i=1}^n x_i x_i^\\top \\right)^{-1} \\left( \\frac{1}{n} \\sum\\limits_{i=1}^n x_i \\eps_i \\right) && (\\left(\\frac{1}{n}\\right)^{-1}, \\frac{1}{n} \\text{ cancel out})\n\\end{align}\n\\]\nNow, we apply probability limits to both sides, and then use the law of large numbers and zero-conditional mean and exogeniety condition to simplify:\n\\[\n\\begin{align}\n\\mathrm{plim}\\hat\\beta & = \\beta + \\left( \\mathrm{plim} \\frac{1}{n}\\sum\\limits_{i=1}^n x_i x_i^\\top \\right)^{-1} \\left( \\mathrm{plim}\\frac{1}{n} \\sum\\limits_{i=1}^n x_i \\eps_i \\right) \\\\\n& = \\beta + (\\E(x_i x_i^\\top))^{-1} \\underbrace{\\E(x_i \\eps_i)}_{= 0} = \\beta && \\text{(law of large numbers)}\n\\end{align}\n\\]\n\n\n\n\n\n\nProbability Limits and Law of Large Numbers\n\n\n\n\n\nWe have a random variable \\(X_i\\) with a true population mean \\(\\mu\\). Let us take a sample of \\(n\\) units \\((x_1, \\dots, x_n)\\). Our sample average will be \\(\\bar x_n\\). The law of large numbers states that:\n\\[\n\\mathrm{plim}(\\bar x_n) = \\mu\n\\]\nWhere plim (probability limit) states that as \\(n\\) approaches infinity, the probability distribution of \\(\\bar x_n\\) collapses around \\(\\mu\\). Thus, the law of large numbers states that the sample average estimator \\(\\bar x_n\\) is an asymptotically consistent estimator of the true population average \\(\\mu\\).\nProof: Recall from your basic statistics courses that the variance of a sample mean \\(\\bar x_n\\) is \\(\\sigma^2/ n\\). Now, as sample size \\(n\\) increases to infinity, we get:\n\\[\n\\lim\\limits_{n \\rightarrow ∞} \\frac{\\sigma^2}{n} = 0\n\\]\nThus, as the sample size increases to \\(∞\\), our sample mean \\(\\bar x_n\\)’s variance becomes 0, so the probability distribution collapses around the true population mean.\n\n\n\n\n\n\nRobust Standard Errors\nSo far, we have assumed that the classical assumptions are met. However, this is often not the case, especially with the final assumption: spherical errors (specifically homoscedasticity).\nHeteroscedasticity is when homoscedasticity is violated - which implies each \\(i\\) (based on its \\(X\\) values) has its own error: \\(\\V(\\eps_i|X) = \\sigma_i^2\\). We still assume no autocorrelation.\n\\[\n\\V(\\eps| X) = \\Omega = \\begin{pmatrix}\n\\sigma^2_1 & 0 & \\dots & 0 \\\\\n0 & \\sigma^2_2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & 0 \\\\\n0 & 0 & \\dots & \\sigma^2_n\n\\end{pmatrix}\n\\]\nWhat are the implications of heteroscedasticity? First, heteroscedasticity does not bias OLS, as the unbiasedness proof only relies on the first 4 classical assumptions.\nSecond, heteroscedasticity means OLS is no longer BLUE - i.e. there exists are more efficient unbiased linear estimator (the generalised least squares estimator, which we will not cover since it is rarely used).\nThird, and most relevant to us, is that the OLS variance formula (and standard errors) are no longer valid. Instead, we have to use the Huber-White Standard Errors (also called robust standard errors). This is because if we recall from Equation 5, we originally simplified this equation using the homoscedasticity assumption. Instead, for robust standard errors, we start from Equation 5, and plug in our error covariance matrix \\(\\Omega\\) from above:\n\\[\n\\V(\\hat{\\beta}| X)  = (X^\\top X)^{-1} X^\\top \\color{blue}{\\begin{pmatrix}\n\\sigma^2_1 & 0 & \\dots & 0 \\\\\n0 & \\sigma^2_2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & 0 \\\\\n0 & 0 & \\dots & \\sigma^2_n\n\\end{pmatrix}}\\color{black} X ( X^\\top X)^{-1}\n\\]\n\n\n\n\n\n\nStatistical Inference\n\nNormality and Standard Errors\nWe need to know the form of the \\(\\hat\\beta\\) sampling distribution for statistical tests. We can impose a condition of normality of the error terms: \\(\\eps | X \\sim \\mathcal N(0, \\sigma^2 I)\\). This will ensure the sampling distribution of \\(\\hat\\beta\\) is normally distributed. If our sample size is sufficiently large, we can invoke the central limit theorem, which says that the sampling distribution of \\(\\hat\\beta\\) is approximately normal if our sample size \\(n\\) is large enough.\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\n\n\nLet us say we have some random sample \\((z_1, \\dots, z_n)\\). We have some estimator, for simplicity, let us say sample mean \\(\\bar z_n\\) (a random variable). The expected value of this estimate \\(\\E (\\bar z_n) = \\mu\\), and the variance of this estimator is \\(\\sigma^2/ n\\).\nNow, let us define a new random variable \\(w_n\\) as the standardised verision of \\(\\bar z_n\\):\n\\[\nw_n =\\frac{\\bar z_n - \\mu}{\\sigma / \\sqrt{n}}\n\\]\nThe central limit theorem states that:\n\\[\nw_n \\rightarrow \\mathcal N(0, 1), \\ \\mathrm{as} \\ n \\rightarrow ∞\n\\]\nThe central limit theorem states that as sample size \\(n\\) approaches infinity, the standardised sampling distribution approximates a normal distribution. This theorem applies to any estimator, including \\(\\beta\\).\nMore importantly for us, it states that as \\(n\\) increases, our \\(w_n\\) becomes closer and closer to \\(\\mathcal N (0, 1)\\), which means we can say with large enough samples, our sampling distribution is approximately normal.\n\n\n\nWe need the standard error (std. devation) of the sampling distribution. We know the standard error is the square root of the variance of the sampling distribution, which we derived (under the classical assumptions) as:\n\\[\n\\V(\\hat\\beta | X) = \\sigma^2 (X^\\top X)^{-1}\n\\]\nWe do not know the value of \\(\\sigma^2\\) as it is a population parameter. So, we estimate it with an unbiased estimator \\(s^2\\):\n\\[\n\\sigma^2 \\approx s^2 = \\frac{\\hat{\\eps}^\\top \\hat{\\eps}}{n-k-1}\n\\]\nWhile this estimator is unbiased, the estimator \\(s^2\\) has variance. The implication of this is that we can no longer use the standard normal distribution for our sampling distribution, and instead, use the t-distribution, which has fatter tails and a lower peak to account for this variance in \\(s^2\\).\nIf we believe heteroscedasticity is violated (which we assume by default), we should use the heteroscedasticity variance of OLS that we derived:\n\\[\n\\V(\\hat{\\beta}| X)  = (X^\\top X)^{-1} X^\\top \\Omega\\color{black} X ( X^\\top X)^{-1}\n\\]\nThe \\(\\sigma^2_i\\) in the \\(\\Omega\\) matrix can be estimated with \\(s_i^2 = \\hat\\eps_i^2\\). The robust standard errors are the square root. We typically use robust-standard errors in causal inference by default, unless we can prove homoscedasticity.\n\n\n\nHypothesis Testing\nWith our (robust) standard errors, we can run hypothesis tests on our coefficients to see if we have a relationship between two variables \\(X_{ij}\\) and \\(Y_i\\). Our typical hypotheses are:\n\n\\(H_0 : \\beta_j = 0\\) (i.e. there is no relationship between \\(X_{ij}\\) and \\(Y_i\\)).\n\\(H_1:\\beta_j ≠ 0\\) (i.e. there is a relationship between \\(X_{ij}\\) and \\(Y_i\\)).\n\nFirst, we calculate the t-test statistic, where \\(H_0\\) is the value set in the null hypothesis (typically 0):\n\\[\nt = \\frac{\\hat\\beta_j - H_0}{\\widehat{se}(\\hat\\beta_j)}\n\\]\nNow, we consult a t-distribution of \\(n-k-1\\) degrees of freedom. We find points \\(t\\) and \\(-t\\) on our t-distribution, and highlight the areas under the curve further away from the 0 at these two points. In the figure below, \\(t = 2.228\\):\n\n\n\n\n\nThe area highlighted, divided by the entire area under the curve, is the p-value. The p-value we get is the probability of getting a test statistic equally or more extreme than the one we got, given the null hypothesis is true.\n\nIf \\(p&lt;0.05\\), we believe the probability of a null hypothesis is low enough, such that we reject the null hypothesis (that there is no relationship between \\(X_{ij}\\) and \\(Y_i\\)), and conclude our alternate hypothesis (that there is a relationship between \\(X_{ij}\\) and \\(Y_i\\)).\nIf \\(p &gt; 0.05\\), we cannot reject the null hypothesis, and cannot reject there is no relationship.\n\n\n\n\n\n\n\nConfidence Intervals\n\n\n\n\n\nThe 95% confidence intervals of coefficients have the following bounds:\n\\[\n(\\hat\\beta_j - 1.96 \\widehat{se}(\\hat\\beta_j), \\ \\ \\hat\\beta_j + 1.96 \\widehat{se}(\\hat\\beta_j))\n\\]\n\nThe 1.96 is an approximation assuming a normal distribution. The actual confidence intervals (calculated by computers) will use a t-distribution of \\(n-k-1\\), which will result in a slightly different multiplicative factor.\n\nThe confidence interval means that under repeated sampling and estimating \\(\\hat\\beta_j\\), 95% of the confidence intervals that we construct will include the true \\(\\beta_j\\) value in the population.\nIf the confidence interval contains 0, we cannot conclude a relationship between \\(X_{ij}\\) and \\(Y_i\\), as 0 is a plausible value of \\(\\beta_j\\). These results will always match those of the t-test.\n\n\n\n\n\n\nF-Tests\nF-tests are used to test more than one coefficient at a time. For example, you might want to test if two variables or coefficients are jointly significant. The utility of this will become more clear when we talk about categorical explanatory variables and polynomial transformations. Our hypotheses in a F-test will be:\n\n\\(M_0 : Y_i = \\beta_0 + \\sum\\limits_{j=1}^g \\beta_{j} X_{ij} + \\eps_i\\) (the smaller null model with \\(g\\) variables).\n\\(M_a : Y_i = \\beta_0 + \\sum\\limits_{j=1}^g \\beta_{j} X_{ij} + \\sum\\limits_{j=g+1}^p \\beta_{j} X_{ij} + \\eps_i\\) (the bigger model with the original \\(g\\) variables + additional variables up to \\(p\\)).\n\nF-tests compare the R-squared of the two models through the F-statistic:\n\\[\nF = \\frac{(SSR_0 - SSR_a) / (k_a - k_0)}{SSR_a /(n - k_a - 1)}\n\\]\nWe then consult a F-distribution with \\(k_a - k_0\\) and \\(n-k_a - 1\\) degrees of freedom, obtaining a p-value (in the same way as the t-test). The p-value we get is the probability of getting a test statistic equally or more extreme than the one we got, given the null hypothesis is true.\n\nIf \\(p&lt;0.05\\), the we believe the probability of the null hypothesis is low enough, such that we reject the null hypothesis (that \\(M_0\\) is the better model), and conclude our alternate hypothesis (that \\(M_a\\) is a better model). This also means the extra coefficients in \\(M_a\\) are jointly statistically significant.\nIf \\(p &gt; 0.05\\), we cannot reject the null hypothesis, and cannot reject that \\(M_0\\) is a better model. Thus, the extra coefficients in \\(M_a\\) are jointly not statistically significant.\n\n\n\n\n\n\n\nModel Specification Issues\n\nOmitted Variable Bias\nSuppose there is some variable \\(Z_i\\) that we have not included in a “short” regression. The actual, “true” regression of the population, would include this confounder \\(Z_i\\)\n\\[\n\\underbrace{y = X\\beta + \\eps}_{\\text{short regression}}\n\\qquad \\underbrace{y = X\\beta + z\\delta + \\eps}_{\\text{true regression with z} }\n\\]\nWe can find the expected value of the OLS estimate of the “short regression” excluding confounder \\(Z_i\\):\n\\[\n\\begin{align}\n\\hat\\beta & = (X^\\top X)^{-1}X^\\top y \\\\\n& = (X^\\top X)^{-1}X^\\top(\\color{blue}{X\\beta + z\\delta + \\eps}\\color{black}) && (\\because \\color{blue}{y = X\\beta + z\\delta + \\eps}\\color{black}) \\\\\n& = \\underbrace{(X^\\top X)^{-1}X^\\top X}_{= \\ I}\\beta + (X^\\top X)^{-1}X^\\top z\\delta + (X^\\top X)^{-1} X^\\top \\eps && \\text{(multiply out)} \\\\\n& = \\beta + (X^\\top X)^{-1}X^\\top z\\delta + (X^\\top X)^{-1} X^\\top \\eps \\\\\n\\end{align}\n\\]\n\\[\n\\begin{align}\n\\E(\\hat\\beta | X, z) & = \\beta + (X^\\top X)^{-1}X^\\top z \\delta + (X^\\top X)^{-1} X^\\top \\underbrace{\\E(\\eps | X, z)}_{= 0} \\\\\n& = \\beta + (X^\\top X)^{-1}X^\\top z \\delta\n\\end{align}\n\\]\nNow, imagine a regression of outcome variable being the confounder \\(z\\), on the explanatory variables \\(X\\), such that \\(z = X\\eta + u\\). Our OLS estimate would of \\(\\eta\\) would be \\(\\hat\\eta = (X^\\top X)^{-1} X^\\top z\\). Now, we can plug \\(\\hat\\eta\\) into our expected value of \\(\\hat\\beta\\). Assume our estimator \\(\\hat{\\eta}\\) is unbiased:\n\\[\n\\begin{align}\n\\E(\\hat\\beta | X, z) & = \\beta + (X^\\top X)^{-1}X^\\top z \\delta\\\\\n& = \\beta + \\color{blue}{\\hat\\eta}\\color{black}\\delta && (\\because \\color{blue}{\\hat\\eta = (X^\\top X)^{-1} X^\\top z }\\color{black}) \\\\\n\\E\\hat\\beta & = \\E(\\E(\\hat\\beta|X, z)) && \\text{(law of iterated expect.)} \\\\\n& = E(\\color{blue}{\\beta + \\hat\\eta \\delta}\\color{black}) && (\\because \\color{blue}{\\E(\\hat\\beta|X, z) = \\beta + \\hat\\eta\\delta} \\color{black}) \\\\\n& = \\beta + \\E\\hat\\eta \\ \\delta && \\text{(take out constants from exp.)} \\\\\n& = \\beta + \\eta\\delta && (\\text{unbiased estimator } \\E\\hat\\eta = \\eta)\n\\end{align}\n\\]\nThus, we can see by not including confounder \\(z\\) in our “short regression”, the estimator is now biased by \\(\\hat\\eta \\delta\\). In later chapters when we start discussing causality, we will see omitted confounders as a huge issue in our estimation.\n\n\n\nHeterogeneity and Interactions\nHeterogeneity is when we believe the magnitude of the relationship between \\(X_{i1}\\) and \\(Y_i\\) in the population is affected by another variable \\(X_{i2}\\), called the moderating variable. An interaction between \\(X_{i1}\\) and the moderating variable \\(X_{i2}\\) means they are multiplied in the regression equation:\n\\[\nY_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\underbrace{\\beta_3 X_{i1} X_{i2}}_{\\text{interaction}}\n\\]\nIn an interaction, \\(\\hat\\beta_0\\) is still the expected \\(Y_i\\) when all explanatory variables equal 0. The other coefficient’s interpretations are:\n\n\n\n\n\n\n\n\n\nBinary \\(X_{i2}\\)\nContinuous \\(X_{i2}\\)\n\n\nBinary \\(X_{i1}\\)\nWhen \\(X_{i2} = 0\\), the effect of \\(X_{i1}\\) (going from 0 to 1) on \\(Y_i\\) is \\(\\hat\\beta_1\\).\nWhen \\(X_{i2} = 1\\), the effect of \\(X_{i1}\\) (going from 0 to 1) on \\(Y_i\\) is \\(\\hat\\beta_1 + \\hat\\beta_3\\).\nThe effect of \\(X_{i1}\\) (going from 0 to 1) on \\(Y_i\\) is \\(\\hat\\beta_1 + \\hat\\beta_3 X_{i2}\\).\n\n\nContinuous \\(X_{i1}\\)\nWhen \\(X_{i2} = 0\\), for every increase in one unit of \\(X_{i1}\\), there is an expected \\(\\widehat{\\beta_1}\\) unit change in \\(Y_i\\).\nWhen \\(X_{i2} = 1\\), for every increase in one unit of \\(X_{i1}\\), there is an expected \\(\\hat\\beta_1+ \\hat\\beta_3\\) change in \\(Y_i\\).\nFor every increase of one unit in \\(X_{i1}\\), there is an expected \\(\\hat\\beta_1 + \\hat\\beta_3 X_{i2}\\) change in \\(Y_i\\).\n\n\n\nThe hypothesis test for \\(\\hat\\beta_3\\) tests the null hypothesis that there is no interaction in the population between \\(X_{i1}\\) and \\(X_{i2}\\). If our coefficient \\(\\hat\\beta_3\\) is statistically significant, we can conclude that this null is wrong, and there indeed is an interaction. If \\(\\hat\\beta_3\\) is insignificant, we fail to reject the null, and can drop the interaction effect from our regression.\n\n\n\nPolynomial Transformations\nSometimes the relationship between two variables in the population is not a straight linear line. We want to accurately reflect this non-linear relationship in our regression to mantain accurate \\(\\hat\\beta\\) estimates. The most common form of polynomial transformation is the quadratic transformation:\n\\[\nY_i = \\beta_0 + \\beta_1X_i + \\beta_2 X_i^2 + \\eps_i\n\\]\nNote that while \\(X_i\\) is non-linear, the actual regression is still linear in parameters. We can see this because it can still be written as \\(y = X\\beta + \\eps\\) when you consider \\(X_i\\) and \\(X_i^2\\) to be two different explanatory variables.\nOur estimated \\(\\hat\\beta_0\\) remains the expected value of \\(Y_i\\) when all explanatory variables equal 0. Unfortunately, the \\(\\hat\\beta_1\\) and \\(\\hat\\beta_2\\) coefficients are not directly interpretable.\n\n\\(\\hat\\beta_2\\)’s sign can tell us if the best-fit parabola opens upward or downward.\nThe significance of \\(\\hat\\beta_2\\) also indicates if the quadratic term is statistically significant. If it is not, we can remove the transformation.\n\nWe can interpret two things about the quadratic transformation:\n\nFor every one unit increase in \\(X_i\\), there is an expected \\(\\hat\\beta_1 + 2 \\hat\\beta_2X_i\\) unit increase in \\(Y_i\\).\nThe minimum/maximum point in the best-fit parabola occurs at \\(X_i = - \\hat\\beta_1/2 \\hat\\beta_2\\).\n\n\n\n\nLogarithmic Transformations\nLogarithmic transformations are often used to change skewed variables into normally distributed variables. Often, skewed variables will violate homoscedasticity, which means the Gauss-Markov theorem of OLS being BLUE no longer applies. By applying a logarithmic transformation, you can meet homoscedasticity and retain the BLUE properties of OLS. We have 3 types of logarithmic transformations:\n\nLinear-Log model: \\(Y_i = \\beta_0 + \\beta_1 \\log X_i + \\eps_i\\).\nLog-Linear model: \\(\\log Y_i = \\beta_0 + \\beta_1 X_i + \\eps_i\\).\nLog-Log model: \\(\\log Y_i = \\beta_0 + \\beta_1 \\log X_i + \\eps_i\\).\n\nThe coefficient interpretations become a little more complex with logarithms:\n\n\n\n\n\n\n\n\n\n\\(X_i\\)\n\\(\\log (X_i)\\)\n\n\n\\(Y_i\\)\nLinear Model:\nWhen \\(X_i\\) increases by one unit, there is an expected \\(\\hat\\beta_1\\) unit change in \\(Y_i\\).\nLinear-Log Model:\nWhen \\(x\\) increases by 10%, there is an expected \\(0.096 \\hat\\beta_1\\) unit change in \\(Y_i\\).\n\n\n\\(\\log (Y_i)\\)\nLog-Linear Model:\nFor every one unit increase in \\(X_i\\), the expected \\(Y_i\\) is multiplied by \\(e^{\\hat\\beta_1}\\).\nLog-Log Model:\nMultiplying \\(X_i\\) by \\(e\\) will multiply the expected value of \\(Y_i\\) by \\(e^{\\hat\\beta_1}\\).x\n\n\n\n\n\n\nCategorical Explanatory Variables\nTake an explanatory variable \\(X_i\\), which has \\(g\\) number of categories \\(1, \\dots, g\\). To include \\(X_i\\) in our regression, we would create \\(g-1\\) dummy (binary) variables, to create the following regression model:\n\\[\n\\E(Y_i|X_i) = \\beta_0 + \\sum\\limits_{j=1}^{g-1} \\beta_j X_{ij}\n\\]\n\nCategories \\(1, \\dots, g-1\\) get there own binary variable \\(X_{i1}, \\dots, X_{ig-1}\\).\nCategory \\(g\\) (the reference category) does not get its own variable. We can change which category we wish to be the reference.\n\nInterpretation is as follows (category \\(j\\) is any one of category \\(1, \\dots, g-1\\)).\n\n\\(\\beta_j\\) is the difference in expected \\(Y_i\\) between category \\(j\\) and the reference category \\(g\\).\n\\(\\beta_0\\) is the expected \\(Y_i\\) of the reference category \\(g\\).\nThus, category \\(j\\) has an expected \\(Y_i\\) of \\(\\beta_0 + \\beta_j\\).\n\nTo test the categorical variable’s statistical significance, we will need to test all the coefficients together. The most common way is to do it with a f-test of multiple coefficients.\nOne type of categorical explanatory variable is a fixed effect. This is done in panel or clustered data, where your categorical variable is the cluster variable. We will explore this more near the end of the course on differences-in-differences.\n\n\n\nLinear Probability Model\nThe standard linear model assumes a continuous \\(Y_i\\) variable. However, we can adapt the linear model to fit binary \\(Y_i\\) variables. When \\(Y_i\\) is binary and only has values \\(Y_i \\in \\{0, 1\\}\\), our linear model is actually no longer a predictor of \\(Y_i\\), since our regression will output values that are not 0 and 1.\nInstead, our linear model will now predict the probability of unit \\(i\\) having \\(Y_i = 1\\). The is due to the conditional expectation interpretation of regression, and the expectation of the binomial distribution:\n\\[\n\\begin{align}\n\\E(Y_i|X_i) & = \\underbrace{0 \\times \\P(Y_i = 0|X_i) \\ + \\ \\P(Y_i = 1|X_i)}_{\\text{a weighted avg. formula}} \\\\\n& = \\P(Y_i=1|X_i)\n\\end{align}\n\\]\nThus, we can rewrite our linear model with the primary outcome being \\(\\P(Y_i = 1|X_i)\\). This model is called the linear probability model:\n\\[\n\\P(Y_i = 1|X_i) = \\beta_0 + \\beta_1X_{i1} + \\dots + \\beta_pX_{ip} + \\eps_i\n\\]\nOur interpretations of coefficients also slightly change.\n\n\n\n\n\n\n\n\n\nContinuous \\(X_{ij}\\)\nBinary \\(X_{ij}\\)\n\n\n\\(\\hat\\beta_j\\)\nFor every one unit increase in \\(X_{ij}\\), there is an expected \\(\\hat\\beta_j \\times 100\\) percentage point change in the probability of a unit being in category \\(Y_i=1\\), holding all other explanatory variables constant.\nThere is a \\(\\hat\\beta_j\\times 100\\) percentage point difference in the probability of a unit being in category \\(Y_i=1\\) between category \\(X_{ij} = 1\\) and category \\(X_{ij} = 0\\), holding all other explanatory variables constant.\n\n\n\\(\\widehat{\\beta_0}\\)\nWhen all explanatory variables equal 0, the expected probability of a unit being in category \\(Y_i=1\\) is \\(\\hat\\beta_0 \\times 100\\)\nFor category \\(X_{ij} = 0\\), the expected probability of a unit being in category \\(Y_i=1\\) is \\(\\hat\\beta_j \\times 100\\) (when all other explanatory variables equal 0).\n\n\n\nThe main downside to the linear probability model relates to prediction - i.e. \\(\\P(Y-i = 1|X_i)\\) as a probability should be restricted between 0 and 1, however, the linear model does not guarantee this restriction. We will introduce methods to deal with this in the next chapter.\n\n\n\n\n\n\nImplementation in R\nYou will need package fixest and estimatr.\n\nlibrary(fixest)\nlibrary(estimatr)\n\nRegression with normal standard errors can be done with the lm() function:\n\nmodel &lt;- lm(y ~ x1 + x2 + x3, data = mydata)\nsummary(model)\n\nRegression with robust standard errors can be done with the feols() function or lm_robust() function:\n\n# feols\nmodel &lt;- feols(y ~ x1 + x2 + x3, data = mydata, se = \"hetero\")\nsummary(model)\n\n# lm robust\nmodel &lt;- lm_robust(y ~ x1 + x2 + x3, data = mydata)\n\nOutput will include coefficients, standard errors, p-values, and more.\n\n\n\n\n\n\nBinary and Categorical Variables\n\n\n\n\n\nYou can include binary and categorical variables by using the as.factor() function:\n\nfeols(y ~ x1 + as.factor(x2) + x3, data = mydata, se = \"hetero\")\n\nYou can do the same for \\(y\\) or \\(x\\). Just remember, \\(y\\) cannot be a categorical variable (use multinomial logsitic regression instead).\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n\n\n\n\nYou can include one-way fixed effects by adding a | after your regression formula in feols():\n\nmodel &lt;- feols(y ~ x1 + x2 + x3 | cluster,\n               data = mydata, se = \"hetero\")\nsummary(model)\n\nYou can add two-way fixed effects as follows:\n\nmodel &lt;- feols(y ~ x1 + x2 + x3 | unit + year,\n               data = mydata, se = \"hetero\")\nsummary(model)\n\n\n\n\n\n\n\n\n\n\nInteraction Effects\n\n\n\n\n\nTwo interact two variables, use * between them. This will automatically include both the interaction term, and the two variables by themselves.\n\nfeols(y ~ x1 + x2*x3, data = mydata, se = \"hetero\")\n\nIf for some reason, you only want the interaction term, but not the variables by themselves, you can use a colon : between the two variables:\n\nfeols(y ~ x1 + x2:x3, data = mydata, se = \"hetero\")\n\n\n\n\n\n\n\n\n\n\nPolynomial Transformations\n\n\n\n\n\nTo conduct a polynomial transformation, you can use the I() function. The second argument is the degree of the polynomial:\n\nfeols(y ~ x1 + I(x2, 3), data = mydata, se = \"hetero\") #cubic for x2\n\n\n\n\n\n\n\n\n\n\nLogarithmic Transformations\n\n\n\n\n\nThe best way to do a logarithmic transformation is to create a new variable that is the log of the variable you want to transform using the log() function, before you even start the regression:\n\nmydata$x1_log &lt;- log(mydata$x1)\n\n\n\n\n\n\n\n\n\n\nConfidence Intervals\n\n\n\n\n\nTo find the confidence intervals for coefficients, first estimate the model with lm() or feols() as shown previously, then use the confint() command:\n\nconfint(model)\n\n\n\n\n\n\n\n\n\n\nF-Tests\n\n\n\n\n\nTo run a f-test, use the anova() command, and input your two different models, with the null model going first.\n\nanova(model1, model2)\n\nNote: F-tests only work with models that are run with homoscedastic standard errors. Robust standard errors will not work.\n\n\n\n\n\n\n\n\n\nLaTeX Regression Tables\n\n\n\n\n\nYou can use the texreg package to make nice regression tables automatically.\n\nlibrary(texreg)\n\nThe syntax for texreg() is as follows:\n\ntexreg(l = list(model1, model2, model3),\n       custom.model.names = c(\"model 1\", \"model 2\", \"model 3\"),\n       custom.coef.names = c(\"intercept\", \"x1\", \"x2\"),\n       digits = 3)\n\nYou can replace texreg() with screenreg() if you want a nicer regression table in the R-console.\nNote: you must have the same amount of model names as total models in your texreg, and you must have the same amount of coeficient names as the total amount of coefficients in all of your models.\n\n\n\n\n\n\n\n\n\nPrediction\n\n\n\n\n\nWe can use the predict() function to generate fitted value predictions in R:\n\nmy_predictions &lt;- predict(model, newdata = my_new_data)\n\nmy_new_data is a dataframe with a bunch of explanatory variable values (for every explanatory variable) for a collection of observations, that you wish to predict \\(\\hat y\\) for.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "1 The Classical Linear Model"
    ]
  },
  {
    "objectID": "rct.html",
    "href": "rct.html",
    "title": "Randomised Controlled Trials",
    "section": "",
    "text": "Up until now, we have focused on theory of statistics and causal inference. But now, we are ready to dive into methodology - different designs to measure and identify causal effects.\nThis chapter introduces the “gold standard” of causal inference: randomised controlled trials. This chapter also covers extensions, such as stratified experiments and survey experiments.\nUse the right sidebar for quick navigation.\n\n\nRandomisation\n\nRandomised Experiments\nExperiments are a research design where the assignment mechanism is controlled by the researcher.\nRandomised Experiments use randomisation as the assignment mechanism. Treatment values are assigned to \\(N\\) units at random, with both known and positive probabilities of being assigned to treatment and control groups.\nQuick notation for randomised experiments:\n\nWe have \\(N\\) total number of units in our experiment.\nA randomly subset of \\(N_1\\) units are assigned to treatment \\(D = 1\\).\nThe remaining \\(N_0 = N - N_1\\) are assigned to control.\n\n\\(N_1\\), the number of individuals assigned to treatment, does not necessarily need to be 50% (although this is quite a common number).\nAlso, when you fix the number of units to be treated at \\(N_1\\) before randomisation, technically, not all units have an independent probability of being selected. This is because once you have assigned \\(N_1\\) individuals to treatment, we know the remaining individuals must be assigned to control. This usually is not a huge issue.\nYou can use bernoulli randomisation (simple randomisation) to avoid this issue. Bernoulli gives every individual a certain chance of being selected based on a bernoulli distribution. This does mean that with different randomisation trials, we will get different numbers of treated individuals for each trial.\n\n\n\nIdentification Assumptions\nRandomisation implies that assignment probabilities do not depend on the potential outcomes. The potential outcome values do not affect our chances of being selected for treatment.\n\\[\n\\P(D=1|Y_0, Y_1) = P(D=1)\n\\]\nOr in other words, treatment is independent of potential outcomes (also unconfounded or ignorability):\n\\[\n(Y_1, Y_0)  \\perp\\!\\!\\!\\!\\perp D\n\\]\nThis implies that \\(E(Y_{0i})\\) is the same between treatment and control groups, and \\(E(Y_{1i})\\) is also the same between treatment and control:\n\\[\n\\begin{split}\n& \\E(Y_{0i} | D_i = 1) = \\E(Y_{0i} | D_i = 0) = \\E Y_{0i}\\\\\n& \\E(Y_{1i} | D_i = 1) = \\E(Y_{1i} | D_i = 0) = \\E Y_{1i}\n\\end{split}\n\\tag{1}\\]\n\n\n\nCausal Identification\nLet us return to our naive estimator, and our problem of selection bias. Using the above property in Equation 1, we can simplify:\n\\[\n\\begin{align}\n& \\underbrace{\\E(Y_{1i}|D_i = 1)- \\E(Y_{0i}|D_i = 1)}_{\\tau_{ATT}} + \\underbrace{\\E(Y_{0i}|D_i = 1) - \\E(Y_{0i} | D_i = 0)}_{\\text{Selection Bias}} \\\\\n& = \\underbrace{\\E(Y_{1i}|D_i = 1)- \\E(Y_{0i}|D_i = 1)}_{\\tau_{ATT}} + \\underbrace{\\E Y_{0i} - \\E Y_{0i}}_{\\text{Selection Bias}} && (\\because \\text{eq. (1)} \\ )\\\\\n& = \\underbrace{\\E(Y_{1i}|D_i = 1)- \\E(Y_{0i}|D_i = 1)}_{\\tau_{ATT}} + 0\n\\end{align}\n\\]\nThus, under randomisation, selection bias is equal to 0, and thus our comparison of observed outcomes is now an unbiased estimator of \\(\\tau_{ATT}\\). Now look at the formula for the ATT estimand. We can simplify as follows using Equation 1:\n\\[\n\\begin{align}\n\\tau_{ATT} & = \\E(Y_{1i} -Y_{0i}|D_i = 1)\\\\\n& = \\E(Y_{1i} |D_i = 1) - \\E(Y_{0i} | D_i = 1) \\\\\n& = \\E Y_{1i}  - \\E Y_{0i} && (\\because \\text{equation (1)} \\ )\\\\\n& = \\underbrace{\\E(Y_{1i} - Y_{0i})}_{\\tau_{ATE}}\n\\end{align}\n\\]\nAnd now we see that \\(\\tau_{ATT}\\) and \\(\\tau_{ATE}\\) are equivalent under randomisation, and we can identify the \\(\\tau_{ATE}\\) with our observed data.\n\n\n\n\n\n\nGraphical Identification\n\n\n\n\n\nLet us look at a direct acyclic graph:\n\n\n\n\n\nBecause we are randomly assigning treatment \\(D\\), we are exogenously determining \\(D\\). Thus, values of \\(D\\) are not being caused by \\(U\\), they are being caused by randomisation.\nThus, we can eliminate the arrow between \\(U \\rightarrow D\\). This allows us to estimate \\(D \\rightarrow Y\\) without any confounders.\n\n\n\n\n\n\nThe Balancing Property\nRandomisation balances all observed and unobserved pre-treatment characteristics between units between the treatment and control. This is because not only is \\((Y_1, Y_0) \\perp\\!\\!\\!\\!\\perp D\\), but also any covariate \\(X\\) is also independent of treatment: \\(X \\perp\\!\\!\\!\\!\\perp D\\).\nThis means that if randomisation is successful, we should expect minimal differences between control and treatment groups for all pre-treatment characteristics values.\nWe can test this assumption by finding the average \\(X\\) values for both control and treatment groups, and see if there are any statistical significant differences in \\(X\\) between control and treatment. This is typically done with a t-test or a regression:\n\\[\nX_i = \\alpha + \\gamma D_i + \\varepsilon_i \\quad \\text{test if } \\gamma \\text{ is significant}\n\\]\nIn any one sample, we actually are likely to have some imbalances in \\(X\\) between control and treatment simply due to chance. You could control for imbalanced covariates, but you do not have to (we will discuss this later).\nYou can adopt other randomisation procedures, such as stratified randomisation, to guarantee balance on \\(X\\).\n\n\n\nInternal Validity\nInternal validity is the validity of the research design in identifying causal effects. Randomisation can be complicated by a few factors:\n\nMissing data (often due to individuals dropping out). We are concerned that there is some covariate that is causing some people to drop out, which re-introduces selection bias.\nMeasurement Problems: Hawthorne Effect - subjects know what you are studying, and will change their behaviour as a result.\nNon-Compliance: Some units assigned to treatment might not take the treatment, and some units assigned to control may take the treatment (this can often be dealt with by using an instrumental variable design).\n\nSome of these issues are very hard to overcome, so it is important to identify these limitations in our experiments. One way to check validity is to do a balance check between key covariates.\n\n\n\nExternal Validity\nExternal validity has little to do with causal identifiction with the subjects in our experiment. Instead, External validity asks if we can generalise our conclusions from our subjects, to other subjects outside our experiment. Can we extrapolate our estimates to to other populations? This is important - if we cannot extrapolate, some results may be very niche.\nFor example, if we measured the effect of migration on tolerance for our subjects in India, can we say the same effect is true of someone in Japan, the US, or Europe?\nRandomisation does not help with external validity - the ability to extrapolate our results to external situations. To extrapolate to a greater population, our actual sample of observations in our experiment, should be representative of the greater population. This is called 𝑋-Validity: we can study this with data - to see how representative our population is compared to the population.\nNon-representative programme of treatment is another threat: Sometimes, treatments will differ between areas. For example, if we are encouraging people to migrate to test how that changes their tolerance, how are the governmental/ngo/private agencies working with you affecting the process. Would less capable agencies create different effects?\nThis is called \\(C\\)-validity, and we cannot measure this with data, unless you redo your experiment in another context.\n\n\n\n\n\n\nCausal Estimation\n\nDifference in Means Estimator\nOur causal estimand is the Average Treatment Effect (ATE):\n\\[\n\\tau_{ATE} = \\E(Y_{1i}) - \\E(Y_{0i})\n\\]\nEarlier in the causal identification section, we illustrated that there is no selection bias. That means we can estimate the ATE using the difference-in-means estimator, by taking the sample mean \\(Y\\) of the treatment group, minus the sample mean \\(Y\\) of the control group:\n\\[\n\\hat\\tau_{ATE} = \\bar Y_1 - \\bar Y_0\n\\]\nThis is an unbiased estimator because selection bias is eliminated with randomisation. This is also an asymptotically consistent estimator due to the law of large numbers.\n\n\n\nOrdinary Least Squares Estimator\nWe can also estimate the \\(\\tau_{ATE}\\) with a regression:\n\\[\nY_i = \\hat\\gamma + \\hat\\tau D_i + \\hat\\varepsilon_i\n\\]\nRemember that OLS is the best approximation of the conditional expectation function \\(\\E(Y_i|X_i)\\). Thus, we can write the regression as:\n\\[\n\\E(Y_i|D_i) = \\hat\\gamma + \\hat\\tau D_i\n\\]\nNow, let us find the difference between treatment \\(E(Y_i|D_i =1)\\) and control \\(E(Y_i|D_i = 0)\\):\n\\[\n\\begin{split}\n\\E(Y_i|D_i = 1) - \\E(Y_i|D_i = 0) \\  = & \\  \\hat\\gamma + \\hat\\tau(1) - (\\hat\\gamma + \\hat\\tau(0)) \\\\\n= & \\ \\hat\\gamma + \\hat\\tau - \\hat\\gamma \\\\\n= & \\ \\hat\\tau\n\\end{split}\n\\]\nThus, the difference in means is equivalent to \\(\\hat\\tau\\) regression coefficient. Here, \\(\\hat\\tau\\) is our estimator of the ATE. This gives the same estimate as the difference-in-means estimator. Furthermore, \\(\\hat\\gamma\\) is equivalent to the average \\(Y\\) in the control group \\(\\bar Y_0\\).\nWe do not need to include covariates. This is because randomisation allows us to meet the conditions of random sampling and zero-conditional mean neccesary for the unbiasedness of OLS.\nHowever, there are several reasons one might want to include pre-treatment covariates:\n\nCan increase precision (reduce standard error), by getting better predictions of \\(Y\\).\nCan control for observable imbalance that was observed in the balance tables. Many researchers will compare a model without and with an imbalanced covariate, to show that the covariate does not matter significantly.\nCan allow for estimation of heterogenous treatment effects by including interactions in the model.\n\nWe should not include post-treatment covariates. Anything that is measured post-treatment could be measuring a treatment effect (something that results from the treatment \\(D\\)). This may “model away” your treatment effect.\n\n\n\n\n\n\nStatistical Inference\n\nStandard Errors\nStandard errors are needed if we want to conduct classical hypothesis testing. If we use the OLS estimator, we can use heteroscedasticity-robust standard errors.\nIf we use the difference-in-means estimator, we can calculate the standard errors with the following formula:\n\\[\n\\widehat{se}(\\hat\\tau) = \\sqrt{\\frac{\\hat\\sigma_1^2}{N_1} + \\frac{\\hat\\sigma_0^2}{N_0}}, \\quad \\hat\\sigma^2_d = \\sum\\limits_{D_i = d}\\frac{Y_i - \\bar Y_d}{N_d} \\quad \\forall \\ d \\in \\{0, 1\\}\n\\]\nWe can also use Nonparametric Bootstrap to create our sampling distribution and standard errors for statistical inference, instead of relying on asymptotic normality of the standard t-test.\nFor blocked experiments (see later), you should randomly sample blocks, not units, to create your bootstrap re-samples. For example, if your data is clustered in cities, you should re-sample by cities.\n\n\n\nClassical Hypothesis Testing\nWe can use a t-test for statistical inference. We first estimate the \\(\\hat\\tau_{ATE}\\) and heteroscedasticity-robust standard error \\(\\widehat{rse}(\\hat\\tau_{ATE})\\).\nThen, we state our hypotheses, which are normally:\n\\[\nH_0 : \\tau_{ATE} = 0, \\quad H_1:  \\tau_{ATE} ≠ 0\n\\]\nFinally, we can calculate the t-test statistic \\(\\hat\\tau /\\widehat{se}(\\hat\\tau)\\), refer to the relevant t-distribution, and calculate the p-value.\nGenerally, we use a statistical significance level of \\(\\alpha = 0.05\\), so we reject the null if \\(|t|&gt;1.96\\).\nFor more complex randomisation schemes, you will need different standard errors. For example, if you use a cluster randomisation scheme, you might need clustered standard errors.\nWe can also use Nonparametric Bootstrap to create our sampling distribution for statistical inference, instead of relying on asymptotic normality of the standard t-test.\n\n\n\nRandomisation Inference\nConsider a new sharp null hypothesis, that all individual causal effects are zero (not just the average causal effect):\n\\[\nH_0^s : Y_1 = Y_0, \\quad H_A^s : Y_1 ≠ Y_0\n\\]\nAssuming \\(H_0\\) is true, we can actually fully construct the potential outcomes \\(Y_{0i}\\) and \\(Y_{1i}\\), since we know every unit has 0 individual treatment effect. Then, we can “recreate” the sampling distribution without asymptotic properties.\nFirst, we want to calculate the total number of randomisations possible. If we have \\(N\\) total units, and \\(N_1\\) in the treatment group and \\(N_0\\) in the control group, we can calculate all possible randomisation permutations as follows:\n\\[\n\\begin{pmatrix} N \\\\ N_1 \\end{pmatrix} = \\frac{N!}{N_1 ! N_0 !}\n\\]\nThis is the total number of assignments possible given \\(N\\), \\(N_1\\), and \\(N_0\\). Then, we can calculate the \\(\\widehat{\\tau_j}\\) of every possible randomisation assignment. The figure below shows this:\n\n\n\n\n\nNow, plot all \\(\\widehat{\\tau_j}\\) in our sampling distribution:\n\n\n\n\n\nLet us say our sample \\(\\hat\\tau = 6\\). We would simply find the area under the curve that is above \\(\\hat\\tau = 6\\), and below \\(-\\hat\\tau = -6\\). This would be our p-value.\n\n\n\n\n\n\nPros/Cons of Randomisation Inference\n\n\n\n\n\nRandomisation Inference is assumption free - we do not need asymptotic properties or hypothetical sampling distributions.\nSince we also do not need asymptotic property inferences, we can do inference with very small samples as well.\nDownsides: the randomisation inference only tests if the sharp null hypothesis is true, but sometimes, that might not be something we want to test.\n\n\n\n\n\n\n\n\n\nExtension: Other Randomisations\n\nStratified Randomisation\nStratified (also called blocked or conditional) randomisation are when randomisation occurs separately within levels of some covariates(s) \\(X\\). Generally, you separate your sample of \\(N\\) units into \\(J\\) subgroups. For example, you could split people up into male or female, and random sample within each group, rather than everyone together.\nWhy would you want to do this? Let us say you have 4 subjects, with pre-treatment potential outcomes of \\(Y_{0i} = \\{2, 2, 8, 8 \\}\\).\nIf you just randomly assign, then there is a 33% chance that you end up with the random assignment where \\(\\{8, 8\\}\\) are placed in one group, and \\(\\{2, 2\\}\\) are placed in another group.\nThis is a concern: our treatment and control groups would be very imbalanced in this situation, which violates our independence assumption.\nWith blocking, we could divide our sample into \\(J = 2\\) subgroups, having group 1 being \\(\\{2, 2\\}\\), and group 2 being \\(\\{8, 8\\}\\). Then, we randomly sample one from each group into the treatment. This way, we are guaranteed better balance. This can prevent imbalances as normal randomisation can have a high probability (in certain situations) of creating imbalances.\n\n\n\n\n\n\nEstimation with Stratification\n\n\n\n\n\nTo estimate the ATE, you will need a weighted average of the ATE for each subgroup \\(j\\), with the weights being the proportion of units each group \\(j\\) accounts for:\n\\[\n\\tau_{ATE} = \\sum\\limits_{j=1}^J \\frac{N_j}{N} \\tau_j\n\\]\n\n\n\n\n\n\nCluster Randomisation\nCluster randomisation is when we randomly assign units (or have individuals naturally) in groups. Every unit within a group (called a cluster) will get the same treatment. We randomly sample the groups to get the treatment or control.\nFor example, we could randomise development treatment at the village level, or randomise treatment of a cirriculum at the school level.\nThe main reason for this is to prevent SUTVA violations.\nFor example, imagine you are testing the effects of a new curriculum. If you randomise by each student, students will talk to their friends, and treated individuals may teach control individuals about the new curriculum. But by randomising by school (either an entire school gets or does not get the new curriculum), this concern is not a huge issue.\nIf you do implement cluster randomisation, you should note that units within each cluster are likely not independent of each other. Thus, you should use clustered standard errors to account for this.\n\n\n\n\n\n\nExtension: Survey Experiments\n\nFraming/Endorsement Experiments\n\n\nPriming Experiments\n\n\nList Experiments\n\n\n\n\n\n Back to top",
    "crumbs": [
      "4 Randomised Controlled Trials"
    ]
  },
  {
    "objectID": "soo.html",
    "href": "soo.html",
    "title": "Selection on Observables",
    "section": "",
    "text": "In the last chapter, we discussed randomisation. Randomisation is great, but, it requires specific circumstances of the research having control over the assignment mechanism. However, in the social sciences, this rarely occurs.\nThis chapter introduces the selection on observables framework, which allows us to identify causal effects in an observational setting by controlling for observable pre-treatment covariates. We discuss the main estimators, including regression, matching, and weighting.\nUse the right sidebar for quick navigation. R-code provided at the bottom.\n\n\nIdentification\n\nBlocking Backdoor Paths\nWithout randomisation, we need some other way to account for pre-treatment covariates that may be confounding and causing selection bias. Controlling for a set of nodes/confounders \\(X\\) can identify the causal effect of \\(D \\rightarrow Y\\), if the following conditions are met:\n\nNo node within set \\(X\\) is a descendant of \\(D\\) (no element within \\(X\\) results from \\(D\\)).\nThe nodes within set \\(X\\) block all back-door paths from \\(D \\rightarrow Y\\).\n\n\n\n\n\n\nIn the figure above, let us block the backdoor paths between \\(D \\rightarrow Y\\):\n\nBackdoor path \\(D \\rightarrow X \\rightarrow Y\\). To block this path, we must control for \\(X\\).\nBackdoor path \\(D \\rightarrow V \\rightarrow Y\\). We do not need to control for \\(V\\), since it is post-treatment (a descendant of \\(D\\)). In fact, \\(V\\) is a bad control (see below).\n\nThus, to identify \\(D \\rightarrow Y\\) here, we only need to control for \\(X\\), and no other variable.\n\n\n\n\n\n\nGood and Bad Controls\n\n\n\n\n\nGood controls block backdoor paths, which facilitate identification of the causal effect.\nBad controls are when we control for post-treatment variables. For example, \\(P\\) below is a bad control, since it is caused by \\(D\\), so it is post-treatment.\n\n\n\n\n\nYou also never want to control variables that only predict \\(D\\). These are bad because controlling for these removes variation in \\(D\\) that could be useful.\nNeutral controls are ones that don’t identify the causal effect, but improve efficiency. For example, \\(Q\\) below affects \\(Y\\), but there is no backdoor path. Controlling \\(Q\\) will not help identification, but can control noise in \\(Y\\) which may increase efficiency.\n\n\n\n\n\n\n\n\n\n\n\nIdentification Assumptions\nOnce we have determined the set of confounders \\(X\\) that we need to control to block all backdoor paths, the assumptions needed for identification of causal effects are:\n\nConditional Ignorability (also known as exogeneity or independence): Among units with identical confounder values \\(X_i\\), treatment \\(D_i\\) is as-if randomly assigned. Or in other words, potential outcomes are independent from treatment within each specific confounder value \\(X_i = x\\).\n\n\\[\n(Y_{0i}, Y_{1i}) \\perp\\!\\!\\!\\!\\perp D_i  \\ | \\ X_i = x, \\quad \\forall \\ x \\in \\mathcal X\n\\]\nThis implies that for any given value of all confounders \\(X_i = x\\), we know that potential outcomes \\(Y_{di}\\) are equivalent between treatment and control:\n\\[\n\\begin{split}\n\\E(Y_{1i}|X_i = x) = \\E(Y_{1i}|D_i = 1, X_i = x) = \\E(Y_{1i}|D_i = 0, X_i = x) \\\\\n\\E(Y_{0i}|X_i = x) = \\E(Y_{0i}|D_i = 1, X_i = x) = \\E(Y_{0i}|D_i = 0, X_i = x)\n\\end{split}\n\\tag{1}\\]\n\nCommon Support: for any unit \\(i\\) with value of \\(X_i\\), there is a non-zero probability that they could be assigned to both control \\(D_i = 0\\) or treatment \\(D_i = 1\\).\n\n\\[\n0 &lt; \\P(D_i = 1 \\ | X_i = x) &lt; 1 \\quad \\forall \\ x \\in \\mathcal X\n\\]\n\n\n\n\n\n\nExample of Identification Assumptions\n\n\n\n\n\nImagine we have a theory that being abducted \\(D\\) causes turning out to vote.\nBlattman (2009) finds that age is the primary way violent groups chose to abduct individuals: abduction parties released young children and older adults, but kept all adolescent and young males.\nThat means our theory is that age \\(X\\) affects selection into treatment \\(D\\). Young children and older adults are less likely to get abducted \\(D\\), while adolescent and young males are more likely \\(D\\).\n\n\n\n\n\n\nIdentification of the ATE\nWith our assumptions above, we can identify the ATE. We start with the conditional average treatment effect, conditional on some value of confounders \\(X_i = x\\). Note the properties shown in Equation 1 .\n\\[\n\\begin{align}\n\\tau_{CATE}(x) & = \\E(Y_{1i} - Y_{0i} \\ | \\ X_i = x) \\\\\n& = \\E(Y_{1i}|X_i = x) - \\E(Y_{0i}|X_i = x) && (\\text{property of expectation}) \\\\\n& = \\E(Y_{1i}|D_i = 1, X_i = x) - \\E(Y_{0i}|D_i = 0X_i = x) &&( \\because \\text{equation (1)} \\ ) \\\\\n& = \\underbrace{\\E(Y_i|D_i = 1, X_i = x)}_{\\because \\text{ observable}} - \\underbrace{\\E(Y_i|D_i = 0, X_i = x)}_{\\because \\text{ observable}}\n\\end{align}\n\\tag{2}\\]\nNow, let us discuss the ATE, and plug in the CATE from Equation 2 to identify it:\n\\[\n\\begin{align}\n\\tau_{ATE} & = \\E(Y_{1i} - Y_{0i}) \\\\\n& = \\int \\underbrace{\\E(Y_{1i} - Y_{0i} \\ | \\ X_i = x)}_{\\tau_{CATE}(x)} d \\ \\underbrace{\\P(X_i = x)}_{\\text{weight}} && (\\text{weighted average})\\\\\n& = \\int(\\underbrace{\\E(Y_i|D_i = 1, X_i) - \\E(Y_i|D_i = 0, X_i)}_{\\because \\text{ equation (2)}})d \\ \\P(X_i = x)\n\\end{align}\n\\tag{3}\\]\nThus \\(\\tau_{ATE}\\) is identified as the weighted average of all the CATEs, who themselves are difference-in-means of the observed \\(Y_i\\) at every possible value of \\(X_i = x\\).\nWe assumed that the pre-treatment covariate \\(X\\) is continuous. This is why we need an integral. However, we can simplify this if \\(X\\) is discrete:\n\\[\n\\tau_{ATE} = \\sum\\limits_{x \\in \\mathcal X} ( \\E(Y_i|D_i = 1, X_i = x) - \\E(Y_i|D_i = 0, X_i = x)) \\P(X_i = x)\n\\tag{4}\\]\n\n\n\nIdentification of the ATT\nWe can weaken conditional ignorability, and still identify the ATT. Only \\(Y_{0i}\\) needs to be independent of \\(D_i\\) for units with the same covariates \\(X_i\\). Or in other words, \\((Y_{0i}) \\perp\\!\\!\\!\\perp D_i | X_i = x\\). This implies:\n\\[\n\\E(Y_{0i}|X_i = x) = \\E(Y_{0i}|D_i = 0, X_i = x) = \\E(Y_{0i}|D_i = 1, X_i = x)\n\\tag{5}\\]\nStart with the conditional ATT, using weakened conditional ignorability from Equation 5 :\n\\[\n\\begin{split}\n\\tau_{CATT}(x) & = \\E(Y_{1i}-Y_{0i}|D_i = 1, X_i = x) \\\\\n& = \\E(Y_{1i}|D_i = 1, X_i = x) - \\E(Y_{0i}|D_i = 1, X_i = x) \\\\\n& = \\E(Y_{1i}|D_i = 1, X_i = x) - \\underbrace{\\E(Y_{0i}|D_i = 0, X_i = x)}_{\\because \\text{ equation (5)}} \\\\\n& = \\underbrace{\\E(Y_i|D_i=1, X_i = x)}_{\\because \\text{ observable}} - \\underbrace{\\E(Y_1|D_i = 0, X_i x)}_{\\because \\text{ observable}}\n\\end{split}\n\\tag{6}\\]\nNow, look at the ATT, and plug in CATT from Equation 6 to identify it.\n\\[\n\\begin{align}\n\\tau_{ATT} & = \\E(Y_{1i} - Y_{0i}|D_i = 1) \\\\\n& = \\int \\underbrace{\\E(Y_{1i} - Y_{0i}|D_i = 1, X_i = x)}_{\\tau_{CATT}(x)}d \\ \\underbrace{\\P(X_i = x|D_i = 1)}_{\\P(X_i = x) \\text{ within treated}} \\\\\n& = \\int (\\underbrace{\\E(Y_i|D_i = 1, X_i = x) - \\E(Y_i|D_i = 0, X_i = x)}_{\\because \\text{ equation (6)}})d \\ \\P(X_i = x|D_i = 1)\n\\end{align}\n\\]\nWe can simplify this if \\(X\\) is discrete:\n\\[\n\\tau_{ATT} = \\sum\\limits_{x \\in \\mathcal X} ( \\E(Y_i|D_i = 1, X_i = x) - \\E(Y_i|D_i = 0, X_i = x)) \\P(X_i = x | D_i = 1)\n\\]\nEven when all assumptions are met for identification of the ATE, the \\(\\tau_{ATE}\\) can be different than the \\(\\tau_{ATT}\\). This is because the weights \\(\\P(X_i = x|D_i = 1)\\) for the ATT are different than the ATE \\(\\P(X_i = x)\\).\n\n\n\n\n\n\nParametric Estimators\n\nOrdinary Least Squares Estimator\nOLS is a natural approach for controlling for confounders \\(X\\), since \\(\\hat\\beta_{OLS}\\) estimates partial out the effects of covariates. OLS is a good estimator of \\(\\tau_{ATE}\\) under 2 conditions:\n\nConstant treatment effect: \\(\\tau_i = Y_{1i} - Y_{0i}\\) for all units \\(i\\).\nLinearity: Potential outcomes are linear, and can be written as:\n\n\\[\nY_i(d) = \\beta_0 + d\\beta_1 + \\mathbf X_i \\gamma + \\varepsilon_i \\quad \\text{for} \\quad d = 0, 1\n\\]\nWhy these conditions? Suppose we have the above linear potential outcomes. We can show:\n\\[\n\\begin{align}\n\\tau_i & = Y_{1i} - Y_{0i} && (\\text{definition of } \\tau_i) \\\\\n& = (\\beta_0 + (1)\\beta_1 + \\mathbf X_i \\gamma + \\varepsilon_i) - (\\beta_0 + (0)\\beta_1 + \\mathbf X_i \\gamma + \\varepsilon_i) && (\\text{plug in } Y_i(1), Y_i(0) \\ )\\\\\n& = (\\beta_0 + \\beta_1 + \\mathbf X_i \\gamma + \\varepsilon_i) - (\\beta_0 + \\mathbf X_i \\gamma + \\varepsilon_i) && (\\text{multiply}) \\\\\n& = \\beta_0 + \\beta_1 + \\mathbf X_i \\gamma + \\varepsilon_i - \\beta_0 - \\mathbf X_i\\gamma - \\varepsilon_i && (\\text{distribute negative sign})\\\\\n& = \\beta_1 && (\\text{cancel out terms})\n\\end{align}\n\\]\nThus, we see \\(\\tau_i = \\beta_1\\). We also know that conditional ignorability implies zero-conditional mean. Thus OLS is an unbiased and asymptotically consistent estimator of both \\(\\beta_1\\) and the ATE.\nYou should be cautious using OLS when assumption 2, linearity, is violated. We know the OLS is the best linear predictor of the conditional expectation function in terms of mean squared error. Thus, \\(\\beta_1\\) will provide the best linear approximation to the population regression function.\nThis does not mean it is good - just the best linear approximation. How far your actual data is away from linearity will determine how good OLS is here.\nYou should not use OLS if you believe assumption 1, heterogeneity, is violated. The reasoning is explained below.\n\n\n\nOLS Bias under Heterogeneity\nWhat if there are heterogenous treatment effects (where \\(\\tau_i\\) is different between units)? Standard OLS in this case is no longer an unbiased estimator of the ATE. Recall the discrete identification of the ATE (in equation Equation 4 ) is a weighted average of CATEs:\n\\[\n\\hat\\tau_{ATE} = \\sum\\limits_{x \\in \\mathcal X} ( \\hat\\tau_{CATE}(x)) \\underbrace{\\P(X_i = x)}_{\\text{weight}} \\\\\n\\]\nOLS, when there are non constant treatment effects, can also be rewritten as a weighted average of CATEs:\n\\[\n\\hat\\beta_{OLS} = \\sum\\limits_{x \\in \\mathcal X} ( \\hat\\tau_{CATE}(x)) \\underbrace{ \\frac{\\V(D_i|X_i = X)\\P(X_i = x)}{\\sum \\V(D_i | X_i = x')\\P(X_i = x')} }_{\\text{weight}} \\\\\n\\]\nNotice how the weights are different. The weights in the OLS are the conditional variances of \\(D_i\\). This means that OLS is not an unbiased estimator of the ATE or ATT, but rather, a weighted average of the ATT and ATU.\nOLS, under heterogeneity, actually provides an unbiased estimator of the conditional variance weighted average treatment effect. This is not the same as the ATE or the ATT. This estimand can also be described as a weighted average of the ATT and the ATU:\n\\[\n\\tau_{OLS} = w_1 \\cdot \\tau_{ATT} + w_0 \\cdot \\tau_{ATU}\n\\]\nWhere:\n\\[\n\\begin{split}\nw_1 & = \\frac{(1 - \\P(D=1)) \\V(\\pi(X)|D = 0)}{\\P(D=1)\\V(\\pi(X)|D=1) + (1-\\P(D=1)\\V(\\pi(X)|D=0)} \\\\\nw_0 & = 1 - w_1\n\\end{split}\n\\]\nThe reason for this is because regression is prone to extrapolation beyond common support - i.e. it can “estimate” potential outcomes for units that are not observed. This can lead to bias. This is in contrast to the subclassification estimator, which cannot be computed if there are missing observable outcomes for a substratum/category of \\(X\\).\nOLS also minimises estimation uncertainty by downweighting groups of \\(X_i\\) where group-specific ATEs are less precisely estimated.\n\n\n\nFully Interacted Estimator\nThe Fully-Interacted Estimator, a newly developed large-sample regression estimator (Lin 2013), solves the heterogeneity bias in the OLS estimator. The fully-interacted estimator takes the form:\n\\[\nY_i = \\hat\\alpha + D_i \\widehat{\\tau}_{int} + (\\mathbf X_i - \\mathbf {\\bar X}) \\hat\\beta +D_i (\\mathbf X_i - \\mathbf{\\bar X}) \\hat\\gamma + \\hat\\varepsilon_i\n\\]\n\nWhere \\(X_i\\) are covariate values sufficient to satisfy conditional independence.\nWhere \\(\\bar X\\) contains the sample means of all \\(X_i\\) covariates.\n\nThis estimator \\(\\hat\\tau_{int}\\) is technically biased when estimating \\(\\tau_{ATE}\\). However, the bias is arbitrarily small in large samples under conditional ignorability.\nThis estimator thus allows us to accurately estimate the ATE even under heterogenous treatment effects, assuming our sample size is sufficiently large.\n\n\n\n\n\n\nOther Solutions to the OLS Bias under Heterogeneity\n\n\n\n\n\nThere are a few other solutions to this issue of OLS bias under heterogeneity:\n\nDoubly-robust estimation uses a weighted average of regression and IPW estimators, which will be asymptotically consistent as long as the regression model is correctly specified.\nMatching as pre-processing uses matching to make treatment and control groups similar, then runs regression models to estimate causal effects.\n\n\n\n\n\n\n\n\n\n\nNonparametric Estimators\n\nSubclassification Estimator\nUsing the discrete identification of the ATE shown in Equation 4 , we can instead use the sample equivalents to get the subclassification estimator:\n\\[\n\\hat\\tau_{ATE} = \\sum\\limits_{j=1}^M \\underbrace{(\\bar Y_{1j} - \\bar Y_{0j})}_{\\tau_{CATE}(j)} \\underbrace{\\frac{n_j}{n}}_{\\text{weight}}\n\\]\n\nWhere \\(M\\) is the number of levels/categories of \\(X\\), and \\(j\\) is one specific level/category of \\(X\\).\nWhere \\(n_j\\) is the number of units in a level/category \\(j\\) of \\(X\\).\nWhere \\(\\bar Y_{dj}\\) is the mean outcome for units with \\(D_i = d\\) in level/category \\(j\\) of \\(X\\).\n\nMore intuitively, the procedure is as follows:\n\nChoose one specific value for all covaraites \\(X\\). Find the ATE within this specific value of \\(X\\).\nMultiply that average treatment effect by the number of observations that meet this specific value of \\(X\\) divided by the total number of units.\nDo this for every possible values of all covaraites \\(X\\), then sum up all the weighted average treatment effects to get the overall ATE.\n\nFor subclassificaion to be possible, within each level \\(j\\) of covariate \\(X\\), there must be at least one unit in control \\(D=0\\) and treatment \\(D=1\\). Subclassification is not a particularly popular estimator, because it only works if all covariates are discrete, and only if you have a manageable amount of categories and covariates.\n\n\n\n\n\n\nSubclassification with Multiple Confounders\n\n\n\n\n\nLet us say we have 2 confounders, \\(X_1\\) and \\(X_2\\). Both confounders are categorical with 3 categories.\nWe would need to create \\(M=9\\) levels of strata, for every possible combination of values of \\(X_1\\) and \\(X_2\\). Then, we would estimate the within-strata average treatment effect, and weight them.\nThis illustrates how with large amounts of confounders, you will need a huge number of stratum. This makes subclassification infeasible in many cases.\n\n\n\n\n\n\n\n\n\nSubclassification for the ATT\n\n\n\n\n\nWhen pre-treatment covariate \\(X\\) is discrete, the identification result of the ATT is:\n\\[\n\\tau_{ATT} = \\sum\\limits_{x \\in \\mathcal X} ( E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)) P(X_i = x | D_i = 1)\n\\]\nWe can calculate this within our give sample to get the subclassificaiton estimator:\n\\[\n\\hat\\tau_{ATT} = \\sum\\limits_{j=1}^M(\\bar Y_{1j} - \\bar Y_{0j}) \\frac{n_{1j}}{n_1}\n\\]\n\nWhere \\(M\\) is the number of strata (levels/categories of \\(X\\)).\nWhere \\(n_j\\) is the number of units in a level/category \\(j\\) of \\(X\\).\nWhere \\(n_{1j}\\) is the number of treated cells \\(D = 1\\) in a level/category \\(j\\) of \\(X\\).\nWhere \\(\\bar Y_{dj}\\) is the mean outcome for units with \\(D_i = d\\) in level/category \\(j\\) of \\(X\\).\n\n\n\n\n\n\n\nMatching Estimator\nWe have a missing data problem in causal inference: we do not know all the potential outcomes. Matching “estimates” missing potential outcomes of a unit.\nFor each observation in the treated group, matching finds an observation in the untreated group that have the most similar values of a set of pre-treatment covariates \\(X\\). Thus, we have pairs of treatment-control observations that act as counterfactuals. We can estimate the ATT as the average difference in observed outcomes within the pairs:\n\\[\n\\hat\\tau_{ATT} = \\frac{1}{n_1} \\sum\\limits_{i:D_i = 1}(Y_i - \\widetilde{Y_i})\n\\]\n\nWhere \\(n_1\\) is the number of units in the treatment group.\nWhere \\(Y_i\\) is the unit’s observed \\(Y\\) in the treatment group.\nWhere \\(\\tilde Y_i\\) is unit \\(i\\)’s closest neighbour in the untreated group.\n\nSometimes, a treatment unit may not have one close control unit to match to. Instead, we could use a combination of control units to match to the treatment unit, and use the average \\(Y\\) of those combination of control units to approximate a more accurate match.\nSuppose we use \\(M_i\\) number of close control units to match to a treatment unit \\(i\\). Then, the matching estimator would be defined as follows:\n\\[\n\\hat\\tau_{ATT} = \\frac{1}{n_1} \\sum\\limits_{i:D_i = 1}(Y_i - \\left(\\frac{1}{M_i} \\sum\\limits_{m=1}^{M_i} \\widetilde{Y_{i_m}}\\right))\n\\]\nWhere \\(\\widetilde{Y_{i_m}}\\) is the obsered outcome for the \\(m\\)th match of unit \\(i\\).\n\n\n\n\n\n\nChoices during Matching\n\n\n\n\n\nWe have to make several choices when conducting matching.\n\nWhat covariates to match on. We generally want to select a set of pre-treatment covariates \\(X\\) such that these covariates ensure the conditional ignorability assumption is met.\nMatch with or without replacement. Matching with replacement means that once you have used one control unit to match to a treatment unit, you can still use that same control unit to match to another treatment unit. This has advantages since you can ensure better and closer matches. However, matching without replacement is also possible.\nHow many to match. You can decide to match multiple control units to one treatment unit, and use the average of the treatment units to approximate a true control unit. This may result in more accurate matches for treatment units that may not have a good single control unit to match to.\n\nWe can also choose to use more advanced matching methods, such as Mahalanobis Distance matching or Propensity Score matching, which are shown below. These are good for matching on more \\(X\\).\n\n\n\n\n\n\n\n\n\nWeaknesses of Matching\n\n\n\n\n\nMatching does not always create “perfect” matches. This means that the pairs matched together may not be identical to each other in terms of covariates \\(X\\) or potential outcomes.\nThe inability to find exact matches can cause bias, especially for the more covariates we match on (see below).\n\n\n\n\n\n\nMatching with Multiple Covariates\nConsider that we \\(k&gt;1\\) number of confounders \\(X\\). Now, we have to match observations in \\(k\\) variables, which implies we are in a multidimensional \\(\\mathbb R^k\\) space. The most commonly used distance metric is Mahalanobis Distance - which measures the distance in \\(X_i\\) between units \\(i\\) and \\(j\\):\n\\[\nD_M (\\mathbf X_i, \\mathbf X_j) = \\sqrt{(\\mathbf X_i - \\mathbf X_j)^T \\boldsymbol\\Sigma_X^{-1} (\\mathbf X_i - \\mathbf X_j)}, \\quad \\boldsymbol\\Sigma_X \\text{ is the variance-covariance matrix of X}\n\\]\nHowever, when we try to match on more than one \\(X\\) variable, we go from matching on a number line \\(\\mathbb R^1\\) to a \\(n\\)-dimensional space, \\(\\mathbb R^n\\). The search space increases exponentially as you increase the number of dimensions.\n\n\n\n\n\nIf we match on a one dimensional plane (say the horizontal line between 0 and 1 on the left), we can see our red line covers approximately 30% between 0 and 1. But in 3 dimensions, our red box covers a significantly less proportion of the entire cube. On the right, \\(d\\) represents the dimensions. We can see as \\(d\\) increases, the fraction of volume increases significantly slower relative to distance. Thus, with a bigger space, the distance between two units increases, so you get worse matches.\nThis curse of dimensionality creates a bias problem - since we get non-exact matches. The more dimensions you add, the worse it becomes.\n\n\n\n\n\n\nMore on Bias\n\n\n\n\n\nThe poor matches caused by increased dimensionality inject error into our estimates of missing potential outcomes.\nThe bias term as you increase the number of dimensions \\(k\\), changes by \\(N^{(-1/k)}\\). This implies no \\(\\sqrt{n}\\) consistency for \\(k&gt;2\\).\nIf \\(N_0\\) (number of untreated units) is much larger than \\(N_1\\) (number of treated units), bias will typically be smaller.\nThere are ways to correct this bias, including Abadie and Imbens (2011) Bias Correction method.\nThere is a new method: Bias-corrected matching, which estimates bias ineherent to mathching estimators via regression, then subtracts it from the matching estimate to correct for it.\n\n\n\n\n\n\nPropensity Scores Matching\nPropensity Score matching is an alternative way to match over many dimensions. The propensity score is an unobserved property, defined as the probability of a unit \\(i\\) of receiving treatment:\n\\[\n\\pi(X_i) \\equiv \\P(D_i = 1|X_i)\n\\]\nWhen supposing the conditional ignorability and common support assumptions, the propensity score \\(\\pi(X_i)\\) has the balancing property: \\(D_i \\perp X_i \\ | \\ \\pi(X_i)\\). This implies that conditional ignorability holds on the propensity scores alone:\n\\[\n(Y_{1i}, Y_{0i}) \\perp\\!\\!\\!\\!\\perp D_i \\ | \\ \\pi(X_i)\n\\]\nThus, instead of conditioning on \\(X_i\\) as we did in selection on observables, we can instead condition on \\(\\pi (X_i)\\), and still identify the causal estimand.\nHowever, we do not actually observe \\(\\pi (X_i)\\). We estimate \\(\\pi (X_i)\\) with a binary response model, with outcome variable \\(D_i\\), and explanatory variables \\(X_i\\). This will get us a fitted probability \\(\\P(D_i = 1) = \\hat\\pi(X_i)\\).\nThen, once we have the propensity score estimates \\(\\hat\\pi(X_i)\\), we can do nearest neighbour matching with the propensity scores (in \\(\\mathbb R^1\\)). This will allow us to identify the \\(\\tau_{ATT}\\).\nThe accurate estimation of \\(\\tau_{ATT}\\) implies an accurate prediction of the propensity scores \\(\\pi(X_i)\\). We can test our matched treatment and control groups to see if the balancing property holds for covariates \\(X_i\\).\n\n\n\nGenetic Matching\n\n\n\n\n\n\nWeighting Estimator\n\nIdentification with Weighting\nWe know that the ATE can be written as a weighted average, as shown in Equation 4 . We can rewrite the \\(\\tau_{ATE}\\) as follows using observed potential outcomes outcomes and conditional ignorability ( Equation 1 ).\n\\[\n\\begin{split}\n& = \\sum\\limits_{x \\in \\mathcal X} \\underbrace{(\\E(Y_{1i}|D_i = 1, X_i = x)}_{\\because \\text{ observed}} - \\underbrace{\\E(Y_{0i}|D_i = 0, X_i = x)}_{\\because \\text{ observed}})\\P(X_i = x) \\\\\n& = \\sum\\limits_{x \\in \\mathcal X}  (\\underbrace{\\E(Y_{1i}|X_i = x)}_{\\because \\text{ eq. (1)}} - \\underbrace{\\E(Y_{0i}|X_i = x)}_{\\because \\text{ eq. (1)}})\\P(X_i = x) \\\\\n& = \\underbrace{\\E[\\E(Y_{1i}|X_i = x) - \\E(Y_{0i}|X_i = x)]}_{\\text{definition of weighted average}}\n\\end{split}\n\\]\nLet us do an algebra trick - multiply both terms within the CATE by 1 (in blue):\n\\[\n\\begin{split}\n& = \\E \\left [\\E(Y_{1i}|X_i=x) \\color{blue}{\\frac{\\pi(X_i)}{\\pi(X_i)}}\\color{black} - (\\E(Y_{0i}|X_i=x) \\color{blue}{\\frac{1-\\pi(X_i)}{1-\\pi(X_i)}} \\right] \\\\\n& \\color{black} = \\E \\left[ \\frac{\\E(Y_{1i}|X_i = x) \\pi(X_i)}{\\pi(X_i)} -  \\frac{\\E(Y_{0i}|X_i = x) (1-\\pi(X_i))}{1-\\pi(X_i)} \\right]\n\\end{split}\n\\]\nWe know that propensity score \\(\\pi(X_i) := \\E(D_i|X_i = x)\\). Thus, we can convert the above to:\n\\[\n\\begin{split}\n& = \\E \\left[ \\frac{\\E(Y_{1i}|X_i = x)\\E(D_i|X_i = x)}{\\pi(X_i)} - \\frac{\\E(Y_{0i}|X_i = x)(1-\\E(D_i|X_i = x))}{1-\\pi(X_i)}\\right] \\\\\n& = \\E \\left[ \\frac{\\E(Y_{1i}|X_i = x)\\E(D_i|X_i = x)}{\\pi(X_i)} - \\frac{\\E(Y_{0i}|X_i = x)\\E(1-D_i|X_i = x)}{1-\\pi(X_i)}\\right]\n\\end{split}\n\\]\n\\[\n\\begin{align}\n& = \\E \\left[ \\frac{\\E(Y_{1i}D_i|X_i = x)}{\\pi(X_i)} - \\frac{\\E(Y_{0i}(1-D_i)|X_i = x)}{1 - \\pi(X_i)}\\right] && (\\text{property of expectation})\\\\\n& = \\E \\left[ \\E \\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} |X_i = x\\right) - \\E \\left( \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)} | X_i = x \\right) \\right] && (\\text{property of expectation})\\\\\n& = \\E\\left[ \\E\\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} - \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)} |X_i = x \\right) \\right]  && (\\text{property of expectation})\n\\end{align}\n\\]\n\\[\n\\begin{align}\n& = \\E\\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} - \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)}\\right) && (\\text{LIE: } \\E(X) = E[E(X|Y)] \\ ) \\\\\n& = \\E\\left( \\frac{Y_{i}D_i}{\\pi(X_i)} - \\frac{Y_{i}(1-D_i)}{1-\\pi(X_i)}\\right) && (\\text{observered outcome}) \\\\\n& = \\E \\left( \\frac{\\color{blue}{Y_i} \\color{black}D_i(1-\\pi(X_i))-\\color{blue}{Y_i}\\color{black}(1-D_i)\\pi(X_i)}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{getting common denom.}) \\\\\n& = \\E\\left( Y_i \\frac{D_i(1-\\pi(X_i))-(1-D_i)\\pi(X_i)}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{factor out }Y_i) \\\\\n& = \\E\\left( Y_i \\frac{D_i - D_i\\pi(X_i)-(\\pi(X_i) -D_i\\pi(X_i))}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{distribute out}) \\\\\n&  = \\E\\left( Y_i \\frac{D_i \\color{blue}{- D_i\\pi(X_i)} \\color{black}-\\pi(X_i) \\color{blue}{+ D_i\\pi(X_i)}}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{distribute out negative})\\\\\n& = \\E\\left( Y_i \\frac{D_i -\\pi(X_i) }{\\pi(X_i)(1-\\pi(X_i))}\\right) && (\\text{cancel out})\\\\\n\\end{align}\n\\tag{7}\\]\nAnd thus, we have identified the ATE.\n\n\n\nInverse Probability Weighting Estimator\nAn alternative use of propensity scores is weighting. As shown above, under conditional ignorability and common support, we can identify the ATE as:\n\\[\n\\tau_{ATE} = \\E\\left[ Y_i \\times \\underbrace{\\frac{D_i - \\pi(X_i)}{\\pi(X_i) (1 - \\pi(X_i))}}_{\\text{weight}}\\right]\n\\]\nThe inverse probability weighting (IPW) estimator is the sample estimator:\n\\[\n\\begin{split}\n\\hat\\tau_{ATE} & = \\frac{1}{N} \\sum\\limits_{i=1}^N \\left(Y_i \\frac{D_i - \\hat\\pi(X_i)}{\\hat\\pi(X_i) (1 - \\hat\\pi(X_i))} \\right) \\\\\n& = \\frac{1}{N} \\sum\\limits_{i=1}^N \\left(\\frac{D_i Y_i}{\\hat\\pi(X_i)} - \\frac{(1-D_i) Y_i}{1 - \\hat\\pi(X_i)} \\right)\n\\end{split}\n\\]\n\nThe second equation is equivalent to the first, shown by Equation 7 .\n\nEssentially, those who are unlikely to be treated but do get treated get weighted more, and individuals who are likely to be treated but do not get treated get weighted more.\n\n\n\n\n\n\nWeighting Estimator for ATT\n\n\n\n\n\nThe identification of the ATT under both conditional ignorability and common support are:\n\\[\n\\tau_{ATT} = \\frac{1}{\\P(D = 1)} \\times \\E\\left[ Y_i \\times \\underbrace{\\frac{D_i - \\pi(X_i)}{(1 - \\pi(X_i))}}_{\\text{weight}}\\right]\n\\]\nThe sample IPW estimator would be:\n\\[\n\\begin{split}\n\\hat\\tau_{ATT} & = \\frac{1}{N_1}\\sum\\limits_{i=1}^N \\left( Y_i \\frac{D_i - \\hat\\pi(X_i)}{1 - \\hat\\pi(X_i)} \\right) \\\\\n& = \\frac{1}{N_1} \\sum\\limits_{i=1}^N \\left( D_iY_i - (1-D_i)Y_i \\frac{\\hat\\pi(X_i)}{1 - \\hat\\pi(X_i)} \\right)\n\\end{split}\n\\]\n\n\n\nThe IPW estimator is asymptotically consistent, but has very poor small sample properties. They are highly sensitive to extreme values of \\(\\hat\\pi(X_i)\\). This generates high variance (inefficiency), and can produce significant bias under model mispecification.\n\n\n\n\n\n\nFalsification Tests\n\nTesting Assumptions with Falsification\nThe stronger (bolder) our assumptions for identification, the less credible our results are. Selection on Observables involves a very strong and hard to verify assumption: conditional ignorability. Can we really be sure that we have controlled for all confounders \\(X_i\\) needed to satisfy conditional ignorability?\nPlacebo tests are a type of falsification test to show evidence against our assumptions. Suppose that we make the assumption of conditional ignorability \\((Y_{0i}, Y_{1i}) \\perp D_i | X_i\\). Suppose we are concerned about the presence of another confounder \\(U\\) that is not included in \\(X_i\\).\n\n\n\n\n\nThe presence of \\(U\\) will falsify our conditional ignorability assumption, and means we cannot identify the causal effect of \\(D \\rightarrow Y\\).\n\n\n\n\n\n\nFalsification vs. Validation\n\n\n\n\n\nFalsification is a principle of trying to criticise our own research, rather than defend it. Falsification is about testing if our assumptions are not met. Failing a test provides evidence that our assumption is not met.\n\nEx. Covariates are balanced - thus there is no evidence that our assumptions are not met. We are not saying that our assumption is correct, just that there is no evidence against it.\n\nValidation is the opposite - we test to see if there is evidence in favour of our assumptions.\n\nEx. Covariates are balanced - thus our assumptions are met.\n\n\n\n\nFor falsification tests, we should not just pay attention to statistical significance - we must also pay attention to the magnitude of the point estimation.\n\n\n\nPlacebo Outcome Test\nA placebo outcome test utilities another alternative outcome variable \\(Y'\\) that is caused by our hypothesised unobserved confounder \\(U\\):\n\n\n\n\n\nWe can see that if \\(U\\) does not exist, \\(D\\) should have zero effect on the new outcome \\(Y'\\). Thus, if \\(U\\) is present, we should find a relationship between \\(D\\) and \\(Y'\\).\n\\[\nY'_i = \\gamma + \\delta D_i + \\varepsilon_i\n\\]\n\nIf we find that there is an effect of \\(D\\) on the new outcome \\(Y'\\) (non-zero \\(\\delta\\)), that is evidence that \\(U\\) exists, and is evidence to reject our conditional ignorability assumption (falsifies our design - red flag!).\nIf you do not find an effect of \\(D\\) on new outcome \\(Y'\\) (\\(\\delta = 0\\)) you find no evidence of \\(U\\), and no evidence to reject our conditional ignorability assumption (fails to falsify our design).\n\nWe must be sure that \\(Y\\) is not related to \\(Y'\\) except through \\(D\\) and \\(U\\). If this is true, we just run our original research design but replace \\(Y\\) with \\(Y'\\).\n\n\n\nPlacebo Treatment Test\nA placebo treatment test involves some other treatment \\(D'\\), that was assigned at the same time\n\n\n\n\n\nWe can see that if \\(U\\) does not exist, the effect of \\(D'\\) should have no effect on \\(Y\\). If \\(U\\) does exist, there should be some effect of \\(D'\\) on \\(Y\\).\n\\[\nY_i = \\gamma + \\delta D'_i + \\varepsilon_i\n\\]\n\nIf we find that there is an effect of \\(D'\\) on \\(Y\\) (non-zero \\(\\delta\\)), that is evidence that \\(U\\) exists, and is evidence to reject our conditional ignorability assumption (falsifies our design - red flag!).\nIf you do not find an effect of \\(D'\\) on \\(Y\\) (\\(\\delta = 0\\)) you find no evidence of \\(U\\), and no evidence to reject our conditional ignorability assumption (fails to falsify our design).\n\nWe must be sure that \\(Y\\) is not related to \\(D'\\) except through \\(D\\) and \\(U\\). If this is true, we just run our original research design but replace \\(D\\) with \\(D'\\).\n\n\n\n\n\n\nExtension: Partial Identification\n\nDecomposing the ATE\nWith falsification, we were concerned with what assumptions we needed to be not-false in order to identify the ATE. However, we can take a different approach - what can we learn about the ATE without any assumptions?\nLet us decompose the ATE into parts:\n\\[\n\\begin{align}\n\\tau_{ATE}  = & \\E(Y_{1i} - Y_{0i}) \\\\\n& \\\\\n= & \\E(Y_{1i} - Y_{0i} | D_i = 1) \\P(D_i = 1) \\\\\n& \\quad - \\E(Y_{1i} - Y_{0i}|D_i = 0)\\P(D_i = 0)  && (\\text{def. of weighted avg.})\\\\\n& \\\\\n= & [ \\color{blue}{\\E(Y_i |D_i = 1)} \\color{black}- \\color{red}{\\E(Y_{0i}|D_i = 1)} \\color{black}] \\P(D_i = 1) \\\\\n& \\quad + [ \\color{red}{\\E(Y_{1i}|D_i = 0)} \\color{black} - \\color{blue}{\\E(Y_i|D_i = 0)} \\color{black}]\\P(D_i = 0) && (\\text{observed + unobserved})\n\\end{align}\n\\]\nSome of the quantities are observed (in blue), and some of the quantities are unobserved (in red). Previously, we made assumptions (conditional ignorability, common support) to fill the unobserved quantities. But, we can make actually any assumption as possible.\n\n\n\nNonparametric Bounds\nOne way to fill in our unobserved outcomes through the “best” and “worst” possible outcomes. This allows us to construct a plausible range of the ATE.\nFirst, let us construct the worst-case scenario - the lowest possible \\(\\tau\\).\n\n\\(\\E(Y_{0i}|D_i = 1) = Y_H\\). Units in the treated \\(D_i=1\\), their potential outcome \\(Y_{0i}\\) will be the highest \\(Y\\) possible, \\(Y_H\\).\n\\(\\E(Y_{1i}|D_i = 0) = Y_L\\). Units in the control \\(D_i=0\\), their unobserved potential outcome \\(Y_{1i}\\) will be the lowest \\(Y\\) possible, \\(Y_L\\).\n\nThus, the lowest possible \\(\\tau\\) (sharp lower bound) is:\n\\[\n\\begin{split}\n\\tau_L = & [ \\color{blue}{\\E(Y_i |D_i = 1)} \\color{black}  - \\color{red}{Y_H} \\color{black}] \\P(D_i = 1) \\\\\n& \\quad + [ \\color{red}{Y_L} \\color{black} - \\color{blue}{\\E(Y_i|D_i = 0)} \\color{black}]\\P(D_i = 0)\n\\end{split}\n\\]\nNow, let us construct the best-case scenario - the highest possible \\(\\tau\\).\n\n\\(\\E(Y_{0i}|D_i = 1) = Y_L\\). Units in the treated \\(D_i=1\\), their potential outcome \\(Y_{0i}\\) will be the lowest \\(Y\\) possible, \\(Y_L\\).\n\\(\\E(Y_{1i}|D_i = 0) = Y_H\\). Units in the control \\(D_i=0\\), their unobserved potential outcome \\(Y_{1i}\\) will be the highest \\(Y\\) possible, \\(Y_H\\).\n\nThus, the highest possible \\(\\tau\\) (sharp upper bound) is:\n\\[\n\\begin{split}\n\\tau_H = & [ \\color{blue}{\\E(Y_i |D_i = 1)} \\color{black}  - \\color{red}{Y_L} \\color{black}] \\P(D_i = 1) \\\\\n& \\quad + [ \\color{red}{Y_H} \\color{black} - \\color{blue}{\\E(Y_i|D_i = 0)} \\color{black}]\\P(D_i = 0)\n\\end{split}\n\\]\nWe know that the true \\(\\tau_{ATE} \\in [\\tau_L, \\tau_H]\\).\n\n\n\nMonotone Treatment Selection Assumption\nOur extreme case from above is not very useful. However, we can layer on assumptions to lower the possible \\(\\tau\\) values.\nOne assumption is the Monotone Treatment Selection (MTS) assumption. This assumption basically says that potential outcomes for units in treatment, are always higher than for those in the control.\n\\[\n\\begin{split}\n& \\E(Y_{0i}|D_i = 0) ≤ \\overbrace{\\E(Y_{0i}|D_i = 1)}^{\\text{unobserved}} \\\\\n& \\underbrace{\\E(Y_{1i} |D_i = 0)}_{\\text{unobserved}} ≤ \\E(Y_{1i} |D_i = 1)\n\\end{split}\n\\]\nThis is basically saying that selection bias is one-direction.\nThis implies a tighter sharp upper bound on \\(\\tau\\).\n\\[\n\\begin{align}\n\\tau_H = & [ \\underbrace{\\E(Y_i |D_i = 1)}_{\\text{observed}}  - \\color{red}{\\E(Y_{0i}|D_i = 0)} \\color{black}] \\P(D_i = 1) \\\\\n& \\quad + [ \\color{red}{\\E(Y_{1i}|D_i =1)} \\color{black} - \\underbrace{\\E(Y_i|D_i = 0)}_{\\text{observed}}]\\P(D_i = 0) \\\\\n= & \\E(Y_i|D_i = 1) - \\E(Y_i|D_i = 0) && (\\text{def. of weighted avg.})\\\\\n\\end{align}\n\\]\nThis indicates that the upper bound of plausible \\(\\tau_{ATE}\\) values is the naive estimator of differences in observed outcomes.\nWe can also make the reverse assumption, where selection bias is in the opposite direction. This means a tighter sharp lower bound \\(\\underline\\tau\\). These assumptions help narrow our possible \\(\\tau_{ATE}\\) values, and can allow us to test if our estimated \\(\\hat\\tau\\) is reasonable (within the plausible bounds).\n\n\n\n\n\n\nImplementation in R\nFor all methods, you will need the tidyverse package:\n\nlibrary(tidyverse)\nlibrary(MatchIt)\nlibrary(estimatr)\n\nSee how to perform each estimator in R:\n\n\n\n\n\n\nDistance Matching\n\n\n\n\n\nFirst, let us conduct nearest neighbour matching with Mahalanobis distance by using the matchit() function.\n\nmatch_object = MatchIt::matchit(D ~ X1 + X2 + X3,\n                                data = my_data,\n                                method = \"nearest\", #distance matching\n                                distance = \"mahalanobis\")\n\n# for output summary\nsummary(match_object)\n\nSecond, let us save the matched data with the match.data() function.\n\nmatch_data &lt;- MatchIt::match.data(match_object,\n                                  weights = 'nn_weights')\n\nThird, we can test if matching worked by using a balance table and a love plot:\n\n# balance table\ncobalt::bal.tab(D ~ X1 + X2 + X3, \n                data = match_data, # from the 2nd step\n                weights = \"nn_weights\",\n                disp = c(\"means\", \"sds\"))\n\n#love plot\ncobalt::love.plot(match_object,\n                  data = my_data, #original dataset\n                  stars = 'raw')\n\nFinally, we can estimate the treatment effect. There are two options - either using a weighted regression, or using the matching algorithm:\n\n# using weighted regression\nestimate &lt;- lm_robust(Y ~ D,\n                      data = match_data, #data from step 2\n                      weights = nn_weights)\nsummary(estimate)\n\n## using the Matching package:\nestimate = Matching::Match(Y = my_data$Y, #outcome\n                           Tr = my_data$D, #treatment\n                           X = my_data[,c(\"X1\", \"X2\", \"X3\")], #covariates\n                           M=1, #number of neighbours\n                           BiasAdjust = TRUE, #for biased adjustment\n                           Weight = 2)\nsummary(estimate)\n\nYou will have the estimates that you can use.\n\n\n\n\n\n\n\n\n\nPropensity Score Matching\n\n\n\n\n\nFirst, we want to estimate propensity scores with a logistic regression (or a random forest):\n\n#logistic model\npscore_model = glm(D ~ X1 + X2,\n                   data = my_data,\n                   family = \"binomial\")\n\n# estimate propensity scores\nmy_data$pscore_estimate = predict(pscore_model,\n                                  my_data,\n                                  type = \"response\")\n\nNow, let us match with propensity scores:\n\n# match\nmatch_object = MatchIt::matchit(D ~ pscore_estimate,\n                                data = my_data,\n                                method = \"nearest\",\n                                distance = \"Mahalanobis\")\n\n# save matched data\nmatch_data &lt;- MatchIt::match.data(match_object,\n                                  weights = 'pscore_weights')\n\nThird, we can test if matching worked with a balance table and a love plot:\n\n#balance table\ncobalt::bal.tab(D ~ X1 + X2 + X3,\n                data = match_data, #matched data from step 2\n                weights = \"pscore_weights\",\n                disp = c(\"means\", \"sds\"))\n\n#love plot\ncobalt::love.plot(match_object,\n                  data = my_data, #original dataset\n                  addl = ~ X1 + X2 + X3,\n                  stars = 'raw')\n\nFinally, let us do the estimation:\n\nestimate &lt;- lm_robust(Y ~ D,\n                      data = match_data, #from step 2\n                      weights = pscore_weights)\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nInverse Probability Weighting\n\n\n\n\n\nFirst, we want to estimate propoensity scores with a logistic regression (or a random forest):\n\n#logistic model\npscore_model = glm(D ~ X1 + X2,\n                   data = my_data,\n                   family = \"binomial\")\n\n# estimate propensity scores\nmy_data$pscore_estimate = predict(pscore_model,\n                                  type = \"response\")\n\nSecond, we calculate the inverse probability weights based on the formula from earlier:\n\nmy_data$ipweight = ifelse(my_data$D == 1, # condition\n                       1/my_data$pscore_estimate,\n                       1/(1-my_data$pscore_estimate))\n\nFinally, we can estimate the ATE, or ATT, or use a weighted regression for the ATE:\n\n# ATE estimator\nmean((my_data$D * my_data$Y) * my_data$ipweight - ((1 - my_data$D) * my_data$Y) * my_data$ipweight)\n\n# ATT estimator\nsum(my_data$D * my_data$Y - (1 - my_data$D) * my_data$Y * (my_data$pscore_estimate/(1 - my_data$pscore_estimate)))/sum(my_data$D)\n\n# ATE with weighted regression\nestimate &lt;- lm_robust(Y ~ D, \n                      data = my_data,\n                      weights = ipweight)\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nOLS Estimator\n\n\n\n\n\nFor the OLS estimator, we can use the lm_robust() function:\n\nestimate &lt;- lm_robust(Y ~ D + X1 + X2 + X3,\n                      data = my_data)\nsummary(estimate)\n\nWe can also use the fixest package and the feols() function:\n\nlibrary(fixest)\n\nestimate &lt;- feols(Y ~ D + X1 + X2 + X3,\n                  data = my_data,\n                  se = \"hetero\")\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nFully Interacted Estimator\n\n\n\n\n\nFor the fully interacted estimator, we can use the lm_lin() function.\n\nestimate &lt;- lm_lin(Y ~ D,\n                   covariates = ~ X1 + X2 + X3,\n                   data = my_data)\nsummary(estimate)\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "5 Selection on Observables"
    ]
  },
  {
    "objectID": "factor.html",
    "href": "factor.html",
    "title": "Factor Analysis Models",
    "section": "",
    "text": "PCA, as we discussed in the last chapter, is essentially an algorithm based method of dimensional reduction. In this chapter, we will discuss Factor Analysis, a model-based method. These models specify some probability model that represents an approximation of the data-generating process, that allow us to model latent variables.\nUse the right sidebar for navigation. R-code provided at the bottom.\n\n\nFactor Analysis\n\nFactor Analysis Models\nFactor Analysis models are Latent Variable Models, with a few characteristics:\n\n\n\n\n\n\nExplanation of Latent Variables\n\n\n\n\n\nA latent variable is a variable that is not observed. Instead, we observe several variables that are indicators of the latent variable.\nA measurement model represents how the observed indicators measure the true concept of interest, the latent variable.\n\n\n\n\n\nAn example of latent variables is the measurement of personality traits.\nWe can measure personality traits by asking survey respondents many different questions, and asking them to respond on a scale of 0-10 on each question.\nBut, these answers to the questions are all measuring one latent variable - the personality.\n\n\n\n\nAll observed indicator variables (items) and latent variables (factors) are treated as continuous variables.\nAll distributions of the variables are specified as normal distributions.\nAll the observed variables (items) are treated as measures of latent variables.\nThe latent variables (factors) are on an equal footing with each other - associations between factors are represented with correlations.\n\nWe can decide how many factors we need to properly measure the latent variables. We can assign predictions (factor scores) of the latent variable to create constructs of the latent variable to use in statistical analysis.\n\n\n\n\n\n\nExample of Factor Analysis\n\n\n\n\n\nBelow is a model of a factor analysis with 2 factors that are correlated (double sided arrow).\n\n\n\n\n\n\n\n\n\n\n\nExploratory and Confirmatory Analysis\nThere are two types of factor analysis: Exploratory factor analysis and confirmatory factor analysis.\nExploratory factor analysis (EFA):\n\nWe do not assume any number of factors, or what the factor pattern will look like.\nOur aim is to find the smallest number of interpretable factors needed to explain the correlations between the observed items.\nModels have minimum number of constraints on model parameters.\n\nConfirmatory factor analysis (CFA):\n\nModels have more than the minimum number of parameter constraints.\nCan be used to study how well hypothesized measurement models fit the data.\n\n\n\n\n\n\n\nExploratory Factor Analysis\n\nOne-Factor Model Specification\nWe denote the latent variable factor (common factor) with \\(\\xi\\). This latent variable is normally distributed with mean \\(k\\) and variance \\(\\phi\\):\n\\[\n\\xi \\sim \\mathcal N(k, \\phi)\n\\]\nWe denote our observed indicator variables (items) by \\(x_1, \\dots, x_p\\). Each item \\(x_i\\) is related to the latent factor \\(\\xi\\) with a linear regression model:\n\\[\nx_i = \\tau_i + \\lambda_i \\xi + \\delta_i\n\\]\n\\(\\lambda_i\\) is the slope, also called the factor loadings, which determine the associations between \\(\\xi\\) and \\(x_i\\). \\(\\delta_i\\) is the error term, called the unique factors.\n\n\n\n\n\n\n\n\nAssumptions and Identification\nThere are several assumptions that the exploratory factor analysis model makes:\n\nError terms \\(\\delta_i\\) for each regression model between \\(\\xi\\) and \\(x_i\\) is normally distributed with a mean of 0: \\(\\delta_i \\sim \\mathcal N(0, \\theta_{ii})\\).\nError terms \\(\\delta_1, \\dots, \\delta_p\\) are uncorrelated with each other. This implies that correlations between the \\(x_1, \\dots, x_p\\) are entirely explained by the factor. In other words, all \\(x_i\\) are conditionally independent given \\(\\xi\\).\nFactor \\(\\xi\\) is uncorrelated with the error terms \\(\\delta_i\\).\n\nWe also generally fix \\(k\\) and \\(\\phi\\) in \\(\\xi \\sim \\mathcal N(k, \\phi)\\) such that \\(\\xi\\) is a standard normal distribution. This is due to a problem of unique identification:\n\\[\n\\xi \\sim \\mathcal N(k = 0, \\ \\phi = 1)\n\\]\n\n\n\n\n\n\nProblem of Unique Identification\n\n\n\n\n\nMany different values of \\(k, \\phi, \\tau_i, \\lambda_i\\) can give the same observed means, variances, and covariances of the items \\(x_1, \\dots, x_p\\). Thus, we have many potential solutions to the problem, which means it is difficult to estimate.\nThus, we need to fix \\(k\\) and \\(\\phi\\) of \\(\\xi \\sim \\mathcal N(k, \\phi)\\) by assumption. The standard assumption is \\(k = 0\\) and \\(\\phi =1\\), so \\(\\xi \\sim \\mathcal N(0, 1)\\). This makes \\(\\xi\\) take a standard normal distribution.\n\n\n\n\n\n\n\n\n\nOther Choices of Assumptions\n\n\n\n\n\nWe can also, instead of assuming \\(k\\) and \\(\\phi\\) in \\(\\xi \\sim \\mathcal N(k, \\phi)\\), we can instead assume the size of \\(\\tau_i\\) and \\(\\lambda_i\\). This also allows for a unique identification of the model.\nThe most common alternative assumption is to choose one item (generally \\(x_1\\)), and set \\(\\tau_1 = 0\\) and \\(\\lambda = 1\\). Since \\(x_1\\) is normally distributed, this implies that \\(x_i \\sim \\mathcal N(k, \\phi + \\theta_{11})\\).\n\n\n\n\n\n\nEstimation Process\nWe know that our latent factor \\(\\xi \\sim \\mathcal N(k, \\phi)\\). We know that \\(x_i\\) is related to \\(\\xi\\) by \\(x_i = \\tau_i + \\lambda_i \\xi + \\delta_i\\), a linear regression.\nThus, the expected value/mean of \\(x_i\\) is given by \\(E(x_i|\\xi) = \\tau_i + \\lambda_i \\xi\\). Since \\(x_i\\) is also assumed to be normally distributed, we can determine the distribution of each item \\(x_i\\):\n\\[\nx_i \\sim \\mathcal N(\\tau_i + \\lambda_i \\xi, \\ \\lambda_i^2\\phi +\\theta_{ii})\n\\]\nWe can construct a theoretical covariance matrix of all the items \\(x_1, \\dots x_p\\), where the diagonals are the variances:\n\\[\n\\begin{pmatrix}\n\\V(x_1) = \\lambda_1^2\\phi + \\theta_{11} & Cov(x_1, x_2) = \\lambda_1\\phi\\lambda_2 & \\dots & Cov(x_1, x_p) = \\lambda_1\\phi\\lambda_p \\\\\nCov(x_2, x_1) = \\lambda_2\\phi\\lambda_1 & \\V(x_2) = \\lambda_2^2 \\phi + \\theta_{22}& \\dots & Cov(x_2, x_p) = \\lambda_1\\phi\\lambda_p \\\\\n\\dots & \\dots & \\ddots & \\vdots \\\\\nCov(x_p, x_1) = \\lambda_p\\phi\\lambda_1 & Cov(x_p, x_2) = \\lambda_p\\phi\\lambda_2 & \\dots & \\V(x_p) = \\lambda_p^2 \\phi + \\theta_{pp}\n\\end{pmatrix}\n\\]\nIf we fix \\(\\xi \\sim \\mathcal N(0, 1)\\), that implies \\(\\phi = 1\\). This allows us to simplify the above theoretical covariance matrix:\n\\[\n\\begin{pmatrix}\n\\V(x_1) = \\lambda_1^2 + \\theta_{11} & Cov(x_1, x_2) = \\lambda_1\\lambda_2 & \\dots & Cov(x_1, x_p) = \\lambda_1\\lambda_p \\\\\nCov(x_2, x_1) = \\lambda_2\\lambda_1 & \\V(x_2) = \\lambda_2^2  + \\theta_{22}& \\dots & Cov(x_2, x_p) = \\lambda_1\\lambda_p \\\\\n\\dots & \\dots & \\ddots & \\vdots \\\\\nCov(x_p, x_1) = \\lambda_p\\lambda_1 & Cov(x_p, x_2) = \\lambda_p\\lambda_2 & \\dots & \\V(x_p) = \\lambda_p^2  + \\theta_{pp}\n\\end{pmatrix}\n\\]\nThe estimation process is to find the values of \\(\\lambda_i\\) and \\(\\theta_{ii}\\), that make the above hypothetical covariance matrix, as close as possible to our observed covariance matrix from our sample with \\(x_1, \\dots, x_p\\). This is generally done with maximum likelihood estimation.\n\n\n\nInterpretation of Factor Loadings\nRecall the relationship between each item \\(x_i\\) and the latent factor \\(\\xi\\):\n\\[\nx_i = \\tau_i + \\lambda_i \\xi + \\delta_i\n\\]\nThe estimated factor loading, \\(\\widehat{\\lambda_i}\\), is the estimated covariance between the observed item \\(x_i\\), and the latent factor \\(\\xi\\). If item \\(x_i\\) has been standardised to a standard normal distribution, \\(\\widehat{\\lambda_i}\\) is also the correlation between \\(x_i\\) and \\(\\xi\\).\n\n\n\n\n\n\nAdditional Note on Multiple Factors\n\n\n\n\n\nWhen there are two or more factors, these interpretations of \\(\\widehat{\\lambda_i}\\) only hold if the multiple factors are uncorrelated.\nFor correlated factors, the covariances and correlations between the factors are still dependent on \\(\\widehat{\\lambda_i}\\), but they need to be calculated with an additional calculation.\n\n\n\nInterpretation of the latent factor \\(\\xi\\) is based on the items \\(x_i\\) that have large (positive or negative) loadings \\(\\widehat{\\lambda_i}\\) on the factor \\(\\xi\\). For example, take this example below:\n\n\n\n\n\nWe can see all the items are positively correlated with the factor. Thus, we could interpret the personality factor \\(\\xi\\) as a measure of status/power-oriented personality. We can see that admire and success carry larger weights (more important for the factor), while rich and respect carry less weight.\n\n\n\n\n\n\nRotation of Factors\n\n\n\n\n\nIn a one-factor model, the direction of the factor \\(\\xi\\) is not identified, so it can be chosen freely.\nFor example, we can change the sign of the values of the factors from \\(\\xi\\) to \\(-\\xi\\).\nRotating the factors would simply inverse the signs of the loading from \\(\\lambda_i\\) to \\(-\\lambda_i\\).\n\n\n\n\n\n\nCommunality and Reliability\nWhen we set our model such that \\(\\xi \\sim \\mathcal N(0, 1)\\), the model implies that the variances of \\(x_i\\) is:\n\\[\n\\V(x_i) = \\lambda_i^2 + \\theta_{ii}\n\\]\n\\(\\lambda_i^2\\) is the part of the variance of \\(x_i\\) explained by the common factor \\(\\xi\\), known as the communality of \\(x_i\\). \\(\\theta_{ii}\\) is the residual variance, also called the specific variance.\n\\(\\theta_{ii}\\) is the unique variance (or error variance), which is the the part of the variance in \\(x_i\\) not explained by the factor \\(\\xi\\).\nThe proportion \\(\\lambda_i^2/ (\\lambda_i^2 + \\theta_{ii})\\) is called the reliability \\(\\rho_i\\) of \\(x_i\\). It is the \\(R^2\\) of the measurement model for \\(x_i\\) on \\(\\xi\\): the proportion of variance in \\(x_i\\) that is explained by \\(\\xi\\).\nWhen all \\(x_i\\) are standardised to have a variance \\(\\V(x_i) = 1\\), then the communality \\(\\lambda_i^2\\) is equal to the reliability: \\(\\lambda_i^2 = 1 - \\theta_{ii}\\).\nThe mean of all the communalities of each item \\(x_1, \\dots, x_p\\) is the proportion of total variance of all the items explained by the common factor \\(\\xi\\).\n\n\n\nFactor Scores\nOnce we have estimated the factor analysis model, we can then use \\(x_1, \\dots, x_k\\) values for a individual, and predict their latent variable values \\(\\xi\\), called factor scores. These are calculated as a linear combination of the observed items:\n\\[\n\\tilde\\xi = w_0 + w_1x_1 + w_2x+2 + \\dots + w_px_p\n\\]\nThe coefficients \\(w_1, \\dots w_p\\) depend on the model parameters. Coefficients \\(w_i\\) are highest for items \\(x_i\\) which are the strongest measures of \\(\\xi\\) according to the model.\n\\(w_0\\) is the intercept. \\(w_0 = 0\\) if both \\(\\xi\\) has a mean of 0, and all \\(x_i\\) are standardised (to a standard normal distribution \\(\\mathcal N(0, 1)\\)) to have a mean of 0.\nThese weights \\(w\\) are determined by three different situations:\n\nCongeneric Measures: This is when the communality \\(\\lambda_i^2\\) and unique variance \\(\\theta_{ii}\\) are differerent for different items \\(x_i\\) and \\(x_k\\). this indicates that the items measure the same factor, but on different scales, and with different amount of errors and reliabilities \\(\\rho\\).\nTau-equivalent measures are when the communality \\(\\lambda_i^2\\) are equivalent for different items \\(x_i\\) and \\(x_k\\), but unique variance \\(\\theta_{ii}\\) is different. That implies the items measure the same factor on the same scale, but with different amounts of error and reliability \\(\\rho\\).\nParallel Measures are when the communality \\(\\lambda_i^2\\) and unique variance \\(\\theta_{ii}\\) are equivalent for different items \\(x_1\\) and \\(x_k\\). These items measure the same factor, at the same scale, with the same amount of error.\n\nIf items \\(x_i, x_k\\) are parallel measures, they recieve equal weights \\(w_i = w_k\\) in the calculation of factor scores for \\(\\xi\\).\n\n\n\n\n\n\nMultiple Factor EFA\n\nModel Specification\nWe can have a model with \\(q\\) number of latent factors \\(\\boldsymbol\\xi = (\\xi_1, \\dots, \\xi_q)\\). The model for the distribution of vector \\(\\boldsymbol\\xi\\) is:\n\\[\n\\boldsymbol\\xi \\sim \\mathcal N(\\boldsymbol k, \\boldsymbol\\Phi)\n\\]\n\nWhere \\(\\boldsymbol k = (k_1, k_2, \\dots, k_q)\\).\nWhere \\(\\boldsymbol\\Phi\\) is a matrix with diagonal elements being variances \\(Var(\\xi_j) = \\phi_{jj}\\), and other elements being covariances \\(Cov(\\xi_j, \\xi_k) = \\phi_{jk}\\).\n\nThis implies that each latent factor \\(\\xi_j\\) is a standard normal distribution:\n\\[\n\\xi_j \\sim \\mathcal N(k_j, \\phi_{jj})\n\\]\nNow, the measurement model of how \\(x_i\\) is related to each factor \\(\\xi_j\\) is as follows:\n\\[\nx_i = \\tau_i + \\lambda_{i1} \\xi_1 + \\lambda_{i2} \\xi_2 + \\dots + \\lambda_{iq} \\xi_q + \\delta_i\n\\]\nThe assumptions remain the same from the first model:\n\nError term \\(\\delta_i\\) is normally distributed with a mean of 0: \\(\\delta_i \\sim \\mathcal N(0, \\theta_{ii})\\). The variance \\(\\theta_{ii}\\) depends on which factor \\(x_i\\) we are using.\nError terms \\(\\delta_{1j}, \\dots, \\delta_{pj}\\) are uncorrelated with each other. This implies that correlations between the items, are entirely explained by the factor. In other words, all \\(x_i\\) are conditionally independent given \\(\\xi_j\\).\nAll factors \\(\\xi_j\\) is uncorrelated with the error terms \\(\\delta_i\\).\n\n\n\n\nModel Identification\nFor a given number of items \\(p\\), you must have a sufficiently small number of factors \\(q\\). Generally, the maximum amount of factors \\(q\\) is given by:\n\\[\ndf = \\frac{(p-q)^2 - (p+q)}{2} ≥ 0\n\\]\nAs in the 1-factor model, we need to specify the scales of the factors. The most conventional way is to set all means of all factors \\(\\xi_j\\) to 0, \\(k_j = 0\\), and set all variances of all factors \\(\\xi_j\\) to 1, \\(\\phi_{jj} = 1\\). Thus, every factor will be defined as:\n\\[\n\\xi_j \\sim \\mathcal N(k_j = 0, \\ \\Phi_{jj} = 1)\n\\]\nFixing \\(\\Phi_{jj} = 1\\) means our variance-covariance matrix \\(\\boldsymbol\\Phi\\) has diagonals of 1. However, the non-diagonal elements \\(Cov(\\xi_j, \\xi_k) = \\phi_{jk}\\) are not defined, so these covariances between factors are parameters that we need to estimate.\n\n\n\n\n\n\nThe Heywood Case\n\n\n\n\n\nEven when the model is formally identified, the number of factors \\(q\\) can be “too large”.\nThe Heywood Case is the name of an estimated variance of \\(x_i\\), \\(\\theta_{ii}\\), which is 0 or even negative for some observed variable \\(x_i\\).\nIt is possible that this means that \\(x_i\\) is a perfect measure of the factors \\(\\xi_j\\). However, it is far more likely that this indicates that the model has too many factors .\nThe best way to deal with this is to fit the model with one fewer factor. You can also use a confirmatory factor analysis model which sets some loadings to 0.\n\n\n\n\n\n\nFactor Rotation\nChoosing the scale of the latent factors does not fully resolve their identification. This is because there are actually infinitely many rotations of our latent factors, that all produce the same fit.\n\n\n\n\n\n\nDetails on the Rotation Identification Issue\n\n\n\n\n\nSuppose we start with two factors, \\(\\xi_1\\) and \\(\\xi_2\\). Let us transform them to 2 new factors with some linear combinations with coefficients \\(a\\):\n\\[\n\\begin{split}\n& \\xi_1^* = a_{11}\\xi_1 + a_{12}\\xi_2 \\\\\n& \\xi_2^* = a_{21} \\xi_1 + a_{22} \\xi_2\n\\end{split}\n\\]\nThis transformation can be interpreted as a rotation (change in coordinate axes) of the space of these factors. Both pairs \\((\\xi_1, \\xi_2)\\) and \\((\\xi_1^*, \\xi_2^*)\\) both produce the exactly same fit for the observed items. Thus, this causes a unique identification issue.\nIn fact, any choice of coefficients \\(a\\) (there are infinitely many of them) will produce the same model fit.\n\n\n\nGenerally, we choose the rotation based on the interpretability of the resulting factors. Interpretation is easiest when each factor has high loadings for some variables, and small (near 0) loadings for all the rest. This allows us to clearly identify what each factor is representing.\nOrthogonal factors (perpendicular to each other in vector space) imply that the factors are uncorrelated. However, some rotations can be oblique rotations, which allow the factors to be correlated.\n\n\n\n\n\n\nVisualisation of Orthogonal and Oblique Rotations\n\n\n\n\n\nThe below illustrates orthogonal and oblique rotations of factors:\n\n\n\n\n\nWe can see that the obliquely rotated axes are not exactly perpendicular to each other.\n\n\n\nIf the main goal of analysis is interpretation, we generally want to use an oblique rotation, as they are easier to interpret. This also shows if there are correlations between the factors.\nFor data reduction purposes, the orthogonal rotation can be useful, as it avoids multicollinearity.\n\n\n\nFactor Interpretation\nInterpretation is very similar to the one-factor models.\nInterpretation of a factor \\(\\xi\\) is based on the items \\(x_i\\) that have large (positive or negative) loadings \\(\\widehat{\\lambda_{ij}}\\) on the factor \\(\\xi\\).\nWhen there are two or more factors, these interpretations of \\(\\widehat{\\lambda_{ij}}\\) (which were the covariance between \\(x_i\\) and \\(\\xi_j\\)) only hold if the multiple factors are uncorrelated.\nFor correlated factors, the covariances and correlations between the factors are still dependent on \\(\\widehat{\\lambda_i}\\), but they are not exactly the value of \\(\\widehat{\\lambda_i}\\).\nWe can still calculate our factor scores as before.\n\\[\n\\tilde\\xi_j = w_{0j} + w_{1j}x_1 + w_{2j}x+2 + \\dots + w_{pj}x_p\n\\]\n\n\n\n\n\n\nConfirmatory Factor Analysis\n\nIntroduction\nExplanatory factor analysis allows us to learn from our data. However, we also make some arbitrary restrictions for indentification:\n\nWe rely on arbitrary guidelines/personal judgement to determine how many factors \\(\\xi\\) to include.\nWe rely on personal judgement to interpret factors by determining which loadings \\(\\widehat{\\lambda}_i\\) are large and small.\nWe have to make arbitrary rotation decisions (oblique or orthogonal).\n\nConfirmatory Factor Analysis is a different approach. Instead of letting the data “speak for itself”, CFA allows us to test our own theories that we already have about the relationships of indicators to factors.\nWe can test theories about relationships between factors \\(\\xi\\) by constraining them according to our theory. For example, based on our theories, we might set some component loadings to zero, and sometimes, set measurement parameters for different items \\(x_i, x_k\\) equal to each other.\n\n\n\n\n\n\nExample of Confirmatory Factor Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nModel Specification\nThe model specification of CFA is almost identical to multiple-factor EFA.\nRemember our factor analysis model, with observed items \\(x_1, \\dots, x_p\\), and latent factors \\(\\xi_1, \\dots, \\xi_q\\) is composed of a series of regressions between one item \\(x_i\\) and all the factors \\(\\xi_1, \\dots, \\xi_q\\):\n\\[\nx_i = \\tau_i + \\lambda_{i1} \\xi_1 + \\lambda_{i2} \\xi_2 + \\dots + \\lambda_{iq} \\xi_q + \\delta_i  \n\\]\nAs with EFA, we assume that all latent factors \\(\\boldsymbol\\xi = \\xi_1, \\dots, \\xi_q\\) are normally distributed with means \\(\\boldsymbol\\kappa = \\kappa_1, \\dots, \\kappa_q\\) and variances \\(\\boldsymbol\\Phi = \\phi_1, \\dots \\phi_q\\):\n\\[\n\\boldsymbol\\xi \\sim \\mathcal N(\\boldsymbol\\kappa, \\boldsymbol\\Phi)\n\\]\nAnd as with EFA, the error terms \\(\\delta_i\\) are normally distributed \\(\\delta_i \\sim \\mathcal N(0, \\theta_{ii})\\), and \\(\\delta_i\\) is uncorrelated with the \\(\\xi_j\\)’s.\nThe estimation process is identical to EFA estimation that was previously discussed.\n\n\n\nRotation and Scales of Factors\nIn EFA, all loadings \\(\\lambda_{ij}\\) (coefficients in the regression of \\(x_i\\) on \\(\\xi_1, \\dots, \\xi_q\\)) are non-zero, or the minimum number of 0’s to fix a specific ration.\nIn CFA, We can fix the factor rotation by setting enough 0 loadings (putting some \\(\\lambda_i = 0\\)), based on theories on which factors might not have any influence on a certain indicator \\(x_i\\).\nA 0 loading implies that that specific \\(\\xi_j\\) is not being measured by an observed item \\(x_i\\) (since a coefficient of 0 implies no correlation between \\(x_i\\) and \\(\\xi_j\\)). Ideally, for CFA, we want a simple structure where each item \\(x_i\\) has only one non-0 loading. This makes interpretation quite easy, as each item will only be measuring one factor.\n\n\n\n\n\n\nVisualisation of the Ideal Structure\n\n\n\n\n\nLet us say we have 8 items, and 2 factors. The ideal structure of our loadings is that each of the 8 items only has one non-zero loading. Or in other words, each item is only measuring one of the two factors.\nGraphically, such a structure would be visualised as:\n\n\n\n\n\nThis makes interpretation of the factors much easier. Below are the factor loadings of this model in EFA and CFA:\n\n\n\n\n\nWe can clearly see that the first factor is measuring some combination of rich, admire, success, and respect, while the second factor is measuring friend, equal, nature, and care.\n\n\n\n\n\n\n\n\n\nFurther Conditions for Identification\n\n\n\n\n\nIf you want a simple structure where each item \\(x_i\\) only has one non-zero loading, and you assume that errors \\(\\delta_i\\) are all uncorrelated with each other, you will need a certain factor-item ratio.\n\nFor a 1-factor CFA model, you must have at least 3 observed items.\nFor a model with 2 or more factors, you must have at least 2 items per factor.\n\nFurther conditions exist for more specific cases. Generally, the computer software will give you a warning message when a model cannot be identified.\n\n\n\nIn CFA, we can also fix the scales of individual factors through two ways:\n\nWe could, just like in EFA, assume that factors are standard normal distributions \\(\\xi_j \\sim \\mathcal N(0,1)\\). This option is the more common one, since it is simple.\nOr, each factor \\(\\xi_j\\) can be standardised to have the same scale of one item \\(x_i\\). This is done by choosing a \\(\\xi_j\\) to standardise to \\(x_i\\), going into that \\(x_i\\)’s regression, and setting the coefficient of \\(\\xi_j\\), \\(\\lambda_{ij} = 1\\) in that \\(x_i\\)’s regression.\n\n\n\n\n\n\n\nModel Selection\n\nSelection Choices in Factor Analysis\nFor both exploratory and confirmatory factor analysis, we have choices to make.\n\nFor exploratory factor analysis, we need to decide how many factors.\nFor confirmator factor analysis, we need to decide the number of factors, and what parameter constraints to use (what components to set equal to 0).\n\nGenerally, we judge models on a few set of criteria:\n\nThe error variance of each item, or reliability of each item. If the reliability is very low, we may not want to include it. We will show a few tests to determine this later.\nInterpretation: once we have selected the number of factors, can those factors be interpreted?\n\nGenerally, if we have any concerns with the above criteria, we will typically omit items with poor reliability, or just omit items such that the smaller number of interpretable factors fits the model better.\nThere are a number of statistical tests and metrics to help us make these decisions.\n\n\n\nGlobal Goodness of Fit Test\nThe estimation process of factor analysis involes finding parameters to estimate the covariance matrix \\(\\boldsymbol\\Sigma\\). The likelihood ratio test compares the fit of two models, relating to how well the fit the covariance matrix \\(\\boldsymbol\\Sigma\\) of the observed items \\(x_1, \\dots, x_p\\).\nFor the global goodness of fit test, we want to estimate the “goodness” of one model we have fitted. The two models in the test are:\n\nThe Saturated “Full” Model \\(f\\) , which reproduces the sample covariance matrix \\(\\boldsymbol\\Sigma\\) perfectly, using \\(\\frac{p(p+1)}{2}\\) parameters.\nOur fitted model \\(R\\) (also called the restricted model), which produces a fitted covariance matrix \\(\\hat{\\boldsymbol\\Sigma}\\), using \\(\\nu_r\\) number of parameters. All the parameters used in the fitted model are also present in the saturated model (which means our fitted model is “nested” in the saturated model).\n\nThe hypothesis of the test is:\n\n\\(H_0\\): the fitted/restricted model \\(r\\) is correct.\n\\(H_1\\): the fitted/restricted model \\(r\\) is incorrect.\n\nSo unlike many other hypothesis tests, we actually do not want to reject the null hypothesis \\(H_0\\).\n\n\n\n\n\n\nDetails on the Global Goodness of Fit Test\n\n\n\n\n\nThe test statistic is the likelihood ratio statistic:\n\\[\nL^2 = 2(L-L_r)\n\\]\nWhere \\(L_r\\) is the log-likelihood for the fitted/restricted model, and \\(L\\) is the log-likelihood for the saturated/full model. Essentially, \\(L^2\\) is comparing the difference between the sample covariance matrix \\(\\boldsymbol\\Sigma\\) and our fitted covariance matrix \\(\\hat{\\boldsymbol\\Sigma}\\).\nThe degrees of freedom of the test is the difference in number of parameters between the two models:\n\\[\n\\text{df} = \\nu_f - \\nu_r \\ = \\  \\frac{p(p+1)}{2} - \\nu_r\n\\]\nOnce we calculate the test-statistic \\(L^2\\), we consult a \\(\\chi^2\\) distribution with the above degrees of freedom.\nIf the p-value is small (p&lt;0.05), we reject \\(H_0\\), and conclude that \\(\\boldsymbol\\Sigma\\) and \\(\\hat{\\boldsymbol\\Sigma}\\) are not very similar, and the restricted fitted model is not a good fit of the data.\nIf the p-value is not small (p&gt;0.05), we refuse to reject \\(H_0\\), conclude that \\(\\boldsymbol\\Sigma\\) and \\(\\hat{\\boldsymbol\\Sigma}\\) are similar, and the restricted fitted model is a good fit of the data.\n\n\n\nOwn drawback of this goodness-of-fit test is that it is sensitive to sample size. That means the larger the sample size, the more likely the test rejects the restricted model, even if the differences between the two models is minimal.\n\n\n\nGlobal Fit Indicies\nAs discussed above, the global goodness-of-fit test is sensitive to sample size. It also assumes we are testing against exact fit in the population, which may not make sense.\nThere are some alternative fit indicies that address this limitation of the goodness-of-fit tests, that are more specialised. These statistics are often all reported alongside a factor analysis model, but are not too important to understand.\n\n\n\n\n\n\nRoot Mean Square Error of Approximation (RMSEA)\n\n\n\n\n\nRMSEA is a global goodness-of-fit metric that determines how well a fitted model does. The value is calculated as:\n\\[\n\\text{RMSEA} = \\sqrt{\\frac{T_m - \\text{df}_m}{\\text{df}_m(N-1)}}\n\\]\n\nWhere \\(T_m\\) is the overal goodness of fit \\(\\chi^2\\) statistic of the implied model.\nWhere \\(\\text{df}_m\\) is the corresponding degrees of freedom.\nWhere \\(N\\) is the sample size.\n\nA RMSEA of 0 is a perfect fit. Values smaller than 0.05 indicate close fit. Values greater than 0.1 indicate poor fit.\n\nAlthough these cutoffs can be questionable for smaller sample sizes (less than 100).\n\nWe can conduct statistical hypothesis tests of \\(H_0 : RMSEA = 0.05\\). This means the null hypothesis assumes a close fit - and if our null hypothesis is rejected, that means we have a poor fit (when doing a one tailed test towards higher values of RMSEA).\n\n\n\n\n\n\n\n\n\nStandard Root Mean Square Residual (SRMR)\n\n\n\n\n\nThe standard root mean square residual is the average covariance residual of our estimates to the actual sample covariances.\nThe smaller the value, the better the fit. Generally, values less tha 0.08 are indications of good fit.\n\n\n\n\n\n\n\n\n\nTucker and Lewis Index (TLI)\n\n\n\n\n\nThe Tucker and Lewis Index compares our fitted model \\(m\\) with some baseline model \\(b\\). It is as follows:\n\\[\nTLI = \\frac{\\frac{T_b}{df_b} - \\frac{T_m}{df_m}}{T_b \\ df_b - 1}\n\\]\n\nWhere \\(T_b\\) and \\(T_m\\) are the chi-squared statistics for the baseline model \\(b\\) and the implied model \\(m\\).\n\\(df\\) are the degrees of freedom.\n\nValues less than 0.9 indicate poor fit. 1 indicates a very good fit. If you get a value over 1, that may indicate overfitting.\nThis statistic does not differ too much with sample size.\n\n\n\n\n\n\n\n\n\nComparative Fit Index (CFI)\n\n\n\n\n\nThe Comparative Fit Index compares our fitted model \\(m\\) with some baseline model \\(b\\). It is as follows:\n\\[\nCFI = \\frac{(T_b - df_b) - (T_m - df_m)}{T_b - df_b}\n\\]\n\nWhere \\(T_b\\) and \\(T_m\\) are the chi-squared statistics for the baseline model \\(b\\) and the implied model \\(m\\).\n\\(df\\) are the degrees of freedom.\n\nThe values are always between 0 and 1. Values closer to 1 indicate a good fit.\nThis statistic does not differ too much with sample size.\n\n\n\n\n\n\nNested Likelihood Ratio Test\nIn the global goodness-of-fit test, we compared one fitted model to an “accurate” model.\nHowever, we can also compare two fitted models, just like the F-test in Linear Regression or Likelihood Ratio test in Logistic Regression.\nWe will have two models that we fit: one is a smaller (restricted model) \\(M_0\\), and one is the larger (less restricted model) \\(L_1\\). Our hypotheses are:\n\nNull hypothesis: \\(H_0 : M_0\\).\nAlternate Hypothesis: \\(H_1: M_1\\).\n\nThis essentially tests if the additional parameters in the larger model are statistically significant - i.e. are they worth including.\n\nFor exploratory factor analysis, a larger model is a model with more factors. The test is to see if including an extra factor makes the model statistically significantly better.\nFor confirmatory factor analysis, we can do the same tests as EFA. We can also test if setting additional loadings to 0 in the larger model is statistically significant.\n\n\n\n\n\n\n\nDetails on Likelihood Ratio Tests\n\n\n\n\n\nThe test statistic is the likelihood ratio statistic:\n\\[\nL^2 - 2(L_1- L_0)\n\\]\nWhere \\(L_r\\) is the log-likelihood for the restricted model \\(M_0\\), and \\(L\\) is the log-likelihood for the less-restricted model \\(M_1\\). Essentially, \\(L^2\\) is comparing the difference between the fitted covariance matrices \\(\\hat{\\boldsymbol\\Sigma}\\) for both models.\nThe degrees of freedom of the test is the difference in number of parameters between the two models:\n\\[\n\\text{df} = \\nu_1 - \\nu_0\n\\]\nOnce we calculate the test-statistic \\(L^2\\), we consult a \\(\\chi^2\\) distribution with the above degrees of freedom.\nIf the p-value is small (p&lt;0.05), we reject \\(H_0\\), and conclude that \\(\\boldsymbol\\Sigma\\) and \\(\\hat{\\boldsymbol\\Sigma}\\) are not very similar, and the larger model \\(M_1\\) is the better model (and the additional parameters are statistically significant).\nIf the p-value is not small (p&gt;0.05), we refuse to reject \\(H_0\\), conclude that \\(\\boldsymbol\\Sigma\\) and \\(\\hat{\\boldsymbol\\Sigma}\\) are similar, and the larger model \\(M_1\\) is no better than the smaller model \\(M_0\\) (and the additional parameters are not statistically significant.\n\n\n\n\n\n\nTests of Single Coefficients\nYou can also conduct singificance tests of individual parameter estimates. This is useful for CFA, if you want to determine if a loading should be 0 or not. The hypotheses are:\n\nNull hypothesis: \\(H_0 : \\lambda_i = 0\\).\nAlternate Hypothesis: \\(H_1 : \\lambda_i ≠ 0\\).\n\nThe CFA software will produce standard errors for the parameter estimates. Using these standard errors, we can conduct \\(z\\)-tests (or Wald tests):\n\\[\nz = \\frac{\\widehat{\\lambda_i}}{se(\\widehat{\\lambda_i})}, \\quad W = \\left( \\frac{\\widehat{\\lambda_i}}{se(\\widehat{\\lambda_i})} \\right)^2\n\\]\nFor the \\(z\\) test, you consult a standard normal distribution to find the p-values. For the wald test, consult a \\(\\chi^2\\) distribution with 1 degree of freedom.\n\nIf p&lt;0.05, we can reject the null hypothesis, and conclude that \\(\\lambda_i\\) is not 0, and should not be set to 0 in our CFA.\nIf p&gt;0.05, we cannot reject the null, so we cannot conclude that \\(\\lambda_i\\) is not 0, and thus, we can set it to 0 in our CFA.\n\n\n\n\nInformation Criteria Statistics\nInformation Criteria Statistics (the same as in Logistic regression) can be used to compare different models, and how good they fit the model.\nThe two most common are:\n\nAkaike’s Information Criterion: \\(AIC = L^2 - 2df\\), where \\(L^2\\) is the likelihood ratio statistic of the fitted restricted model against the full/satruated model, and \\(df\\) is the degrees of freedom.\nBayesian Information Criterion: \\(BIC = L^2 - \\log(N) df\\), where \\(N\\) is the sample size.\n\nSmaller values indicates a better model. Generally, these values cannot be interpreted on their own - they need to be interpreted in relation to other models (comparison/ordinal, not cardinal). BIC rewards having fewer parameters more strongly than AIC.\n\n\n\n\n\nImplementation in R\nFor exploratory factor analysis, you will need the package psych and GPArotation:\n\nlibrary(psych)\nlibrary(GPArotation)\n\nBefore starting factor analysis, you want a dataset with only complete observations (no NA’s) for the variables you are items for factor analysis:\n\nno_na &lt;- apply(mydata, 1, FUN=function(x){all(!is.na(x))})\nmydata &lt;- mydata[no_na,]\n\nAlso subset the data so that only the items you want to use are in the dataframe.\n\n\n\n\n\n\nEFA with One Factor\n\n\n\n\n\nWe can use the fa() command to conduct factor analysis:\n\nfa_object &lt;- fa(mydata, nfactors=1, fm=\"ml\")\nprint(fa_object)\n\nThe output provides\n\n\n\n\n\n\n\n\n\nEFA with Two Factors\n\n\n\n\n\nWe can use the fa() command to conduct factor anlaysis.\nFor a non-rotated (orthogonal) rotation, the code is as follows:\n\nfa_object &lt;- fa(mydata, nfactors=1, fm=\"ml\", rotate = \"none\")\nprint(fa_object)\n\nFor a oblique rotation, the code is as follows:\n\nfa_object &lt;- fa(mydata, nfactors=1, fm=\"ml\", rotate = \"oblimin\")\nprint(fa_object)\n\nThe output provides a table of the loadings \\(\\lambda_i\\) for each item. ML1 represents the first factor \\(\\xi_1\\), and ML2 represents the second factor \\(\\xi_2\\), and so on…\nFor the oblique rotation, there is also a table of correlations between all the factors \\(\\xi_j\\).\n\n\n\n\n\n\n\n\n\nFactor Scores\n\n\n\n\n\nTo calculate factor scores, we can extract them from the fa_object in which we stored our factor analysis.\n\nfa_object$scores\n\nThis will give you a table, with the rows being different units in the data, and the columns being different factor scores for each factor.\nIf you are wanting to just view the scores, it is recommended to subset the data if you have too many observations:\n\nfa_object$scores[1:10,] #first 10 units\n\nYou can also subset the number of factors (although if you don’t need extra factors, you would just specify less when estimating):\n\npca_object$scores[,1:2] #first 2 factors\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Further Statistical Methods",
      "Factor Analysis Models"
    ]
  },
  {
    "objectID": "unsupervised.html",
    "href": "unsupervised.html",
    "title": "Unsupervised Learning Methods",
    "section": "",
    "text": "This chapter introduces unsupervised learning methods commonly used in Data Science. First, we will discuss Principle Components Analysis, a popular dimensional reduction approach. Then, we will discuss cluster analysis, a popular method of learning more about our data.\nUse the right sidebar for navigation. R-code provided at the bottom.\n\n\nPrinciple Components Analysis\n\nIntroduction\nPrinciple Components Analysis (PCA) combines multiple observed variables into one or new observed variables (called principle components).\n\n\n\n\n\n\n\n\n\n\n\nExample: Trust in Institutions\n\n\n\n\n\nA survey asked respondents to respond, on a scale of 0-10, how much they trust the institutions.\n\nParliament: \\(x_1\\)\nLegal: \\(x_2\\)\nPolice: \\(x_3\\)\nPoliticians: \\(x_4\\)\nPolitical Parties: \\(x_5\\)\nEuropean Parliament: \\(x_6\\)\nUnited Nations: \\(x_7\\)\n\nWe don’t care about each variable individually. We want to summarise them into one variable that describes trust in institutions.\nWe can use PCA to create \\(y_1\\) (and \\(y_2\\), and more).\n\n\n\nPCA can be used for dimensional reduction (reducing the number of variables), or for summarising multiple variables into one measure.\n\n\n\nObserved Variables\nWe have \\(p\\) observed variables \\(x_1, \\dots, x_p\\), measured for each unit in a sample of data (note, not population. We are not lying in the statistical inference world here).\nEach of these observered variables \\(x_i\\) can be summarised by:\n\nTheir sample means \\(\\bar x_i\\).\nTheir sample variances \\(s^2_i = (x_i)\\) and sample standard deviations \\(s_i = \\sqrt{Var(x_i)}\\).\nSample correlations \\(Corr(x_i, x_k)\\) for each pair of observed variables \\(i ≠ k\\).\nTotal variance of the \\(p\\) variables: \\(\\V(x_1) + \\V(x_2)+ \\dots + \\V(x_p)\\).\n\nThe sample correlations \\(Corr(x_i, x_k)\\) can be summarised in a covariance or correlation matrix. We notate the covariance or correlation matrix as \\(\\boldsymbol\\Sigma\\).\nBelow is an example of a correlation matrix between 7 variables (the same 7 in the example above).\n\n\n\n\n\n\n\n\nPrinciple Components\nPCA takes these original variables \\(x_1, \\dots, x_p\\), and calculates a set of new variables (principle components) \\(y_1, \\dots, y_p\\). Each principle component \\(y_j\\) are linear combinations of the original variables:\n\\[\n\\begin{split}\ny_1 = & a_{11}x_1 + a_{21}x_2 + \\dots + a_{p1}x_p \\\\\ny_2 = & a_{12}x_1 + a_{22}x_2 + \\dots + a_{p2}x_p \\\\\n& \\vdots \\\\\ny_p = & a_{1p}x_1 + a_{2p}x_2 + \\dots + a_{pp}x_p\n\\end{split}\n\\]\nWith \\(a_{ij}\\) being the weights of the linear combinations. The sum of all the weights for each principle component should be 1: \\(\\sum_i a_{ij}^2 = 1\\) for each \\(j = 1, 2, \\dots, p\\).\nWe can rewrite each principle component \\(y_j\\) in terms of linear algebra:\n\\[\ny_j = \\mathbf a_j^\\mathsf{T}\\mathbf x = \\underbrace{\\begin{pmatrix}\na_{1j} & a_{2j} &\\dots & a_{pj}\n\\end{pmatrix}}_{\\text{weight vector for } y_i} \\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_p\n\\end{pmatrix}\n\\]\nAnd all principle components \\(\\mathbf y = (y_1, \\dots, y_p)\\) can be expressed as:\n\\[\n\\mathbf y = \\mathbf A^\\mathsf{T} \\mathbf x, \\quad \\mathbf A = \\begin{pmatrix} \\mathbf a_1 & \\mathbf a_2 & \\dots & \\mathbf a_p \\end{pmatrix}\n\\]\nThe PCs together have the same total variance as the original variables: \\(\\sum Var(y_i) = \\sum Var(x_i)\\). Thus, the PCs carry the same information/variation as the original variables, just with a different distribution. The principle components are also numbered, with each subsequent principle component having less variance: \\(Var(y_j) ≥ Var(y_{j+1})\\).\nThe principle components are all uncorrelated with each other: \\(Corr(y_j , y_k) = 0, \\ \\forall \\ j ≠ k\\). This also implies a property with weights: \\(\\sum_i a_{ij}a_{ik} = 0\\) for every pair \\(j ≠ k\\). Thus, the difference components convey distinct aspects of the data.\n\n\n\nCalculating Principle Components\nThe weights \\(a_{ij}\\) are calculated from eigenvalue decomposition of the covariance matrix \\(\\boldsymbol\\Sigma\\) of variables \\(x_1, \\dots, x_p\\).\nWe assume that \\(\\boldsymbol\\Sigma\\) has \\(p\\) distinct positive eigenvalues, denoted \\(\\lambda_1 &gt; \\dots, \\lambda_p &gt; 0\\). Each eigenvalue corresponds to a eigenvector \\(\\mathbf a_j\\), which serves as the weight vector for the \\(i\\)th principle component:\n\\[\n\\boldsymbol\\Sigma \\mathbf a_j = \\lambda_j \\mathbf a_j\n\\]\nWe can apply eigenvalue decomposition to matrix \\(\\boldsymbol\\Sigma\\):\n\\[\n\\boldsymbol\\Sigma = \\mathbf{ADA}^{-1}\n\\]\nWhere \\(\\mathbf Q\\) is made up of the eigenvectors of matrix \\(\\boldsymbol\\Sigma\\), and \\(\\mathbf D\\) is a diagonal matrix with the eigenvalues \\(\\lambda\\) on its diagonals:\n\\[\n\\mathbf D = \\begin{pmatrix}\n\\lambda_1 & 0 & \\dots & 0 \\\\\n0 & \\lambda_2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\lambda_p\n\\end{pmatrix}, \\quad \\mathbf A = \\begin{pmatrix} \\mathbf a_1 & \\mathbf a_2 & \\dots & \\mathbf a_p \\end{pmatrix}\n\\]\nThe principle components will be as follows:\n\\[\n\\mathbf y = \\mathbf A^\\mathsf{T} \\mathbf x\n\\]\n\n\n\nStandardisation of Items\nThe results of PCA will be affected by the variances of individual variables. If \\(Var(x_1) &gt; Var(x_2)\\), \\(x_1\\) will receive a larger weight in PCA.\nThis means that if we have different measurement scales for variables, we can get very different PCA results. This is very similar to the issue of comparing covariances, which generally need to be standardised into correlation coefficients.\nTo prevent measurement scales from affecting PCA, we will standardise the observations for each variable, creating a new variable \\(z_{it}\\) (\\(t\\) is observations \\(t = 1, \\dots, n\\) in the sample):\n\\[\nz_{it} = \\frac{x_{it} - \\bar x_i}{s_i}\n\\]\nA standardised variable has sample mean 0, and standard deviation and variance of 1. Thus, the total variance of all standardised variables \\(z_i\\) is equal to the number of variables:\n\\[\n\\sum\\limits_{i=1}^p Var(z_i) = p\n\\]\nWe can also avoid the standardisation step by performing PCA on the correlation matrix instead of the covariance matrix (since correlation coefficients are already standardised).\n\n\n\n\n\n\nInterpreting Principle Components\n\nChoosing Number of Components\nThe variance of \\(y_j\\), is equivalent to the \\(j\\)th eigenvalue: \\(Var(y_j) = \\lambda_j\\). Principle components are listed in decreasing order of variance \\(\\lambda_1, ≥ \\lambda_2 ≥ \\dots ≥ \\lambda_p\\). The proportion of the first \\(q\\) principle components is thus:\n\\[\n\\frac{\\lambda_1 + \\lambda_2 + \\dots + \\lambda_q}{\\lambda_1 + \\lambda_2 + \\dots + \\lambda_p}\n\\]\nYou could just choose the first principle component \\(y_1\\), and use it. However, the best practice is to retain as few as possible principle components, without losing a significant amount of variability. There are a few different rules that people often use to determine how many principle components to use:\n\nRetain as many components \\(q\\) such that it explains 70-80% of the variation.\nRetain components with large eigenvalues \\(\\lambda_j\\), generally at least 0.7 if using the covariance matrix, and 1 for the correlation matrix.\nOmit components once \\(\\lambda_j\\) stops decreasing significantly (using a scree plot).\n\n\n\n\n\n\n\nScree Plot of \\(\\lambda_j\\)\n\n\n\n\n\nBelow is a scree plot of \\(\\lambda_j\\) on the \\(y\\) axis, and the principle component on the \\(y\\) axis.\n\n\n\n\n\nWe can see after the first two principle components, the plot becomes relatively flat. Thus, we can keep components 1 and 2, and we can ignore the rest.\n\n\n\nGenerally, you will normally keep around 2 or 3 principle components following these guidelines. You should also consider the interpretation of the principle components when deciding to keep the components.\n\n\n\nWeights and Component Loadings\nEach principle component is a weighted sum of the original variables, with \\(a_{ij}\\) being weights:\n\\[\ny_j = a_{1j}x_1 + a_{2j}x_2 + \\dots + a_{pj}x_p\n\\]\nThus, a larger weight for a variable means that variable contributes more, and a smaller weight for a variable means that the variable contributes less.\n\n\n\n\n\n\nExample of Interpreting Weights\n\n\n\n\n\nBelow is a table of weights:\n\nWe can see that it seems that police is contributing the least to the first principle component \\(y_1\\).\n\n\n\nWe can also consider the weights in a normalised form, called component loadings:\n\\[\na^*_{ij} = \\sqrt{\\lambda_j}  \\ a_{ij} = sd(y_j) a_{ij}\n\\]\nWhen PCA is based on the correlation matrix, we will have the property that the component loadings equals the correlation between the given variable \\(x_i\\) and the principle component \\(y_j\\):\n\\[\na^*_{ij} = Corr(x_i, y_j)\n\\]\nThis property can help us interpret what higher values of \\(y_j\\) and lower values of \\(y_j\\) mean for each principle component.\n\n\n\n\n\n\nExample of Component Weights\n\n\n\n\n\nBelow is a table of component weights:\n\nWe can see that all of the component weights for the first component are positive. That means that the first principle component \\(y_1\\) is positively correlated with all \\(x_i\\).\nWe know that all \\(x_i\\) are scales of 0 - 10 on how much they trust the institution in question. Thus, we know that for principle component \\(y_1\\), that higher values mean higher trust in institutions.\nFor the second principle component \\(y_2\\), there are some positives and some negatives. The negative correlations are with legal, police, and UN, and the positive correlations are with parliament, politicians, political parties, and european parliament.\nWe could interpret this as the second principle component \\(y_2\\) increases, the trust in legal/police decreases, but the trust in political institutions increase. Thus, \\(y_2\\) could be a measure of trust in legal/police versus political institutions.\nFor the third principle component, we can see only the trust in european parliament and UN is negative. Thus, we could interpret \\(y_2\\) as a measure of trust in national versus international institutions.\n\n\n\n\n\n\nWeights and Correlations of Original Variables\nPCA is derived from the sample correlation matrix. Thus, the weights also reflect patterns in the correlations of the original variables \\(x_i\\).\nPatterns between the weights of the first principle component and correlation matrix include the following:\n\nIf the correlations of all variables is positive, then the first principle component will be positive. This is because all the variables that are correlated mean they are measuring the “same thing” in the same direction.\nVariables which on average have the largest correlations with other variables will get the largest weights. This makes sense, since variables with the largest correlations seem to be measuring the “concept” the most.\nIf a variable has 0 correlation with the other variables, it will get a 0 weight, since it is measuring a completely different thing than the other variables.\n\n\n\n\n\n\n\nCorrelation Matrix and the First Principle Component\n\n\n\n\n\nBelow is a correlation matrix between variables:\n\nWe can see the variable politicians (highlighted in blue) have the largest correlation with the other variables on average.\nWe can see the variable police (highlighted in yellow) have the smallest correlation with other variables on average. It might be the “odd one out”, not as lined up as the rest, so it gets a smaller weight in the first principle component.\n\n\n\nThe second principle component is the contrast of the first principle component (remember, the covariance between the two is 0).\nIt will identify two subgroups of variables, such that each subgroup will have high correlations within them, but the correlation between groups is lower.\n\n\n\nPrinciple Component Scores\nFor each unit \\(i\\) (individual), we can calculate a component score for the \\(j\\)th principle component:\n\\[\ny_j = a_{1j}z_1 + a_{2j}z_2 + \\dots + a_{pj}z_p\n\\]\nBy doing this for each individual unit \\(i\\), we now have a new variable \\(y_j\\) that we can use for further analysis. Such analyses can include:\n\nWe could plot the principle component scores for the first and second principle component score, and look for clusters with similar values.\nWe could use the principle component scores as summary statistics for some phenomena.\nWe could also use the scores of \\(y_j\\) and input into a regression model or another statistical model.\n\n\n\n\n\n\n\nCluster Analysis\n\n\n\n\nImplementation in R\nYou will need packages psych and GPArotation.\n\nlibrary(psych)\nlibrary(GPArotation)\n\nTo estimate the principle components, you should use the princomp() command:\n\npca_object &lt;- princomp(~x1 + x2 + x3,\n                      data = mydata,\n                      cor = TRUE, #to do on corr matrix\n                      scores = TRUE, #calculate scores\n                      na.action = na.exclude)\nsummary(newObject)\n\nThe summary command will provide the standard deviation of each principle component, the proportion of total variance each component explains, and the cumulative proportion of variance explained.\n\n\n\n\n\n\nObtaining Variances/Eigenvalues\n\n\n\n\n\nThe variances of each principle component (also the eigenvalues) are the square of the standard deviation. We can obtain as following:\n\npca_object$sdev^2\n\n\n\n\n\n\n\n\n\n\nScreeplot\n\n\n\n\n\nTo create a screeplot to help identify how many principle components to keep, we use the screeplot() command:\n\nscreeplot(pca_object, type='l', main=\"\")\n\n\n\n\n\n\n\n\n\n\nWeights and Component Loadings\n\n\n\n\n\nWe can calculate weights for each principle component by using the loadings() command:\n\npca_weights &lt;- loadings(pca_object)\nprint(pca_loadings, cutoff = 0, digits = 4)\n\nFor component loadings (weights in a normalised form), we first get the square root of the eigenvalues \\(\\lambda_i\\), then we multiply our weights with those square roots.\n\nsqrt_lambda &lt;- pca_object$sdev\nprint(t(t(pca_weights)*sqrt_lambda), cutoff = 0, digits = 4)\n\n\n\n\n\n\n\n\n\n\nComponent Scores\n\n\n\n\n\nWe can access component scores in the pca_object we originally created:\n\npca_object$score\n\nThis will give you a table, with the rows being different units in the data, and the columns being different principle component scores for each principle component.\nIf you are wanting to just view the scores, it is recommended to subset the data if you have too many observations:\n\npca_object$score[1:10,] #first 10 units\n\nYou can also subset the number of principle components:\n\npca_object$score[,1:3] #first 3 PC\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Further Statistical Methods",
      "Unsupervied Learning Methods"
    ]
  },
  {
    "objectID": "predict.html",
    "href": "predict.html",
    "title": "Forecasting and Prediction Models",
    "section": "",
    "text": "This chapter covers methods for forecasting and prediction. We start with time-series models for forecasting future values. Then, we explore a series of machine learning models that help with prediction/classification tasks.\nUse the right sidebar for navigation. R-code provided at the bottom.\n\n\nTime Series Models\n\nTime Series and Stationary Processes\nTime series are a set of data collected at successive time points \\(t \\in \\mathbb Z\\). Each observation \\(X_t\\) is at time point \\(t\\). For example, a time series with \\(T\\) number of time periods would be \\((X_1, X_2, \\dots, X_T)\\). Examples of time series include monthly unemployment data, daily currency exchange rates, and monthly politician approval rates.\nThe goal of time series modelling is to predict \\(X_t\\) with past data. There are several factors that might affect \\(X_t\\):\n\nPrevious values of \\(X_t\\) in the time series.\nLong-term trends: these are long-term movements in the mean of a section of the time series. This can be an upward trend, a downward trend, or a stationary trend (no changes).\nSeasonality: cyclical repeating patterns in certain periods, for example, business cycles, or sales being higher during the holiday season.\nRandomness: other random/systematic fluctuations.\n\nOne important idea is a (weak) stationary process, where there is zero trend over time. A weak stationary process is when the expected value and variance of our time series is constant over different (but same-sized) groups of time periods: \\(\\E X_t = \\mu\\), \\(\\V X_t = \\sigma^2\\).\nMost time-series data will not be weak stationary. However, we can transform most time-series data into weak stationary processes through differencing. We refer to the new differenced time series as \\(X'_t\\):\n\nZero differencing \\(d=0\\) is no differencing: \\(X_t' = X_t\\). We just use our original time-series.\nFirst order differencing \\(d = 1\\) is \\(X_t' = X_t - X_{t-1}\\). Basically we take our \\(X_t\\) at time period \\(t\\), and subtract away the previous time period \\(X_{t-1}\\).\nSecond order differencing \\(d = 2\\) is \\(X'_t = (X_t - X_{t-1}) - (X_{t-1}-X_{t-2})\\).\n\nAny further order of differencing \\(d\\) is increasingly uncommon. The point of differencing is that we keep increasing \\(d\\) until we have a weak stationary process. We denote a time series that has been differenced at \\(d\\)-order with \\(X_i'^d\\).\n\n\n\nMA and AR Models\nMA (Moving Average) models are when \\(X_t\\) at time \\(t\\) depends on past error terms \\(\\epsilon_{t-1}, \\dots, \\epsilon_{t-q}\\). This implies that \\(X_t\\) is dependent on short-term noise/fluctuation. The model is specified with stationarity assumed for \\(X'_t\\):\n\\[\n\\begin{align}\nMA(q) \\ : \\ X_t'^d & = \\omega +  \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2} + \\dots + \\theta_q \\varepsilon_{t-q} + \\varepsilon_t \\\\\n& = \\omega + \\sum\\limits_{i=1}^q \\theta_i \\epsilon_{t-i} \\ + \\varepsilon_t\n\\end{align}\n\\]\nWhere \\(\\omega\\) is some constant (the mean of the time series), and \\(\\theta_1, \\dots, \\theta_q\\) are the coefficients. This model is estimated with maximum likelihood. To choose \\(q\\), we find the model with the lowest AIC or BIC.\nAR (Auto-regressive) models are when \\(X_t\\) at time \\(t\\) depends on past values \\(X_{t-1}, \\dots, X_{t-p}\\). This implies that \\(X_t\\) is dependent on previous values in the time series, for example, in variables with momentum (ex. stock prices, sales). The model is specified with stationarity assumed for \\(X'_t\\):\n\\[\n\\begin{align}\nAR(p) \\ : \\ X_t'^d& = \\omega + \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\dots + \\phi_p X_{t-p} + \\varepsilon_t \\\\\n& = \\omega + \\sum\\limits_{i=1}^p \\phi_i X_{t-i} \\ + \\varepsilon_t\n\\end{align}\n\\]\nLet us demonstrate a unique property of autoregressive models. Consider AR(1):\n\\[\nAR(1) : X_t' = \\omega + \\phi_1 X_{t-1} + \\varepsilon_i\n\\]\nKnowing this, we can also predict \\(X'_{t-1}\\), and then plug this back into our original \\(X'_t\\). Then, we can do the same with \\(X'_{t-2}\\), plug it in, and then \\(X'_{t-3}\\), and so on \\(k\\) number of times, until we can get the following model:\n\\[\nX'_t = \\phi^kX_{t-k} + \\sum\\limits_{j=0}^{k-1} \\phi_j \\varepsilon_{t-j}\n\\]\nAnd when \\(|\\phi| &lt; 1\\), the model is equivalent to a moving average model where \\(q = ∞\\):\n\\[\nX'_t = \\sum\\limits_{j=0}^∞ \\phi_j \\varepsilon_{t-j} = MA (∞)\n\\]\n\n\n\nARIMA Model\nThe most popular model is the ARIMA (Auto-regressive Integrated Moving Average Model), which combines AR and MA models. ARIMA assumes that we have stationary trends:\n\\[\n\\text{ARIMA}(p, d, q) \\ : \\ X_t'^d = \\underbrace{\\sum\\limits_{i=1}^p \\phi_iX_{t-i}}_{\\text{AR}} + \\underbrace{\\sum\\limits_{i=1}^q \\theta_i\\varepsilon_{t-i}}_{\\text{MA}} + \\varepsilon_t\n\\]\n\n\\(X_t'^d\\) is \\(d\\)-ordered differenced time series that is stationary.\nThe AR (Autoregressive) section is how previous values of \\(X\\) before time period \\(t\\) predict the current \\(X_t\\). \\(p\\) is the number of previous time periods to include (which we have to choose).\nThe MA (Moving Average) section is how previous values of error \\(\\epsilon\\) before time period \\(t\\) predict the current \\(\\epsilon_t\\). \\(q\\) is the number of previous time periods to include (which we have to choose).\n\nThe model is estimated with maximum likelihood estimation. We can choose \\(p\\) and \\(q\\) by first choosing a variety of \\(p\\) and \\(q\\), estimating each model, and choosing the model (and \\(p\\) and \\(q\\)) that has the least AIC or BIC value.\nARIMA is a non-seasonal models, since seasonality is technically non-stationary by definition. An extension of this model for seasonaility is called SARIMA.\nARIMA is very popular for a multitude of reasons.\n\nFirst, most situations in the real world have both AR and MA components: generally, \\(X_t\\) is determined by some combination of past \\(X_t\\) and errors.\nARIMA is a general framework. An \\(\\text{ARIMA}(p, 0, 0) = \\text{AR}(p)\\), and a \\(\\text{ARIM}(0,0,q) = \\text{MA}(q)\\). This allows us to adapt ARIMA to a wide variety of data without being forced into one specific model selection.\nARIMA has quite good accuracy in time series forecasting, even compared to machine learning and modern techniques.\n\n\n\n\nSeasonality Differencing and SARIMA\nSARIMA (Seasonal Auto-regressive Integrated Moving Average) models are the seasonal extension: we add seasons by doing additional seasonal differencing.\nA seasonal differenced is taking \\(X_t\\), and subtracting the \\(X\\) value from the same point in the last season. Let us say each season is \\(S\\) number of time periods long. So, a first-order seasonal difference would be \\(Z_t = X_t - X_{t-S}\\).\nWe can also combine a seasonal differenced \\(Z_t\\) with a normal difference. For example, if \\(d= 1\\) and \\(D=1\\):\n\\[\nZ^{*d}_t = Z_t - Z_{t-1} = (X_t - X_{t-S}) - (X_{t-1} - X_{t-1-S}), \\qquad \\text{where } Z_t = X_t - X_{t-S}\n\\]\nA SARIMA model will take the following form:\n\\[\n\\text{SARIMA}(p, q, d)(P, Q,D)S \\ : \\ Z_t^{*d} = \\underbrace{\\overbrace{\\sum\\limits_{i=1}^p \\phi_iX_{t-i}}^{\\text{AR}} + \\overbrace{\\sum\\limits_{i=1}^q \\theta_i\\epsilon_{t-i}}^{\\text{MA}}}_{\\text{ARIMA}} + \\underbrace{\\overbrace{\\sum\\limits_{i=1}^P \\Phi_iX_{t-Si}}^{\\text{AR}} + \\overbrace{\\sum\\limits_{i=1}^Q \\Theta_i\\epsilon_{t-Si}}^{\\text{MA}}}_{\\text{Seasonality}} + \\epsilon_t\n\\]\nWhere \\(Z^{*d}\\) is a \\(d\\)-order difference of \\(D\\)-order seasonal differences \\(Z_t'^D\\). Note that the seasonality section’s parameters and model components are all specified with capital letters.\nChoosing \\(p, q, P, Q\\) is the same as in a ARIMA model - we should estimate models with different combinations, and see which model has the lowest AIC and BIC.\n\n\n\n\n\n\nTrees and Random Forests\n\nRegression Trees\nRegression Trees are an alternative way to obtain predictions of an outcome \\(y\\) from explanatory variables \\(x\\). Instead of modelling how \\(y\\) changes with every unit increase in \\(x\\), regression trees instead model stratification.\nTree-based methods will divide the independent variable \\(x_j\\) into 2 regions, splitting \\(x\\) at some value \\(x = s\\). For example, if we had an \\(x\\) variable such that \\(x \\in [0, 100]\\), we could split the variable at \\(x = s\\) to create two regions: \\(x^{(1)} \\in [0, s]\\) and \\(x^{(2)} \\in [s, 100]\\).\nThen, tree-based methods will calculate the mean \\(y\\) value in each region created: \\(\\bar y^{(1)}\\) and \\(\\bar y^{(2)}\\). These means will be the predictions of \\(\\hat y\\).\n\nIf a unit \\(i\\) has an \\(x\\) value that falls into region \\(x^{(1)} \\in [0, s]\\), their predicted \\(\\hat y = \\bar y^{(1)}\\).\nIf a unit \\(i\\) has an \\(x\\) value that falls into region \\(x^{(2)} \\in [s,100]\\), their prediction \\(\\hat y = \\bar y^{(2)}\\).\n\nBut how do we decide which threshold \\(s\\)? For continuous \\(y\\) variables, to determine the threshold \\(s\\) in which to split \\(x\\), the algorithm of tree regressions will find the optimal threshold \\(x = s\\) that reduces the residual sum of squares (RSS) of the predictions the most:\n\\[\nRSS = \\sum\\limits_{i=1}^n(y_i - \\hat y_i)^2\n\\]\nEssentially, the computer tests every possible threshold value of \\(x_j = s\\), and finds the threshold that reduces the sum of squares the most.\nFor binary \\(y\\) variables, trees will determine the threshold \\(s\\) in which to split \\(x\\), based on the threshold \\(x=s\\) that reduces the classification error rate. The classification error is essentially how many predictions \\(\\hat y\\) do not match to the actual \\(y\\).\n\n\n\n\n\n\nProcedure of Growing a Tree\n\n\n\n\n\nThe process of growing a tree is as follows. Assume now we have more than one explanatory variable.\n\nTest all possible thresholds \\(x_j = s\\) for each \\(x_j \\in \\{x_1, \\dots, x_k\\}\\).\nIdentify the specific variable \\(x_p \\in \\{x_2, \\dots, x_k\\}\\) which has a specific threshold \\(x_p = s^*\\) that has the greatest reduction in residual sum of squares (or classification error rate), for all combinations of \\(x_j\\) and threshold \\(s\\).\nNow, divide that specific \\(x_p\\) at that specific threshold \\(s^*\\), creating two regions \\(x_p^{(1)}, x_p^{(2)}\\).\nNow, we repeat the process of testing all possible thresholds \\(x_j = s\\) for each \\(x_j \\in \\{x_1, \\dots, x_k \\}\\) and finding the specific variable-threshold \\(x_m = s^{**}\\) that results the in the greatest reduction in RSS (or classification error rate) within \\(x_p^{(1)}\\). Create 2 more groups from within \\(x_p^{(1)}\\).\nThen, do the same for \\(x_p^{(2)}\\). Create 2 more groups from with \\(x_p^{(2)}\\).\nKeep on going, finding the variable-thresholds which reduce the RSS the most from each previous subregion we have created.\nContinue doing this until some stopping criteria. Find the average \\(y\\) in each of the groups you have, and those are the predicted \\(\\hat y\\).\n\nIt is possible for a variable to occur multiple times with different thresholds.\n\n\n\n\n\n\n\n\n\nExample of Growing a Tree\n\n\n\n\n\nBelow is an example of growing a tree:\n\n\n\n\n\nWe can see that the first (top) split is in the variable taxpercent, at threshold 34.2935. That means that specific variable-threshold split reduced the RSS the most of any variable-threshold combination.\nNotice how after the first split, we only divide each subregion. We do not go back to the top/whole data set.\nNotice how taxpercent re-appears again on the right side of the tree. It is possible for a variable to occur multiple times.\nAt the end, you can see the numbers at the bottom. These are called leaves, and are the final categories/groups of the tree with their mean \\(y\\) labelled. Those mean \\(y\\) will be the predictions for each observation that falls into that group.\n\n\n\nThe earlier a variable is split, the more influential the variable is. We will discuss this idea further later in the chapter.\n\n\n\nLimitations and Bootstrap Aggregation\nOne of the best things about decision trees is that they incorporate interactions between variables. Lower-level splits of variables are interacting with higher-level splits of variables. This allows regression trees to be excellent for non-linear predictions. Regression trees are also great for visualisation, and are quite easy to explain visually without invoking complex statistics or mathematics.\nThe downside of regression trees is that they have extremely high variance. If you just slightly change the data, your predicted results will be completely different, and even the order of variables and thresholds will completely change. This is called overfitting. Overfitting causes simple regression trees to be poor predictors of out-of-sample data.\nBootstrap Aggregation is a solution to this variance and overfitting problem. This procedure builds on a simple statistical idea: in a set of \\(n\\) samples \\(Z_1, \\dots, Z_n\\) each with variance \\(\\sigma^2\\), the variance of the means \\(\\bar Z\\) of all the samples is \\(\\frac{\\sigma^2}{n}\\). Since \\(n\\) is in the denominator, that implies increasing the number of samples \\(n\\) will reduce the variance of our predictions.\nHowever, we typically only have one sample of data. How can we increase \\(n\\) if we only have one sample? The answer is Bootstrap Sampling. Essentially, we sample with replacement from our original sample.\nTo create a bootstrap sample, we choose 1 observation at random from our original sample. We add that observation to our bootstrap sample, and replace it back into our original sample. Then, we draw another observation, add it to our bootstrap sample, and replace it back. We do this \\(n\\) times (\\(n\\) being the sample size of our sample).\nIf we do this procedure multiple times, we will end up with a few similar, but slightly different data sets. We are essentially replicating the process of obtaining new data sets, without gathering more data. Now, with our multiple data sets, we can use bootstrap aggregation to reduce our variance.\n\n\n\nBagging and Random Forest\nBagging models apply bootstrap sampling to tree regressions.\n\nWe first generate \\(B\\) number of bootstrap samples.\nThen, we train a tree on each different sample \\(b\\), creating a prediction function \\(f_b(x) = \\hat y_b\\) for each sample, which specifies what values of \\(x\\) result in what predicted \\(\\hat y\\) according to the tree.\nThen we average all of the sample predictions together to obtain our final prediction function \\(f_{bag}(x)\\).\n\n\\[\nf_{bag}(x) = \\frac{1}{B}\\sum\\limits_{b=1}^Bf_b(x)\n\\]\nBagging reduces the variance of trees, and is one of the most accurate prediction methods, frequently outperforming both linear and non-linear methods.\nHowever, bagging is still not perfect. This is because bagging trees are heavily correlated: in general, the trees will have the same top-level \\(x_p\\) variable, especially if there is one very strong predictor in our explanatory variables. This is an issue - highly correlated trees, even if averaged, do not reduce the variance as much as we might need.\nRandom Forests solve this issue by not only bootstrapping observations like Bagging does, but also sampling a subset of explanatory variables for each tree. For example, let us say you a set of explanatory variables \\(\\mathcal X = \\{x_1, \\dots  x_k\\}\\). For every bootstrap sample of observations \\(b\\) that we did in bagging, Random forest will also sample a subset of explanatory variables \\(\\mathcal X_b\\), which contains only \\(g&lt;k\\) number of explanatory variables.\nThis means that each tree Random Forest produces only has a subset of \\(g\\) explanatory variables, not the total number of \\(k\\) explanatory variables. This means that sometimes, influential variables \\(x_p\\) will not be included in the subset \\(\\mathcal X_b\\), which will allow for other predictors to get a chance to shine.\nThus, Random Forest will have less correlated trees, and thus, will typically have more accurate predictions.\nGenerally, the size of the subset of explanatory variables \\(\\mathcal X_b\\) will typically be \\(g=\\sqrt{k}\\), the square root of the total number of explanatory variables. However, you can play around with this (discussed in the next section).\n\n\n\nModel Selection\nThere are a few choices you must make when specifying the model you are using.\n\nShould you use Random Forest or Bagging? Typically, Random Forest is better, but this is not always the case.\nIf you do choose Random Forest, what size of the subset of explanatory variables \\(\\mathcal X_b\\) should you use? Typically, the standard is to use the square root of the number of explanatory variables, but this is not a fixed rule.\n\nFor prediction tasks, Mean of squared residuals is a general measure of how well the model performed. It is the mean of the squared errors, where the errors are the differences between the actual dataset \\(y_i\\) value and the predicted \\(\\hat y_i\\) value. In R, the mean of squared residuals metric is calculated on testing data - testing the model on units that were not included in a specific bootstrap sample. Thus, this is a good measure of how predictive your model is, without worrying about overfitting.\nFor classification tasks, the simplest metric to help you make the decision is the error rate. This is simply the percentage of observations the model got wrong (predicted one category when it should have been the other). We can also dig into more detail:\n\nThe False Positive Rate is the observations that are \\(y=0\\), but our model predicted incorrectly as \\(\\hat y = 1\\).\nThe False Negative Rate is the observations that are \\(y=1\\), but our model predicted incorrectly as \\(\\hat y = 0\\).\n\nThe inverses of false positives/negatives are specificity and sensitivity:\n\nSpecificity is the percentage of observations that are \\(y=0\\), that our model correctly predicted as \\(\\hat y =0\\).\nSensitivity is the percentage of observations that are \\(y=1\\), that our model correctly predicted as \\(\\hat y = 1\\).\n\nThe metric on which to focus on depends on our goals of classification. Choosing the right model requires some subjectivity. For example, if we are trying to predict if a patient has a serious disease, we probably want more false positives rather than under-detecting people who are actually sick. But for judicial systems, since we do not want to put an innocent person in jail, we might prefer lower false positives, and more false negatives.\n\n\n\nImportance Statistics\nOne of the downsides of Bagging and Random Forest is that they are harder to interpret than traditional statistical models and also regression trees.\nRegression Trees produce a nice diagram, with the most important variables near the top. However, Bagging and Random Forest average hundreds or thousands of different trees, so we cannot really draw a diagram out.\nThis is where importance statistics comes in. To calculate importance, we “remember” the reduction in RSS (or error rate) every time we split a tree, for every bagging sample \\(b\\). Then, we figure out which variables on average reduce the RSS (or error rate) the most.\n\n\n\n\n\nAbove is an example of the importance plot. This allows us to determine which explanatory variables are the most influential in determining the predictions, which can be useful in interpretation.\n\n\n\nExtension: Causal Forests\n\n\n\n\n\n\nNaive Bayes Classifier\n\n\n\nImplementation in R\nYou will need the randomForest and tidyverse packages.\n\nlibrary(randomForest)\nlibrary(tidyverse)\n\nFor replication purposes (if you are publishing a paper), you may also want to set a random seed, which will ensure your bootstrap and variable sampling remains consistent in replication.\n\nset.seed(1234) #any number will work\n\n\n\n\n\n\n\nBagging Model\n\n\n\n\n\nYou can train a bagging model with the randomForest() function:\n\nmodel &lt;- randomForest(Y ~ x1 + x2 + x3 + x4,\n                      data = mydata,\n                      na.action = na.omit,\n                      mtry = 4, #equal to k\n                      importance = TRUE)\n\n#call object to see output\nmodel\n\nThe output will contain summary statistics.\nNote: you can also do \\(Y \\sim .\\) in the formula. The \\(.\\) symbolises that you want to include all other variables not \\(Y\\) within your dataframe into your model. This speeds up the process of writing every single explanatory variable out.\n\n\n\n\n\n\n\n\n\nRandom Forest Model\n\n\n\n\n\nYou can train a random forest model with the randomForest() function:\n\nmodel &lt;- randomForest(Y ~ x1 + x2 + x3 + x4,\n                      data = mydata,\n                      na.action = na.omit,\n                      mtry = 2, #equal to sqrt of k\n                      importance = TRUE)\n\n#call object to see output\nmodel\n\nThe output will contain summary statistics.\nNote: you can also do \\(Y \\sim .\\) in the formula. The \\(.\\) symbolises that you want to include all other variables not \\(Y\\) within your dataframe into your model. This speeds up the process of writing every single explanatory variable out.\n\n\n\n\n\n\n\n\n\nPredictions\n\n\n\n\n\nYou probably want to make predictions with your model (that is kind of the point of these models).\nWe can use the predict() function to generate predictions:\n\nmypredictions &lt;- predict(model, newdata = my_new_data)\n\nmy_new_data is a dataframe with a bunch of explanatory variable values (for every explanatory variable) for a collection of observations, that you wish to predict \\(\\hat y\\) for.\nmodel is the name of the object in which you stored your trained model to.\n\n\n\n\n\n\n\n\n\nImportance Plot\n\n\n\n\n\nTo see the importance of each explanatory variable, we can use the varImpPlot() function:\n\nvarImpPlot(model,\n           main = \"Title of the Graph\",\n           type = 2)\n\nDo not change type = 2. That specifies something technical that you do not need to worry about.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Further Statistical Methods",
      "Forecasting and Prediction Models"
    ]
  },
  {
    "objectID": "causal.html",
    "href": "causal.html",
    "title": "Causal Frameworks",
    "section": "",
    "text": "In the last 2 chapters, we focused on the classical and generalised linear model, and how it can explain relationships (correlations) between variables.\nThis chapter introduces the main causal frameworks (potential outcomes, causal graphs), the main causal estimands used in causal inference, and the idea of selection bias and confounders. This chapter is the foundation in which the next set of methods for causal inference will be built on.\nUse the right sidebar for quick navigation. R-code for causal diagrams is provided at the bottom.\n\n\nPotential Outcomes Framework\n\nTreatment Variable\nIn causal inference, we are interested in how treatment \\(D\\) causes outcome variable \\(Y\\). How does a voting system affect voter turnout? How does a minimum wage law affect unemployment?\n\\(D\\) is our treatment variable. The indicator of treatment for each unit \\(i\\) is \\(D_i\\).\n\\[\nD_i = \\begin{cases}\n1 \\quad \\text{if unit } i \\text{ recieved the treatment} \\\\\n0 \\quad \\text{if unit } i \\text{ did not recieve the treatment}\n\\end{cases}\n\\]\nCausal variables/treatments \\(D\\) must occur before the outcome \\(Y\\). A variable cannot cause something to occur in the past.\nCausal variables/treatments must be able to be manipulated (in order to imagine a world where the treatment did not occur). For example, \\(D\\) cannot be sex assigned at birth, ethnicity, etc. For example, major global events (how did 9/11 cause the Arab spring?)\n\n\n\nPotential Outcomes\nImagine there are two hypothetical parallel worlds - one where unit \\(i\\) receives the treatment \\(D\\), and one where unit \\(i\\) does not receive the treatment \\(D\\). Everything else in these worlds is identical. Unit \\(i\\) has \\(Y\\) values in both of these parallel worlds.\nPotential Outcomes for unit \\(i\\) are denoted:\n\\[\nY_{di}, Y_i(d) =\\begin{cases}\n& Y_{1i}, \\ Y_i(1) \\quad \\text{Outcome for unit } i \\text{ when } D_i = 1 \\\\\n& Y_{0i}, \\ Y_i(0) \\quad \\text{Outcome for unit } i \\text{ when } D_i = 0 \\\\\n\\end{cases}\n\\]\nFor example, imagine we are interested in finding the effect of democracy \\(D\\) on GDP growth \\(Y\\). Potential outcome \\(Y_{1i}\\) is the potential GDP growth of country \\(i\\) if they were a democracy, and outcome \\(Y_{0i}\\) is the potential GDP growth of a country \\(i\\) if they were not a democracy.\nThe reason we want hypothetical worlds is that since these two parallel worlds are identical except for treatment \\(D_i\\), then the differences between the two parallel worlds must be caused by the treatment \\(D_i\\).\nNote that these are hypothetical worlds - we will obviously not observe both of them at the same time. The next section discusses this.\n\n\n\nObserved Outcomes and “Missing Data”\nOf course, there is not two parallel worlds with 2 potential outcomes. In the real world, each unit \\(i\\) either receives treatment \\(D\\), or does not. We do not observe the other potential outcome.\n\\(Y_i\\) is the observed outcome for unit \\(i\\). This is given by formula:\n\\[\nY_i = D_i \\cdot Y_{1i} + (1-D_i) \\cdot Y_{0i}\n\\]\nIf we plug in \\(D_i = 0, 1\\) to the equation above, we get the observed outcomes:\n\\[\nY_i = \\begin{cases}\nY_{1i} \\quad \\text{if } D_i = 1 \\\\\nY_{0i} \\quad \\text{if } D_i = 0 \\\\\n\\end{cases}\n\\]\nBefore the treatment (A priori), both potential outcomes could be observed. After the treatment, one is observed, and the other is counterfactual. For any given experiment, only one will ever be seen, and the counterfactual will never be seen. This is a “missing data” problem.\n\n\n\n\n\n\nNeyman Urn Model\n\n\n\n\n\nPotential Outcomes can be visualised with the Neyman Urn Model.\nBefore the treatment, we have a box (we cannot see) with both potential outcomes.\n\n\n\n\n\nThen, when we apply treatment, we stick our hand into the box that we cannot see, and pull out one observed outcome.\n\n\n\n\n\nWe are essentially sampling from potential outcomes to get observed outcomes.\n\n\n\nThis missing data problem is called the fundamental problem of causal inference.\n\n\n\nStable Unit Treatment Value Assumption\nThe above given observed and potential outcome frameworks depends on the Stable Unit Treatment Value Assumption (SUTVA).\n\\[\n\\begin{align}\nY_{(D_1, D_2, \\dots, D_N)i} & = Y_{(D_1', D_2', \\dots, D_N')i} \\\\\nY_{di} \\text{ under current randomisation} & = Y_{di} \\text{ under all other randomisations}\n\\end{align}\n\\]\nOr more intuitively, the potential outcomes of unit \\(i\\) only depends on their own treatment status, and no other unit’s treatment status. Thus, changing everyone else’s treatment status has no effects on unit \\(i\\)’s potential outcomes \\(Y_{di}\\). The treatment is also the same for everyone (treatment is stable and consistent)\nSUTVA can be violated in many different ways:\n\nSpill-over effects: If we are testing a new curriculum, one student \\(j\\) getting the new curriculum may teach their friend \\(i\\) the new curriculum, thus affecting the potential outcomes of \\(i\\).\nContagion: If we are studying a disease, diseases can spread, so another unit \\(j\\) getting a disease affects the potential outcomes of unit \\(i\\).\nDilution: If we are studying vaccines - there is herd immunity - other people getting the vaccine also reduces our chances of getting the disease.\nVariable levels of treatment: If we are doing a drug trial, if some people got two doses, while others only got one dose. This is not a consistent treatment.\nTechnical errors: If someone who is supposed to be treated accidentally is not treated. This is not a consistent treatment.\n\nWhen SUTVA is violated, potential outcomes become very messy, and we no longer have the neat framework as before.\n\n\n\n\n\n\nCausal Estimands\n\nIndividual Treatment Effect\nRemember the potential outcomes from parallel worlds \\(Y_{1i}\\) and \\(Y_{0i}\\).\nSince these two parallel worlds are identical except for the fact one receives the treatment \\(D\\) and the other does not, the causal effect of \\(D\\) should be the difference between the potential outcomes of these two worlds. Thus, the individual treatment effect \\(\\tau\\) of a unit \\(i\\) is:\n\\[\n\\tau_i = Y_{1i} - Y_{0i}\n\\]\nThis is the specific treatment effect for a specific unit \\(i\\). This cannot be observed, because we do not see both potential outcomes for the same unit \\(i\\).\nThis is also very hard to estimate, as we cannot reliably fill in the missing potential outcome for any one unit \\(i\\). Thus, we almost never use individual treatment effects, and use group treatment effects.\n\n\n\nAverage Treatment Effect (ATE)\nWe have a hard time estimating individual treatment effects. So let us consider another form of causal estimand: group level estimands. Consider a population of units \\(i = 1, \\dots, N\\). The population has potential outcomes represented in two (only partially observed) vectors:\n\\[\n\\begin{split}\n& Y_1 = (Y_{11}, Y_{12}, \\dots, Y_{1N}) \\\\\n& Y_0 = (Y_{01}, Y_{02}, \\dots, Y_{0N})\n\\end{split}\n\\]\nWe compare these two vectors of potential outcomes. The most common way to do this is to use their expected values.\nThe Average Treatment Effect is defined as:\n\\[\n\\begin{split}\n\\tau_{ATE} & = \\E(Y_{1i} - Y_{0i}) \\\\\n& = \\underbrace{\\frac{1}{N} \\sum\\limits_{i=1}^N (Y_{1i} - Y_{0i})}_{\\text{a formula for average}}\n\\end{split}\n\\]\nWe cannot calculate this with observed data - since we need all potential outcomes to do this. However, we can creatively estimate this quantity through a number of research designs and assumptions, which we will explore throughout causal inference.\n\n\n\nAverage Treatment Effect on the Treated (ATT)\nAn alternative estimand to the ATE is the Average Treatment Effect on the Treated (ATT):\n\\[\n\\begin{split}\n\\tau_{ATT} & = \\E(Y_{1i} - Y_{0i} \\ | \\ D_i = 1) \\\\\n& = \\underbrace{\\frac{1}{N_1} \\sum\\limits_{i=1}^N D_i (Y_{1i} - Y_{0i}) \\quad  \\text{where } N_1 = \\sum\\limits_{i=1}^ND_i}_{\\text{a formula for the average only for treated units}}\n\\end{split}\n\\]\nThis is the causal effect of only units who have received the treatment. Note that frequently the ATT is not equal to the ATE, so be aware of which estimand you are trying to estimate/identify.\nSometimes, \\(\\tau_{ATT} = \\tau_{ATE}\\). This occurs when the expectation of the potential outcomes of both the treated and control are the same, then the two equal each other.\nThe opposite is also true: if the expectation of the potential outcomes of both the treated and control are different, then the two are not equal.\nThe opposite estimand is the Average Treatment effect on the Untreated (ATU), which only measures the causal effect of units who did not recieve the treatment.\nThis is not used very often, since it is kind of uninituive to think about treatment effects on individuals who did not recieve treatment. However, it can be useful in understanding identification assumptions.\n\n\n\nConditional Average Treatment Effect (CATE)\nThe conditional average treatment effect is any treatment effect where there is a condition on a characteristic/covariate:\n\\[\n\\tau_{CATE}(x) = \\E(Y_{1i} - Y_{0i} \\ | \\ \\underbrace{X_i = x)}_{\\text{condition}}\n\\]\nThis is the causal effect of only variables who meet the condition of the covariate specified. For example, you could find the conditional average treatment effect of only women (so the covariate which we are conditioning on is gender). You can also condition on multiple covariates.\nThis is often used for tailoring products/medicine/advertising to certain groups of people. It is also frequently used in identification strategies.\nThis estimand will go by other names, including the Local Average Treatment Effect (LATE).\n\n\n\n\n\n\nSelection Bias and Confounders\n\nNaive Estimator and Selection Bias\nA natural way to estimate the ATE is to use a naive estimator: find the average difference of observed outcomes. This is called the naive estimator:\n\\[\n\\hat\\tau_{naive} = \\underbrace{\\E(Y_i|D_i = 1)}_{\\text{for treated}} - \\underbrace{\\E(Y_i|D_i = 0)}_{\\text{for control}}\n\\]\nHowever, there is an issue - we can show this with algebra:\n\\[\n\\begin{align}\n\\hat\\tau_{naive} & = \\E(Y_i|D_i = 1) - \\E(Y_i|D_i = 0) \\\\\n& = \\underbrace{\\E(Y_{1i}|D_i = 1) - \\E(Y_{0i} | D_i = 0)}_{\\because \\text{ observed potential outcomes}} \\\\\n& = \\E(Y_{1i}|D_i = 1) - \\E(Y_{0i} | D_i = 0) + \\underbrace{\\E(Y_{0i}|D_i = 1) \\color{red}{- \\E(Y_{0i}|D_i = 1)}}_{\\because \\text{ this equals 0, so we can add it}} \\\\\n& = \\underbrace{\\E(Y_{1i}|D_i = 1) \\color{red}{- \\E(Y_{0i}|D_i = 1)}}_{\\tau_{ATT}} + \\underbrace{\\E(Y_{0i}|D_i = 1) - \\E(Y_{0i} | D_i = 0)}_{\\text{Selection Bias}} \\\\\n\\end{align}\n\\]\nWe can see that our naive estimator produces the \\(\\tau_{ATT}\\) plus an extra bit (called the selection bias). Thus, our naive estimator is biased, so we should be careful about using this naive estimator (correlation does not equal causation).\n\n\n\n\n\n\nNaive Estimator Biased for ATU\n\n\n\n\n\nThe proof above shows how the naive estimator is a biased estimator for the \\(\\tau_{ATT}\\). We can also prove it is a biased estimator of the ATU:\n\\[\n\\begin{split}\n\\hat\\tau_{naive} & = \\E(Y_i|D_i = 1) - \\E(Y_i|D_i = 0) \\\\\n& = \\underbrace{\\E(Y_{1i}|D_i = 1) - \\E(Y_{0i} | D_i = 0)}_{\\because \\text{ observed potential outcomes}} \\\\\n& = \\E(Y_{1i}|D_i = 1) - \\E(Y_{0i} | D_i = 0) + \\underbrace{\\E(Y_{1i}|D_i = 0) - \\E(Y_{1i}|D_i = 0)}_{\\because \\text{ this equals 0, so we can add it}} \\\\\n& = \\underbrace{\\E(Y_{1i}|D_i = 0)- \\E(Y_{0i}|D_i = 0)}_{\\tau_{ATU}} + \\underbrace{\\E(Y_{1i}|D_i = 1) - \\E(Y_{1i} | D_i = 0)}_{\\text{Selection Bias}} \\\\\n\\end{split}\n\\]\n\n\n\n\n\n\n\n\n\nNaive Estimator Biased for ATE\n\n\n\n\n\nThe proofs above shows how the naive estimator is a biased estimator for the \\(\\tau_{ATT}\\) and \\(\\tau_{ATU}\\). We can also prove it is a biased estimator of the ATE.\nLet us first start with the ATE. Let us call \\(Y_{1i} - Y_{0i} := \\tau_i\\) for notation simplicity:\n\\[\n\\begin{align}\n\\tau_{ATE} & = \\E(Y_{1i} - Y_{0i})  = \\E(\\tau_i)\\\\\n& = \\underbrace{\\E(\\tau_i|D_i = 1)\\P(D_i = 1) + \\E(\\tau_i|D_i = 0)\\P(D_i = 0)}_{\\because \\text{ weighted average of ATE and ATU by proportion}} \\\\\n& = \\E(\\tau_i|D_i = 1) \\underbrace{(1 -\\P(D_i = 0))}_{\\because \\text{ complement prob.}} + \\E(\\tau_i|D_i = 0)\\P(D_i = 0) \\\\\n\\end{align}\n\\]\nLet us call \\(\\P(D_i = 0) := \\pi\\) for notation simplicity. Now, continue:\n\\[\n\\begin{split}\n& = \\underbrace{\\E(\\Delta|D_i = 1) - \\pi \\E(\\Delta|D_i = 1)}_{\\because \\text{ distribute out}} + \\E(\\tau_i|D_i = 0)\\pi \\\\\n& = \\E(\\tau_i|D_i = 1) + \\underbrace{\\pi[\\E(\\tau_i|D_i = 0) - \\E(\\tau_i|D_i = 1)]}_{\\because \\ \\pi \\text{ factored out }} \\\\\n& = \\E(Y_{1i} |D_i = 1) - \\E(Y_{0i}|D_i = 1) + \\pi[\\E(\\tau_i|D_i = 0) - \\E(\\tau_i|D_i = 1)] \\\\\n\\end{split}\n\\]\nLet us call the part \\(\\pi[\\E(\\tau_i|D_i = 0) - \\E(\\tau_i|D_i = 1)] := \\Pi(\\tau_i)\\). Now, continue to simplify:\n\\[\n\\begin{split}\n& = \\E(Y_{1i} |D_i = 1) - \\E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) + \\underbrace{\\E(Y_{1i} |D_i = 0) - \\E(Y_{0i}|D_i = 0)}_{\\because \\text{ these two cancel out so we add 0}}  \\\\\n& = \\E(Y_{1i} |D_i = 1) - \\E(Y_{0i}|D_i = 0) + \\E(Y_{1i} |D_i = 0)- \\E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& = \\underbrace{\\E(Y_i|D_i = 1)}_{\\because \\text{ observed outcome}} - \\underbrace{\\E(Y_i|D_i = 0)}_{\\because \\text{ observed outcome}} + \\E(Y_{1i} |D_i = 0)- \\E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& = \\underbrace{\\E(Y_{i} |D_i = 1) - \\E(Y_{i}|D_i = 0)}_{\\hat\\tau_{naive}} + \\E(Y_{1i} |D_i = 0)- \\E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& = \\hat\\tau_{naive}+ \\E(Y_{1i} |D_i = 0)- \\E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i)\n\\end{split}\n\\]\nThus, we can see that \\(\\tau_{ATE}\\) is not equivalent to \\(\\hat\\tau_{naive}\\). Let us isolate \\(\\hat\\tau_{naive}\\) to identify the selection bias.\n\\[\n\\begin{split}\n& \\tau_{ATE} = \\hat\\tau_{naive}+ \\E(Y_{1i} |D_i = 0)- \\E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& -\\hat\\tau_{naive} = -\\tau{ATE} + \\E(Y_{1i} |D_i = 0)- \\E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& \\hat\\tau_{naive} = \\tau_{ATE} - \\E(Y_{1i} |D_i = 0) + \\E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& \\hat\\tau_{naive} = \\tau_{ATE} + \\underbrace{\\E(Y_{0i}|D_i = 1)- \\E(Y_{1i} |D_i = 0) + \\Pi(\\tau_i)}_{\\text{selection bias}}\n\\end{split}\n\\]\n\n\n\n\n\n\nConfounders\nTake the selection bias formula from above:\n\\[\n\\underbrace{\\E(Y_{0i}|D_i = 1)}_{Y_{0i}\\text{ (treated)}} - \\underbrace{\\E(Y_{0i} | D_i = 0)}_{Y_{0i}\\text{ (control)}}\n\\]\nIf selection bias is non-zero, this essentially states that the expected potential outcome before treatment \\(Y_{0i}\\) between the treatment and control groups is not equal.\nOr in other words, the treatment and control groups have some other variable causing differences even before treatment has begun. This implies that the differences between the treatment and control group may not be due to treatment \\(D\\), but due to the underlying differences before treatment even occurred.\nConfounders are variables that cause the differences between treatment and control groups before the treatment has started. Confounders correlate with both the treatment variable and the outcome. If a variable only correlates with \\(D\\) or \\(Y\\), then it is not a confounder. If must correlate with both \\(D\\) and \\(Y\\).\nThis is why correlation does not equal causation - if the treatment and control group are different before we start the experiment, we cannot say the difference between the two is purely a result of treatment \\(D\\).\nWe can also think about confounders in terms of omitted variable bias. The formula for omitted variable bias is:\n\\[\n\\E\\hat{\\beta} = \\beta + \\eta \\delta\n\\]\nWhere \\(\\beta\\) are the true population parameters, \\(\\eta\\) is the relationship between the confounder and treatment, and \\(\\delta\\) is the relationship between confounder and outcome.\nWe can see that if either \\(\\eta = 0\\) or \\(\\delta = 0\\), then there is 0 omitted variable bias - which implies that a confounder that causes bias must be correlated with both the treatment and outcome.\n\n\n\nAssignment Mechanism\nThe Assignment Mechanism is the procedure that determines the treatment status of each unit. In causal inference, we want to restrict the assignment mechanism, in order to remove the effect of selection bias.\nThere are two types of studies that use different assignment mechanisms:\n\nRandomised Experiments: The assignment mechanism is both known, and controlled by the researcher. Generally, the researcher chooses some type of randomisation.\nObservational Studies: The assignment mechanism is not known to, or not under the control of the researcher. This means that confounders may be driving selection into treatment and control, inducing bias.\n\nGenerally, the most credible studies are randomisation studies, since we can control interventions to parse out the effect of confounders. Observational studies generally rely on more assumptions that need to be met, and need to be defended for the study to be credible.\nIn the next chapter, we will start off with Randomised Controlled Trials, which are a form of randomisation experiment. After that, we will start discusses observational studies, and what assumptions we need in these studies to identify the causal effect without selection bias.\n\n\n\n\n\n\nDirected Acyclic Graphs\n\nCausal Diagrams\nCausal Diagrams are a visual way to represent causal theories and frameworks, which allows us to visualise how variables interact with each other. Each Directed Acyclic Graph (DAG) has the following components:\n\nNodes: representing variables (which are also called vertices).\nDirected Edges: Arrows that encode one-way causal theories between variables. For example, we might believe \\(Z\\) causes \\(X\\), so we will draw an arrow from \\(Z\\) to \\(X\\). These connections are observable (solid) or unobservable (dashed).\n\n\n\n\n\n\nIn the figure above, we have two unobserved variables: \\(Q\\) and \\(Y\\) We have three observed variables: \\(Z\\), \\(D\\), and \\(Y\\). We can see the causal theories represented by arrows.\nWe can see the causal effec \\(Z \\rightarrow Y\\) is confounded by \\(W\\): \\(W\\) is affecting who gets treatment \\(Z\\), and causing \\(Y\\). Thus, \\(W\\) is affecting who gets selected into treatment \\(Z\\), and selecting your potential outcome \\(Y\\). Thus, this is an example of selection bias. \\(D \\rightarrow Y\\) is confounded by \\(Q\\). \\(Z \\rightarrow D\\) is not confounded, so we can estimate this causal effect.\n\n\n\nRepresenting Interventions\nTreatments (interventions by the researcher, for example) are when we determine one variable exogenously (such as by randomisation). Or in other words, one variable is determined randomly externally, not caused by any variables within the directed acyclic graphs.\nTreatments are represented by the do() operator. When the treatment is exogenous, we can break all the connections into that variable’s node. This is because we are determining the value of the variable, not any other variables.\n\n\n\n\n\nAn intervention here is on variable \\(D\\). That means the value of \\(D\\) is being chosen outside of this graph (by randomisation, or the researcher). This allows us to delete the arrow between \\(Q \\rightarrow D\\) and \\(Z \\rightarrow D\\). This is because we are exogenously determining \\(D\\), so \\(Q\\) and \\(Z\\) are not determining the value of \\(D\\).\nWith exogenously determined variables, we can find the causal effect that variable is causing on another.\n\n\n\nBlocked Paths\nA set of nodes \\(\\{ \\mathbf S \\}\\) blocks a path \\(p\\) if either:\n\nIf the path \\(p\\) contains at least one arrow-emitting node included in the set of nodes \\(S\\), or\nThe path \\(p\\) contains at least one collision node (multiple arrows point into it) that is outside the set of nodes \\(S\\), and the collision node has no descendant within the set of nodes \\(S\\) (no arrows go out of it to another node).\n\n\n\n\n\n\nTake this above causal diagram. We can see the following:\n\nThe path \\(D \\rightarrow P \\rightarrow Y\\) is blocked by set \\(\\{P\\}\\), because node \\(P\\) is one arrow-emitting node in the path \\(D \\rightarrow P \\rightarrow Y\\).\nThe path \\(D \\leftarrow M \\rightarrow Y\\) is blocked by set \\(\\{M\\}\\), because node \\(M\\) is one arrow-emitting node in the path \\(D \\leftarrow M \\rightarrow Y\\).\n\\(D \\leftarrow Z \\rightarrow M \\rightarrow Y\\) is blocked by \\(\\{M\\}\\), \\(\\{Z\\}\\), or \\(\\{M, Z\\}\\).\n\nBlocking paths is important, since in order to estimate \\(D \\rightarrow Y\\), we need to block any other path between \\(D\\) and \\(Y\\) that is not directly \\(D \\rightarrow Y\\).\n\n\n\n\n\n\nImplementation in R\nThis section will show how you can create DAGs in R. We will need the ggdag and dagitty packages.\n\nlibrary(ggdag)\nlibrary(dagitty)\n\n\n\n\n\n\n\nSimple DAGs with Dagify\n\n\n\n\n\nYou can create a very simple DAG with dagify as follows:\n\ndag_object &lt;- dagify(\n  Y ~ X + D, #Y is caused by X and D\n  D ~ X #D is caused by X\n)\n\nggdag(dag_object) + theme_dag()\n\nThis dag is not very customisable. This can be an issue if you want nodes to be in a specific location. See below for a more customisable DAG.\n\n\n\n\n\n\n\n\n\nCustom DAGs with Dagitty\n\n\n\n\n\nYou can create more complex DAGs with Dagitty. Dagitty allows us to position nodes in a coordinate system, which is useful in some purposes.\n\ndag_object &lt;- dagitty('dag {\n      D [pos = \"0, 1\"]\n      Y [pos = \"2, 1\"]\n      X [pos = \"1, 2\"]\n      \n      D -&gt; Y\n      D &lt;- X -&gt; Y\n  }')\n\nggdag(dag_object) + theme_dag()\n\nThe pos arguments have the coordinates of where you want to put each node.\nBelow are the path connections, where you can use -&gt; and &lt;- to indicate relationships.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "3 Causal Frameworks"
    ]
  },
  {
    "objectID": "glm.html",
    "href": "glm.html",
    "title": "The Generalised Linear Model",
    "section": "",
    "text": "In the last chapter, we discussed the classical linear model. However, the classical linear model has a few limitations, which are addressed by the generalised linear model (GLM). In this chapter, we explore the generalised linear model, including logistic and negative binomial regression, and the maximum likelihood estimator used to estimate GLMs.\nUse the right sidebar for quick navigation. R-code is provided at the bottom.\n\n\nGLMs and Maximum Likelihood\n\nThe Generalised Linear Model\nThere are some limitations to the classical linear model. Consider the linear probability model:\n\\[\n\\P(Y_i = 1|X_i)= \\pi_i = \\beta_0 + \\beta_1X_{i1} + \\dots + \\beta_pX_{ip} + \\eps_i\n\\]\nThe classical assumptions assume homoscedasticity. However, probability of a binary event is given by the bernoulli distribution, for which \\(\\V \\pi_i = \\pi_i(1-\\pi_i)\\), which is clearly a function of outcome \\(\\pi_i\\), meaning it is homoscedastic. And more importantly, the linear model will predict probabilities \\(\\P(Y_i = 1 | X_i)\\) that are higher than 1 and less than 0, which, if we know the rules of probability, is nonsensical. This nonsensical outcome issue is also present in count and rate outcomes.\nThe generalised linear model allows us to “transform” the outcome variable (either \\(Y_i\\) or \\(\\pi_i\\) when dealing with probabilities) through a link function \\(g(\\cdot)\\), while maintaining the linear structure of the rest of the linear model. This allows us to deal with other distributions and avoid nonsensical predictions.\nOf course, we have the Classical Linear Model, which is considered a GLM that has no link function:\n\\[\nY_i = \\beta_0 + \\beta_1X_{i1} + \\beta_p X_{ip} + \\eps_i \\ = \\ x_i^\\top\\beta + \\eps_i\n\\]\nThe Logistic Regression Model is a model that deals with probabilities \\(\\pi_i\\), just like the linear probability model. It solves the limitations of the linear probability model through a link function:\n\\[\n\\log \\left( \\frac{\\pi_i}{1 - \\pi_i} \\right) = \\beta_0 + \\beta_1X_{i1} + \\dots + \\beta_p X_{ip} \\ = \\ x_i^\\top\\beta\n\\]\nThe Negative Binomial Model (and a special case called the Poisson model) is a model that deals with count and rate data (only positive values), where \\(\\lambda_i\\) is the \\(\\E Y_i\\) for a negative binomial distribution.\n\\[\n\\log \\lambda_i = \\beta_0 + \\beta_1X_{i1} + \\dots + \\beta_p X_{ip} \\ = \\ x_i^\\top \\beta\n\\]\nWe will explore each of these models in more detail later.\n\n\n\nMaximum Likelihood Estimation\nAll GLMs are estimated with an estimator called the maximum likelihood estimator (MLE). This is also one of the most used estimators in statistics for a bunch of other methods, so it is useful to understand.\nWe have a set of population parameters in the vector \\(\\theta\\) we want to estimate. This set of parameters determines our population distribution of \\(Y_i\\), which we can describe with some probability density function \\(\\varphi(y, \\theta)\\). For example, in linear regression, our population \\(y\\) is determined by the population parameters \\(\\beta_0, \\dots, \\beta_k\\).\nWhen we are estimating parameters, we will have sample data with \\(n\\) number of observations, with each observation \\(i\\) having its own \\(Y_i\\) value. Thus, our sample looks something like \\((y_1, \\dots, y_n)\\). Based on the probability density function \\(\\varphi(\\cdot)\\) of the population \\(Y_i\\), the probability of getting our observed \\(y_1\\) in our sample from the population data is \\(\\varphi(y_1, \\theta)\\), and the probability of getting observation \\(y_i\\) is \\(\\varphi(y_i, \\theta)\\).\nWe know by the rules of probability, that the probability of multiple independent events is the product of their probabilities. Thus, the probability that we get a specific sample with \\(y\\) values \\((y_1, \\dots ,y_n)\\), based on the value of our population parameters \\(\\boldsymbol\\theta\\) is given by the likelihood function \\(L\\):\n\\[\nL(\\theta, y)  = \\varphi(y_1;\\theta) \\times \\varphi(y_2;\\theta) \\times \\dots \\times \\varphi(y_n;\\theta) = \\prod\\limits_{i=1}^n \\varphi(y_i, \\theta)\n\\]\nWe want to find values of \\(\\theta\\) that make it the highest probability we observe our sample \\(y_1, \\dots ,y_n\\). This is done by maximising the likelihood function \\(L(\\cdot)\\). However, maximising the likelihood function \\(L(\\cdot)\\) is very difficult. Luckily, we can use the log of the likelihood function \\(\\ell(\\cdot)\\), which retains the same maximum/minimum points as \\(L(\\cdot)\\) why being easier to work with.\n\\[\n\\begin{align}\n\\ell (\\theta; y)& = \\log L(\\theta, y)  \\\\\n& = \\log \\left(\\prod_{i=1}^n \\varphi(y_i; \\theta) \\right) \\\\\n& = \\log[\\varphi(y_1, \\boldsymbol\\theta) \\times \\varphi(y_2, \\boldsymbol\\theta) \\times \\dots \\times \\varphi(y_n, \\boldsymbol\\theta)] && \\text{(expand product notation)} \\\\\n& = \\log[\\varphi(y_1, \\boldsymbol\\theta)] + \\log [\\varphi(y_2, \\boldsymbol\\theta)] + \\dots + \\log[\\varphi(y_n, \\boldsymbol\\theta)] && \\text{(property of logs)} \\\\\n& = \\sum\\limits_{i=1}^n \\log[\\varphi(y_i, \\boldsymbol\\theta)] && \\text{(condense into sum)}\n\\end{align}\n\\]\n\n\n\nScore Function and Expectation\nThe gradient of \\(\\ell(\\theta; y)\\) with respect to vector \\(\\theta\\) is known as the score function \\(s(\\theta, y)\\):\n\\[\ns(\\theta;y) = \\frac{\\partial}{\\partial \\theta} \\ell(\\theta; y) = \\frac{\\partial}{\\partial \\theta}\n\\sum\\limits_{i=1}^n \\log[\\varphi(y_i; \\theta)]\n\\]\nThe \\(\\hat\\theta\\) values of MLE are the \\(\\theta\\) that solve \\(s(\\theta; y) = 0\\). If we have \\(p\\) parameters, there will be \\(p\\) partial derivatives within the gradient.\nLet us define the true parameter value in the population as \\(\\theta_0\\), which implies the score function of \\(\\theta_0\\) is \\(s(\\theta_0; Y_i)\\), where \\(Y_i\\) is the random variable. Vector \\(y\\), our sample, is a realisation of this random variable - since we are sampling for our observed \\(y\\), our sampled \\(y\\) will change with a different sample. Thus, the score function is also a random variable with expectation and variance.\nThe expectation of the score function in respect to random \\(Y_i\\), evaluated at true population parameter value \\(\\theta_0\\) is:\n\\[\n\\begin{align}\n\\E[s(\\theta_0; Y_i)] & = \\int s(\\theta_0;y) \\overbrace{\\varphi(y; \\theta_0)}^{\\P \\ \\mathrm{ of\\ sample \\ y}}dy && (\\text{definition of continuous } \\E) \\\\\n& = \\int \\left[ \\frac{\\partial}{\\partial\\theta} \\ell(\\theta; y)\\right]\\varphi(y \\theta_0)dy && \\text{(plug in score function)} \\\\\n& = \\int\\left[\\frac{\\partial}{\\partial\\theta} \\varphi(y; \\theta) \\right] \\varphi (y; \\theta_0)dy && \\text{(plug in } \\ell \\text{, y is vector so no summation needed)} \\\\\n& = \\int \\frac{\\frac{\\partial}{\\partial\\theta} \\varphi(y; \\theta_0)}{\\varphi(y; \\theta_0)}\\varphi(y; \\theta_0) && (\\because [\\log(u(x))]' = u'(x)/u(x) \\ ) \\\\\n& = \\int \\frac{\\partial}{\\partial\\theta} \\varphi(y; \\theta_0) && \\text{(cancel out)} \\\\\n& = \\frac{\\partial}{\\partial\\theta} \\int \\varphi(y; \\theta_0) && \\text{(can flip deriv. and anti-deriv.)} \\\\\n& = \\frac{\\partial}{\\partial\\theta} 1 = 0 && \\text{(indef int. of PDF = 1)}\n\\end{align}\n\\]\nThus, the expectation of the score at true population parameter \\(\\theta_0\\) in respect to the random variable vector \\(y\\) is 0.\n\n\n\nInformation Matrix and Variance\nWith the expectation from above, we can also find the variance-covariance matrix of the score function at \\(\\theta_0\\):\n\\[\n\\begin{align}\n\\V s(\\theta_0; y) & = \\E [(s(\\theta_0; y) - \\E(s(\\theta_0; y))^2 ] && (\\because \\V Z = \\E[Z - \\E Z]) \\\\\n& = \\E[(s(\\theta_0; y) - 0)^2] && (\\because \\E [s(\\theta_0 ; y)] = 0) \\\\\n& = \\E\\left [\\frac{\\partial \\ell (\\theta_0; y)}{\\partial \\theta} \\frac{\\partial \\ell (\\theta_0; y)}{\\partial \\theta^\\top} \\right] && \\text{(plug in and square } s(\\theta_0, y)) \\\\\n& = \\E\\left[ -\\frac{\\partial^2 \\ell (\\theta_0; y)}{\\partial\\theta\\partial \\theta^\\top}\\right] \\equiv \\mathcal I(\\theta_0)\n\\end{align}\n\\]\nWhere \\(\\mathcal I (\\theta_0)\\) is also known as the expected fisher information matrix. This is also the matrix of second derivatives of the log-likelihood, meaning this is the negative of the Hessian matrix of the log-likelihood.\nWe can also get the observed equivalent of our parameter estimate \\(\\theta\\), called the observed information matrix, which does not involve expectation (which can be difficult):\n\\[\nI(\\theta;y) = -\\frac{\\partial^2 \\ell(\\theta; y)}{\\partial \\theta \\partial \\theta^\\top}\n\\]\nThrough complex math beyond this course, we can show that the asymptotic variance of any MLE estimate \\(\\hat\\theta\\), \\(\\V \\hat\\theta\\), is the inverse of the information matrix:\n\\[\n\\V (\\hat\\theta) = \\mathcal I(\\hat\\theta)^{-1}\n\\]\nWhen we have finite (but sufficiently large samples), this is our variance estimate. We can also use the observed information matrix \\(I(\\hat\\theta)^{-1}\\).\n\n\n\nAsymptotic Properties of MLE\nThrough complex proofs beyond the scope of this chapter, we can determine that asymptotically as \\(n \\rightarrow ∞\\), the distribution of the MLE estimates \\(\\hat\\theta\\) becomes:\n\\[\n\\hat\\theta \\sim \\mathcal N(\\theta_0, \\mathcal I(\\theta_0)^{-1})\n\\]\nThis tells us two things. First, maximum likelihood estimates are asymptotically consistent, since the asymptotic distribution has an expectation \\(\\E \\hat\\theta = \\theta_0\\).\nSecond, we know that the asymptotic variance of MLE estimates is \\(\\mathcal I(\\theta_0)^{-1}\\). This is notable because of the Carmer-Rao bound. The cramer rao bound states that the variance of any unbiased estimator \\(\\hat\\theta\\) is bounded by the reciprocal of the Fisher Information matrix:\n\\[\n\\V \\hat\\theta_n ≥ \\frac{1}{\\mathcal I(\\theta)}\n\\]\nWe will not prove this here since it is technical. However, this is the lowest possible variance of any unbiased estimator. As we see above, the asymptotic variance of the MLE \\(\\hat\\theta\\) is exactly \\(1/\\mathcal I(\\theta)\\). Thus, this tells us there is no other asymptotically consistent (unbiased) estimator that has a lower variance than the MLE.\nHowever, do note that the MLE is biased in finite-samples (but the bias becomes small in large-samples). This becomes an issue when we are dealing with some applications of causal inference, such as fixed effects.\n\n\n\nOLS as a Maximum Likelihood Estimator\nEarlier, we noted that all linear models are estimated with a Maximum Likelihood Estimator. This includes the classical linear model.\nOLS is a MLE under the classical assumptions, and assuming \\(Y_i\\) is normally distributed at \\(Y_i \\sim \\mathcal N(X\\beta, \\sigma^2)\\) (where \\(\\mu = \\E(Y_i|X_i) = X\\beta\\), and \\(\\sigma^2\\) as the homoscedastic variance of \\(\\eps_i\\)). By the PDF of a normal distribution and ?@eq-loglike, the log-likelihood function \\(\\ell\\) of our sample for linear regression is:\n\\[\n\\begin{align}\n\\ell(\\beta, \\sigma^2; y_i)\n& = \\sum\\limits_{i=1}^n \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{\\left( -\\frac{1}{2 \\sigma^2}(y_i - x_i^\\top \\beta)^2\\right)} \\right) \\\\\n& = \\sum\\limits_{i=1}^n \\log (1) - \\log (\\sqrt{2\\pi\\sigma^2})   + \\log\\left( e^{\\left( -\\frac{1}{2 \\sigma^2}(y_i - x_i^\\top\\beta )^2\\right)}\\right) && \\text{(prop. of logs)} \\\\\n& = \\sum\\limits_{i=1}^n 0 - \\frac{1}{2}\\log ({2\\pi\\sigma^2})  + \\left( -\\frac{1}{2 \\sigma^2}(y_i - x_i^\\top\\beta)^2\\right) && \\text{(prop. of logs)} \\\\\n& = -\\frac{n}{2} \\log (2\\pi\\sigma^2)  -\\frac{1}{2 \\sigma^2}\\sum\\limits_{i=1}^n(y_i - x_i^\\top\\beta)^2 && \\text{(prop. of sums)}\n\\end{align}\n\\]\nLook at the right part of the above equation. We can see that is the SSR. Thus, we can rewrite:\n\\[\n\\ell(\\beta, \\sigma^2; y) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(y-X\\beta)^\\top (y-X\\beta)\n\\]\nAnd if we take the gradient in respect to \\(\\beta\\), we can see we get the same first order condition as OLS:\n\\[\n\\frac{\\partial \\ell}{\\partial \\beta} = -2X^\\top y + 2 X^\\top X\\beta = 0\n\\]\nWith the same first order condition, we will get the same \\(\\beta\\) estimate as in OLS. Thus, we can see under the condition of normality of \\(Y_i\\), MLE is equivalent to OLS.\n\n\n\nNewton-Raphson Algorithm\nWe need to solve \\(s(\\theta; y) = 0\\) to find our MLE estimates \\(\\hat\\theta\\). However, there is often no closed form solution due to differentiation issues. Thus, we must use iterated algorithms to solve for \\(\\hat\\theta\\). Suppose \\(\\theta\\) is a scalar (only one parameter). For values of \\(\\theta\\) that are close to the true population \\(\\theta_0\\), the first-order taylor series expansion of \\(s(\\theta)\\) about \\(\\theta_0\\) states:\n\\[\ns(\\theta; y) \\approx s(\\theta_0; y) + s'(\\theta_0;y)(\\theta - \\theta_0)\n\\]\nWhere \\(s'(\\theta_0; y)\\) is the first derivative of the score function \\(s(\\theta; y)\\) evaluated at \\(\\theta = \\theta_0\\). When dealing with a vector \\(\\theta\\), the equivalent first derivative is the hessian matrix of \\(\\ell(\\cdot)\\), or the negative of the observed information matrix:\n\\[\n\\frac{\\partial s(\\theta; y)}{\\partial \\theta} = \\frac{\\partial^2 \\ell(\\theta; y)}{\\partial \\theta \\partial \\theta^\\top} = - I(\\theta)\n\\]\nThis gives us the following first-order taylor series expansion of \\(s(\\theta)\\):\n\\[\ns(\\theta; y) \\approx s(\\theta_0; y) - I(\\theta_0)(\\theta - \\theta_0)\n\\]\nAnd when we solve \\(s(\\theta; y) = 0\\) to maximimse, we can solve for \\(\\theta\\):\n\\[\n\\begin{align}\n0 & \\approx s(\\theta_0; y) - I(\\theta_0)(\\theta - \\theta_0) && (\\because s(\\theta;y) = 0) \\\\\nI(\\theta_0)(\\theta - \\theta_0) & \\approx s(\\theta_0; y)  && (+I(\\theta_0)(\\theta - \\theta_0) \\text{ to both sides})\\\\\n\\theta - \\theta_0 & \\approx I(\\theta_0)^{-1}s(\\theta_0; y) && (\\times I(\\theta_0)^{-1} \\text{ to both sides}) \\\\\n\\theta & \\approx \\theta_0 + \\approx I(\\theta_0)^{-1}s(\\theta_0; y) && (+\\theta_0 \\text{ to both sides})\n\\end{align}\n\\]\nSince \\(\\theta_0\\) is unknown, we cannot use this formula directly. Thus, we use an interative procedure. We start with some initial value \\(\\theta^{(0)}\\) that is randomly chosen. Then, we “update” for a new value \\(\\theta^{(1)}\\):\n\\[\n\\theta^{(1)} = \\theta^{(0)} + I (\\theta^{(0)})^{-1} s(\\theta^{(0)}; y)\n\\]\nAnd we do the same updating for \\(\\theta^{(2)}\\) with \\(\\theta^{(1)}\\), and \\(\\theta^{(m+1)}\\) using \\(\\theta^{(m)}\\), until the algorithm converges and the differences between \\(\\theta^{(m+1)}\\) using \\(\\theta^{(m)}\\) becomes minimal (under some pre-specified threshold of “tolerance”). An alternative is fisher-scoring, which does the same thing but with the expected fisher information matrix \\(\\mathcal I(\\theta)\\) instead of the observed \\(I(\\theta)\\) when \\(\\mathcal I(\\theta)\\) is not too difficult to compute.\n\n\n\nInformation Criterion Statistics\nRecall that the likelihood function \\(L(\\cdot)\\) is the probability of observing a particular sample given the parameters \\(\\theta\\), or in other words, \\(\\P (y | \\theta)\\).\nThis property of \\(L(\\cdot)\\) also allows us to compare models between each other. For example, let us say we have two models of the same outcome variable \\(Y_i\\), one model with parameters \\(\\theta_1\\), and another with parameters \\(\\theta_2\\) (perhaps one has more parameters/explanatory variables, etc). The model with a higher likelihood \\(L(\\theta; y)\\) is the model that is considered the better fit.\nThus, the likelihood value \\(L(\\theta; y)\\) allows us to compare the explanatory power of models, like \\(R^2\\) does for the classical linear model. This will become useful when we discuss likelihood ratio tests.\nA group of statistics, called information criterion (IC) statistics, use the likelihood \\(L(\\cdot)\\) to compare different models. The idea behind these statistics is to not only reward higher likelihoods \\(L\\), but also reward simplicty of models with less parameters.\nThe most commonly used is Akaike’s Information Criterion (AIC). The formula for AIC is as follows:\n\\[\nAIC = -2 \\log L + 2p\n\\]\n\nWhere \\(L\\) is the likelihood of the model in question evaluated as \\(L(\\theta ; y)\\), and \\(p\\) is the number of parameters in the model.\n\nThe lower the AIC is, the better the model is considered. There are also alternative IC statistics, such as the Bayesian Information Criterion (BIC).\n\n\n\n\n\n\nLogistic Regression Model\n\nModel Specification\nInclude definition of \\(\\pi_i\\)., and odds \\(pi_i / (1- \\pi_i)\\).\nInclude regression writeen in terms of \\(\\pi_i\\). (in in for box, show derivation).\nShow fitted values. plot.\n\n\n\nInterpretation and Odds Ratios\n\n\n\nOrdinal Logistic Regression\n\n\n\nMultinomial Logistic Regression\n\n\n\n\n\n\nNegative Binomial Regression\n\nModel Specification\nTalk about negative binomial distribution.\nInclude model for rates, and fitted probabilities.\n\n\n\nInterpreting Coefficients\n\n\n\nPoisson Regression\n\n\n\n\n\n\nStatistical Inference\n\nHypothesis Testing\n\n\n\nConfidence Intervals\nadd odds ratios\n\n\n\nLikelihood Ratio Test\n\n\n\n\n\n Back to top",
    "crumbs": [
      "2 The Generalised Linear Model"
    ]
  },
  {
    "objectID": "glm.html#gradient-descent-algorithms",
    "href": "glm.html#gradient-descent-algorithms",
    "title": "The Generalised Linear Model",
    "section": "Gradient Descent Algorithms",
    "text": "Gradient Descent Algorithms\nIn some more complex models, we cannot mathematically find the minimum of the log-likelihood function. So instead, we resort to a series of computer algorithms called gradient descent. The algorithm takes the following form:\n\nWe randomly choose values of \\(\\boldsymbol\\theta\\) to start (let us notate the chosen as \\(\\boldsymbol\\theta^*\\)), and calculate the likelihood \\(L\\) with those chosen at \\(\\boldsymbol\\theta^*\\).\nWe then slightly shift the values of \\(\\boldsymbol\\theta^*\\) upwards and downwards, calculating all the likelihoods. We see in which shift-direction does the likelihood \\(L\\) increase the most.\nOnce we determine the direction that \\(L\\) increases the most, we shift int hat direction to a new \\(\\boldsymbol\\theta'\\) value. Once again, we slightly shift values of \\(\\boldsymbol\\theta'\\) upwards and downwards, and see which shift-direction does the likelihood \\(L\\) increase the most.\nWe keep repeating this process of moving in the direction that increases \\(L\\) the most, shifting around in all directions at that point, and once again moving in the direction that increases \\(L\\) the most.\nWe stop when we are at some point \\(\\boldsymbol\\theta^!\\) where all directions of shits decrease \\(L\\). We are “at the top of the mountain”, and that becomes our estimate.\n\nThis is a very simple gradient descent algorithm. You might point out that this algorithm only works if there is one global maximum, and no local maxima (since we would stop the algorithm at a local extrema if this were the case). This usually is not an issue since in common regression models (linear, logistic, poisson), their is only one global maximum. The figure below shows this:\n\n\n\n\n\nA solution for this problem of local maxima (which becomes more an issue with machine learning models) is to basically do the estimation algorithm multiple times, each time starting at some random \\(\\boldsymbol\\theta^*\\). Then, we find the time with the largest \\(L\\). This will in theory help us determine which are local and global maxima.\n\n\nProperties of the MLE\nLet us find the variance of the score function \\(s(\\theta_0, y)\\), where \\(\\theta_0\\) is the value of the parameter \\(\\theta\\) when \\(s(\\theta ; y_i) = 0\\) is solved, and \\(y\\) is the vector of our observed values of \\(Y_i\\):\n\\[\n\\begin{align}\n\\V s(\\theta_0; y_i) & = \\E [(s(\\theta_0; y) - \\E(s(\\theta_0; y))^2 ] && (\\because \\V Z = \\E[Z - \\E Z]) \\\\\n& = \\E[(s(\\theta_0; y) - 0)^2] && (\\because \\E [s(\\theta_0 ; y)] = 0) \\\\\n& = \\E\\left [\\frac{\\partial \\ell (\\theta_0; y)}{\\partial \\theta} \\frac{\\partial \\ell (\\theta_0; y)}{\\partial \\theta^\\top} \\right] && \\text{(plug in and square } s(\\theta_0, y)) \\\\\n& = \\E\\left[ -\\frac{\\partial^2 \\ell (\\theta_0; y)}{\\partial\\theta\\partial \\theta^\\top}\\right] \\equiv \\mathcal I(\\theta_0)\n\\end{align}\n\\]\nWhere \\(\\mathcal I (\\theta_0)\\) is also known as the expected fisher information matrix. Through complex math that I will not do here, we can show that asymptotically as \\(n \\rightarrow ∞\\), the distribution of the MLE estimates \\(\\hat\\theta\\) becomes:\n\\[\n\\hat\\theta \\sim \\mathcal N(\\theta_0, \\mathcal I(\\theta_0)^{-1})\n\\]\nThis tells us two things. First, maximum likelihood estimates are asymptotically consistent, since the asymptotic distribution has an expectation \\(\\E \\hat\\theta = \\theta_0\\).\nSecond, this tells us the asumptotic variance of the MLE. To calculate the variance, we can estimate it with the expected fisher information matrix of our estimate \\(\\hat\\theta\\):\n\\[\n\\V \\hat\\theta = \\mathcal I(\\hat\\theta)^{-1}\n\\]\nIt can also be proven with the Cramer-Rao lower bound that the MLE is the asymptotic consistent estimator with the lowest asymptotic variance. However, do note that the MLE is biased in finite-samples (but the bias becomes small in large-samples). This becomes an issue when we are dealing with some applications of causal inference, such as fixed effects.\n\n\n\nOLS as a Maximum Likelihood Estimator\nEarlier, we noted that all linear models are estimated with a Maximum Likelihood Estimator. This includes the classical linear model, as OLS is a MLE under classical conditions.\nWe know in the classical model, \\(\\mu = E(y|x) = x_i^\\top \\beta\\). Let us plug that into the probability density function of a normal distribution to get the PDF of \\(Y_i\\) in a simple linear regression. By Equation 1, the log-likelihood function \\(\\ell\\) of our sample for linear regression is:\n\\[\n\\begin{align}\n\\ell(\\beta, \\sigma^2; y_i)\n& = \\sum\\limits_{i=1}^n \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{\\left( -\\frac{1}{2 \\sigma^2}(y_i - x_i^\\top \\beta)^2\\right)} \\right) \\\\\n& = \\sum\\limits_{i=1}^n \\log (1) - \\log (\\sqrt{2\\pi\\sigma^2})   + \\log\\left( e^{\\left( -\\frac{1}{2 \\sigma^2}(y_i - x_i^\\top\\beta )^2\\right)}\\right) && \\text{(prop. of logs)} \\\\\n& = \\sum\\limits_{i=1}^n 0 - \\frac{1}{2}\\log ({2\\pi\\sigma^2})  + \\left( -\\frac{1}{2 \\sigma^2}(y_i - x_i^\\top\\beta)^2\\right) && \\text{(prop. of logs)} \\\\\n& = -\\frac{n}{2} \\log (2\\pi\\sigma^2)  -\\frac{1}{2 \\sigma^2}\\sum\\limits_{i=1}^n(y_i - x_i^\\top\\beta)^2 && \\text{(prop. of sums)}\n\\end{align}\n\\]\nNow, let us take the derivative in respect to vector \\(\\beta\\) to solve for our score function:\n\\[\n\\begin{align}\n\\frac{\\partial \\ell}{\\partial \\beta} & = 0 + \\frac{1}{\\sigma^2}\\sum\\limits_{i=1}^n(y_i - x_i^\\top\\beta)x_i && \\text{(by chain and power rule)} \\\\\n& = \\frac{1}{\\sigma^2} \\left[ \\sum\\limits_{i=1}^n y_i x_i - \\sum\\limits_{i=1}^n x_i x_i^\\top \\beta\\right] && \\text{(multiply out, prop. of sums)}\\\\\n& = \\frac{1}{\\sigma^2} (X^\\top y - (X^\\top X)\\beta) && \\text{(vector to matrix notation)}\n\\end{align}\n\\]\nAnd now set the score function equal to 0 and solve for \\(\\beta\\). We can ignore the \\(1/\\sigma^2\\) out front since if the rest equals 0, the score function equals 0.\n\\[\n\\begin{align}\n0 & = X^\\top y - (X^\\top X)\\beta \\\\\n(& X^\\top X)\\beta = X^\\top y && (+(X^\\top X)\\beta \\text{ to both sides}) \\\\\n\\beta & = (X^\\top X)^{-1} X^\\top y && (\\times (X^\\top X)^{-1} \\text{ to both sides})\n\\end{align}\n\\]\nAnd we can see, our MLE estimates are exactly the same form as the OLS estimates.\n\n\n\nInformation Criterion Statistics",
    "crumbs": [
      "2 The Generalised Linear Model"
    ]
  }
]