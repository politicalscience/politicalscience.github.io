[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Political Science and Political Economy",
    "section": "",
    "text": "This is a collection of resources relating to Political Science and Political Economy, that I have gathered during my masters degree at LSE."
  },
  {
    "objectID": "index.html#navigating-the-collection",
    "href": "index.html#navigating-the-collection",
    "title": "Political Science and Political Economy",
    "section": "Navigating the Collection",
    "text": "Navigating the Collection\nUse the left sidebar for navigation between topics. The topics in this collection include:\n\nGame Theory (formal mathematical modelling of politics)\nSome quick refresher resources on Mathematics\nQuantitative Methods for Causal Inference. R-code is provided for implementation.\nFurther Statistical Models (other regressions, multivariate latent models, etc.). R-code is provided for implementation purposes."
  },
  {
    "objectID": "index.html#further-resources",
    "href": "index.html#further-resources",
    "title": "Political Science and Political Economy",
    "section": "Further Resources",
    "text": "Further Resources\nMathematics:\n\nDavid Siegel’s Youtube Channel (Mathematics for Political Scientists)\nKhan Academy\n\nQuantitative Methods (Causal Inference):\n\nBen Lambert’s Youtube Channel\nBurkey Academy\nMY457 Causal Inference for Experimental and Observational Studies\nAngrist and Pischke: Mastering Metrics (undergrad level applied econometrics)\nAngrist and Pishke: Mostly Harmless Econometrics (graduate level applied econometrics)\nWooldridge: Introductory Econometrics (the classic)\nStock and Watson: Introduction to Econometrics (another classic)\nImbens and Rubin: Causal Inference for Statistics, Social, and Biomedical Sciences\nHuntington-Klein: The Effect"
  },
  {
    "objectID": "index.html#about-the-creator",
    "href": "index.html#about-the-creator",
    "title": "Political Science and Political Economy",
    "section": "About the Creator",
    "text": "About the Creator\nThis collection was created by Kevin.\n\nk.l.li1@lse.ac.uk\nhttps://kevinli03.github.io"
  },
  {
    "objectID": "games.html",
    "href": "games.html",
    "title": "Guide to Game Theory",
    "section": "",
    "text": "Use the right sidebar for easy navigation.\n\n\nBasics of Game Theory\n\nGames and Notations\nGames consist of:\n\nA set of players \\(i = 1, \\dots, N\\)\nFor each player \\(i\\), a set of actions \\(a_i\\) and strategies \\(s_i\\) or \\(\\sigma_i\\).\nAn action profile of all player’s actions \\(a = (a_1, \\dots, a_N)\\), also notated \\(a = (a_i, a_{-i})\\), where \\(a_{-i}\\) is the actions of all other players not player \\(i\\).\nA strategy profile of all player’s strategies \\(s = (s_1, \\dots, s_N)\\), also notated \\(s = (s_i, s_{-i})\\).\nFor each player \\(i\\), preferences over the action profiles, specified by a payoff function \\(u_i(s_i, s_{-i})\\).\n\n\n\n\n\n\n\nStrategies vs. Actions\n\n\n\n\n\nStrategies and Actions are two different things.\nAn action is something you do on a particular moment a player has to make a move.\n\nFor example, in chess, an action might be move the knight to some square.\n\nA strategy is a complete plan of action - i.e. a plan of all the actions you will take in the game, for all the possible alternate scenarios that could occur in the game.\n\nFor example, in chess, a strategy would be a complete plan of what actions to take, for every possible response of your opponent, for every possible move throughout the entire game.\n\nFor static-complete information games, these two are often equivalent, but it is important to make this distinction for dynamic and incomplete information games.\n\n\n\n\n\n\nPreference Theory\nLet us define \\(X\\) as a set of alternatives that some agent \\(i\\) can achieve. Alternatives are both mutually exclusive and exhaustive.\n\n\n\n\n\n\nMutually Exclusive and Exhaustive Alternatives\n\n\n\n\n\nMutually exclusive means that if you end up with one alternative, you cannot end up with any other alternative at the same time.\n\nFor example, if I get a 95 on my exam, I cannot possibly also get a 85 on the same exam. Having gotten one score on my exam, the other alternatives cannot be obtained.\n\nExhaustive means \\(X\\) consists of all possibilities - the set of all possible outcomes.\n\nFor example, the potential scores on an exam are 0 - 100, so the set \\(X\\) includes all of these potential scores. There are no potential outcomes I can get outside of \\(X\\).\n\n\n\n\nA preference relation is a relationship between two elements of \\(X\\):\n\nIf \\(x \\succ y\\), the agent strictly prefers \\(x\\) to \\(y\\) - i.e. when given the choice, they will always choose \\(x\\).\nIf \\(x \\sim y\\), the agent is indifferent between \\(x\\) and \\(y\\).\nIf \\(x \\succsim y\\), the agent weakly prefers \\(x\\) to \\(y\\) - i.e. the agent either prefers \\(x\\) to \\(y\\), or is indifferent, but never prefers \\(y\\) to \\(x\\).\n\nPreferences are assumed to be rational if they are complete and transitive.\n\n\n\n\n\n\nComplete and Transitive Preferences\n\n\n\n\n\nComplete preferences means for all \\(x, y \\in X\\), either \\(x \\succsim y\\) or \\(y \\succsim x\\).\n\nFor any two alternatives, the agent always has some type of preference relation (never “no opinion”).\nThis does not mean they cannot be indifferent (that is a preference relation). It just means that if you ask the agent how they feel between \\(x\\) and \\(y\\), they must be able to say they prefer one, or are indifferent, and not say “I don’t have an opinion”.\n\nTransitive preferences means that for \\(x, y, z \\in X\\), if \\(x \\succsim y\\), and \\(y \\succsim z\\), then \\(x \\succsim z\\).\n\nNote: group preferences are not always transitive (condercet’s paradox), and thus not rational. But for individual agent purposes, we generally assume to this to be true.\n\n\n\n\n\n\n\nUtility Functions and Maximal Set\nA utility function is a mapping \\(u: X \\rightarrow \\mathbb R\\), such that for any \\(x, y \\in X\\), \\(u(x) ≥ u(y)\\) if and only if \\(x \\succsim y\\). Or in other words, the utility function should match the preference relations.\n\n\n\n\n\n\nNotes on Utility Functions\n\n\n\n\n\nThe mapping of utility functions is often ordinal, not continuous.\n\nFor example, let us say \\(u(x) = 2\\) and \\(u(y) = 1\\). If \\(u\\) is ordinal, we can say that \\(x\\) is preferred to \\(y\\). But, we cannot say \\(x\\) is preferred twice as much as \\(y\\), since for ordinal, the distance means nothing - only the magnitude.\n\nAny rational set of preferences (both complete and transitive) can be represented by a utility function.\n\nThe proof is beyond the scope of this lesson. But we can quickly show why if either are violated, they cannot be represented.\nIf the preferences are incomplete, we cannot assign a number utility to the alternative that the individual is unsure about/does not have an opinion on.\nIf the preferences are not transitive, we cannot create a utility function, because numbers \\(\\mathbb R\\) (which the utility function uses as a mapping) are transitive, and you cannot map something non-transitive into something transitive.\n\n\n\n\nWhat is the best choice from \\(X\\) that an agent can make (rationality)?\nWe define the “best choice” as outcomes/alternatives included in the maximal set of \\(X\\), notated \\(M(\\succsim, X)\\):\n\\[\n\\begin{split}\nM(\\succsim, X) & = \\{ x \\in X : x \\succsim y, \\ \\forall \\ y \\in X \\} \\\\\n& = \\arg \\max\\limits_{x \\in X} \\{u(x) \\}\n\\end{split}\n\\]\nOr in more intuitive terms, the inputs \\(x \\in X\\) that make the utility function \\(u \\in \\mathbb R\\) achieve its maximum value, are the elements of \\(x\\) in the maximal set \\(M(\\succsim, X)\\).\n\n\n\n\n\n\nStatic Games of Perfect Information\n\nDominant Strategy\nA strategy \\(s_i^D\\) is a strictly dominant strategy for player \\(i\\) if it gives a higher payoff than any other strategy of player \\(i\\), no matter the choice of strategy of other players.\n\\[\n\\underbrace{u_i(s_i^D, s_{-i})}_{S_i^D \\text{ utility} } &gt; \\underbrace{u_i (s_i, s_{-i})}_{\\text{utility of others}} \\quad \\forall \\ s_{-i}, \\ \\forall s_i ≠ s_i^D\n\\]\nA strategy \\(s_i^1\\) that always produces a lower payoff than \\(s_i^2\\) is said to be strictly dominated.\n\n\n\n\n\n\nWeakly Dominant Strategies\n\n\n\n\n\nA strategy \\(s_i^D\\) is weakly dominant for player \\(i\\) if it gives an equal or higher payoff than any other strategy of player \\(i\\), no matter the choice of strategy of other players.\n\\[\n\\underbrace{u_i(s_i^D, s_{-i})}_{S_i^D \\text{ utility}} ≥ \\underbrace{u_i (s_i, s_{-i})}_{\\text{utility of others}} \\quad \\forall \\ s_{-i}, \\ \\forall s_i ≠ s_i^D\n\\]\nA strategy \\(s_i^1\\) that always produces a lower or equal payoff than \\(s_i^2\\) is said to be weakly dominated.\n\n\n\n\n\n\n\n\n\nEliminating Dominated Strategies\n\n\n\n\n\nA strictly dominated strategy can be eliminated, as it will never be played. A strictly dominated strategy will never form a part of any equilibrium (pure or mixed).\nA weakly dominated strategy cannot be eliminated. It can be a best response in certain cases.\n\nHowever, if you have multiple equilibria, some that have weakly dominated strategies, you can eliminate some of them as they are often considered non-pivotal or nonsensical, which narrows the amount of equilibrium you have.\n\n\n\n\nA strategy profile \\(s^D\\) is a dominant strategy equilibrium, if every player is playing a dominant strategy. Formally, a strategy profile \\(s^D = (s_1^D, \\dots, s_N^D)\\) is a dominant strategy equilibrium, if \\(s_i^D\\) is a dominant strategy for all \\(i\\):\n\\[\n\\underbrace{u_i(s_i^D, s_{-i})}_{S_i^D \\text{ utility} } &gt; \\underbrace{u_i (s_i, s_{-i})}_{\\text{utility of others}} \\quad \\forall \\ s_{-i}, \\ \\forall s_i ≠ s_i^D, \\ \\forall \\ i\n\\]\nDominant Strategy Equilibrium, if it exists, is always unique. However, it often does not exist.\n\n\n\n\n\n\nExample: Prisoner’s Dilemma\n\n\n\n\n\nTake the classic game: the prisoner’s dilemma. The matrix representation is as follows:\n\\[\n\\begin{matrix}\n& C_2 & D_2 \\\\\nC_1 & (3, 3) & (0, 4) \\\\\nD_1 & (4, 0) & (1,1)\n\\end{matrix}\n\\]\n\nPayoffs are given as (player picking between rows, player picking between columns)\n\nLet us look at the row player’s decision. First, let us hold constant their opponent (the column player) and assume they choose cooperate.\n\\[\n\\begin{matrix}\n& C_2  \\\\\nC_1 & (3, 3) \\\\\nD_1 & (4, 0)\n\\end{matrix}\n\\]\nWe can see \\(4 &gt; 3\\), so the row player should choose defect.\nNow, let us hold constant their opponent (the column player) and assume they choose defect.\n\\[\n\\begin{matrix}\n& D_2 \\\\\nC_1 & (0, 4) \\\\\nD_1 & (1,1)\n\\end{matrix}\n\\]\nWe can see \\(1&gt;0\\), so the row player should choose defect.\nThus, the row player, no matter what their opponent does, should choose defect. Thus, defect is a dominant strategy.\n\nSince this game is symmetrical, the column player also has a dominant strategy of defect.\nThus, the dominant strategy equilibrium of this game is (defect, defect).\n\n\n\n\n\n\nBest Responses\nThe strategy \\(s_i\\) for player \\(i\\), is a best response to a specific opponent’s strategy \\(s_{-i}\\), if it yields maximum payoff to \\(i\\) holding the opponents specific strategy \\(s_{-i}\\) constant.\n\\[\n\\underbrace{u_i(s_i, s_{-i})}_{\\text{utility of } s_i} ≥ \\underbrace{u_i(s_i', s_{-i})}_{\\text{utility for others}} \\quad \\forall \\ s_i'\n\\]\nIn other words, that means, holding the opponents strategy constant, player \\(i\\) has no profitable deviation to any other strategy to obtain a better payoff.\nBest responses of player \\(i\\) are denoted as the following:\n\\[\nBr_i(s_{-i}) = s_i\n\\]\n\n\n\n\n\n\nExample: Stag-Hunt Game\n\n\n\n\n\nTake the classic game: the stag-hunt game. The matrix representation is as follows:\n\\[\n\\begin{matrix}\n& S_2 & H_2 \\\\\nS_1 & (2, 2) & (0, 1) \\\\\nH_1 & (1, 0) & (1,1)\n\\end{matrix}\n\\]\nLet us compute the best responses of the player choosing between rows, to each of the opponents possible strategies.\nLet us say the opponent plays stag. What is the row player’s best response?\n\\[\n\\begin{matrix}\n& S_2 \\\\\nS_1 & (2, 2) \\\\\nH_1 & (1, 0)\n\\end{matrix}\n\\]\nWe can see \\(2&gt;1\\), thus the row player’s best response in this case is to play stag.\nLet us say the opponent plays hunt. What is the row player’s best response?\n\\[\n\\begin{matrix}\n& H_2 \\\\\nS_1  & (0, 1) \\\\\nS_2 & (1,1)\n\\end{matrix}\n\\]\nWe can see \\(1&gt;0\\), thus the row player’s best response in this case is to play hunt.\nWe can notate the best responses of the row player (player 1) as follows:\n\\[\n\\begin{cases}\nBr_1(S_2) = S_1 \\\\\nBr_1(H_2) = H_1\n\\end{cases}\n\\]\n\n\n\n\n\n\nPure Strategy Nash Equilibrium\nA strategy profile \\(s^*\\) is a Nash Equilibrium, when all players are playing best responses (and no player has any profitable deviations, holding the opponents strategy consistent):\n\\[\n\\underbrace{u_i(s_i^*, s_{-i}^*)}_{s_i^* \\text{ utility}} ≥ \\underbrace{u_i(s_i', s_{-i}^*)}_{\\text{utility of others}} \\quad \\forall \\ s_i', \\ \\forall \\ i\n\\]\nThere are a few characteristics of Nash Equilibrium:\n\nThere can be more than one Nash Equilibrium in a game, thus, they are not unique.\nAll dominant strategy equilibrium are Nash Equilibrium as well, but not all Nash Equilibrium are dominant strategy equilibrium.\n\nNash Equilibrium can be computed in 2 ways:\n\nCalculate all the best responses of each player. The strategy profiles where all players are playing best responses is a Nash Equilibrium.\nTest different cases of strategy profiles. If no profitable deviation exists while holding other player’s strategies constant, then that is a Nash Equilibrium.\n\n\n\n\n\n\n\nExample: Assurance Game\n\n\n\n\n\nTake this famous game - the assurance game. The matrix form is as follows:\n\\[\n\\begin{matrix}\n& B_2 & R_2 \\\\\nB_1 & (1, 1) & (3, 0) \\\\\nR_1 & (0, 3) & (4,4)\n\\end{matrix}\n\\]\nTo find the Nash Equilibrium, let us compute best responses for both players. Start with the row player (best responses underlined and bolded):\n\\[\n\\begin{matrix}\n& B_2 & R_2 \\\\\nB_1 & (\\underline{\\mathbf 1}, 1) & (3, 0) \\\\\nR_1 & (0, 3) & (\\underline{\\mathbf 4},4)\n\\end{matrix}\n\\]\nNow, let us find the best responses of the column player:\n\\[\n\\begin{matrix}\n& B_2 & R_2 \\\\\nB_1 & (\\underline{\\mathbf 1}, \\underline{\\mathbf 1}) & (3, 0) \\\\\nR_1 & (0, 3) & (\\underline{\\mathbf 4},\\underline{\\mathbf 4})\n\\end{matrix}\n\\]\nWe can see that strategy profiles (nukes, nukes) and (no nukes, no nukes) are both strategy profiles where both players are playing best responses. Thus, these two strategy profiles are the Nash Equilibrium.\n\n\n\n\n\n\n\n\n\nTrembling Hand Perfect Equilibrium\n\n\n\n\n\nOne assumption of Game Theory is that players are always rational - they will always aim to maximise their payoff. However, in the real world, this isn’t always the case. Sometimes, players will play an unoptimal action.\nIf our goal of our models is to make predictions about outcomes, we want our models to be robust.\nThe Trembling Hand Perfect Equilibrium assumes that one of the players plays an unoptimal (non-Nash Equilibrium) action \\(\\epsilon\\) proportion of the time.\nIf the Nash Equilibrium remains the Nash Equilibrium, even with one player acting unoptimally \\(\\epsilon\\) of the time, then the equilibrium is robust.\nTake this following game:\n\\[\n\\begin{matrix}\n& L & R \\\\\nU & (1, 1) & (2, 0) \\\\\nD & (0, 2) & (2,2)\n\\end{matrix}\n\\]\nThis game has two Nash Equilibriua: (Up, Left) and (Down, Right).\nLet us test the robustness of the (Up, Left) Equilibrium. Let us assume player 1 (row player) is irrational at probability \\(\\epsilon\\), and selects Down. The remainder of the time \\(1 - \\epsilon\\) they still play the optimal Up. Let us call this strategy \\(s_1^\\epsilon\\)\nWill player 2 still prefer to play Left and mantain the current equilibrium?\nWe can solve this by finding the expected utility of player 2 playing Left and Right, and seeing if the utility of Left is still higher. Remember, player 1 is playing strategy \\(s_1^\\epsilon\\).\n\\[\n\\begin{cases}\nu_2(L, s_i^\\epsilon) = Pr(U) \\times 1 + Pr(D) \\times 2 = (1-\\epsilon)(1) + \\epsilon(2) = 1 + \\epsilon \\\\\nu_2(R, s_i^\\epsilon) = Pr(U) \\times 0 + Pr(D) \\times 2 = (1-\\epsilon)(0) + \\epsilon(2) = 2 \\epsilon\n\\end{cases}\n\\]\nIs the utility for player 2 of playing Left greater or equal than playing Right?\n\\[\n\\begin{split}\nu_2(L, s_i^\\epsilon) & ≥ u_2(R, s_i^\\epsilon) \\\\\n1 + \\epsilon & ≥ 2 \\epsilon \\\\\n\\epsilon & ≤ 1\n\\end{split}\n\\]\nThis tells us that Left is the preferred strategy of player 2, as long as \\(\\epsilon ≤ 1\\). Remember, \\(\\epsilon\\) is a probability between 0 and 1. That means, all values of \\(\\epsilon\\) ensure Left is the preferred strategy.\nThus, (Up, Left) is still the equilibrium, no matter how irrational player 1 might be.\n\n\n\n\n\n\nMixed Strategies\nLet us say player \\(i\\) has possible strategies \\(\\{s_1, \\dots, s_m \\}\\). A mixed strategy for player \\(i\\) is a probability distribution over their set of strategies:\n\\[\n\\sigma_i = (\\sigma_i(s_1), \\dots, \\sigma_i(s_m))\n\\]\nWhere \\(\\sigma_i(s_k)\\) is the probability of player \\(i\\) playing their strategy \\(s_k\\).\n\n\n\n\n\n\nExample of a Mixed Strategy\n\n\n\n\n\nLet us say player \\(i\\) has some mixed strategy:\n\\[\n\\sigma_i = (0.25, 0.75)\n\\]\nWhat this means is that player 1 will play their first strategy \\(s_1\\) 25% of the time, and play their second strategy \\(s_2\\) 75% of the time.\n\n\n\nMixed Strategies have a few properties:\n\n\\(\\sigma_i(s_k) \\in [0, 1], \\quad \\forall \\ s_k \\in \\{s_1, \\dots, s_m \\}\\). This is because of the laws of probability - a probability must be between 0 and 1.\n\\(\\sum\\limits_k \\sigma_i(s_k) = 1\\), In other words - all probabilities in the mixed strategy should sum to 1.\n\nPure strategies are technically mixed-strategies where one strategy has probability 1, and all other strategies have probability 0.\n\n\n\nMixed Strategy Nash Equilibrium\nThe mixed-strategy profile \\(\\sigma^* = (\\sigma_1^*, \\dots, \\sigma_N^*)\\) is a Nash Equilibrium if \\(\\sigma_i^*\\) is a best response to \\(\\sigma_{-i}^*\\) for all players \\(i\\).\n\\[\n\\underbrace{u_i(\\sigma_i^*, \\sigma_{-i}^*)}_{\\sigma_i^* \\text{utility}} ≥ \\underbrace{u_i(\\sigma_i', \\sigma_{-i}^*)}_{\\text{utility of others}} \\quad \\forall \\ \\sigma_i', \\ \\forall \\ i\n\\]\nA few notes on Mixed Strategy Nash Equilibrium.\n\nIf a mixed strategy for player \\(i\\), \\(\\sigma_i^*\\) is in a Nash Equilibrium, player \\(i\\) must be indifferent between the strategies they are mixing. This is because if player \\(i\\) preferred one strategy over all others, they would simply play that strategy, not mix.\nAll pure-strategy Nash equilibrium are also mixed-strategy nash equilibrium, just that the mixed strategy in question has a probability of playing one strategy 100% of the time.\nAll simultaneous games where all players have a finite set of strategies, has a mixed strategy Nash Equilibrium (this proof got John Nash as Nobel Prize).\n\nFinding Mixed Strategy Nash Equilibrium should take this procedure:\n\nFind all pure-strategy Nash Equilibrium.\nIf there are any strategies that are strictly dominated by one other strategy, eliminate them, as they are never a part of mixed-strategy Nash.\nFind the mixed strategy nash equilibrium (shown below).\n\n\n\n\n\n\n\nExample: Matching Pennies Game\n\n\n\n\n\nTake the following Matching Pennies game:\n\\[\n\\begin{matrix}\n& H_2 & T_2 \\\\\nH_1 & (1, -1) & (-1, 1) \\\\\nT_1 & (-1, 1) & (1,-1)\n\\end{matrix}\n\\]\nFirst, let us find the pure-strategy Nash Equilibrium.\n\\[\n\\begin{matrix}\n& H_2 & T_2 \\\\\nH_1 & (\\underline{\\mathbf 1}, -1) & (-1, \\underline{\\mathbf 1}) \\\\\nT_1 & (-1, \\underline{\\mathbf 1}) & (\\underline{\\mathbf 1},-1)\n\\end{matrix}\n\\]\nWe can see there are no pure-strategy Nash Equilibrium. We also do not have any strictly dominated strategies.\nWe now need to find the probabilities each player plays each strategy at their disposal.\nLet us define \\(p\\) as player 1’s (row player) probability of playing Heads, which means they will play Tails with probability \\(1-p\\). Similarly, let us define \\(q\\) as player 2’s probability of playing Heads, which means they will play tails at probability \\(1-q\\):\n\\[\n\\begin{matrix}\n& H_2 & T_2 \\\\\nH_1 & (1, -1) & (-1, 1) & p \\\\\nT_1 & (-1, 1) & (1,-1) & 1-p \\\\\n& q & 1-q\n\\end{matrix}\n\\]\nLet us look at player 1. What is their expected utility when playing Heads or Tails (which depends on player 2’s probability \\(q\\)):\n\\[\n\\begin{cases}\nu_1(H_1) = 1q -1(1-q) = 2q - 1 \\\\\nu_1(T_1) = -1q + 1(1-q) = -2q + 1\n\\end{cases}\n\\]\nWe know that for player 1 to mix between these two strategies, they must be indifferent. Thus:\n\\[\n\\begin{split}\nu_1(H_1) & = u_1(T_1) \\\\\n2q - 1 & = -2q + 1 \\\\\n4q & = 2 \\\\\nq & = \\frac{1}{2}\n\\end{split}\n\\]\nThus, player 1 will play Heads 50% of the time.\nWe can do the same exercise with player 2, and we will also get \\(p = \\frac{1}{2}\\).\nThus, the Mixed Strategy Nash Equilibrium is: \\(\\left( (\\frac{1}{2}, \\frac{1}{2}), (\\frac{1}{2},\\frac{1}{2}) \\right)\\).\n\nThis is in the form \\(((\\sigma_1(H_1), \\sigma_1(T_1), (\\sigma_2(H_2), \\sigma_2(T_2))\\)\n\n\n\n\n\n\n\n\n\n\nDynamic Games of Complete Information\n\nStrategies and Decision Nodes\nIn dynamic games, players move sequentially. This means that if player 1 moves first, player 2 may have different “states” of the game in which to make a decision.\nEach potential location in the game in which a player has to choose an action is called a decision node.\nA strategy, by definition, is a complete plan of action. This implies that a strategy of a player must include what a player would do at every possible decision node in the game. Even if the decision node is never reached, the strategy profile of the player must specify an action to do at that decision node.\nThe total number of different strategies is the product of the number of actions possible at every decision node.\n\n\n\n\n\n\nExample of Strategies: Budget Game\n\n\n\n\n\nTake a typical dynamic game (where players move sequentially):\n\n\n\n\n\nIn this game, Congress has one decision node, and the President has 2 decision nodes.\nThus, congress has 2 strategies: Small Budget, or Large Budget.\nPresident has 4 strategies, since he has 2 in the first decision node, and 2 in the second, and the total number of strategies is the product: \\(2 \\times 2\\).\n\nThe strategies are: (Approve, Approve), (Approve, Reject), (Reject, Approve), (Reject, Reject)\nThe first part of each represents what they would do in the top decision node (after P1 plays small budget), the second part of each represents the bottom decision node.\n\n\n\n\n\n\n\nNash Equilibrium in Dynamic Games\nNash Equilibrium have the same definition in Dynamic Games - a strategy profile where all players are playing best responses, and no player has a profitable deviation.\nHowever, finding Nash Equilibrium for Dynamic Games requires you to convert the tree-form into normal form.\n\n\n\n\n\n\nExample of Normal Form: Budget Game\n\n\n\n\n\nTake a typical dynamic game (where players move sequentially):\n\n\n\n\n\nWe discussed the potential strategies of each player above (let us simplify the strategies to the first letter of their name):\n\nCongress: S or L\nPresident: (AA), (AR), (RA), (RR) - the first letter represents what they would do in the top decision node (after P1 plays small budget).\n\nWe put this into normal form as follows:\n\\[\n\\begin{matrix}\n& AA & AR & RA & RR \\\\\nS & (10, 2) & (10, 2) & (2, 1) & (2, 1)\\\\\nL & (5, 10) & (0, 0) & (5, 10) & (0, 0)\n\\end{matrix}\n\\]\nWe can quickly find the Nash Equilibrium in all the same ways as before:\n\\[\n\\begin{matrix}\n& AA & AR & RA & RR \\\\\nS & (\\underline{\\mathbf{10}}, \\underline{\\mathbf 2}) & (\\underline{\\mathbf{10}}, \\underline{\\mathbf 2}) & (2, 1) & (\\underline{\\mathbf 2}, 1)\\\\\nL & (5, \\underline{\\mathbf{10}}) & (0, 0) & (\\underline{\\mathbf 5}, \\underline{\\mathbf{10}}) & (0, 0)\n\\end{matrix}\n\\]\nWe have three Nash Equilibria: \\((S, AA), \\ (S, AR), \\ (L, RA)\\).\n\n\n\nHowever, Nash Equilibrium have a problem in dynamic games: actions at nodes not reached on the path of equilibrium, do not affect payoffs.\n\nThis means that Nash Equilibrium include solutions where players do not play optimally off the path (what we call non-credible threats).\nThis is a big issue, since off-the-path behaviour determines how previous players anticipate what to do.\n\n\n\n\n\n\n\nExample of Non-Credible Threats: Budget Game\n\n\n\n\n\nTake a typical dynamic game (where players move sequentially):\n\n\n\n\n\nFrom above, we found three Nash Equilibrium: \\((S, AA), \\ (S, AR), \\ (L, RA)\\).\nLet us take a look at the third Nash Equilibrium, \\(L, RA\\):\nThis equilibrium says that the President will choose to Reject in the top node (after congress passes a small budget). But, this makes no sense - we can see President can increase their payoff to 2 if they choose approve.\nIf president does play optimally and moves to Approve in the top node, now Congress has a profitable deviation from \\(L\\) to \\(S\\), as they can go from a payoff of 5 to 10.\nThus, this \\(L, RA\\) equilibrium does not make much sense - it completely depends on the assumption that off-the-path on the top-node, the President will choose an unoptimal action of \\(R\\).\nThis shows the issue with Nash Equilibrium in Dynamic Games.\n\n\n\n\n\n\nSubgame Perfect Nash Equilibrium\nA Subgame Perfect Nash Equilibrium (SPNE) is a subset of Nash Equilibria, where all players act optimally, even in nodes that are not reached in equilibrium.\nSubgame Perfect Nash Equilibrium can also be defined as a strategy profile that is a Nash Equilibrium in all Subgames of the Game.\n\nA subgame of an extensive game is a game consisting of any node and its subsequent actions and nodes.\n\nTo find SPNE, we use backwards induction:\n\nStart with the last player to move. Determine their optimal action, and eliminate their non-optimal actions.\nNow move forward to the second-to-last player to move. Determine their optimal action - assuming that the outcomes eliminated in step 1 are not feasible to obtain.\nNow keep moving forward one player, and find their optimal action, until you reach the front.\n\n\n\n\n\n\n\nExample of Backwards Induction\n\n\n\n\n\nTake this following dynamic game:\n\n\n\n\n\nLet us start with the last player to move, player 3.\n\nOn the left decision node, \\(9&gt;4\\), so player 3 chooses \\(r\\).\nOn the middle decision node, \\(8&gt;7\\), so player 3 chooses \\(l\\).\nOn the right decision node, \\(3 &gt; 0\\), so player 3 chooses \\(r\\).\n\nNow, go one step backward to player 2:\n\nPlayer 2 can choose between \\(a\\) and \\(b\\) - however, they should only consider outcomes that player 3 will choose (they are anticipating what player 3 will do).\nUnder this assumption, if player 2 chooses \\(a\\), they know player 3 will choose \\(l\\), and player 2 will get a payoff of 2.\nIf player 2 chooses \\(b\\), they know player 3 will choose \\(r\\), and player 2 will get a pyyoff of 5.\nSince \\(5&gt;2\\), player 2 will choose \\(b\\).\n\nNow, go one step backward to player 1:\n\nPlayer 1 can choose between \\(L\\) and \\(R\\)- however, they should only consider outcomes that player 3 and player 2 will choose (they are anticipating what later players will do).\nUnder this assumption, if player 1 chooses \\(L\\), they know player 3 will choose \\(r\\), and that will get player 1 a payoff of 5.\nIf player 1 chooses \\(R\\), they know player 2 will choose \\(b\\), and player 3 will choose \\(r\\), and that will get player 1 a payoff of 1.\nSince \\(5&gt;1\\), that means player 1 will play \\(L\\).\n\nThus, the SPNE of this game is: \\((L, b, rlr)\\).\n\nRemember, the strategy profile consists of what a player will do at each node in the game. Player 3 has 3 nodes, so we have to indicate that in our SPNE.\n\n\n\n\n\n\n\n\n\n\nIncomplete Information Games\n\nInformation Sets and Beliefs\nIn incomplete information games, sometimes, a player is unsure which decision node they are on. An information set is a set of decision nodes in which a player is unsure which one they are at.\n\nFor example, if player \\(i\\) is unsure they are at decision node \\(A\\) or \\(B\\), those two decision nodes will be a part of the same information set.\n\n\n\n\n\n\n\nExample of Information Sets\n\n\n\n\n\nTake this following game: Alex moves first, then Chris - however, Chris cannot observe the action Alex took:\n\n\n\n\n\nThus, Chris is unsure if they are at the left side of the game, or the right side of the game, since Chris did not observe what Alex did.\nThus, these two nodes are on the same information set for Chris - he knows he is in this information set, but he is unsure which node he is at within this information set.\n\n\n\nPlayers have to choose the same action to do for all nodes in an information set. This is because they are unsure which node within the information set they are at, so they have to do the same action for all of the nodes.\nPlayers have a set of beliefs regarding which node they are at within the information set. This is often described as a probability distribution over each node within the information set.\n\nFor example, player \\(i\\) might believe they have an 80% chance of being at node \\(A\\), and 20% chance at being in node \\(B\\). Beliefs within an information set should sum to 1.\n\n\n\n\n\n\n\nExample of Beliefs\n\n\n\n\n\nTake this following game: Alex moves first, then Chris - however, Chris cannot observe the action Alex took:\n\n\n\n\n\nChris is unsure if he is at the left or right node. However, Chris can put a belief that he is at each node - we can label these beliefs \\(\\mu\\) and \\(1-\\mu\\).\n\n\\(\\mu\\) is Chris’s belief of the chance he is at the left node.\n\\(1-\\mu\\) is Chris’s belief of the chance he is not at the left node, so at the right node.\n\nChris will use these beliefs to help him decide if he should play \\(o\\) or \\(f\\).\n\n\n\nPlayers will use these beliefs to determine what strategy they should play at the information set. Players can also “update” their beliefs based on new information from prior events in the game.\n\n\n\nBayes’ Rule\nPlayers can also “update” their beliefs based on new information from prior events in the game. They do this updating via Bayes’ Rule.\nLet us assume \\(Pr(A)\\) is the probability of us being at a specific node. Let us assume \\(Pr(B)\\) is the probability of us being in the information set that contains the specific node.\nThus, \\(Pr(A|B)\\) is the probability of us being at the specific node, assuming we have entered the information set. Bayes’ Rule says the following about \\(Pr(A|B)\\):\n\\[\n\\underbrace{Pr(A|B)}_{\\text{posterior}} = \\frac{\\overbrace{Pr(B|A)}^{\\text{likelihood}} \\overbrace{Pr(A)}^{\\text{prior}}}{\\underbrace{Pr(B|A)Pr(A) + Pr(B|A^C)Pr(A^C)}_{\\text{evidence}}}\n\\]\n\n\n\n\n\n\nProof of Bayes’ Rule\n\n\n\n\n\nLet us start off with the definition of conditional probability:\n\\[\nPr(A|B) = \\frac{Pr(A \\cap B)}{Pr(B)}\n\\]\nLet us solve for \\(Pr(A \\cap B)\\) to get:\n\\[\nPr(A\\cap B) = Pr(A|B)Pr(B)\n\\]\nNow, let us take the opposite conditional probability:\n\\[\nPr(B|A) = \\frac{Pr(B \\cap A)}{Pr(A)}\n\\]\nLet us solve for \\(Pr(B \\cap A)\\):\n\\[\nPr(B\\cap A) = Pr(B|A) Pr(A)\n\\]\n\nBy the commutative property of sets, we know that:\n\\[\nPr(A \\cap B) = Pr(B \\cap A)\n\\]\nThus, the following must also be true.\n\\[\nPr(A|B)Pr(B) = Pr(B|A)Pr(A)\n\\]\nNow, let us solve for \\(Pr(A|B)\\):\n\\[\nPr(A|B) = \\frac{Pr(B|A)Pr(A)}{Pr(B)}\n\\]\nThus, we have proved Bayes’ Rule.\n\n\n\nWe often call \\(Pr(A)\\) the prior probability, and \\(Pr(A|B)\\) the posterior probability. Bayes’ Rule allows us to use new information to update our original prior probability \\(Pr(A)\\) to get our posterior probability \\(Pr(B|A)\\).\n\n\n\nPerfect Bayesian Equilibrium\nA Perfect Bayesian Equilibrium (PBE) consists of:\n\nA strategy profile: specifying the action to take for each player, at every information set in the game.\nA belief profile: a probability distribution over each node in each information set.\n\nFor every player, players should act sequentially rational. This means that:\n\nPlayers act optimally at every information set (maximise their payoffs), given their belief profiles.\nPlayer’s beliefs are consistent with the strategy profiles, if on-the-equilibrium-path beliefs are consistent with Bayes Rule, and if off-the-equilibrium-path beliefs are sufficiently restricted to ensure players are acting optimally with regard to their beliefs.\n\nTo find a Perfect Bayesian Equilibrium, you should generally start with backwards induction to find strategies, then check if the beliefs are consistent with these strategies.\n\n\n\n\n\n\nExample: Selten’s Horse\n\n\n\n\n\nTake this game:\n\n\n\n\n\nLet us find the Perfect Bayesian Equilibria of this game.\nFirst, let us start off with the strategy profiles by doing backwards induction.\nWhat should player 3 play in their node?\n\nWe can see that no matter what their belief \\(p\\) is, \\(L\\) always has a better payoff than \\(R\\). Thus, 3 will play \\(L\\).\n\nNow, what should player 2 do?\n\nIf player 2 chooses \\(A\\), they get a payoff of 3.\nIf player \\(A\\) chooses \\(D\\), they know that player 3 will choose \\(L\\), so player 2 will get a payoff of 5.\nSince \\(5&gt;2\\), player 2 chooses \\(D\\).\n\nNow, what should player 1 do?\n\nIf player 1 plays \\(D\\), they know player 3 will play \\(L\\), thus getting player 1 a payoff of 4.\nIf player 1 plays \\(A\\), they know player 2 will play \\(D\\) and player 3 will play \\(L\\), getting them a payoff of 5.\nSince \\(5&gt;4\\), player 1 will play \\(A\\).\n\nSo now, we have found the strategy profile \\((A, D, L)\\) is optimal. But what about the beliefs?\nWell, what beliefs are consistent with this strategy? We can see that under this equilibrium path, player 3’s information set is on-the-equilibrium path (it will be reached).\n\nWe see that on the equilibrium path, we will always end up on the \\((1-p)\\) node (since we know player 1 does not want to play \\(D\\)).\nThus, the belief that makes sense should be \\(p=0\\), since player 1 never plays \\(D\\), so if we end up in the information set, we must be on \\(1-p\\),\n\nThus, the PBE is \\((A, D, L, p = 0)\\).\n\n\n\n\n\n\nSignalling Games\nSignalling games are a very common form of incomplete-information game in Game Theory. They take the following form:\n\nNature chooses with probability \\(p\\) the type \\(\\theta\\) of the world. Player 1 observes \\(\\theta\\). Player 2 does not observe \\(\\theta\\).\nPlayer 1 chooses one action \\(m\\) in set \\(M\\), which act as signals for \\(\\theta\\).\nPlayer 2 receives \\(m\\) (not \\(\\theta\\)), and chooses some action \\(a \\in A\\).\n\nThis game basically says that player 1 has some private information \\(\\theta\\) about the world, and they can either decide to reveal or conceal this information to player 2 through their action \\(m\\).\n\n\n\n\n\n\nExample: Value of Education\n\n\n\n\n\nThe value of education model, developed by Spence (1973), is an example of a signalling game.\nThere are two players: the Job-Seeker, and the Boss\n\nNature chooses the Job-Seeker’s ability: High ability (H), or Low ability (L), with some probability \\(p\\).\nJob-Seeker decides whether or not to get a Degree (D). The Degree acts as a signal as whether the Job-Seeker is type high ability or low ability.\nThe Boss observes whether or not the Job-Seeker gets a Manager (M) or Blue-Collar (B) job. The Boss makes this decision based on their beliefs on if the job-seeker is High ability or Low ability, according to their signal and updating beliefs by Bayes’ Rule.\n\nThe idea is that getting a degree is more costly for a low ability job-seeker, but it is a positive signal that you are type high ability. Bosses want to put High-ability people in manager roles, and low-ability people in blue-collar roles.\n\n\n\n\n\n\n\n\nSignalling Games have 3 types of equilibrium:\n\nSeparating Equilibrium (see below)\nPooling Equilibrium (see below)\nSemi-separating equilibrium (we are not too concerned with this).\n\n\n\n\nSeparating Equilibrium\nA separating equilibrium is a PBE of a signalling game, where each type \\(\\theta\\) of sender, sends a different message \\(m\\).\nBy sending a different message \\(m\\) for each type \\(\\theta\\), the sender is essentially “revealing” their type \\(\\theta\\) to the second player who does not know \\(\\theta\\). With this information, the second player can update their beliefs on what type \\(\\theta\\) the sender is.\nTo solve for a separating equilibrium, you should do the following:\n\nChoose one potential separating equilibrium.\nUpdate beliefs of player 2 using Bayes’ Rule.\nCalculate best responses for Player 2 given these updated beliefs.\nNow, holding Player 2’s strategies constant, check if player 1 has any profitable deviations.\nIf player 1 has no profitable deviations, you have a Seperating Equilibrium.\nNow, check all other potential separating equilibria.\n\n\n\n\n\n\n\nFinding Seperating Equilibrium: Value of Education\n\n\n\n\n\nTake the value of education game from earlier:\n\n\n\n\n\nThere are 2 potential seperating equilibrium:\n\nIn type \\(H\\), player 1 plays \\(U\\), and in type \\(L\\), player 1 plays \\(D\\) (we will call this \\(UD\\))\nIn type \\(H\\), player 1 plays \\(D\\), and in type \\(L\\), player 1 plays \\(U\\) (we will call this \\(DU\\))\n\nLet us check \\(UD\\) first.\n\nGiven this set of strategies, we know Player 2’s beliefs \\(\\mu_U = 1\\) and \\(\\mu_D = 0\\).\nWith these beliefs, we know on the left side, player 2 should play \\(M\\), and on the right side, player 2 should play \\(B\\).\n\nNow does player 1 have any profitable deviation?\n\nIn type \\(H\\), Player 1 is playing \\(U\\). He knows that after \\(U\\), player 2 plays \\(M\\), getting player 1 a payoff of 10. If player 1 deviates to \\(D\\), he knows player 2 plays \\(B\\) on the right side, so player 1 gets a payoff of 4, which is less than 10. So this is not a profitable deviation.\nIn type \\(L\\), player 1 is playing \\(D\\). He knows that after \\(D\\), player 2 plays \\(B\\), getting player 1 a payoff of 1. If player 1 deviates to \\(U\\), he knows player 2 plays \\(M\\) on the right side, so player 1 gets a payoff of 10. That is a profitable deviation.\n\nThus, Player 1 has a profitable deviation in type \\(L\\), so this is not a separating equilibrium.\n\nNow, let us check \\(DU\\):\n\nGiven this set of strategies, we know player 2’s beliefs \\(\\mu_U = 0\\) and \\(\\mu_D = 1\\).\nWith these beliefs, we know on the left side, player 2 should play \\(B\\), and on the right side, player 2 should play \\(M\\).\n\nNow does player 1 have any profitable deviation?\n\nIn type \\(H\\), player 1 is playing \\(D\\). He knows that after \\(D\\), player 2 plays \\(M\\), getting player 1 a payoff of 8. If player 1 deviates to \\(U\\), he knows player 2 plays \\(B\\), getting him a payoff of 6. That is not a profitable deviation.\nIn type \\(L\\), player 1 is playing \\(U\\). He knows that after \\(U\\), player 2 plays \\(B\\), getting player 1 a payoff of 6. If player 1 deviates to \\(D\\), he knows player 2 plays \\(M\\), getting him a payoff of 5. This is not a profitable deviation.\n\nThus, there is a seperating equilibrium \\((DU, BM, \\mu_U = 0, \\mu_D = 1)\\).\n\nWhere \\(DU\\) is in the form of type \\(H\\), type \\(L\\); and \\(BM\\) is in the form of left information set, right information set.\n\n\n\n\n\n\n\nPooling Equilibrium\nA pooling equilibrium is a PBE of a signalling game, where no matter what type \\(\\theta\\) the sender is, the sender always sends the same message \\(m\\).\nBy sending the same message \\(m\\) for all types \\(\\theta\\), the sender refuses to reveal any information about their \\(\\theta\\). Thus, player 2’s posterior and prior beliefs about sender’s type \\(\\theta\\) remain the same.\nTo solve for a separating equilibrium, you should do the following:\n\nChoose one potential pooling equilibrium.\nUpdate beliefs of player 2 on-the-equilibrium-path using Bayes’ Rule (actually, this will stay the same as the prior beliefs). The off-the-equilibrium path information set will be dealt with later.\nCalculate best responses for Player 2 given these beliefs.\nNow, holding Player 2’s strategies constant, check if player 1 has any profitable deviations.\nIf player 1 has no profitable deviations, you have a Seperating Equilibrium. If player 1 might have a profitable deviation depending on what player 2 chooses off-the-equilibrium path, you will need to find the beliefs off-the-equilibrium path that eliminate this profitable deviation.\nNow, check all other potential pooling equilibria.\n\n\n\n\n\n\n\nFinding Pooling Equilibrium: Value of Education\n\n\n\n\n\nTake the value of education game from earlier (assume \\(p = 0.25\\)):\n\n\n\n\n\nThere are 2 potential pooling equilibrium:\n\nPlayer 1 plays \\(UU\\) (playing \\(U\\) no matter the type).\nPlayer 1 plays \\(DD\\) (playing \\(D\\) no matter the type).\n\nLet us test \\(UU\\) (I will not test \\(DD\\), but the same process applies).\nFirst, we know that information set \\(\\mu_U\\) is on the equilibrium path. Since we cannot update our beliefs, our beliefs remain the same as the prior (\\(p = 0.25\\) as given above):\n\nThus, \\(\\mu_U = 0.25\\), and \\(1-\\mu_U = 0.75\\).\n\nNow, let us find the optimal action of player 2 in this \\(\\mu_U\\) information set, based on the beliefs above:\n\\[\n\\begin{cases}\nu_2(M) = 10\\mu_U + 0(1-\\mu_U) = 10(0.25) = 2.5 \\\\\nu_2(B) = 5\\mu_u + 3(1-\\mu_u) = 5(0.25) +3(0.75) = 3.5\n\\end{cases}\n\\]\nWe see that \\(3.5&gt;2.5\\), so we know player 2 will play \\(B\\) on the left information set.\nNow, does player 1 have a profitable deviation?\n\nIn type \\(H\\), player 1 is playing \\(U\\). As calculated above, we know player 2 plays \\(B\\) here, so player 1 gets a payoff of 6. If player 1 deviates to \\(D\\), he could potentially get either 8 or 4 (depending on what player 2 plays on the right side).\nIn type \\(L\\), player 1 is playing \\(U\\). As calculated above, we know player 2 plays \\(B\\) here, so player 1 gets a payoff of 6. If player 1 deviates to \\(D\\), he could potentially get either 5 or 1, which or both less than 6. Thus, there is no profitable deviation.\n\nThus, player 1 only has a potential profitable deviation in type \\(H\\), if player 2 plays \\(M\\) on the right side information set (which will give player 1 a deviation payoff of 8, higher than his original 6 sticking with \\(U\\)).\nThus, for this to be a equilbrium, we must eliminate this profitable deviation, by making sure player 2 does not play \\(M\\) on the right-side, but instead plays \\(B\\).\n\nIf player 2 plays \\(B\\) on the right side, the player 1 deviation in type \\(H\\) would only yield payoff 4, so there would be no profitable deviation for player 1.\n\nThis means that the expected utility of \\(B\\) for player 2 must be higher than the expected utility of \\(M\\) on the right-side information set:\n\\[\n\\begin{split}\nu_2(B) & ≥ u_2 (M) \\\\\n5\\mu_D + 3(1-\\mu_D) & ≥ 10\\mu_D + 0(1-\\mu_D) \\\\\n5\\mu_D + 3 - 3\\mu_D &≥ 10\\mu_D \\\\\n2 \\mu_D + 3&≥ 10\\mu_D \\\\\n8\\mu_D&≤3 \\\\\n\\mu_D&≤\\frac{3}{8}\n\\end{split}\n\\]\nThus, this set of strategies is only a pooling PBE if \\(\\mu_D ≤ \\frac{3}{8}\\), since only this off-the-equilibrium-path belief condition ensures that there is no profitable deviation for player 1.\nThus, the pooling PBE is: \\((UU, BB, \\mu_U = \\frac{1}{4}, \\mu_D ≤ \\frac{3}{8})\\).\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Guide to Game Theory"
    ]
  },
  {
    "objectID": "linear.html",
    "href": "linear.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "This chapter introduces the essentials of linear algebra for political science (for mostly statistics/quantitative methods).\nUse the right sidebar for easy navigation.\n\n\nVectors\n\nScalars and Vectors\nA scalar is any single element or component, like a real number (ex. \\(x_1 \\in \\mathbb{R}\\)).\nA vector is a collection of scalars: \\((x_1 \\ x_2 \\ x_3 \\ x_4)\\). Each scalar is considered a element/component. Vectors can be both row vectors (horiztonal), and column vectors (vertical).\nThe dimension of a vector is the number of components/scalars/elements in the vector. Vectors will be denoted with a bold lowercase letter: \\(\\mathbf x\\), to distinguish them from scalars \\(x\\).\nVectors can be visualised graphically. Each element corresponds to a distance in a direction. For example, take the vector \\((3 \\ 2)\\). Graphically:\n\n\n\n\n\nIf a vector has 3 dimensions, the graphical representation will be in 3 dimensions, and so on.\n\n\n\nVector Addition/Subtraction\nVector addition and subtraction is done just by adding the respective elements to each other:\n\\[\n\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} + \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 1+3 \\\\ 2+4 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 6 \\end{pmatrix}\n\\]\n\nVector addition only works if the two vectors are of the same dimensionality.\n\nVector addition can be viewed graphically. Take two 2-dimensional vectors \\(\\mathbf A\\) and \\(\\mathbf B\\):\n\n\n\n\n\n\n\n\nScalar Multiplication and Normalisation\nVector scalar multiplication is done by multiplying all elements of the vector by the scalar:\n\\[\n4 \\times \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 4 \\times 1 \\\\ 4 \\times 2 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 8 \\end{pmatrix}\n\\]\nGraphically, this just multiplies the length of the vector by the scalar (and if the scalar is negative, the direction switches 180 degrees).\nThe norm of a vector is its length (when thinking graphically):\n\\[\n|| \\mathbf{x}|| = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2}\n\\]\nNormalizing a vector is scalar multiplying the vector by \\(1/||\\mathbf{x} ||\\).\n\nThis results in the norm of the vector equaling 1. This can be useful if you want to standardise and compare vectors.\n\n\n\n\nScalar Product\nScalar product, also known as dot product, takes two vectors and creates a scalar.\n\\[\n\\mathbf a \\cdot \\mathbf b = \\sum_i a_ib_i = a_1 b_1 + a_2 b_2+ \\dots a_n b_n\n\\]\n\nEssentially, multiply each respective element with each other. Then sum all of the products.\n\n\n\n\n\n\n\nExample of Scalar Product\n\n\n\n\n\nLet us do an example of scalar product:\n\\[\n\\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix} \\cdot \\begin{pmatrix} 4 \\\\ 5 \\end{pmatrix}\n\\]\nWe multiply each respective element with each other, then sum all of the products:\n\\[\n2 \\times 4 + 3 \\times 5 = 8 + 15 = 23\n\\]\n\n\n\nDot product calculates the projection/shadow of vector \\(\\mathbf a\\) on vector \\(\\mathbf b\\). In the figure below, the dot product calcualtes the length of the blue-highlighted line segment:\n\n\n\n\n\nFrom here, we can tell if the two vectors are perpendicular, then the dot product would be 0. This is useful for measures of similarity/correlation.\n\n\n\n\n\n\nMatrices\n\nTypes of Matrices\nA matrix is a collection of scalars, that are put in a \\(n \\times m\\) order with \\(n\\) number of rows, and \\(m\\) number of columns.\n\\[\n\\mathbf A_{2 \\times 3} = \\begin{pmatrix}\n2 & 3 & 2 \\\\\n1 & 4 & 1\n\\end{pmatrix}\n\\]\nMatrices can be viewed as a set of row vectors combined, or a set of column vectors combined (this is how datasets are arranged). Each element by the matrix can be denoted \\(a_{ij}\\), which is the element in the \\(i\\)th row and \\(j\\)th column:\n\\[\n\\mathbf A_{2 \\times 3} = \\begin{pmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23}\n\\end{pmatrix}\n\\]\nThere are several very common types of matrices that you need to know.\n\n\n\n\n\n\nSquare and Zero Matrix\n\n\n\n\n\nSquare Matrix is a matrix that have an equal number of rows and columns.\n\\[\n\\mathbf A_{2 \\times 2} = \\begin{pmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{pmatrix}\n\\]\nThese are useful because many matrix manipulations, like inversions and determinants.\nZero Matrix is a square matrix with all 0’s.\n\n\n\n\n\n\n\n\n\nDiagonal, Identity, and Lower/Upper Triangular Matrix\n\n\n\n\n\nDiagonal matrices only have elements along the top-left bottom-right diagonal.\n\\[\n\\mathbf A_{3 \\times 3} = \\begin{pmatrix}\na_{11} & 0 & 0 \\\\\n0 & a_{22} & 0 \\\\\n0 & 0 & a_{33} \\\\\n\\end{pmatrix}\n\\]\nAn identity matrix (notated \\(\\mathbf I\\))is a diagonal matrix, but all the diagonal elements equal 1:\n\\[\n\\mathbf I_{2 \\times 2} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\\]\n\nAny matrix times \\(\\mathbf I\\) equals itself (like a 1 in normal multiplication).\n\nA Lower/Upper Triangular Matrix is a matrix where only has values above/below the diagonal. For example, the following is a lower triangular matrix:\n\\[\n\\mathbf A_{3 \\times 3} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n3 & 4 & 0 \\\\\n3 & 3 & 4\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nSubmatrix\n\n\n\n\n\nA submatrix is a matrix if you were to remove a row and a column (that is specified by an element).\nFor example, take this 3 by 3 matrix:\n\\[\n\\mathbf A_{3 \\times 3} = \\begin{pmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{pmatrix}\n\\]\nLet us find the submatrix of \\(a_{21}\\). This means we will eliminate the 2nd row, and 1st column:\n\\[\n\\mathbf A_{2 \\times 2} = \\begin{pmatrix}\na_{12} & a_{13} \\\\\na_{32} & a_{33}\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nPermutation Matrix\n\n\n\n\n\nA permutation matrix is a matrix that only has one non-zero element in each row and column.\n\\[\n\\mathbf A_{3 \\times 3} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{pmatrix}\n\\]\nThe identity matrix is a permutation matrix.\n\n\n\n\n\n\n\n\n\nSingular/Non-Singular Matrix\n\n\n\n\n\nA singular matrix is one who’s determinant is zero. These cannot be inverted.\nA non-singular matrix is one who’s determinant is not zero. These can be inverted. For non-singular matrices:\n\\[\nAA^{-1} = I\n\\]\n\n\n\n\n\n\n\n\n\nBlock/Partitioned/Block Diagonal Matrix\n\n\n\n\n\nA block or partitioned matrix is a matrix which contains matrices within.\n\\[\n\\mathbf A_{4 \\times 4} = \\begin{pmatrix}\n\\mathbf A_{2 \\times 2} & \\mathbf B_{2 \\times 2} \\\\\n\\mathbf C_{2 \\times 2} & \\mathbf D_{2 \\times 2}\n\\end{pmatrix}\n\\]\n\nNote how the block matrix is 4 by 4, since if we expand out each matrix within, we would get a 4 by 4 matrix.\n\nA block diagonal matrix is a block/partitioned matrix with only matrices on its diagonal:\n\\[\n\\mathbf A_{4 \\times 4} = \\begin{pmatrix}\n\\mathbf A_{2 \\times 2} & 0 \\\\\n0 & \\mathbf D_{2 \\times 2}\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nOrthogonal/Orthonormal Matrix\n\n\n\n\n\nAn orthogonal matrix is one with columns perpendicular to each other (when treating each column as a vector). In other words, the dot product of any two columns is zero.\n\\[\n\\mathbf A_{3 \\times 3} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 5 \\\\\n0 & 3 & 0\n\\end{pmatrix}\n\\]\n\nThe identity matrix is also orthogonal.\nAny matrix with one element in each row and column will be orthogonal.\n\nAn orthonormal matrix is an orthogonal matrix but the lengths/norms of all the columns is 1:\n\\[\n\\mathbf A_{3 \\times 3} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\nMatrix Transpose\nThe matrix transpose is a matrix flipped along its diagonal. It is denoted either \\(\\mathbf A^\\mathsf{T}\\) or \\(\\mathbf A '\\).\nIn other words, the rows and column locations of each element are inverted (essentially elements \\(a^\\mathsf{T}_{ij} = a_{ji}\\)):\n\\[\n\\begin{pmatrix}\n2 & 3 & 5 \\\\\n1 & 4 & 6\n\\end{pmatrix}^\\mathsf{T} = \\begin{pmatrix}\n2 & 1 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{pmatrix}\n\\]\n\nNotice how the first column became the first row, the second column became the second row.\nYou can also get the transpose of a vector.\n\n\n\n\n\n\n\nProperties of Transposes\n\n\n\n\n\nThe Vector Property says that the dot product of vectors can be written with transposes:\n\\[\n\\mathbf a \\cdot \\mathbf b = \\mathbf a^\\mathsf{T}\\mathbf b\n\\]\nThe Inverse Property says that the transpose of a transpose is the original matrix:\n\\[\n\\left(\\mathbf A^\\mathsf{T} \\right)^\\mathsf{T} = \\mathbf A\n\\]\nThe Addition Property states that the transpose of a sum of two matrices, is equal to the individual transposes of both matrices added together:\n\\[\n(\\mathbf A + \\mathbf B)^\\mathsf{T} = \\mathbf A^\\mathsf{T} + \\mathbf B^\\mathsf{T} = \\mathbf B^\\mathsf{T} + \\mathbf A^\\mathsf{T}\n\\]\nThe Multiplication Property says the following (note the order of multiplication):\n\\[\n( \\mathbf{AB})^\\mathsf{T} = \\mathbf B^\\mathsf{T} \\mathbf A^\\mathsf{T}\n\\]\nThe Symmetrical Property says that a matrix that is symmetrical does not change when inversed:\n\\[\n\\mathbf A^\\mathsf{T} = \\mathbf A \\quad \\text{s.t.} \\quad \\mathbf A \\text{ is symmetrical}\n\\]\nThe Inverse Transpose Property says the following about inverses and transposes:\n\\[\n\\left( \\mathbf A^{-1} \\right)^\\mathsf{T} = \\left( \\mathbf A^\\mathsf{T} \\right)^{-1}\n\\]\n\n\n\n\n\n\nMatrix Algebra\nMatrix addition/subtraction is the same as vector addition - add the respective elements together:\n\\[\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{pmatrix} + \\begin{pmatrix}\n5 & 6 \\\\\n7 & 8\n\\end{pmatrix} = \\begin{pmatrix}\n1 + 5 & 2 +6 \\\\\n3+7 & 4 + 8\n\\end{pmatrix} = \\begin{pmatrix}\n6 & 8 \\\\\n10 & 12\n\\end{pmatrix}\n\\]\nMatrix scalar multiplication is the same as vector scalar multiplication - multiply each element by the scalar:\n\\[\n3 \\times \\begin{pmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{pmatrix} = \\begin{pmatrix}\n3 \\times 1 & 3 \\times 2 \\\\\n3 \\times 3 & 3 \\times 4\n\\end{pmatrix} = \\begin{pmatrix}\n3 & 6 \\\\\n9 & 12\n\\end{pmatrix}\n\\]\nMatrix Plain Multiplication is a little more complicated. Let us say you want to multiply \\(\\mathbf A\\) and \\(\\mathbf B\\) to get a new matrix \\(\\mathbf C\\). The elements of \\(\\mathbf C\\) are calculated as follows:\n\\[\nc_{ij} = \\sum_ka_{ik}b_{kj} = a_{i1}b_{1j} + a_{i2}b_{2j} + a_{i3}b_{3j}\\dots\n\\]\nIn other words, \\(c_{ij}\\) is the dot product of the \\(i\\)th row of \\(\\mathbf A\\), and the \\(j\\)th column of \\(\\mathbf B\\).\n\n\n\n\n\n\nExample of Matrix Multiplication\n\n\n\n\n\nLet us solve the following problem:\n\\[\n\\begin{pmatrix}\n2 & 1 \\\\\n3 & 5 \\end{pmatrix} \\begin{pmatrix}\n6 & 1 \\\\\n2 & 3 \\end{pmatrix} = \\mathbf C\n\\]\nLet us do each dot product for each element of \\(\\mathbf C\\):\n\n\\(c_{11}\\) is the dot product of the 1st row of the 1st matrix, and the 1st column of the 2nd matrix: \\((2 \\ 1) \\cdot (6 \\ 2)\\). That means \\(c_{11} = 2 \\times 6 + 1 \\times 2 = 12+2 = 14\\).\n\\(c_{12}\\) is the dot product of the 1st row of the 1st matrix, and the 2nd column of the 2nd matrix: \\((2 \\ 1) \\cdot (1 \\ 3)\\). That means \\(c_{12} = 2 \\times 1 + 1 \\times 3 = 2 + 3 = 5\\).\n\\(c_{21}\\) is the dot product of the 2nd row of the 1st matrix, and the 1st column of the 2nd matrix: \\((3 \\ 5) \\cdot (6 \\ 2)\\). That means \\(c_{21} = 3 \\times 6 + 5 \\times 2 = 18 + 10 = 28\\).\n\\(c_{22}\\) is the dot product of the 2nd row of the 1st matrix, and the 2nd column of the 2nd matrix: \\((3 \\ 5) \\cdot (1 \\ 3)\\). That means \\(c_{22} = 3 \\times 1 + 5 \\times 3 = 3 + 15 = 18\\).\n\nThus, we now have our answer:\n\\[\n\\begin{pmatrix}\n2 & 1 \\\\\n3 & 5 \\end{pmatrix} \\begin{pmatrix}\n6 & 1 \\\\\n2 & 3 \\end{pmatrix} = \\begin{pmatrix}\n14 & 5 \\\\\n28 & 18 \\end{pmatrix}\n\\]\n\n\n\nMatrix multiplication is only possible when the number of columns in \\(\\mathbf A\\) is equal to the number of rows in \\(\\mathbf B\\). So for example, we can multiply \\(\\mathbf A_{2 \\times 3}\\) and \\(\\mathbf B_{3 \\times 4}\\). We cannot multiply \\(\\mathbf A_{2 \\times 3}\\) and \\(\\mathbf B_{2 \\times 3}\\).\nThe dimensions of product \\(\\mathbf C\\) is the number of rows in \\(\\mathbf A\\) and the number of columns in \\(\\mathbf B\\). So for example, if we multiply \\(\\mathbf A_{2 \\times 3}\\) and \\(\\mathbf B_{3 \\times 4}\\), we will get \\(\\mathbf C_{2 \\times 4}\\).\n\n\n\n\n\n\nProperties of Matrix Algebra\n\n\n\n\n\nThe Associative property applies to addition/subtraction and multiplication:\n\\[\n\\begin{split}\n& (\\mathbf A + \\mathbf B) + \\mathbf C = \\mathbf A + (\\mathbf B + \\mathbf C) \\\\\n& (\\mathbf A \\mathbf B)\\mathbf C = \\mathbf A(\\mathbf B \\mathbf C)\n\\end{split}\n\\]\nThe Distributive Property states the following is true:\n\\[\n(\\mathbf A + \\mathbf B) \\mathbf C = \\mathbf A \\mathbf C + \\mathbf B \\mathbf C\n\\]\nThe Commutative Property applies only to addition/subtraction, not multiplication. Commutative property also applies to dot products.\n\\[\n\\begin{split}\n& \\mathbf A + \\mathbf B = \\mathbf B + \\mathbf A \\\\\n& \\mathbf a \\cdot \\mathbf b = \\mathbf b \\cdot \\mathbf a\n\\end{split}\n\\]\nMatrix Multiplication does not have the commutative property: \\(\\mathbf A \\mathbf B ≠ \\mathbf B \\mathbf A\\). Although there are two exceptions: \\(\\mathbf A \\mathbf I = \\mathbf I \\mathbf A\\), and \\(\\mathbf A \\mathbf A^{-1} = \\mathbf A^{-1}  \\mathbf A\\).\n\n\n\n\n\n\nKronecker Product\nTake the Kronecker Product of \\(\\mathbf A\\) and \\(\\mathbf B\\):\n\\[\n\\mathbf A \\otimes \\mathbf B = \\mathbf C\n\\]\nLet us define \\(\\mathbf A\\) and \\(\\mathbf B\\) as the following:\n\\[\n\\mathbf A_{2 \\times 2} = \\begin{pmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{pmatrix}, \\ \\mathbf B_{2 \\times 2} = \\begin{pmatrix}\nb_{11} & b_{12} \\\\\nb_{21} & b_{22}\n\\end{pmatrix}\n\\]\nThe resulting Kronecker Product \\(\\mathbf C\\) would be defined as a block matrix:\n\\[\n\\mathbf C_{4 \\times 4} = \\begin{pmatrix}\na_{11}\\mathbf B & a_{12} \\mathbf B \\\\\na_{21} \\mathbf B & a_{22} \\mathbf B\n\\end{pmatrix}\n\\]\nEssentially, we treat \\(\\mathbf A\\) as a collection of scalars. We scalar multiply each scalar element of \\(\\mathbf A\\) by the matrix of \\(\\mathbf B\\).\nIf \\(\\mathbf A\\) has dimensions \\(n \\times m\\), and \\(\\mathbf B\\) has dimensions \\(p \\times q\\), then \\(\\mathbf C\\) will have dimensions \\(np \\times mq\\).\n\n\n\nTraces and Determinants\nThe trace of \\(\\mathbf A\\) is a sum of all diagonal elements. Traces are used in Eigenvalues.\n\\[\nTr(\\mathbf A) = \\sum_i a_{ii} = a_{11} + a_{22} + \\dots\n\\]\n\n\n\n\n\n\nProperty of Traces\n\n\n\n\n\nThe Addition Property of traces states that the trace of the sum of two matrices is equivalent to the sum of the traces of each individual matrix:\n\\[\nTr(\\mathbf A + \\mathbf B) =Tr(\\mathbf A) + Tr(\\mathbf B) = Tr(\\mathbf B) + Tr(\\mathbf A)\n\\]\nThe Transpose Property says the trace of the transpose is equal to the trace of the original (since the diagonal remains the same):\n\\[\nTr(\\mathbf A^\\mathsf{T}) = Tr(\\mathbf A)\n\\]\nThe Multiplication Property says that the trace of multiplication has the commutative property (only for two matrices):\n\\[\nTr(\\mathbf{AB}) = Tr(\\mathbf{BA})\n\\]\n\n\n\nDeterminants tell us if a matrix is singular (and thus has no inverse). If the determinant is 0, then the matrix is singular. The determinant is only computable for square matrices. For a 2 by 2 matrix:\n\\[\n|\\mathbf A_{2 \\times 2}| = \\left| \\begin{pmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{pmatrix} \\right | = a_{11} a_{22} - a_{12}a_{21}\n\\]\n\n\n\n\n\n\nExample of a Determinant\n\n\n\n\n\nLet us find the determinant of the following matrix:\n\\[\n\\mathbf B_{2 \\times 2} = \\begin{pmatrix}\n2 & 3 \\\\\n1 & 4\n\\end{pmatrix}\n\\]\nUsing the formula above:\n\\[\n|\\mathbf B| = 2 \\times 4 - 3 \\times 1 = 8 - 3 = 5\n\\]\nThus, this matrix is non-singular.\n\n\n\nFor 3 by 3, there is a method called the butterfly method to find the determinant. Take this matrix.\n\\[\n\\mathbf A_{3 \\times 3} = \\begin{pmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{pmatrix}\n\\]\nThe determinant is defined as following:\n\\[\n\\begin{split}\n| \\mathbf A| = & a_{11}a_{22}a_{33} + a_{12} a_{23} a_{31} + a_{13}a_{21}a_{32} \\\\\n& -a_{31}a_{22}a_{13} - a_{11}a_{23}a_{32} - a{12}a_{21}a_{33}\n\\end{split}\n\\]\n\n\n\n\n\n\nProperties of Determinants\n\n\n\n\n\nThe Transpose Property states that the determinant of a transpose is equal to the determinant of the original:\n\\[\n\\det(\\mathbf A^\\mathsf{T}) = \\det(\\mathbf A)\n\\]\nThe Identity Property states that the determinant of an identity matrix is 1:\n\\[\n\\det (\\mathbf I)=1\n\\]\nThe Multiplication Property states that the determinant of a product is equal to the individual determinants multiplied:\n\\[\n\\det (\\mathbf {AB}) = \\det (\\mathbf A) \\det (\\mathbf B)\n\\]\nThe Inverse Property says that the determinant of an inverse is the inverse of the determinant of the original matrix:\n\\[\n\\det(\\mathbf A^{-1}) = \\frac{1}{\\det(\\mathbf A)}\n\\]\nThe Triangular/Diagonal Property is the product of all diagonal elements:\n\\[\n\\det (\\mathbf A) = \\prod_i a_{ii}\n\\]\n\n\n\n\n\n\nLaplace Expansion and Cofactors\nFor anything larger than a 3 by 3 matrix, we should us a Laplace expansion to find the determinant.\nFirst, you choose a row or column of the matrix.\n\nFor every element in that row or column, find the submatrix of that element.\nCalculate the determinant of each of the submatrices. This is called the minor.\nNow, convert the minors to cofactors. The cofactor is the minor times \\((-1)^{i+j}\\).\nThen, take each element, multiply by its cofactor. Sum all of these products together.\n\nThe final sum is the determinant of the matrix.\n\n\n\n\n\n\nExample of Laplace Expansion\n\n\n\n\n\nFor example, take this matrix:\n\\[\n\\mathbf A = \\begin{pmatrix}\n1 & 2 & 1 \\\\\n0 & 1 & 1 \\\\\n5 & 3 & 0\n\\end{pmatrix}\n\\]\nLet us expand over the 1st row \\((1 \\ 2 \\ 1 )\\). We expand over the submatrices of each element in that row.\n\nFor \\(a_{11} = 1\\), the submatrix is \\(\\begin{pmatrix} 1 & 1 \\\\ 3 & 0 \\end{pmatrix}\\), and the determinant/minor of that is \\(1 \\times 0 - 1 \\times 3 = -3\\).\nFor \\(a_{12} = 2\\), the submatrix is \\(\\begin{pmatrix} 0 & 1 \\\\ 5 & 0 \\end{pmatrix}\\). The determinant/minor of that is \\(0 - 5 = -5\\).\nFor \\(a_{13} = 1\\), the submatrix is \\(\\begin{pmatrix} 0 & 1 \\\\ 5 & 3 \\end{pmatrix}\\). The determinant/minor of that is \\(0 - 5 = -5\\).\n\nNow, let us find the cofactors \\((-1)^{i + j} \\times \\text{minor}\\):\n\nFor \\(a_{11}\\), the cofactor is \\((-1)^2 \\times -3 = 1 \\times -3 = -3\\).\nFor \\(a_{12}\\), the cofactor is \\((-1)^3 \\times -5 = -1 \\times -5 = 5\\).\nFor \\(a_{13}\\), the cofactor is \\((-1)^4 \\times -5 = 1 \\times -5 = -5\\).\n\nNow, take each element, multiply by its cofactor. Sum all of these products together.\n\\[\n1(-3) + 2(5) + 1(-5) = -3 + 10 - 5 = 2\n\\]\nThus, the determinant of the matrix is \\(2\\).\n\n\n\nThis works for any matrix of any size, for any row or any column.\n\nSo, you should choose rows/columns with more 0’s, since the final sum involves multiplying the element with the cofactor, and if the element is 0, then you don’t have to consider it.\n\n\n\n\nMatrix Inverse\nIf you take a matrix \\(\\mathbf A\\), and multiply by the inverse \\(\\mathbf A^{-1}\\), the result will be the identity matrix \\(\\mathbf{I}\\).\nYou can invert any square matrix that does not have a determinant of a 0. This is because the inverse is defined as the following:\n\\[\n\\mathbf A^{-1} = \\frac{1}{|\\mathbf A|} \\mathbf C^\\mathsf{T}\n\\]\n\nWhere \\(| \\mathbf A|\\) is the determinant of the matrix \\(\\mathbf A\\), and \\(\\mathbf C^\\mathsf{T}\\) is the transpose of the cofactor matrix (consisting of the cofactor of every element of \\(\\mathbf A\\).\n\n\n\n\n\n\n\nExample of a 2 by 2 Matrix Inverse\n\n\n\n\n\nLet us solve for the matrix inverse of a 2 by 2 matrix.\n\\[\n\\mathbf A_{2 \\times 2} = \\begin{pmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{pmatrix}\n\\]\nWe know the determinant of \\(\\mathbf A\\) with the formula for 2 by 2 matrix determinants:\n\\[\n| \\mathbf A | = a_{11}a_{22} - a_{12} a_{21}\n\\]\nNow, let us find the cofactors (note, the determinant of a scalar is just the scalar):\n\n\\(c_{11} = (-1)^{1+1}a_{22} = a_{22}\\)\n\\(c_{12} = (-1)^{1+2}a_{21} = -a_{21}\\)\n\\(c_{21} = (-1)^{2+1} a_{12} = -a_{12}\\)\n\\(c_{22} = (-1)^{2+2} a_{11} = a_{11}\\)\n\nThus, our cofactor matrix is:\n\\[\n\\mathbf C_{2 \\times 2} = \\begin{pmatrix}\na_{22} & -a_{21} \\\\\n-a_{12} & a_{11}\n\\end{pmatrix}\n\\]\nThe transpose of the cofactor matrix is thus (flipping rows to columns):\n\\[\n\\mathbf C^\\mathsf{T} = \\begin{pmatrix}\na_{22} & -a_{12} \\\\\n-a_{21} & a_{11}\n\\end{pmatrix}\n\\]\nThus, the inverse is:\n\\[\n\\mathbf A^{-1} = \\frac{1}{|\\mathbf A|} \\mathbf C^\\mathsf{T} = \\frac{1}{a_{11}a_{22} - a_{12} a_{21}} \\begin{pmatrix}\na_{22} & -a_{12} \\\\\n-a_{21} & a_{11}\n\\end{pmatrix}\n\\]\nThus, for example, the following is true:\n\\[\n\\mathbf A = \\begin{pmatrix}\n3 & 1 \\\\\n5 & 2\n\\end{pmatrix}, \\ \\mathbf A^{-1}  = \\begin{pmatrix}\n2 & -1 \\\\\n-5 & 3\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nProperties of Inverses\n\n\n\n\n\nThe Inverse Property states that the inverse of an inverse is the original matrix:\n\\[\n\\left( \\mathbf A^{-1} \\right)^{-1} = \\mathbf A\n\\]\nThe Multiplication Property states the inverse of a product is the following (note the order of the multiplication):\n\\[\n(\\mathbf{AB})^{-1}=\\mathbf B^{-1} \\mathbf A^{-1}\n\\]\nThe Scalar Multiplication Property states that the scalar product is the following:\n\\[\n(c \\mathbf A)^{-1} = \\frac{1}{c} \\mathbf A^{-1}, \\quad \\text{s.t.} \\quad  c ≠ 0\n\\]\n\n\n\n\n\n\n\n\n\nVector Spaces\n\nLinear Mapping\nA mapping is any rule that maps elements from one set to another. A function \\(f\\) is a mapping \\(f: A \\rightarrow B\\).\nA linear mapping is a mapping that is linear, which must meet the following properties:\n\n\\(f(a+b) = f(a) + f(b)\\)\n\\(f(ca) = c f(a)\\)\n\nWe can represent linear mappings for finite sets by matrices. Let us say \\(\\mathbf X_{n \\times m}\\) is a matrix, and \\(\\mathbf y_{m}\\) is a vector. Let us find their product:\n\\[\n\\mathbf {Xy} = \\mathbf z_m\n\\]\nWhat this is saying is take the vector \\(\\mathbf y\\), and operate on it using the matrix \\(\\mathbf X\\), to produce a new vector \\(\\mathbf z\\). Here, the matrix \\(\\mathbf X\\) is an operator that maps \\(\\mathbf y \\rightarrow \\mathbf z\\).\n\n\n\nLinear Combinations and Independence\nA linear combination is a combination of vectors that is linear (i.e. vectors can be added, and scalar multiplied). For example, this is a linear combination:\n\\[\nt \\mathbf x + (1-t) \\mathbf y\n\\]\nLinear combinations either represents lines (in \\(\\mathbb R^2\\)), planes (in \\(\\mathbb R^3\\)), and hyperplanes (a plane of one less dimensions than the space) in higher dimensions.\nNow, let us take some linear combination:\n\\[\na_1 \\mathbf x_1 + a_2 \\mathbf x_2+\\dots + a_n \\mathbf x_n\n\\]\nThis set of vectors \\((\\mathbf x_1, \\dots, \\mathbf x_n)\\) is linearlly independent if you cannot go from one vector \\(\\mathbf x_j \\in \\{ \\mathbf x_1, \\dots, \\mathbf x_n \\}\\), and linearly transform it (by adding/subtracting/multiplying a constant) into another vector.\n\n\n\n\n\n\nExample of Linear Independence\n\n\n\n\n\nLet us say we have these two vectors:\n\\[\n\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n\\]\nAre these linearly independent? That means I cannot use a linear transformation to go from one to another.\nNo, there is no constant you can multiply to get from vector 1 to vector 2, and there are no other vectors to add/subtract to to go from one to another.\nNow consider these two vectors:\n\\[\n\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix}\n\\]\nThese are not independent - you can multiply the first vector by a scalar of 2 to get the second vector.\n\n\n\nThis can be complicated to see in higher dimensions (since it is hard to consider how multiple vectors can be combined to match another). There, we use the matrix rank (see below).\n\n\n\nSpanning Vectors and Dimension\nA collection of spanning vectors spans some space, if you can write any vector in that space, as a linear combination of the spanning vectors.\nFor example, take vector \\(\\mathbf e_1 = (1 \\ 0)\\) and \\(\\mathbf e_2 = (0 \\ 1)\\). These vectors span some space including \\(\\mathbf z\\), if vector \\(\\mathbf z\\) can be written as:\n\\[\n\\mathbf z = a \\mathbf e_1 + b \\mathbf e_2, \\quad \\text{e.x.} \\quad \\mathbf z = (a \\ b)\n\\]\n\nIn fact, \\(\\mathbf e_1\\) and \\(\\mathbf e_2\\) span the set of all 2-dimensional vectors.\n\nThis allows us to write vectors in terms of the core vectors, and to understand the dimension of the space. The dimension of the space is the smallest number of linearly independent vectors that span the space.\n\n\n\n\n\n\nExample of Dimensionality\n\n\n\n\n\nTake these two vectors:\n\\[\n\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\]\nThese are linearly independent - no factor multipled can get the other vector. Thus, this is 2 dimensional space\nNow, let us add a third vector.\n\\[\n\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}\n\\]\nIs the third vector linearly independent? No. We can write the third vector with a combination of the other two:\n\\[\n\\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix} = 3 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + 2 \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\]\nThus, the third vector is in the space spanned by the first 2 vectors. The first two vectors spans this space, and thus, it is 2 dimensional space.\n\n\n\nGenerally, the dimensionality of the space matches the number of vectors that span the space (so a 2 dimensional space is often spanned by 2 vectors, 3 spanned by 3, etc.).\nNote: Dimensionality of a vector space is not always the same as the dimensionality of the vectors.\n\n\n\nMatrix Rank\nAs we discussed before, it can be difficult to find if vectors are linearly independent in higher dimensions.\nWe can stack the row vectors into a matrix (we can also do them in columns):\n\\[\n\\begin{pmatrix} \\mathbf x_1 \\\\ \\mathbf x_2 \\\\ \\mathbf x_3 \\end{pmatrix} =\n\\begin{pmatrix} x_{11} & x_{12} & x_{13} \\\\\nx_{21} & x_{22} & x_{23} \\\\\nx_{31} & x_{32} & x_{33} \\end{pmatrix}\n\\]\nThe Rank of a matrix is the number of linearly independent rows/columns in a matrix.\nIf the Rank is equal to the total number of rows/columns (all rows/columns are linearly independent), the matrix has full rank. A matrix with full rank is non-singular, and thus, can be inverted, and has a non-zero determinant.\nThus, if we find the determinant of the matrix, if it is 0, the vectors are not linearly independent, and if it is not 0 , they are linearly independent.\nWe can also know a matrix is not full rank, if the space of the dimension is less than the number of vectors (as explained above).\n\n\n\nQuadratic Forms\nA quadratic form takes the following form:\n\\[\n\\mathbf x^\\mathsf{T} \\mathbf A \\mathbf x\n\\]\nWhere matrix \\(\\mathbf A\\) is a square matrix.\nIf \\(\\mathbf x^\\mathsf{T} \\mathbf A \\mathbf x &gt; 0\\) for all values, then the matrix is positive definite. If \\(\\mathbf x^\\mathsf{T} \\mathbf A \\mathbf x &lt; 0\\), then negative definite. If \\(\\mathbf x^\\mathsf{T} \\mathbf A \\mathbf x ≤ 0\\) it is positive semi-definite.\nThis will be useful for optimisation and multivariable calculus\n\n\n\n\n\n\nSolving Systems of Equations\n\nMatrices and Systems of Equations\nYou can write any system of linear equations as a matrix times a vector. Let us take this set of equations:\n\\[\n\\begin{cases}\na_{11}x_1 + a_{12} x_2 + a_{13} x_3 = y_1 \\\\\na_{21}x_1 + a_{22}x_2 + a_{23}x_3 = y_2 \\\\\na_{31}x_1 + a_{32}x_2 + a_{33}x_3 = y_3\n\\end{cases}\n\\]\nWe can write this system of equations as follows:\n\\[\n\\begin{pmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}\n= \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix}\n\\]\nOr even simpler, we can represent it as:\n\\[\n\\mathbf A\\mathbf x = \\mathbf y\n\\]\nA unique solution exists when you have the same amount of equations as unknowns, and the equations are non-contradictory.\nOverdetermined systems are when there are more equations than unknowns, which might contradict each other. Underdetermined systems are when there are not enough equations compared to unknowns, so we cannot solve it.\n\n\n\n\n\n\nNon-Linearly Independent and Identification\n\n\n\n\n\nTake this system of linear equations:\n\\[\n\\begin{cases}\nx+y=1 \\\\\n2x + 2y = 2\n\\end{cases}\n\\]\nWe can see that the two equations are a common factor of each other. Or in other words, these two vectors are non-linearly independent.\nWe know when solving for these equations, we cannot actually solve for a unique solution.\nWe know if a matrix is full rank, then all rows/columns are linearly independent.\n\n\n\n\n\n\nSolving Systems of Equations\nMatrix inversion is a way to solve a system of equations. Take this system of equations:\n\\[\n\\mathbf{Ax} = \\mathbf y\n\\]\nYou can solve for \\(\\mathbf x\\) by inverting matrix \\(\\mathbf A\\) (assuming matrix a is full rank):\n\\[\n\\begin{align}\n\\mathbf{Ax} & = \\mathbf y \\\\\n\\color{blue}{\\mathbf{A}^{-1}}\\color{black}{\\mathbf{Ax}} & = \\color{blue}{\\mathbf A^{-1}}\\color{black}{\\mathbf y} && (\\text{multiply both sides by } \\color{blue}{\\mathbf A^{-1}}\\color{black}) \\\\\n\\mathbf x &= \\mathbf A^{-1}\\mathbf y && (\\mathbf A^{-1} \\mathbf A\\text{ inverses cancel})\n\\end{align}\n\\]\n\n\n\n\n\n\nExample of Matrix Inversion\n\n\n\n\n\nTake this system of equations:\n\\[\n\\begin{cases}\n3x - 7y = -11 \\\\\n5x + 10y = 25\n\\end{cases}\n\\]\nWe can write this in linear algebra:\n\\[\n\\begin{pmatrix}\n3 & -7 \\\\\n5 & 10\n\\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} =\n\\begin{pmatrix} -11 \\\\ 25 \\end{pmatrix}\n\\]\nNow, let us find the inverse of the first matrix:\n\\[\n\\mathbf A^{-1} = \\frac{1}{|\\mathbf A|}\\mathbf C^\\mathsf{T}\n\\]\nWe know the determinant \\(|\\mathbf A| = 3(10) - (-7)(5) = 65\\).\nNow, let us find the cofactor matrix:\n\n\\(c_{11} = (-1)^{1+1}\\times 10 = 10\\)\n\\(c_{12} = (-1)^{1+2} \\times 5 = -5\\)\n\\(c_{21} = (-1)^{2+1} \\times -7 = 7\\)\n\\(c_{22} = (-1)^{2+2} \\times 3 = 3\\)\n\nThus, the cofactor matrix transposed should be:\n\\[\n\\mathbf C^\\mathsf{T} = \\begin{pmatrix} 10 & -5 \\\\ 7 & 3 \\end{pmatrix}^\\mathsf{T} = \\begin{pmatrix} 10 & 7 \\\\ -5 & 3 \\end{pmatrix}\n\\]\nThus, the inverse of matrix \\(\\mathbf{A}\\) should be:\n\\[\n\\mathbf A^{-1} = \\frac{1}{65} \\begin{pmatrix} 10 & 7 \\\\ -5 & 3 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{13} & \\frac{7}{65} \\\\ -\\frac{1}{13} & \\frac{3}{65} \\end{pmatrix}\n\\]\nWe know the solution should be:\n\\[\n\\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{13} & \\frac{7}{65} \\\\ -\\frac{1}{13} & \\frac{3}{65} \\end{pmatrix} \\begin{pmatrix} -11 \\\\ 25 \\end{pmatrix}\n\\]\nThus, doing matrix multiplication to obtain \\(x\\) and \\(y\\):\n\n\\(x =\\frac{2}{13}(-11) + \\frac{7}{65}(25) = -\\frac{22}{13} + \\frac{35}{13} = \\frac{13}{13}=1\\)\n\\(y=-\\frac{1}{13}(-11) + \\frac{3}{65}(25) = \\frac{11}{13}+ \\frac{15}{13} = \\frac{26}{13} = 2\\)\n\nThe solution to this system of equations is: \\((1, 2)\\).\n\n\n\nCramer’s rule is another rule to solve equations, only for square matrices. It is not super commonly used. Given the system of equations:\n\\[\n\\mathbf{Ax} = \\mathbf y\n\\]\nThe element \\(x_i\\) is defined as:\n\\[\nx_i = \\frac{|\\mathbf B_i|}{\\mathbf A}\n\\]\nWhere \\(\\mathbf B_i\\) is a matrix obtained by taking the matrix \\(\\mathbf A\\), and replacing the \\(i\\)th column with the column vector \\(\\mathbf y\\).\n\n\n\n\n\n\nEigenvalues and Eigenvectors\n\nDefinitions\nA matrix, as we discussed, can act as an operator that maps some vector to another \\(\\mathbf A : \\mathbf x \\rightarrow \\mathbf y\\).\nGeometrically, vector \\(\\mathbf x\\) points in some dimensional space. Then the matrix \\(\\mathbf A\\) comes along, and transforms it into a different vector \\(\\mathbf x\\), which might flip the direction, or rotate, or change its norm, etc. Sometimes, \\(\\mathbf y\\) will stay in the same direction or opposite direction (along the same line) as the original, even after the operator.\nBasically, an eigenvector of matrix \\(\\mathbf A\\) is a vector \\(\\mathbf x\\), that does not change its direction (stays on the same line) when you apply the operator \\(\\mathbf A\\) to get vector \\(\\lambda \\mathbf x\\). Mathematically:\n\\[\n\\mathbf{Ax} = \\lambda \\mathbf x\n\\]\nThe \\(\\lambda\\) is an eigenvalue that corresponds to the eigenvector.\n\n\n\nComputing Eigenvalues\nLet us start with the eigenvector formula:\n\\[\n\\mathbf{Ax} = \\lambda \\mathbf x\n\\]\nWe can multiply the right side by the identity matrix (which does not change the result):\n\\[\n\\mathbf{Ax} = \\lambda \\mathbf {Ix}\n\\]\nNow, let us move everything to one side, and simplify:\n\\[\n\\begin{split}\n\\mathbf{Ax} - \\lambda \\mathbf {Ix} & = 0 \\\\\n(\\mathbf A - \\lambda \\mathbf I) \\mathbf x & = 0\n\\end{split}\n\\]\n\\((\\mathbf A - \\lambda \\mathbf I)\\) is an singular matrix, meaning determinant \\(|(\\mathbf A - \\lambda \\mathbf I)| = 0\\). All values of \\(\\lambda\\) that solve this determinant equation will be eigenvalues of the matrix.\n\n\n\n\n\n\nExample of Solving for Eigenvalues\n\n\n\n\n\nWe know \\(|(\\mathbf A - \\lambda \\mathbf I)| = 0\\). Let us say:\n\\[\n\\mathbf A = \\begin{pmatrix} 2 & 1 \\\\ 3 & 4 \\end{pmatrix}, \\quad \\lambda \\mathbf I \\begin{pmatrix} \\lambda & 0 \\\\ 0 & \\lambda \\end{pmatrix}\n\\]\nThus:\n\\[\n\\mathbf A - \\lambda \\mathbf I = \\begin{pmatrix} 2 - \\lambda & 1 \\\\ 3 & 4 - \\lambda \\end{pmatrix}\n\\]\nWe know the determinant should equal 0. We can solve for \\(\\lambda\\):\n\\[\n\\begin{split}\n0 & = |(\\mathbf A - \\lambda \\mathbf I)| \\\\\n0 & = (2-\\lambda)(4-\\lambda) - 1(3) \\\\\n0 & = 8-2\\lambda - 4 \\lambda +\\lambda^2 - 4 \\\\\n0 & = \\lambda^2 - 6\\lambda +4 \\\\\n\\end{split}\n\\]\nLet us use the quadratic formula:\n\\[\n\\begin{split}\n\\lambda & = \\frac{-b ± \\sqrt{b^2 - 4ac}}{2a} \\\\\n\\lambda & = \\frac{6 ± \\sqrt{36 - 4(1)(4)}}{2(1)} \\\\\n\\lambda & = \\frac{6 ± \\sqrt{20}}{2} \\\\\n\\lambda & = 3 ± \\frac{\\sqrt{20}}{2} \\\\\n\\lambda & = 3 ± \\frac{2 \\sqrt{5}}{2} \\\\\n\\lambda & = 3 ± \\sqrt{5}\n\\end{split}\n\\]\nThus, we have found our eigenvalues.\n\n\n\n\n\n\nCalculating Eigenvectors\nLet us say we have the matrix \\(\\mathbf{A}\\):\n\\[\n\\mathbf A = \\begin{pmatrix} 2 & 3 \\\\ 2 & 1 \\end{pmatrix}\n\\]\nFirst, you need to calculate the eigenvalues \\(\\lambda\\) (as shown in the previous section).\n\n\n\n\n\n\nExample of Calculating Eigenvalues\n\n\n\n\n\nWe know \\(|(\\mathbf A - \\lambda \\mathbf I)| = 0\\).\n\\[\n\\mathbf A - \\lambda \\mathbf I = \\begin{pmatrix} 2 - \\lambda & 3 \\\\ 2 & 1 - \\lambda \\end{pmatrix}\n\\]\nNow solve \\(|(\\mathbf A - \\lambda \\mathbf I)| = 0\\).\n\\[\n\\begin{split}\n0 & = |(\\mathbf A - \\lambda \\mathbf I)| \\\\\n0 & = (2 - \\lambda)(1- \\lambda) - 2(3) \\\\\n0 & = 2 -2 \\lambda - \\lambda + \\lambda^2 - 6 \\\\\n0 & = \\lambda ^2 - 3\\lambda - 4 \\\\\n0 & = (\\lambda - 4)(\\lambda + 1) \\\\\n\\lambda & = 4, -1\n\\end{split}\n\\]\n\n\n\nFor every eigenvalue, you will have an eigenvector that makes the eigenvector equation \\(\\mathbf{Ax} = \\lambda \\mathbf x\\) true. From above:\n\\[\n\\begin{split}\n& \\mathbf{Ax} = 4 \\mathbf x \\\\\n& \\mathbf{Ax} = -1 \\mathbf x\n\\end{split}\n\\]\nRemember, \\(\\mathbf x\\) is a vector here of 2 elements, \\(x_1, x_2\\) - but we only have one equation for each eigenvalue pair. This means there will be one solution without a unique solution.\n\n\n\n\n\n\nEigenvalues and Non-Unique Solutions\n\n\n\n\n\nIt might seem simple to just solve for our \\(\\mathbf x\\) with two unknowns \\(x_1, x_2\\).\nBut the issue is - our equations are not linearly independent, so we do not have enough information to solve for a unique solution for one.\nThis is actually okay - why? Well, take a look at these potential eigenvectors:\n\\[\n\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}\n\\]\nThese are different vectors - but they are all eigenvectors. This is because of the equation \\(\\mathbf{Ax} = \\lambda \\mathbf x\\) - if both sides have vector \\((1 \\ 0)\\) multiplied by a constant (like the 2nd and 3rd vectors), the equation is still equal.\nThus, eigenvectors are not defined uniquely - only up to a singular multiplicative constant.\n\n\n\nWhat we typically do is define \\(x_1 = 1\\) (there are some cases where this does not work). Using this, we can solve for the answer.\n\n\n\n\n\n\nExample of Calculating Eigenvectors\n\n\n\n\n\nLet us continue the same example from before.\n\\[\n\\mathbf A = \\begin{pmatrix} 2 & 3 \\\\ 2 & 1 \\end{pmatrix}, \\quad \\lambda = 4, -1, \\quad \\mathbf x = \\begin{pmatrix} 1 \\\\ c \\end{pmatrix}\n\\]\nLet us solve for the eigenvalue of \\(\\lambda = 4\\).\n\\[\n\\begin{split}\n\\mathbf{Ax} & = 4 \\mathbf x \\\\\n\\begin{pmatrix} 2 & 3 \\\\ 2 & 1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ c \\end{pmatrix} & = 4\\begin{pmatrix} 1 \\\\ c \\end{pmatrix} \\\\\n\\begin{pmatrix} 2+3c \\\\ 2+c \\end{pmatrix} & = \\begin{pmatrix} 4 \\\\ 4c \\end{pmatrix}\n\\end{split}\n\\]\nThat gives us two equations:\n\\[\n\\begin{split}\n& 2 + 3c = 4 \\\\\n& 2 + c = 4c\n\\end{split}\n\\]\nThe answer is \\(c = \\frac{2}{3}\\) (both answers give us the equation).\nThus, the eigenvector with \\(\\lambda = 4\\) is:\n\\[\n\\mathbf x = \\begin{pmatrix} 1 \\\\ \\frac{2}{3} \\end{pmatrix}\n\\]\nWe can do the same for \\(\\lambda = -1\\), and we will get eigenvector:\n\\[\n\\mathbf x = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n\\]\n\n\n\n\n\n\nEigenvector Decomposition\nMatrix decomposition is to take a matrix \\(\\mathbf A\\), and decompose it into other matrices, that when multiplied together, get the original matrix.\nIf matrix \\(\\mathbf A\\) has unique eigenvalues, you can write:\n\\[\n\\mathbf A = \\mathbf {QDQ}^{-1}\n\\]\nWhere \\(\\mathbf Q\\) is made up of eigenvectors of matrix \\(\\mathbf A\\), and \\(\\mathbf D\\) is a diagonal matrix with the eigenvalues \\(\\lambda\\) on the diagonal:\n\\[\n\\mathbf D = \\begin{pmatrix} \\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{pmatrix}, \\quad\n\\mathbf Q = \\begin{pmatrix} \\mathbf x_1 & \\mathbf x_2\\end{pmatrix}\n\\]\nEigenvectors decomposition has a few uses:\n\n\n\n\n\n\nTaking the Power of Matrices\n\n\n\n\n\nLet us say you want to find \\(\\mathbf A^z\\). We can use matrix decomposition for this:\n\\[\n\\mathbf A^z = {QDQ}^{-1}{QDQ}^{-1}{QDQ}^{-1}\\dots\n\\]\nNotice how the \\(\\mathbf{Q}^{-1} \\mathbf Q\\) occurs quite frequently, and we know by properties of inverses, that \\(\\mathbf{Q}^{-1} \\mathbf Q = \\mathbf I\\), and multiplying by \\(\\mathbf I\\) does nothing.\nThus, we can rewrite the formula above as:\n\\[\n\\mathbf A^z = \\mathbf{QD}^z\\mathbf Q^{-1}\n\\]\nThis is much simpler than calculating \\(\\mathbf A^z\\), since taking diagonal matrix to a power is defined as:\n\\[\n\\mathbf D^z = \\begin{pmatrix} \\lambda_1^z & 0 \\\\ 0 & \\lambda_2^z\\end{pmatrix}\n\\]\nWhich is much easier to do.\n\n\n\n\n\n\n\n\n\nFinding Determinants\n\n\n\n\n\nLet us say you want to find the determinant of \\(\\mathbf A\\). The following is true:\n\\[\n\\begin{split}\n\\det (\\mathbf A) & = \\det(\\mathbf{QD}^z\\mathbf Q^{-1}) \\\\\n& = \\det (\\mathbf Q) \\det (\\mathbf D) \\det (\\mathbf Q^{-1}) \\\\\n& = \\det (\\mathbf Q) \\det (\\mathbf D) \\frac{1}{\\det (\\mathbf Q)} \\\\\n& = \\det (\\mathbf D)\n\\end{split}\n\\]\nSince \\(\\mathbf D\\) is diagonal, the determinant of a diagonal matrix is just the product of the diagonal elements. Thus:\n\\[\n\\det (\\mathbf A) = \\prod_i \\lambda_i\n\\]\n\n\n\n\n\n\n\n\n\nPrinciple Components Analysis\n\n\n\n\n\nPrinciple Components Analysis (PCA) is used when matrix \\(\\mathbf A\\) is symmetric, positive semi-definite.\n\nThis means the eigenvectors are orthogonal (perpendicular) to each other.\nThis means eigenvalues will always be real and non-negative.\n\nPCA is done on the covariance matrix, which is symmetric and positive semi-definite.\nIt is a data reduction technique commonly used in statistics and data science.\n\nThe eigenvectors of the covariance matrix form the principle components of thee system.\nThe eigenvalues tell us how much of the variance each component explains (more important principle components have higher eigenvalues).\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Linear Algebra"
    ]
  },
  {
    "objectID": "2.html",
    "href": "2.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "In the last chapter, we discussed the basics of statistics, and briefly introduced regression as a way to find correlations.\nThis chapter dives deep into multiple linear regression, the foundational model for all of statistics. We cover the specification of the model, estimation and statistical inference, as well as extensions.\nUse the right sidebar for quick navigation. R-code is provided at the bottom.\n\n\nBasics of the Model\n\nModel Specification\nLet us say we have some outcome variable \\(y\\), and several explanatory variables \\(x_1, x_2, \\dots, x_k\\). We have data on \\(n\\) number of observations \\(i = 1, \\dots n\\).\nThe linear regression model can be written as a conditional expectation \\(E(y|x)\\) function:\n\\[\nE(y_i |x_i) = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_kx_{ki}\n\\]\nThe linear model can also be specified for any specific outcome value \\(y_i\\) for unit \\(i\\):\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_kx_{ki} + u_i\n\\]\nWe can also specify the linear model in terms of linear algebra:\n\\[\n\\begin{pmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\\end{pmatrix} =\n\\begin{pmatrix}1 & x_{11} & \\dots & x_{k1} \\\\1 & x_{12} & \\dots & x_{k2} \\\\\\vdots & \\vdots & \\vdots & \\vdots \\\\1 & x_{1n} & \\dots & x_{kn}\\end{pmatrix}\n\\begin{pmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_k\\end{pmatrix}\n+ \\begin{pmatrix}u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n\\end{pmatrix}\n\\]\n\\[\n\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u\n\\]\n\n\n\n\n\n\nMore Info on Conditional Expectations\n\n\n\n\n\nImagine \\(y\\) is income, and \\(x\\) is age.\nAt age \\(x=20\\), not every 20 year old makes the same amount of income. There is some distribution, with some making more, and some making less. This is the distribution \\(y|x=20\\).\nWe can find the expected value of this distribution, \\(E(y|x=20)\\). This is a conditional expectation, and indicates the expected income of a 20 year old if we randomly chose one from the distribution.\nAt \\(x=30\\), the \\(E(y|x)\\) probably is different (30 year olds make more money). Thus, the linear model is essentially stating that the expected \\(y\\) depends on \\(x\\). Or in terms of this example, the expected income depends on the individual’s age.\n\n\n\n\n\n\n\n\n\nMore Info on the Error Term \\(u_i\\)\n\n\n\n\n\nThe \\(u_i\\) is called the error term. This indicates that not every value of \\(y_i\\) in our data will be exactly on the linear best-fit line.\nGraphically, it is the highlighted part:\n\n\n\n\n\nIn social science terms, the \\(u_i\\) is the effect of any other variable not included in our model on \\(y\\).\nFor example, if \\(x\\) is age, and \\(y\\) is income, we will have the following relationship:\n\\[\n\\text{income}_i = \\beta_0 + \\beta_1 \\text{age}_i + u_i\n\\]\nHowever, not every individual lies perfectly on this linear line. This is because there are other factors outside of age that affect \\(y\\) (income), and these other factors are bundled into the error term.\n\n\n\n\n\n\nEstimation Process\nTo estimate the population parameters \\(\\beta_0, \\dots, \\beta_k\\), we use our sample data, and try to find the values \\(\\widehat{\\beta_0}, \\dots, \\widehat{\\beta_k}\\) that minimise the square sum of residuals (SSR):\n\\[\n\\begin{align}\nSSR & = \\sum\\limits_{i=1}^n(y_i - \\hat y_i)^2 \\\\\n& = \\sum\\limits_{i=1}^n(y_i - \\color{blue}{(\\widehat{\\beta_0} + \\widehat{\\beta_1}x_{1i} + \\dots + \\widehat{\\beta_k}x_{ki})}\\color{black})^2 && (\\text{plug in } \\color{blue}{\\hat y = \\widehat{\\beta_0} + \\widehat{\\beta_1}x_{1i} + \\dots }\\color{black}) \\\\\n& = \\sum\\limits_{i=1}^n(y_i - \\widehat{\\beta_0} - \\widehat{\\beta_1}x_{1i} - \\dots - \\widehat{\\beta_k}x_{ki})^2 &&(\\text{distribute negative sign})\n\\end{align}\n\\]\nIn the linear algebra representation (where \\(\\mathbf b\\) is the vector of estimated parameters \\(\\widehat{\\beta_0}, \\dots, \\widehat{\\beta_k}\\)):\n\\[\n\\begin{align}\nS(\\hat{\\boldsymbol\\beta}) & = (\\mathbf y - \\hat{\\mathbf y})^\\mathsf{T} (\\mathbf y - \\hat{\\mathbf y})\\\\\n& = (\\mathbf y - \\color{blue}{\\mathbf X \\hat{\\boldsymbol\\beta}}\\color{black})^\\mathsf{T} (\\mathbf y - \\color{blue}{\\mathbf{X} \\hat{\\boldsymbol\\beta}}\\color{black}) && (\\text{plug in } \\color{blue}{\\hat{\\mathbf y}  = \\mathbf X \\hat{\\boldsymbol\\beta}}\\color{black}) \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\nIntuitive Visualisation of SSR\n\n\n\n\n\nThe residuals are the difference from our predicted best-fit line result \\(\\widehat{y_i}\\), and the actual value of \\(y_i\\) in the data. Below highlighted in red are the residuals.\n\n\n\n\n\nAfter we have the residual values, we simply square each of them, then sum all of them together. That is the sum of squared residuals.\n\n\n\nThis estimation is called the ordinary least squares (OLS) estimator. The solutions to the OLS estimator can be derived mathematically.\n\n\n\nDeriving OLS Estimates\nOLS wants to minimise the sum of squared residuals \\(S(\\hat{\\boldsymbol\\beta})\\) - the differences between the actual \\(\\mathbf y\\) and our predicted \\(\\hat{\\mathbf y}\\):\n\\[\n\\begin{align}\nS(\\hat{\\boldsymbol\\beta}) & = (\\mathbf y - \\hat{\\mathbf y})^\\mathsf{T} (\\mathbf y - \\hat{\\mathbf y})\\\\\n& = (\\mathbf y - \\color{blue}{\\mathbf X \\hat{\\boldsymbol\\beta}}\\color{black})^\\mathsf{T} (\\mathbf y - \\color{blue}{\\mathbf{X} \\hat{\\boldsymbol\\beta}}\\color{black}) && (\\text{plug in } \\color{blue}{\\hat{\\mathbf y}  = \\mathbf X \\hat{\\boldsymbol\\beta}}\\color{black}) \\\\\n& = \\mathbf y^\\mathsf{T} \\mathbf y - \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf y - \\mathbf y^\\mathsf{T} \\mathbf{X}\\hat{\\boldsymbol\\beta} + \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf{Xb} && (\\text{distribute out)} \\\\\n& = \\mathbf y^\\mathsf{T} \\mathbf y - \\color{blue}{2\\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf y}\\color{black} + \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf{X} \\hat{\\boldsymbol\\beta} &&(\\text{combine } \\color{blue}{- \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf y - \\mathbf y^\\mathsf{T} \\mathbf{X}\\hat{\\boldsymbol\\beta}}\\color{black})\n\\end{align}\n\\]\nNow, let us find the first order condition:\n\\[\n\\frac{\\partial S(\\hat{\\boldsymbol\\beta})}{\\partial \\hat{\\boldsymbol\\beta}} = -2\\mathbf X^\\mathsf{T} \\mathbf y + 2 \\mathbf X^\\mathsf{T} \\mathbf{X} \\hat{\\boldsymbol\\beta} = 0\n\\]\nWhen assuming \\(\\mathbf X^\\mathsf{T} \\mathbf X\\) is invertable (which is true if \\(\\mathbf X\\) is full rank), we can isolate \\(\\hat{\\beta}\\) to find the solution to OLS:\n\\[\n\\begin{align}\n-2\\mathbf X^T\\mathbf y + 2 \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol{\\hat{\\beta}} & = 0 \\\\\n2 \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol{\\hat\\beta} & = 2\\mathbf X^\\mathsf{T} \\mathbf y && (+ 2\\mathbf X^\\mathsf{T} \\mathbf y \\text{ to both sides}) \\\\\n\\boldsymbol{\\hat\\beta} & = (2\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} 2 \\mathbf X^\\mathsf{T} \\mathbf y && (\\times (2\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\text{ to both sides})\\\\\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y &&(\\text{cancel out } 2^{-1}\\times 2)\n\\end{align}\n\\]\nThose are our coefficient solutions to OLS. With the estimated parameters \\(\\widehat{\\beta_0}, \\dots, \\widehat{\\beta_k}\\), we now have a best-fit line, called the fitted values.\nFor more detailed analysis of the OLS estimator, see the causal inference section.\n\n\n\n\n\n\nInterpretation\n\nInterpretation of Parameters\nI define \\(\\widehat{\\beta_j} \\in \\{\\widehat{\\beta_1}, \\dots, \\widehat{\\beta_k}\\}\\), multiplied to \\(x_j \\in \\{x_1, \\dots, x_k\\}\\). \\(\\widehat{\\beta_0}\\) is the intercept.\n\n\n\n\n\n\n\n\n\nContinuous \\(x_j\\)\nBinary \\(x_j\\)\n\n\nContinuous \\(y\\)\nFor every one unit increase in \\(x_j\\), there is an expected \\(\\widehat{\\beta_j}\\) unit change in \\(y\\).\nWhen all explanatory variables equal 0, the expected value of \\(y\\) is \\(\\widehat{\\beta_0}\\).\nThere is a \\(\\widehat{\\beta_j}\\) unit difference in \\(y\\) between category \\(x_j = 1\\) and category \\(x_j = 0\\).\nFor category \\(x_j = 0\\), the expected value of \\(y\\) is \\(\\widehat{\\beta_0}\\) (when all other explanatory variables equal 0).\n\n\nBinary \\(y\\)\nFor every one unit increase in \\(x_j\\), there is an expected \\(\\widehat{\\beta_j} \\times 100\\) percentage point change in the probability of a unit being in category \\(y=1\\).\nWhen all explanatory variables equal 0, the expected probability of a unit being in category \\(y=1\\) is \\(\\widehat{\\beta_0} \\times 100\\)\nThere is a \\(\\widehat{\\beta_j}\\times 100\\) percentage point difference in the probability of a unit being in category \\(y=1\\) between category \\(x_j = 1\\) and category \\(x_j = 0\\).\nFor category \\(x_j = 0\\), the expected probability of a unit being in category \\(y=1\\) is \\(\\widehat{\\beta_0} \\times 100\\) (when all other explanatory variables equal 0).\n\n\n\nIf you have multiple explanatory variables, always add: while holding all other explanatory variables not \\(x_j\\) constant.\n\n\n\nStandardised Interpretations\nSometimes, a \\(\\beta_j\\) increase in \\(y\\) for every one unit increase in \\(x\\) is not particularly useful for us to interpret. For example, if \\(y\\) is democracy, what does a 5 unit increase in democracy actually mean?\nWe can add more relevant detail by expressing the change of \\(y\\) and \\(x\\) in terms of their standard deviations. Or in other words, we want to find the change in \\(\\frac{\\hat y_i}{\\sigma_y}\\) for every one standard deviation \\(\\sigma_x\\) increase in \\(x\\). For simplicity, let us use a simple linear regression \\(E(y_i|x_i) = \\beta_0 + \\beta_1 x_i\\):\n\\[\n\\begin{align}\n& E \\left(\\frac{y_i}{\\sigma_y} | x_i = x + \\sigma_x \\right ) - E \\left(\\frac{y_i}{\\sigma_y} | x_i = x \\right ) \\\\\n& = \\frac{E(y_i|x_i = x+ \\sigma_x)}{\\sigma_y} - \\frac{E(y_i|x_i = x)}{\\sigma_y} &&(\\text{property of expectation}) \\\\\n& = \\frac{E(y_i|x_i = x+ \\sigma_x) - E(y_i|x_i = x)}{\\sigma_y} && (\\text{combine into 1 fraction})\\\\\n& = \\frac{\\beta_0 + \\beta_1(x+\\sigma_x) - [\\beta_0 + \\beta_1(x)]}{\\sigma_y} && (\\text{plug in regression models})\\\\\n& = \\frac{\\beta_0 + \\beta_1x + \\beta_1\\sigma_x - \\beta_0 -\\beta_1x}{\\sigma_y} && (\\text{distribute out})\\\\\n& = \\frac{\\beta_1\\sigma_x}{\\sigma_y} && (\\text{cancel and simplify})\n\\end{align}\n\\]\nThus, for a one standard deviation \\(\\sigma_x\\) increase in \\(x_j\\), there is an expected \\(\\frac{\\beta_j\\sigma_x}{\\sigma_y}\\)-standard deviation change in \\(y\\).\n\n\n\nResidual Standard Deviation\nResiduals are the distance of the actual value \\(y_i\\) of observation \\(i\\), compared to the predicted \\(\\widehat{y_i}\\) from our fitted values/best-fit line. They can be obtained after we fit our model:\n\\[\n\\begin{align}\n\\mathbf{\\hat u} & = \\mathbf y - \\mathbf{\\hat y} \\\\\n& = \\mathbf y - \\color{blue}{\\mathbf{X} \\hat{\\boldsymbol\\beta}} && \\color{black}(\\text{plug in } \\color{blue}{\\hat{\\mathbf y} = \\mathbf{X} \\hat{\\boldsymbol\\beta}} \\color{black}) \\\\\n& = \\mathbf y - \\mathbf X \\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y} && \\color{black}(\\text{plug in } \\color{blue}{\\boldsymbol{\\hat\\beta}  = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y}\\color{black})\n\\end{align}\n\\]\nThe residual standard deviation \\(\\hat\\sigma\\) measures the spread/variance of our residuals - so, how far away the actual values \\(y_i\\) are from our predicted values \\(\\widehat{y_i}\\) in general for all observations.\nThe residual variance is estimated with the formula below (with the residual standard deviation being the square root):\n\\[\n\\hat\\sigma^2 = \\frac{\\sum_{i=1}^n \\hat u_i^2}{n-k-1} = \\frac{\\mathbf{\\hat u}^\\mathsf{T} \\mathbf{\\hat u}}{n-k-1}\n\\]\n\n\n\n\n\n\nVisualisation of Residual Standard Deviation\n\n\n\n\n\nBelow is a figure illustrating different residual standard deviations, with the same best-fit line.\n\n\n\n\n\n\n\n\nSmaller \\(\\hat\\sigma\\) mean the actual values are, on average, close to our predicted values, and larger \\(\\hat\\sigma\\) mean the actual values are, on average, further away from our predicted values.\n\n\n\nR-Squared\nOur fitted values equation takes the following form:\n\\[\n\\begin{align}\n\\hat{\\mathbf y} & = \\mathbf X \\hat{\\boldsymbol\\beta}  \\\\\n\\hat{\\mathbf y} & = \\mathbf X\\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y} && \\color{black}(\\text{plug in } \\color{blue}{\\boldsymbol{\\hat\\beta} = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y}\\color{black}) \\\\\n\\hat{\\mathbf y} &= \\color{blue}{\\mathbf P}\\color{black}{\\mathbf y}  && (\\text{plug in } \\color{blue}{\\mathbf P : = \\mathbf X(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}} \\color{black})\n\\end{align}\n\\]\nWe can see that \\(\\mathbf P\\) is a matrix that turns \\(\\mathbf y \\rightarrow \\hat{\\mathbf y}\\). Matrix \\(\\mathbf P\\) is our linear model that projects the true values \\(\\mathbf y\\) into a space of our regressors \\(\\mathbf X\\).\nOne thing we might be interested in is how well our model \\(\\mathbf{Py}\\) explains the actual \\(\\mathbf y\\). One way we can do this is the scalar product of two vectors. We know that the scalar/dot product calculates the project/shadow of one vector on another. Thus, the scalar product \\(\\mathbf y^\\mathsf{T}\\mathbf{Py}\\) describes the shadow the actual \\(y\\) casts on our projected model.\nHowever, this value will change based on the scale of our \\(y\\) variable. Thus, we will divide it by \\(\\mathbf y^\\mathsf{T}\\mathbf y\\), which is the “maximum” shadow possible (perfect shadow). This ratio is called \\(R^2\\).\n\\[\nR^2 = \\frac{\\mathbf y^\\mathsf{T}\\mathbf{Py}}{\\mathbf y^\\mathsf{T}\\mathbf y}\n\\]\nR-Squared (\\(R^2\\)) measures the proportion of variation in \\(y\\) that is explained by our explanatory variables. R-Squared is always between 0 and 1 (or 0-100 as a percentage). Higher values indicate our model better explains the variation in \\(y\\).\nInterpreting R-squared: The Model explains \\(R^2 \\times 100\\) percent of the variation in \\(y\\).\n\n\n\n\n\n\nStatistical Inference\n\nHomoscedasticity and Heteroscedasticity\nHomoscedasticity is defined as:\n\\[\nVar(\\mathbf u | \\mathbf X) = \\sigma^2 \\mathbf I_n = \\begin{pmatrix}\n\\sigma^2 & 0 & \\dots & 0 \\\\\n0 & \\sigma^2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & 0 \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{pmatrix}\n\\]\nOr in other words, no matter the values of any explanatory variable, the error term variance is constant.\nIf this is false, then we have heteroscedasticity.\n\n\n\n\n\n\nIntuitive Visualisation of Homoscedasticity\n\n\n\n\n\nAn easy way to identify homoscedasticity is to look at a residual plot (just the plot of all \\(\\widehat{u_i}\\)):\n\nNotice how the homoscedasticity residuals seem to have the same up-down variance, no matter the value of \\(x\\).\nThe heteroscedasticity residuals have a clear pattern - the up-down variance is smaller when \\(x\\) is smaller, and the up-down variance is larger when \\(x\\) is larger.\nEssentially, if you see a pattern in the residual plot, it is likely heteroscedasticity.\n\n\n\nIf you have homoscedasticity, you should use normal OLS standard errors.\nIf you have heteroscedasticity, you should use robust OLS standard errors. You should also use robust standard errors if you are not sure which errors to use.\n\n\n\nDeriving Standard Errors\nWe will only derive homoscedastic (normal) standard errors. The robust standard error derivation is beyond the scope of this lesson (just trust the computer that it will calculate it properly).\nLet us assume homoscedasticity. We want to find the variance of our estimator, \\(Var(\\boldsymbol{\\hat\\beta} | \\mathbf X)\\). Let us start off with our OLS solution. We can simplify as follows:\n\\[\n\\begin{align}\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}(\\mathbf X \\boldsymbol\\beta + \\mathbf u) && (\\text{plug in } \\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u) \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&(\\text{multiply out})\\\\\n& = \\color{blue}{\\mathbf I}\\color{black}{\\boldsymbol\\beta} + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&( \\ \\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X = \\mathbf I}\\color{black})\\\\\n& = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{identity property of } \\mathbf I)\n\\end{align}\n\\]\n\\[\nVar(\\boldsymbol{\\hat\\beta} | \\mathbf X) = Var(\\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u \\ | \\ \\mathbf X)\n\\]\n\\(\\boldsymbol\\beta\\) is a vector of fixed constants. \\((\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u\\) can be imagined as a matrix of fixed constants, since we are conditioning the above variance on \\(\\mathbf X\\) (so for each \\(\\mathbf X\\), the statement is fixed).\n\n\n\n\n\n\nMathematical Lemma\n\n\n\n\n\nIf \\(\\mathbf u\\) is an \\(n\\) dimensional vector of random variables, \\(\\mathbf c\\) is an \\(m\\) dimensional vector, and \\(\\mathbf B\\) is an \\(n \\times m\\) dimensional matrix with fixed constants, then the following is true:\n\\[\nVar(\\mathbf c + \\mathbf{Bu}) = \\mathbf B Var(\\mathbf u)\\mathbf B^T\n\\]\nI will not prove this lemma here, but it is provable.\n\n\n\nWith the Lemma above, and with the definition of homoscedasticity, we can simplify:\n\\[\n\\begin{align}\nVar(\\boldsymbol{\\hat\\beta} | \\mathbf X) & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} Var(\\mathbf u | \\mathbf X) [(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}]^{-1} && (\\text{lemma})\\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} Var(\\mathbf u | \\mathbf X) \\color{blue}{\\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1}} && \\color{black}( \\ \\color{blue}{[(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}]^{-1} = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1}}\\color{black})\\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\color{blue}{\\sigma^2 \\mathbf I_n}\\color{black}{ \\mathbf X} (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\color{blue}{Var(\\mathbf u | \\mathbf X) = \\sigma^2 \\mathbf I_n}\\color{black}) \\\\\n& =  \\color{red}{\\sigma^2} \\color{black} (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf I_n \\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\text{move scalar } \\color{red}{\\sigma^2}\\color{black})\\\\\n& =  \\sigma^2 (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}  \\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\text{identity property of } \\mathbf I_n)\\\\\n& =  \\sigma^2 (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\text{inverses } \\mathbf X^\\mathsf{T}  \\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\text{ cancel})\n\\end{align}\n\\]\nHowever, we do not actually know what \\(\\sigma^2\\) is. We can estimate it with \\(\\hat\\sigma^2\\) (discussed here).\nThe standard errors are the square root of the variance. Thus, our standard errors for any coefficient estimate \\(\\hat\\beta_j\\) are:\n\\[\nse(\\hat\\beta_j) = \\hat\\sigma \\sqrt{(\\mathbf X^T \\mathbf X)^{-1}_{jj}}\n\\]\n\n\n\nT-Tests\nIn regression, our typical hypotheses are:\n\n\\(H_0 : \\beta_j = 0\\) (i.e. there is no relationship between \\(x_j\\) and \\(y\\)).\n\\(H_1:\\beta_j ≠ 0\\) (i.e. there is a relationship between \\(x_j\\) and \\(y\\)).\n\nUsing the standard error (see above), we calculate the \\(t\\)-statistic, and using the \\(t\\)-statistic, we calculate a p-value.\n\n\n\n\n\n\nDetails of Running a Hypothesis Test\n\n\n\n\n\nFirst, we calculate the t-test statistic:\n\\[\nt = \\frac{\\widehat{\\beta_1} - H_0}{\\widehat{se}(\\widehat{\\beta_1})}\n\\]\n\nWhere \\(H_0\\) is typically 0, but if you do decide to alter the null hypothesis, you would plug it in.\n\nNow, we consult a t-distribution of \\(n-k-1\\) degrees of freedom. We use a t-distribution because the standard error calculation used in OLS is slightly imprecise.\n\nNote: we can only do this step if we believe the central limit theorem is met (that our errors are asymptotically normal). We need a large enough sample size.\n\nWe start from the middle of the t-distribution, and move t-test-statstic number of standard deviations from both sides of the middle.\nThen, we find the probability of getting a t-test statistic even further from the middle than the one we got. The area highlighted in the figure below showcases this. In the figure, the t-test statistic is 2.228.\n\n\n\n\n\nThe area highlighted, divided by the entire area under the curve, is the p-value.\n\n\n\nThe p-value we get is the probability of getting a test statistic equally or more extreme than the one we got, given the null hypothesis is true.\n\nIf \\(p&lt;0.05\\), we believe the probability of a null hypothesis is low enough, such that we reject the null hypothesis (that there is no relationship between \\(x\\) and \\(y\\)), and conclude our alternate hypothesis (that there is a relationship between \\(x\\) and \\(y\\)).\nIf \\(p &gt; 0.05\\), we cannot reject the null hypothesis, and cannot reject that there is no relationship between \\(x\\) and \\(y\\).\n\nNOTE: this is not causality - we are only looking at the relationship. Causality needs to be established with an adequate research design.\n\n\n\nConfidence Intervals\nThe 95% confidence intervals of coefficients have the following bounds:\n\\[\n\\widehat{\\beta_j} - 1.96 \\widehat{se}(\\widehat{\\beta_j}), \\ \\ \\widehat{\\beta_j} + 1.96 \\widehat{se}(\\widehat{\\beta_j})\n\\]\n\nThe 1.96 is an approximation assuming a normal distribution. The actual confidence intervals (calculated by computers) will use a t-distribution of \\(n-k-1\\), which will result in a slightly different multiplicative factor.\n\nThe confidence interval means that under repeated sampling and estimating \\(\\widehat{\\beta_j}\\), 95% of the confidence intervals that we construct will include the true \\(\\beta_j\\) value in the population.\nIf the confidence interval contains 0, we cannot conclude a relationship between \\(x_j\\) and \\(y\\), as 0 is a plausible value of \\(\\beta_j\\). These results will always match those of the t-test.\n\n\n\nF-Tests\nF-tests are used to test more than one coefficient at a time. The hypotheses will be:\n\n\\(M_0 : y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_g x_g + u_i\\) (the smaller null model).\n\\(M_a : y = \\beta_0 + \\beta_1x_1 + \\dots + \\beta_g x_g + \\dots + \\beta_kx_k + u_i\\) (the bigger model with additional variables).\n\n\n\n\n\n\n\nDetails of the F-test\n\n\n\n\n\nF-tests compare the \\(R^2\\) of the two models through the F-statistic:\n\\[\nF = \\frac{(SSR_0 - SSR_a) / (k_a - k_0)}{SSR_a /(n - k_a - 1)}\n\\]\nWe then consult a F-distribution with \\(k_a - k_0\\) and \\(n-k_a - 1\\) degrees of freedom, obtaining a p-value (in the same way as the t-test).\n\n\n\nThe p-value we get is the probability of getting a test statistic equally or more extreme than the one we got, given the null hypothesis is true.\n\nIf \\(p&lt;0.05\\), the we believe the probability of the null hypothesis is low enough, such that we reject the null hypothesis (that \\(M_0\\) is the better model), and conclude our alternate hypothesis (that \\(M_a\\) is a better model). This also means the extra coefficients in \\(M_a\\) are jointly statistically significant.\nIf \\(p &gt; 0.05\\), we cannot reject the null hypothesis, and cannot reject that \\(M_0\\) is a better model. Thus, the extra coefficients in \\(M_a\\) are jointly not statistically significant.\n\n\n\n\nPredictive Inference\nWe can predict using the linear regression by plugging in explanatory variable values, and finding the predicted \\(\\widehat{y_i}\\).\n\\[\n\\begin{align}\n\\hat{\\mathbf y} & = \\mathbf X \\hat{\\boldsymbol\\beta}  \\\\\n\\hat{\\mathbf y} & = \\mathbf X\\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y} && \\color{black}(\\text{plug in } \\color{blue}{\\boldsymbol{\\hat\\beta} = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y}\\color{black}) \\\\\n\\end{align}\n\\]\nWe also have confidence intervals for every predicted \\(\\widehat{y_i}\\). These intervals are calculated with the residual standard deviation (covered previously):\n\\[\n\\widehat{y_i} - 1.96 \\hat\\sigma, \\ \\widehat{y_i} + 1.96 \\hat\\sigma\n\\]\n\n\n\n\n\n\nExtensions\n\nCategorical Explanatory Variables\nTake an explanatory variable \\(x\\), which has \\(n\\) number of categories \\(1, \\dots, n\\). To include \\(x\\) in our regression, we would create \\(n-1\\) dummy variables, to create the following regression model:\n\\[\nE(y_i|x_i) = \\beta_0 + \\beta_1x_{1i} + \\dots + \\beta_k x_{n-1 \\ i}\n\\]\n\nCategories \\(1, \\dots, n-1\\) get there own binary variable \\(x_1, \\dots, x_{n-1}\\).\nCategory \\(n\\) (the reference category) does not get its own variable. We can change which category we wish to be the reference.\n\nInterpretation is as follows (category \\(j\\) is any one of category \\(1, \\dots, n-1\\)).\n\n\\(\\beta_j\\) is the difference in expected \\(y\\) between category \\(j\\) and the reference category.\n\\(\\beta_0\\) is the expected \\(y\\) of the reference category.\nThus, category \\(j\\) has an expected \\(y\\) of \\(\\beta_0 + \\beta_j\\).\n\n\n\n\n\n\n\nExample of a Categorical Explanatory Variable\n\n\n\n\n\nLet us say that \\(x\\) is the variable development level of a country, with 3 categories: low (L), medium (M), and high (H). \\(y\\) will be the crime rate of the country.\nLet us set low development (L) as our reference category. Our regression will be:\n\\[\nE(y|x) = \\beta_0 + \\beta_1x_M + \\beta_2 x_H\n\\]\nNow let us interpret the coefficients:\n\n\\(\\beta_0\\) is the expected crime rate for a country of low (L) development.\n\\(\\beta_1\\) is the difference in expected crime rate between a medium (M) developed country and a low (L) developed country (since low is the reference category).\n\\(\\beta_2\\) is the difference in expected crime rate between a high (H) developed country and a low (L) developed country (since low is the reference category).\n\nThe expected/predicted \\(y\\) (crime rate) for each category is:\n\nLow (L): \\(\\beta_0\\)\nMedium (M): \\(\\beta_0 + \\beta_1\\)\nHigh (H): \\(\\beta_0 + \\beta_2\\).\n\n\n\n\nEach coefficient \\(\\beta_j\\)’s statistical significance is a difference-in-means significance test, not the significance of the categorical variable as a whole. To find if the entire categorical variable is significant, you should use a F-test.\n\n\n\nFixed Effects\nWhen we have hierarchical or panel data, we need to control for differences between clusters. We essentially include the cluster variable as a categorical variable in our regression.\n\n\n\n\n\n\nHierarchical/Clustered Data\n\n\n\n\n\nHierarchical data is data where the basic units of analysis \\(i\\) are clustered, grouped, or nested into clusters.\nFor example, let us say we are measuring how income affects voter turnout in european countries. We have observations from France, Switzerland, Germany, and many other countries. However, these observations can be grouped by the country they came from.\nWhy is this grouping important? This is because there may be something in common between observations within the same cluster. For example, Switzerland might just have higher voter turnout in general due to something about Swiss institutions or culture.\nThis means that observations aren’t random - i.e. we know that if we select from switzerland, it is likely to have higher turnout - observations from the same country are correlated. Thus, we need some way to account for this clustering of observations. We will explore this below.\n\n\n\n\n\n\n\n\n\nPanel Data\n\n\n\n\n\nPanel data is data that can be clustered in two ways - by unit, and by time. For example, let us say we have a dataset on all countries and their GDP between 1960-2020.\n\nWe will have clusters based on country: Germany will have an observation in 1960, in 1961, …, to 2020. Same for every other country. These observations are grouped by the unit (country in this case).\nWe will also have clusters based on time: We will have all GDP observations for all countries in 1960, in 1961, etc. These observations are grouped by the time (year in this case).\n\n\n\n\nLet us say we have \\(m\\) number of clusters \\(i = 1, \\dots, m\\). Within each cluster, we will have units \\(t = 1, \\dots, n\\). Our cluster fixed effects model will take the form:\n\\[\n\\begin{split}\ny_{it} & = \\alpha_i + \\beta_1x_1 + \\dots + \\beta_kx_k + u_{it} \\\\\n& \\text{where } \\alpha_i = \\beta_{00} + \\underbrace{\\beta_{02}D_{i2} + \\beta_{03}D_{i3} + \\dots + \\beta_{0m}D_{im}}_{\\text{unique intercepts for each cluster}}\n\\end{split}\n\\]\n\nWhere \\(D_{i2}, D_{i3}, \\dots, D_{im}\\) are dummy variables for clusters \\(2, \\dots, m\\). Cluster 1 is the reference category.\n\\(y_{it}\\) indicates the \\(y\\) value of the \\(t\\)th individual in the \\(i\\)th cluster.\n\nFor panel data, we use two-way fixed effects, which is basically just two fixed effects for different clustering. Let us say we have \\(i = 1, \\dots, m\\) units with \\(t = 1, \\dots, n\\) different numbers of time periods. Our two way fixed effects model takes the form:\n\\[\n\\begin{split}\ny_{it} & = \\alpha_i + \\gamma_t + \\beta_1x_1 + \\dots + \\beta_kx_k + u_{it} \\\\\n& \\text{where } \\alpha_i =  \\alpha_{00} + \\underbrace{\\alpha_{02}D_{i2} + \\alpha_{03}D_{i3} + \\dots + \\alpha_{0m}D_{im}}_{\\text{unique intercepts for each unit}} \\\\\n& \\text{where } \\gamma_t =  \\gamma_{00} + \\underbrace{\\gamma_{02}T_{i2} + \\gamma_{03}D_{t3} + \\dots + \\gamma_{0n}T_{in}}_{\\text{unique intercepts for each time}} \\\\\n\\end{split}\n\\]\n\nWhere \\(D_{i2}, D_{i3}, \\dots, D_{im}\\) are dummy variables for units \\(2, \\dots, m\\)., and \\(T_{i2}, T_{i3}, \\dots, T_{in}\\) are dummy variables for time periods \\(2, \\dots, n\\).\n\\(y_{it}\\) indicates the observation of unit \\(i\\) in time period \\(t\\).\n\n\n\n\n\n\n\nIntuitive Explanation of Fixed Effects\n\n\n\n\n\nFor one-way fixed effects, we essentially add a unique intercept term for every cluster, accounting for the average differences in \\(y\\) between each category.\n\n\\(\\beta_{00}\\) is the intercept for the reference category 1.\n\\(\\beta_{00} + \\beta_{0i}\\) is the intercept for the \\(i\\)th category.\n\nFor two-way fixed effects, we add a unique intercept term for every year and country, accounting for the average differences in \\(y\\) between each country, and the average differences in \\(y\\) between each year.\n\n\n\n\n\n\nInteraction Effects\nAn interaction between two variables means they are multiplied in the regression equation:\n\\[\ny_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} + \\beta_3 x_{1i} x_{2i}\n\\]\nInterpretation of the relationship between \\(x_1\\) and \\(y\\) is as follows:\n\n\n\n\n\n\n\n\n\nBinary \\(x_2\\)\nContinuous \\(x_2\\)\n\n\nBinary \\(x_1\\)\nWhen \\(x_2 = 0\\), the effect of \\(x_1\\) (going from 0 to 1) on \\(y\\) is \\(\\widehat{\\beta_1}\\).\nWhen \\(x_2 = 1\\), the effect of \\(x_1\\) (going from 0 to 1) on \\(y\\) is \\(\\widehat{\\beta_1} + \\widehat{ \\beta_3}\\).\nThe effect of \\(x_1\\) (going from 0 to 1) on \\(y\\) is \\(\\widehat{\\beta_1} + \\widehat{\\beta_3} x_2\\).\n\n\nContinuous \\(x_1\\)\nWhen \\(x_2 = 0\\), for every increase in one unit of \\(x_1\\), there is an expected \\(\\widehat{\\beta_1}\\) unit change in \\(y\\).\nWhen \\(x_2 = 1\\), for every increase in one unit of \\(x_1\\), there is an expected \\(\\widehat{\\beta_1}+ \\widehat{\\beta_3}\\) change in \\(y\\).\nFor every increase of one unit in \\(x_1\\), there is an expected \\(\\widehat{\\beta_1} + \\widehat{\\beta_3} x_2\\) change in \\(y\\).\n\n\n\n\n\n\n\n\n\nProof of Interpretations of Interactions\n\n\n\n\n\nWe can solve for the change of \\(x_1\\) on \\(y\\) using a partial derivative of \\(y\\) in respect to \\(x_1\\):\n\\[\n\\begin{split}\n\\frac{\\partial \\widehat{y_i}}{\\partial x_{1i}} & = \\frac{\\partial}{\\partial x_{1i}} \\left[ \\widehat{\\beta_0} + \\widehat{\\beta_1}x_{1i} + \\widehat{\\beta_2}x_{2i} + \\widehat{\\beta_3}x_{1i}x_{2i}\\right] \\\\\n\\frac{\\partial \\widehat{y_i}}{\\partial x_{1i}} & = \\widehat{\\beta_1} + \\widehat{\\beta_3}x_2\n\\end{split}\n\\]\nThis gives us the effect of \\(x_1\\) on \\(y\\).\n\n\n\n\\(\\widehat{\\beta_0}\\) is still the expected \\(y\\) when all explanatory variables equal 0.\nThe coefficient of the interaction \\(\\widehat{\\beta_3}\\), when statistically significant, indicates a statistically significant interaction effect. If it is not statistically significant, then the interaction effect is not statistically significant (and can be dropped).\n\n\n\nPolynomial Transformations\nSometimes the relationship between two variables is not a straight line - we can add more flexibility with polynomials. The most common form of polynomial transformation is the quadratic transformation:\n\\[\ny_i = \\beta_0 + \\beta_1x_{i} + \\beta_2 x_{i}^2 + u_i\n\\]\nOur estimated \\(\\widehat{\\beta_0}\\) remains the expected value of \\(y\\) when all explanatory variables equal 0.\nUnfortunately, the \\(\\widehat{\\beta_1}\\) and \\(\\widehat{\\beta_2}\\) coefficients are not directly interpretable.\n\n\\(\\widehat{\\beta_2}\\)’s sign can tell us if the best-fit parabola opens upward or downward.\nThe significance of \\(\\widehat{\\beta_2}\\) also indicates if the quadratic term is statistically significant. If it is not, we can remove the transformation.\n\nWe can interpret two things about the quadratic transformation:\n\nFor every one unit increase in \\(x\\), there is an expected \\(\\widehat{\\beta_1} + 2 \\widehat{\\beta_2}x\\) unit increase in \\(y\\).\nThe minimum/maximum point in the best-fit parabola occurs at \\(x_i = - \\widehat{\\beta_1}/2 \\widehat{\\beta_2}\\)\n\n\n\n\n\n\n\nProof of Polynomial Interpretations\n\n\n\n\n\nWe can derive the change in \\(y\\) given a one unit increase in \\(x\\) by finding the partial derivative of \\(y\\) in respect to \\(x\\):\n\\[\n\\begin{split}\n\\frac{\\partial \\widehat{y_i}}{\\partial x} & = \\frac{\\partial}{\\partial x} \\left[ \\widehat{\\beta_0} + \\widehat{\\beta_1}x_i + \\widehat{\\beta_2}x_i^2 \\right] \\\\\n\\frac{\\partial \\widehat{y_i}}{\\partial x} & = \\widehat{\\beta_1} + 2 \\widehat{\\beta_2}x_i\n\\end{split}\n\\]\nWe can also solve for the \\(x_i\\) that results in the minimum/maximum of the best-fit parabola by setting the partial derivative equal to 0:\n\\[\n\\begin{split}\n0 & = \\widehat{\\beta_1} + 2 \\widehat{\\beta_2}x_i \\\\\nx_i & = -\\widehat{\\beta_1}/2 \\widehat{\\beta_2}\n\\end{split}\n\\]\n\n\n\nWe can go beyond quadratic - as long as we always include lower degree terms in our model:\n\nCubic: \\(y_i = \\beta_0 + \\beta_1x_{i} + \\beta_2 x_{i}^2 + \\beta_3 x_i^3 + u_i\\)\nQuartic: \\(y_i = \\beta_0 + \\beta_1x_{i} + \\beta_2 x_{i}^2 + \\beta_3 x_i^3 + \\beta_4 x_i^4 + u_i\\)\n\n\n\n\nLogarithmic Transformations\nLogarithmic transformations are often used to change skewed variables into normally distributed variables.\n\n\n\n\n\n\nLogging a Skewed Variable\n\n\n\n\n\nMany monetary variables are heavily skewed. Natural logging these variables can turn them into normal distributions. This is useful, since skewed variables tend to have heteroscedasticity, and by making them normal, we can use the smaller normal standard errors.\nFor example, take this variable called expenses with a significant right skew:\n\n\n\n\n\nIf we take the log of this variable, we get the following distribution that is almost normal:\n\n\n\n\n\n\n\n\nWe have 3 types of logarithmic transformations:\n\n\n\n\n\n\n\n\n\n\\(x\\)\n\\(\\log (x)\\)\n\n\n\\(y\\)\nLinear Model:\n\\(y = \\beta_0 + \\beta_1 x + u\\)\nLinear-Log Model:\n\\(y = \\beta_0 + \\beta_1 \\log x + u\\)\n\n\n\\(\\log (y)\\)\nLog-Linear Model:\n\\(\\log(y) = \\beta_0 + \\beta_1 x + u\\)\nLog-Log Model:\n\\(\\log y = \\beta_0 + \\beta_1 \\log x + u\\)\n\n\n\n\nInterpreting the models:\n\n\n\n\n\n\n\n\n\n\\(x\\)\n\\(\\log (x)\\)\n\n\n\\(y\\)\nLinear Model:\nWhen \\(x\\) increases by one unit, there is an expected \\(\\widehat{\\beta_1}\\) unit change in \\(y\\).\nLinear-Log Model:\nWhen \\(x\\) increases by 10%, there is an expected \\(0.096 \\widehat{\\beta_1}\\) unit change in \\(y\\).\n\n\n\\(\\log (y)\\)\nLog-Linear Model:\nFor every one unit increase in \\(x\\), \\(y\\) is multiplied by \\(e^{\\widehat{\\beta_1}}\\).\nLog-Log Model:\nMultiplying \\(x\\) by \\(e\\) will multiply the expected value of \\(y\\) by \\(e^{\\widehat{\\beta_1}}\\).\n\n\n\n\n\n\n\n\n\nProof of Interpretations for Log Transformations\n\n\n\n\n\nProof of Linear-Log Model:\n\\[\n\\begin{split}\n& E(y_i|x_i = x) = \\beta_0 + \\beta_1 \\log x \\\\\n& E(y_i | x_i = e^A x) = \\beta_0 + \\beta_1 \\log(e^A x) \\\\\n& = \\beta_0 + \\beta_1 (\\log(e^A) + \\log x) \\\\\n& = \\beta_0 + \\beta_1 (A + \\log x) \\\\\n& = \\beta_0 + \\beta_1A + \\beta_1 \\log x\n\\end{split}\n\\]\n\\[\n\\begin{split}\nE(y_i|x_i = \\alpha x) - E(y_i|x_i = x) & = \\beta_0 + \\beta_1 A + \\beta_1 \\log (x) - (\\beta_0 + \\beta_1 \\log x) \\\\\n& = \\beta_1 A\n\\end{split}\n\\]\n\nWhen \\(A = 0.095\\), then \\(e^A = 1.1\\). Thus, a 1.1 times increase of \\(x\\) results in a \\(0.095 \\widehat{\\beta_1}\\) change in \\(y\\).\n\n\nProof of Log-Linear Model:\n\\[\n\\begin{split}\nE(\\log y_i | x_i = x) =  \\log y_i & = \\beta_0 + \\beta_1 x \\\\\ny_i & = e^{\\beta_0 + \\beta_1 x} \\\\\ny_i & = e^{\\beta_0}e^{\\beta_1 x} \\\\\nE(\\log y_i|x_i = x+1) = \\log y_i & = \\beta_0 + \\beta_1(x+1) \\\\\ny_i & = e^{\\beta_0 + \\beta_1 + \\beta_1 x} \\\\\ny_i & = e^{\\beta_0}e^{\\beta_1}e^{\\beta_1x}\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\frac{E(\\log y_i|x_i = x+1)}{E(\\log y_i | x_i = x)} & = \\frac{e^{\\beta_0}e^{\\beta_1}e^{\\beta_1x}}{e^{\\beta_0}e^{\\beta_1x}} \\\\\n& = e^{\\beta_1}\n\\end{split}\n\\]\n\nThus, when \\(x\\) increases by one, there is a multiplicative increase of \\(e^{\\beta_1}\\).\n\n\nProof of Log-Log model:\n\\[\n\\begin{split}\nE(\\log y_i | x_i = x) =  \\log y_i & = \\beta_0 + \\beta_1 \\log x \\\\\ny_i & = e^{\\beta_0 + \\beta_1 \\log x} \\\\\ny_i & = e^{\\beta_0}e^{\\beta_1 \\log x} \\\\\nE(\\log y_i|x_i = ex) = \\log y_i & = \\beta_0 + \\beta_1 \\log (ex) \\\\\ny_i & = e^{\\beta_0 + \\beta_1 \\log e + \\beta_1 \\log x} \\\\\ny_i & = e^{\\beta_0}e^{\\beta_1}e^{\\beta_1 \\log x}\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\frac{E(\\log y_i|x_i = ex)}{E(\\log y_i | x_i = x)} & = \\frac{e^{\\beta_0}e^{\\beta_1}e^{\\beta_1 \\log x}}{e^{\\beta_0}e^{\\beta_1 \\log x}} \\\\\n& = e^{\\beta_1}\n\\end{split}\n\\]\n\nThus, when \\(x\\) is multiplied by \\(e\\), there is a multiplicative increase of \\(e^{\\beta_1}\\).\n\n\n\n\n\n\n\n\n\n\nImplementation in R\nYou will need package fixest.\n\nlibrary(fixest)\n\nRegression with normal standard errors can be done with the lm() function:\n\nmodel &lt;- lm(y ~ x1 + x2 + x3, data = mydata)\nsummary(model)\n\nRegression with robust standard errors can be done with the feols() function:\n\nmodel &lt;- feols(y ~ x1 + x2 + x3, data = mydata, se = \"hetero\")\nsummary(model)\n\nOutput will include coefficients, standard errors, p-values, and more.\n\n\n\n\n\n\nBinary and Categorical Variables\n\n\n\n\n\nYou can include binary and categorical variables by using the as.factor() function:\n\nfeols(y ~ x1 + as.factor(x2) + x3, data = mydata, se = \"hetero\")\n\nYou can do the same for \\(y\\) or \\(x\\). Just remember, \\(y\\) cannot be a categorical variable (use multinomial logsitic regression instead).\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n\n\n\n\nYou can include one-way fixed effects by adding a | after your regression formula in feols():\n\nmodel &lt;- feols(y ~ x1 + x2 + x3 | cluster,\n               data = mydata, se = \"hetero\")\nsummary(model)\n\nYou can add two-way fixed effects as follows:\n\nmodel &lt;- feols(y ~ x1 + x2 + x3 | unit + year,\n               data = mydata, se = \"hetero\")\nsummary(model)\n\n\n\n\n\n\n\n\n\n\nInteraction Effects\n\n\n\n\n\nTwo interact two variables, use * between them. This will automatically include both the interaction term, and the two variables by themselves.\n\nfeols(y ~ x1 + x2*x3, data = mydata, se = \"hetero\")\n\nIf for some reason, you only want the interaction term, but not the variables by themselves, you can use a colon : between the two variables:\n\nfeols(y ~ x1 + x2:x3, data = mydata, se = \"hetero\")\n\n\n\n\n\n\n\n\n\n\nPolynomial Transformations\n\n\n\n\n\nTo conduct a polynomial transformation, you can use the I() function. The second argument is the degree of the polynomial:\n\nfeols(y ~ x1 + I(x2, 3), data = mydata, se = \"hetero\") #cubic for x2\n\n\n\n\n\n\n\n\n\n\nLogarithmic Transformations\n\n\n\n\n\nThe best way to do a logarithmic transformation is to create a new variable that is the log of the variable you want to transform using the log() function, before you even start the regression:\n\nmydata$x1_log &lt;- log(mydata$x1)\n\n\n\n\n\n\n\n\n\n\nConfidence Intervals\n\n\n\n\n\nTo find the confidence intervals for coefficients, first estimate the model with lm() or feols() as shown previously, then use the confint() command:\n\nconfint(model)\n\n\n\n\n\n\n\n\n\n\nF-Tests\n\n\n\n\n\nTo run a f-test, use the anova() command, and input your two different models, with the null model going first.\n\nanova(model1, model2)\n\nNote: F-tests only work with models that are run with homoscedastic standard errors. Robust standard errors will not work.\n\n\n\n\n\n\n\n\n\nLaTeX Regression Tables\n\n\n\n\n\nYou can use the texreg package to make nice regression tables automatically.\n\nlibrary(texreg)\n\nThe syntax for texreg() is as follows:\n\ntexreg(l = list(model1, model2, model3),\n       custom.model.names = c(\"model 1\", \"model 2\", \"model 3\"),\n       custom.coef.names = c(\"intercept\", \"x1\", \"x2\"),\n       digits = 3)\n\nYou can replace texreg() with screenreg() if you want a nicer regression table in the R-console.\nNote: you must have the same amount of model names as total models in your texreg, and you must have the same amount of coeficient names as the total amount of coefficients in all of your models.\n\n\n\n\n\n\n\n\n\nPrediction\n\n\n\n\n\nWe can use the predict() function to generate fitted value predictions in R:\n\nmy_predictions &lt;- predict(model, newdata = my_new_data)\n\nmy_new_data is a dataframe with a bunch of explanatory variable values (for every explanatory variable) for a collection of observations, that you wish to predict \\(\\hat y\\) for.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "2 Multiple Linear Regression"
    ]
  },
  {
    "objectID": "3.html",
    "href": "3.html",
    "title": "Classic Least Squares Theory",
    "section": "",
    "text": "Last chapter, we discussed the multiple linear regression model, and how it can help us measure relationships between explanatory and outcome variables.\nThis chapter introduces some key theory regarding the ordinary least squares estimator behind linear regression. Topics covered includes properties of estimators, the OLS estimator, and the Method of Moments estimator.\nUse the right sidebar for quick navigation.\n\n\nEstimators\n\nEstimands and Estimators\nAn estimand is the true value of some true parameter \\(\\theta\\) in the population we are trying to measure.\nWe often do not have data on the population. We typically have a sample from the population, and use an estimator (procedure) to produce a sample estimate \\(\\hat\\theta\\).\nHowever, because of sampling variability (not all random samples will be identical), each sample \\(n\\) will have a different estimate \\(\\hat\\theta_n\\).\nIf we keep taking \\(N\\) number of samples, we will have \\(N\\) number of estimates \\(\\hat\\theta_1, \\hat\\theta_2, \\dots, \\hat\\theta_N\\). Thus, any specific estimate \\(\\theta_n\\) from sample \\(n\\) can be thought of as a random draw from the sampling distribution \\(\\hat\\theta_1, \\hat\\theta_2, \\dots, \\hat\\theta_N\\).\n\n\n\n\n\n\nExample of a Sampling Distribution\n\n\n\n\n\nLet us say we want to find the mean salary of all individuals in the UK. The true value of the mean salary for every individual is \\(\\theta\\).\nHowever, asking all 60 million people is nearly impossible. So, we take a randomly sample of 1000 individuals, and then find the sample mean. Our estimator is thus the sample mean estimator.\nOur first sample of 1000 individuals yields an estimate \\(\\hat\\theta_1\\). If we take another sample, we will get slightly different people in this sample, and get another estimate \\(\\hat\\theta_2\\). We keep taking samples, and get more and more estimates \\(\\hat\\theta_3, \\hat\\theta_4, \\dots, \\hat\\theta_n\\).\nWe plot all of these samples into a distribution as follows:\n\n\n\n\n\nThis indicates the potential estimates we can get. If we were to conduct only one sample, we would essentially be selecting a random \\(\\hat\\theta_i\\) value from this distribution.\n\n\n\nThe sampling distribution of an estimator is the key property of estimators. The two parameters of interest from this sampling distribution are its expectation and variance.\n\n\n\nUnbiased Estimators\nAn estimator of a parameter is unbiased, if its estimates \\(\\hat\\theta_n\\) have an expectation equal to the true population value of the parameter:\n\\[\nE(\\hat\\theta_n) = \\theta\n\\]\nOr in other words, if we repeatedly sample and use the estimator, on average, the estimates will be equal to the true population value.\nWe want an unbiased estimator, because if \\(E(\\hat\\theta_n) = \\theta\\), that means our “best guess” of the estimator value is the true parameter value \\(\\theta\\). That means any one estimate \\(\\hat\\theta_n\\) is on average, correct.\nIf our estimator is biased, we can quantify bias with the following formula:\n\\[\nBias(\\hat\\theta_n) = E(\\hat\\theta_n)-\\theta\n\\]\n\n\n\nVariance and Efficiency\nUnbiasedness is not the only desirable property of estimators - we also care about the variance. After all, if we have two unbiased estimators, the one with less variance will be on average, closer to the true population value, for any one estimate \\(\\hat\\theta\\).\n\n\n\n\n\n\nExample of the Importance of Variance\n\n\n\n\n\nFor example, let us say the true population parameter is \\(\\theta = 0\\). We will have two estimators: estimator \\(A\\) and estimator \\(B\\):\n\nEstimator \\(A\\), after two samples (for simplicity), produces estimates -1 and 1.\nEstimator \\(B\\), after two samples, produces estimates -100 and 100.\n\nBoth estimators are unbiased \\(E(\\hat\\theta_n) = 0\\). However, clearly, estimator \\(A\\) is, on average, closer to \\(\\theta =0\\) than estimator \\(B\\). This is because while both estimators are unbiased, estimator \\(A\\) has a smaller variance than estimator \\(B\\) - that is on average, estimator \\(A\\)’s estimators are more closely “packed around” the expectation of the estimator.\n\n\n\nThe variance of an estimator can be quantified as:\n\\[\nVar(\\hat\\theta_n) = E[(\\hat\\theta_n - E(\\hat\\theta_n))^2]\n\\]\nAn efficient estimator is one that, on average, has the closest estimated value \\(\\hat\\theta_n\\) to the true population parameter. If two estimators are both unbiased, the one with lower variance is more efficient. Efficiency can be quantified as the estimator with the lowest mean squared error:\n\\[\nMSE(\\hat\\theta_n) = E[(\\hat\\theta_n - \\theta)^2] =Var(\\hat\\theta_n) + Bias(\\hat\\theta_n)^2\n\\]\nWe generally want an efficient estimator, since we know it will be giving us the closest guess to the true population parameter \\(\\theta\\).\n\n\n\n\n\n\nEfficient but Biased\n\n\n\n\n\nInterestingly, it is possible for a biased estimator to be more efficient than an unbiased estimator.\nThis is particularly the case when the biased estimator has a slight bias but small variance, while the unbiased estimator has a giant variance. In this case, the biased estimator is producing estimates \\(\\hat\\theta\\) that on average, are closer to the true population parameter \\(\\theta\\).\n\n\n\n\n\n\nAsymptotically Consistent Estimators\nAsymptotic properties are properties of estimators as the sample size \\(n\\) approaches infinity.\nAn estimator is consistent, if as we increase sample size towards infinity, the estimate will become more and more concentrated around the true population value \\(\\theta\\). At \\(n = ∞\\), our sampling distribution collapses to just one value, the true population value \\(\\theta\\). Mathematically:\n\\[\nPr(|\\hat\\theta_n - \\theta|&gt; \\epsilon) \\rightarrow 0, \\text { as } n \\rightarrow ∞\n\\]\nOr in other words, the probability that the distance between an estimate \\(\\hat\\theta_n\\) and the true population value \\(\\theta\\) will be higher than a small close-to-zero value \\(\\epsilon\\) will be 0, since our estimates \\(\\hat\\theta_n\\) will converge at the \\(\\theta\\).\nThis is a useful property, since even if our estimator is biased, if it is asymptotically consistent, we know that with large enough sample sizes, that bias becomes infinitely small and negligible.\n\n\n\n\n\n\nBiased but Consistent\n\n\n\n\n\nAn estimator can be both biased, but consistent. In smaller sample sizes, the estimator might not be on average correct, but over a large enough sample size, it will become “unbiased”.\nFor example, in the figure below, we can see that this estimator is biased at small values of \\(n\\), but as \\(n\\) increases, it becomes more consistent, collapsing its distribution around the true \\(\\theta\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaw of Large Numbers and Consistency\n\n\n\n\n\nThe law of large numbers states that the sample average of a random sample, is a consistent estimator of the population mean.\nFor example, let us say we have a random variable \\(x\\). We take a random sample of \\(n\\) units, so our sample is \\((x_1, \\dots, x_n)\\).\n\nLet us define \\(\\bar x_n\\) as our sample average.\nLet us define \\(\\mu\\) as the true population mean of variable \\(x\\).\n\nThe law of large numbers states that:\n\\[\nplim( \\bar x_n) = \\mu\n\\]\n\nWhere \\(plim\\) states that as \\(n\\) approaches infinity, the probability distribution of \\(\\bar x_n\\) collapses around \\(\\mu\\).\n\n\nWhy is this the case? This sample mean estimator is calculated simply through the formula for mean:\n\\[\n\\bar x_n = \\frac{1}{n}\\sum\\limits_{i=1}^n x_i\n\\]\nLet us define the variance of our sample of \\(x_1, \\dots, x_n\\) as \\(Var(x_i) = \\sigma^2\\). We can now find the variance of our sampling distribution of estimator \\(\\bar x_n\\):\n\\[\n\\begin{split}\nVar(\\bar x_n) & = Var\\left( \\frac{1}{n}\\sum\\limits_{i=1}^n x_i \\right) \\\\\n& = \\frac{1}{n^2} Var \\left(\\sum\\limits_{i=1}^n x_i\\right) \\\\\n& = \\frac{1}{n^2} \\sum\\limits_{i=1}^n Var(x_i) \\\\\n& = \\frac{1}{n^2} \\sigma^2 \\\\\n& = \\frac{\\sigma^2}{n}\n\\end{split}\n\\]\nAnd as sample size \\(n\\) increases to infinity, we get:\n\\[\n\\lim\\limits_{n \\rightarrow ∞} Var(\\bar x_n) = \\lim\\limits_{n \\rightarrow ∞} \\frac{\\sigma^2}{n} = 0\n\\]\nThus, the variance of our estimator \\(\\bar x_n\\) shrinks to zero, so as sample size increases to infinity \\(n\\), the sampling distribution of estimator \\(\\bar x_n\\) collapses around the true population mean.\n\n\n\n\n\n\nAsymptotic Normality\nAnother asymptotic property of estimators, as sample size \\(n\\) approaches infinity, is that the sampling distribution approaches a normal distribution.\nThe central limit theorem establishes asymptotic normality of estimators. Let us say we have \\(N\\) number of random variables \\(\\hat\\theta_1, \\dots, \\hat\\theta_N\\) (estimates are realisations of random variables). The central limit theorem states that:\n\\[\nPr(w_n &lt; w) \\rightarrow \\Phi(w) \\quad \\text{as } n \\rightarrow ∞\n\\]\n\nWhere \\(w_n\\) is a transformed version of the random variable \\(\\hat\\theta_n\\), defined as \\(w_n = \\frac{\\bar\\theta_n - \\mu}{\\sigma / \\sqrt{n}}\\).\nWhere \\(Pr(w_n &lt; w)\\) is the cumulative density function of the random variable \\(w_n\\).\nWhere \\(\\Phi(w)\\) is the cumulative density function (cdf) of the standard normal distribution \\(\\mathcal N(0, 1)\\).\n\nThe importance of CLM comes from the fact that as we increase sample size, our sampling distribution becomes more and more normally distributed. This property is essentially for carrying out statistical inference and significance tests, as they generally assume that our estimator is normally distributed.\n\n\n\nNonparametric Bootstrap\nMost traditional statistical tests rely on asymptotic normality. However, asymptotic normality can only be satisfied if we have a large enough sample size. When we are dealing with small samples, we cannot invoke central limit theorem.\nNonparametric Bootstrap, instead of assuming some sampling distribution, is a method to simulate the sampling distribution. This is done by re-sampling from the sample with replacement. The procedure is as follows:\n\nYou take the sample you observe (with sample size \\(n\\)), and randomly re-sample \\(n\\) observations from that sample with replacement (so allowing observations to repeat in our re-sample).\nContinue to do this over and over again to get \\(B\\) number of re-samples.\nFor each re-sample \\(b\\), you should calculate the \\(\\widehat{\\theta_b}\\). Plot all of the sample \\(\\widehat{\\theta_b}\\) in a distribution.\n\nYou can also estimate the standard error of \\(\\hat\\theta\\) using the standard deviation of the distribution. However, do not use these standard errors for confidence intervals or tests unless you are confident the sampling distribution is approximately normal.\nNonparametric Bootstrap is also used in some more complex estimators where it is very difficult to calculate or estimate the standard errors.\n\n\n\n\n\n\nOrdinary Least Squares Estimator\n\nDeriving the Estimator\nOur linear regression model, and the fitted values \\(\\hat{\\mathbf{y}}\\), take the following form:\n\\[\n\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u, \\qquad \\hat{\\mathbf y} = \\mathbf X \\hat{\\boldsymbol\\beta}\n\\]\nOLS wants to minimise the sum of squared residuals \\(S(\\hat{\\boldsymbol\\beta})\\) - the differences between the actual \\(\\mathbf y\\) and our predicted \\(\\hat{\\mathbf y}\\):\n\\[\n\\begin{align}\nS(\\hat{\\boldsymbol\\beta}) & = (\\mathbf y - \\hat{\\mathbf y})^\\mathsf{T} (\\mathbf y - \\hat{\\mathbf y})\\\\\n& = (\\mathbf y - \\color{blue}{\\mathbf X \\hat{\\boldsymbol\\beta}}\\color{black})^\\mathsf{T} (\\mathbf y - \\color{blue}{\\mathbf{X} \\hat{\\boldsymbol\\beta}}\\color{black}) && (\\text{plug in } \\color{blue}{\\hat{\\mathbf y}  = \\mathbf X \\hat{\\boldsymbol\\beta}}\\color{black}) \\\\\n& = \\mathbf y^\\mathsf{T} \\mathbf y - \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf y - \\mathbf y^\\mathsf{T} \\mathbf{X}\\hat{\\boldsymbol\\beta} + \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf{Xb} && (\\text{distribute out)} \\\\\n& = \\mathbf y^\\mathsf{T} \\mathbf y - \\color{blue}{2\\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf y}\\color{black} + \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf{X} \\hat{\\boldsymbol\\beta} &&(\\text{combine } \\color{blue}{- \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf y - \\mathbf y^\\mathsf{T} \\mathbf{X}\\hat{\\boldsymbol\\beta}}\\color{black})\n\\end{align}\n\\]\nNow, let us find the first order condition:\n\\[\n\\frac{\\partial S(\\hat{\\boldsymbol\\beta})}{\\partial \\hat{\\boldsymbol\\beta}} = -2\\mathbf X^\\mathsf{T} \\mathbf y + 2 \\mathbf X^\\mathsf{T} \\mathbf{X} \\hat{\\boldsymbol\\beta} = 0\n\\]\nWhen assuming \\(\\mathbf X^\\mathsf{T} \\mathbf X\\) is invertable (which is true if \\(\\mathbf X\\) is full rank), we can isolate \\(\\hat{\\beta}\\) to find the solution to OLS:\n\\[\n\\begin{align}\n-2\\mathbf X^T\\mathbf y + 2 \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol{\\hat{\\beta}} & = 0 \\\\\n2 \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol{\\hat\\beta} & = 2\\mathbf X^\\mathsf{T} \\mathbf y && (+ 2\\mathbf X^\\mathsf{T} \\mathbf y \\text{ to both sides}) \\\\\n\\boldsymbol{\\hat\\beta} & = (2\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} 2 \\mathbf X^\\mathsf{T} \\mathbf y && (\\times (2\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\text{ to both sides})\\\\\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y &&(\\text{cancel out } 2^{-1}\\times 2)\n\\end{align}\n\\]\nThose are our coefficient solutions to OLS.\n\n\n\nRegression Anatomy Theorem\nTake our multiple linear regression: \\(y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + ... + \\beta_k x_{ki} + u_i\\).\nLet us say we are interested in \\(x_1\\). Let us make \\(x_1\\) the outcome variable of a regression with explanatory variables \\(x_2, ..., x_k\\):\n\\[\nx_{1i} = \\gamma_0 + \\gamma_1 x_{2i} + ... + \\gamma_{k-1}x_{ki} + \\widetilde{r_{1i}}\n\\]\nThe error term \\(\\widetilde{r_{1i}}\\) is the part of \\(x_1\\) that cannot be explained by \\(x_2, ..., x_k\\).\nNow, take the regression of with outcome variable \\(y\\), with all explanatory variables except \\(x_1\\):\n\\[\ny_i = \\delta_0 + \\delta_1 x_{2i} + ... + \\delta_{k-1} x_{ki} + \\widetilde{y_i}\n\\]\nThe error term \\(\\widetilde{y_i}\\) is the part of \\(y_i\\) that cannot be explained by \\(x_2, ..., x_k\\). That implies \\(x_1\\) must be the one explaining \\(\\widetilde{y_i}\\). But, \\(x_1\\) may also correlated with \\(x_2, ..., x_k\\), and those correlated parts are already picked up in the regression coefficients of \\(x_2, ..., x_k\\). Thus, \\(\\widetilde{y_i}\\) must be explained by the part of \\(x_1\\) that is uncorrelated with \\(x_2, ..., x_k\\), which we derived earlier as \\(\\widetilde{r_{1i}}\\).\nThus, we can create another regression with explanatory variable \\(\\widetilde{x_{1i}}\\) and outcome variable \\(\\widetilde{y_i}\\):\n\\[\n\\widetilde{y_i} = \\alpha_0 + \\alpha_1 \\widetilde{r_{1i}} + u_i\n\\]\nWe plug \\(\\widetilde{y_i}\\) back into our regression of \\(y_i\\) with explanatory variables \\(x_2 ..., x_k\\):\n\\[\n\\begin{align}\ny_i & = \\delta_0 + \\delta_1 x_{2i} + ... + \\delta_{k-1} x_{ki} + \\widetilde{y_i} \\\\\ny_i & = \\delta_0 + \\delta_1 x_{2i} + ... + \\delta_{k-1} x_{ki} + \\alpha_0 + \\alpha_1 \\widetilde{r_{1i}} + u_i && (\\text{plug in } \\widetilde{y_i} = \\alpha_0 + \\alpha_1 \\widetilde{r_{1i}} + u_i)\\\\\ny_i  & = \\underbrace{(\\delta_0 + \\alpha_0)}_{\\beta_0} + \\underbrace{\\alpha_1 \\widetilde{r_{1i}}}_{\\beta_1 x_{1i}} + \\underbrace{\\delta_1x_{2i}}_{\\beta_2 x_{2i}} + ... + \\underbrace{\\delta_{k-1} x_{ki}}_{\\beta_kx_{ki}} + \\underbrace{u_i}_{u_i} && (\\text{rearrange})\n\\end{align}\n\\]\nThis new regression mirrors the original multiple linear regression. Importantly, we see the estimate of \\(\\alpha_1\\) will be the same as \\(\\beta_1\\) in the original regression. This coefficient explains the expected change in \\(y\\), given an increase in the part of \\(x_1\\) uncorrelated with \\(x_2, ..., x_k\\).\nSo essentially, we have partialed out the effect of the other explanatory variables, and only focus on the effect on \\(y\\) of the uncorrelated part of \\(x_1\\) (which is \\(\\widetilde{r_{1i}}\\)). This is what controlling for confounders is.\n\n\n\nOLS as an Unbiased Estimator\nOLS is an unbiased estimator of the relationship between any \\(x_j\\) and \\(y\\) under 4 conditions:\n\nLinearity in parameters: the model of the population (data generating process) can be modelled as \\(\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u\\).\nRandom Sampling: the observations in our sample are randomly sampled.\nNo Perfect Multicolinearity: There is no exact linear relationships between the regressors. This ensures that \\(\\mathbf X^\\mathsf{T} \\mathbf X\\) is invertible, which is required for the derivation of OLS.\nZero Conditional Mean: \\(E(\\mathbf u|\\mathbf X) = 0\\). This implies that no \\(x_j\\) is correlated with \\(\\mathbf u\\) (exogeneity), and no function of multiple regressors is correlated with \\(\\mathbf u\\).\n\nLet us prove OLS is unbiased - i.e. \\(E(\\hat{\\boldsymbol\\beta}) = \\boldsymbol\\beta\\). Let us manipulate our OLS solution:\n\\[\n\\begin{align}\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}\\color{blue}{(\\mathbf X \\boldsymbol\\beta + \\mathbf u)} && \\color{black}(\\text{plug in } \\color{blue}{\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u}\\color{black}) \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&(\\text{multiply out})\\\\\n& = \\color{blue}{\\mathbf I}\\color{black}{\\boldsymbol\\beta} + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&( \\ \\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X = \\mathbf I}\\color{black})\\\\\n& = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{identity property of } \\mathbf I)\n\\end{align}\n\\]\nNow, let us take the expectation of \\(\\boldsymbol{\\hat\\beta}\\) conditional on \\(\\mathbf X\\). Remember condition 4, \\(E(\\mathbf u | \\mathbf X) = 0\\):\n\\[\n\\begin{align}\nE(\\boldsymbol{\\hat\\beta}|\\mathbf X) & = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} E(\\mathbf u | \\mathbf X) &&(\\mathbf u \\text{ conditional on value of } \\mathbf X) \\\\\nE(\\boldsymbol{\\hat\\beta}|\\mathbf X) & = \\boldsymbol\\beta &&(E(\\mathbf u | \\mathbf X) = 0)\n\\end{align}\n\\]\nNow, we can use the law of iterated expectations (LIE) to conclude this proof:\n\\[\n\\begin{align}\nE(\\boldsymbol{\\hat\\beta}) & = E(E(\\boldsymbol{\\hat\\beta}|\\mathbf X)) && (\\text{LIE: E(X) = E(E(X|Y))})\\\\\n& = E(\\boldsymbol\\beta) && (\\text{LIE: E(X) = E(E(X|Y))})\\\\\n& = \\boldsymbol\\beta && (\\text{expecation of a constant})\n\\end{align}\n\\]\nThus, OLS is unbiased under the 4 conditions above.\n\n\n\nGauss-Markov Theorem\nThe Gauss-Markov Theorem states that the OLS estimator is the best linear unbiased estimator (BLUE) - the unbiased linear estimator with the lowest variance, under 5 conditions:\n\nLinearity (see unbiasedness conditions)\nRandom Sampling (…)\nNo Perfect Multicollinearity (…)\nZero-Conditional Mean (…)\nHomoscedasticity (the new condition).\n\nHomoscedasticity is when no matter the values of any explanatory variable, the error term variance is constant at \\(\\sigma^2\\). The error term variance does not change based on the values of the explanatory variables:\n\\[\nVar(\\mathbf u | \\mathbf X) = \\sigma^2 \\mathbf I_n = \\begin{pmatrix}\n\\sigma^2 & 0 & \\dots & 0 \\\\\n0 & \\sigma^2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & 0 \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\nVisualisation of Homoscedasticity\n\n\n\n\n\nAn easy way to identify homoscedasticity is to look at a residual plot (just the plot of all \\(\\widehat{u_i}\\)):\n\n\n\n\n\nNotice how the homoscedasticity residuals seem to have the same up-down variance, no matter the value of \\(x\\).\nThe heteroscedasticity residuals have a clear pattern - the up-down variance is smaller when \\(x\\) is smaller, and the up-down variance is larger when \\(x\\) is larger.\nEssentially, if you see a pattern in the residual plot, it is likely heteroscedasticity.\n\n\n\nThe Gauss-Markov Theorem is one of the main reasons we focus so heavily on the OLS estimator. If we believe our data-generating structure to be linear, then OLS is the best unbiased estimator we can use, since it has the lowest variance.\n\n\n\nDeriving Variance\nLet us assume homoscedasticity. We want to find the variance of our estimator, \\(Var(\\boldsymbol{\\hat\\beta} | \\mathbf X)\\). Let us start off with our OLS solution. We can simplify as follows:\n\\[\n\\begin{align}\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}\\color{blue}{(\\mathbf X \\boldsymbol\\beta + \\mathbf u)} && \\color{black}(\\text{plug in } \\color{blue}{\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u}\\color{black}) \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&(\\text{multiply out})\\\\\n& = \\color{blue}{\\mathbf I}\\color{black}{\\boldsymbol\\beta} + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&( \\ \\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X = \\mathbf I}\\color{black})\\\\\n& = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{identity property of } \\mathbf I)\n\\end{align}\n\\]\n\\[\nVar(\\boldsymbol{\\hat\\beta} | \\mathbf X) = Var(\\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u \\ | \\ \\mathbf X)\n\\]\n\\(\\boldsymbol\\beta\\) is a vector of fixed constants. \\((\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u\\) can be imagined as a matrix of fixed constants, since we are conditioning the above variance on \\(\\mathbf X\\) (so for each \\(\\mathbf X\\), the statement is fixed).\n\n\n\n\n\n\nMathematical Lemma\n\n\n\n\n\nIf \\(\\mathbf u\\) is an \\(n\\) dimensional vector of random variables, \\(\\mathbf c\\) is an \\(m\\) dimensional vector, and \\(\\mathbf B\\) is an \\(n \\times m\\) dimensional matrix with fixed constants, then the following is true:\n\\[\nVar(\\mathbf c + \\mathbf{Bu}) = \\mathbf B Var(\\mathbf u)\\mathbf B^\\mathsf{T}\n\\]\nI will not prove this lemma here, but it is provable.\n\n\n\nWith the Lemma above, and with the definition of homoscedasticity, we can simplify:\n\\[\n\\begin{align}\nVar(\\boldsymbol{\\hat\\beta} | \\mathbf X) & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} Var(\\mathbf u | \\mathbf X) [(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}]^{-1} && (\\text{lemma})\\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} Var(\\mathbf u | \\mathbf X) \\color{blue}{\\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1}} && \\color{black}( \\ \\color{blue}{[(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}]^{-1} = \\mathbf X(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1}}\\color{black})\\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\color{blue}{\\sigma^2 \\mathbf I_n}\\color{black}{ \\mathbf X} (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\color{blue}{Var(\\mathbf u | \\mathbf X) = \\sigma^2 \\mathbf I_n}\\color{black}) \\\\\n& =  \\color{red}{\\sigma^2} \\color{black} (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf I_n \\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\text{move scalar } \\color{red}{\\sigma^2}\\color{black})\\\\\n& =  \\sigma^2 (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}  \\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\text{identity property of } \\mathbf I_n)\\\\\n& =  \\sigma^2 (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\text{inverses } \\mathbf X^\\mathsf{T}  \\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\text{ cancel})\n\\end{align}\n\\]\nHowever, we do not actually know what \\(\\sigma^2\\) is. We can estimate it with \\(\\hat\\sigma^2\\) (discussed here).\nWe can use these standard errors (square root of variance) for hypothesis testing, if we believe homoscedasticity is met. If not, we will need to use robust standard errors, which we will not derive here. In modern econometrics, it has become more common to use robust standard errors by default, unless we can definitively prove homoscedasticity is met.\n\n\n\nAsymptotic Consistency of OLS\nOLS is an asymptotically consistent estimator of the relationship between any \\(x_j\\) and \\(y\\) under 4 conditions. These conditions are identical to the unbiasedness conditions EXCEPT condition 4, which is weakened from the original unbiasedness condition.\n\nLinearity (see unbiasedness)\nRandom Sampling (…)\nNo Perfect Multicolinearity (…)\nZero Mean and Exogeneity: \\(E(u_i) = 0\\), and \\(Cov(x_i, u_i) = 0\\), which implies \\(E(\\mathbf x_i u_i) = 0\\). This means that no regressor should be correlated with \\(\\mathbf u\\). This is weaker than Zero-Conditional mean, since it means a function of regressors can be correlated with \\(\\mathbf u\\).\n\nWe need condition 3 to ensure \\(\\mathbf X^\\mathsf{T} \\mathbf X\\) is invertible, in order to have OLS estimates. Once we have OLS estimates (derivation above), we can manipulate it as following:\n\\[\n\\begin{align}\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}\\color{blue}{(\\mathbf X \\boldsymbol\\beta + \\mathbf u)} && \\color{black}(\\text{plug in } \\color{blue}{\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u}\\color{black}) \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&(\\text{multiply out})\\\\\n& = \\color{blue}{\\mathbf I}\\color{black}{\\boldsymbol\\beta} + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&( \\ \\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X = \\mathbf I}\\color{black})\\\\\n& = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{identity property of } \\mathbf I)\n\\end{align}\n\\]\n\n\n\n\n\n\nVector Notation\n\n\n\n\n\nThe following statements are true:\n\\[\n\\begin{split}\n& \\mathbf X^\\mathsf{T} \\mathbf X = \\sum\\limits_{i=1}^n \\mathbf x_i \\mathbf x_i^\\mathsf{T} \\\\\n& \\mathbf X^\\mathsf{T} \\mathbf  u = \\sum\\limits_{i=1}^n \\mathbf x_i u_i\n\\end{split}\n\\]\n\n\n\nUsing vector notation, law of large numbers, and zero-mean and exogeneity condition, we can simplify the above to:\n\\[\n\\begin{align}\n\\boldsymbol{\\hat\\beta} & = \\boldsymbol\\beta + \\left( \\sum\\limits_{i=1}^n \\mathbf x_i \\mathbf x_i^\\mathsf{T} \\right)^{-1} \\left( \\sum\\limits_{i=1}^n \\mathbf x_i \\mathbf u \\right) && (\\text{vector notation})\\\\\n\\boldsymbol{\\hat\\beta} & = \\boldsymbol\\beta + \\left( \\frac{1}{n}\\sum\\limits_{i=1}^n \\mathbf x_i \\mathbf x_i^\\mathsf{T} \\right)^{-1} \\left( \\frac{1}{n}\\sum\\limits_{i=1}^n \\mathbf x_i \\mathbf u \\right) && ( \\ \\left(\\frac{1}{n} \\right)^{-1} \\text{and } \\frac{1}{n} \\text{ cancel out}) \\\\\n\\text{plim} \\boldsymbol{\\hat\\beta} & = \\boldsymbol\\beta + \\left( \\text{plim} \\frac{1}{n}\\sum\\limits_{i=1}^n \\mathbf x_i \\mathbf x_i^\\mathsf{T} \\right)^{-1} \\left( \\text{plim} \\frac{1}{n}\\sum\\limits_{i=1}^n \\mathbf x_i u_i \\right) && (\\text{apply plim}) \\\\\n\\text{plim} \\boldsymbol{\\hat\\beta} & = \\boldsymbol\\beta + (E(\\mathbf x_i \\mathbf x_i^\\mathsf{T}))^{-1}E(\\mathbf x_i  u_i) && (\\text{law of large numbers})\\\\\n\\text{plim} \\boldsymbol{\\hat\\beta} & = \\boldsymbol\\beta && (E(\\mathbf x_i u_i) = 0)\n\\end{align}\n\\]\nThus, OLS is asymptotically consistent under the 4 conditions above.\n\n\n\nOLS as a Conditional Expectation Function\n\n\n\n\n\n\nConditional Expectation Functions\n\n\n\n\n\nA conditional expectation function (CEF) says that the value of \\(E(y)\\) depends on the value of \\(x\\). We notate a conditional expectation function as \\(E(y|x)\\). As we noted earlier, the linear regression model can be a conditional expectation function of \\(E(y|x)\\).\nA best linear approximation of a conditional expectation function can take the following form:\n\\[\nE(y_i|x_i) = b_0 + b_1x_i\n\\]\nWith parameters \\(b_0, b_1\\) that minimise the mean squared errors (MSE).\n\\[\n\\begin{split}\nMSE & = E(y_i - E(y_i|x_i))^2 \\\\\n& = \\frac{1}{n}\\sum\\limits_{i=1}^n( y_i - E(y_i|x_i))^2\n\\end{split}\n\\]\n\n\n\nOLS is a best-linear approximation of the conditional expectation function. Suppose we have the conditional expectation function, and its mean squared errors:\n\\[\n\\begin{align}\nE(y_i|x_i) & = b_0 + b_1x_i \\\\\nMSE & = E(y_i - E(y_i|x_i))^2 \\\\\n& =  E(y_i - \\beta_0 - \\beta_1x_i)^2\n\\end{align}\n\\]\nThe first order conditions are (using chain rule and partial derivatives):\n\\[\n\\begin{split}\n& E(y_i - b_0 - b_1x_i) = 0 \\\\\n& E(x_i(y_i - b_0 - b_1x_i) = 0\n\\end{split}\n\\]\nNow, recall our OLS minimisation conditions (simple linear regression):\n\\[\n\\begin{split}\n& \\sum\\limits_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\\n& \\sum\\limits_{i=1}^n x_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0\n\\end{split}\n\\]\nSince by definition, average/expectation is \\(E(x) = \\frac{1}{n} \\sum x_i\\), we can rewrite as:\n\\[\n\\begin{split}\n& n \\times E(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i) = 0 \\\\\n& n \\times E(x_i(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i)) = 0\n\\end{split}\n\\]\nAnd since anything multiplied to a zero turns into zero, we can ignore the \\(n\\) in the first order condition. Thus, our conditions are:\n\\[\n\\begin{split}\n& E(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i) = 0 \\\\\n& E(x_i(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i)) = 0\n\\end{split}\n\\]\nWhich as we can see, are the exact same minimisation conditions as the conditional expectation function. Thus, OLS is the best approximation of the conditional expectation function.\nThis property is very useful for causal inference, as it means OLS calculates the expected \\(y\\), which allows us to find causal effects by comparing the expected \\(y\\) of the treatment and control groups (assuming the OLS estimator is unbiased).\n\n\n\n\n\n\nMethod of Moments Estimator\n\nMethod of Moments\nThe Method of Moments Estimator is another estimator of the true value of populations in the parameter. The estimator defines key population moments of interest - which are the population parameters written in terms of expected value functions set equal to 0.\nThen, the Method of Moments uses the sample equivalents of the population moments to estimate the population parameter. For example, to estimate the population mean, the Method of Moments uses the sample mean.\nIn order to define a method of moments for a set of parameters \\(\\theta_1, \\dots, \\theta_k\\), we need to specify at least one population moment per parameter. Or in other words, we must have more than \\(k\\) population moments.\nOur population moments can be defined as the expected value of some function \\(m(\\theta; y)\\) that consists of both the variable \\(y\\) and our unknown parameter \\(\\theta\\). The expectation of the function \\(m(\\theta; y)\\) should equal 0.\n\\[\nE(m(\\theta; y)) = 0\n\\]\nOur sample moments will be the sample analogues of \\(\\theta\\) and \\(y\\), which are \\(\\hat\\theta\\) and \\(y_i\\):\n\\[\n\\frac{1}{n}\\sum\\limits_{i=1}^n m(\\hat\\theta; y_i) = 0\n\\]\nMethod of moments estimators are asymptotically consistent, because of the law of large numbers.\n\n\n\nPopulation Mean Estimator\nLet us say that we have some random variable \\(y\\), with a true population mean \\(\\mu\\). We want to estimate \\(\\mu\\), but we only have a sample of the population.\nHow can we define \\(\\mu\\) in a moment of the form: \\(E(m(\\mu, y)) = 0\\)? Well, we know \\(\\mu\\) is the expectation of \\(y\\), so \\(\\mu = E(y)\\). Since they are equal, \\(\\mu - E(y) = 0\\). Thus, we can define the mean as a moment of the following condition:\n\\[\nE(y - \\mu) = 0\n\\]\nThe method of moments estimator uses the sample equivalent of the population moment. The sample equivalent of \\(\\mu\\), is the sample mean \\(\\bar y\\):\n\\[\nE(y_i - \\hat\\mu) = \\frac{1}{n}\\sum\\limits_{i=1}^n (y_i - \\hat\\mu) = 0\n\\]\nWith this equation, we can then solve for \\(\\hat\\mu\\):\n\\[\n\\begin{align}\n0 & = \\frac{1}{n}\\sum\\limits_{i=1}^n (y_i - \\hat\\mu) \\\\\n0 & = \\frac{1}{n}\\sum\\limits_{i=1}^ny_i - \\frac{1}{n}\\sum\\limits_{i=1}^n \\hat\\mu  && (\\text{multiply out})\\\\\n0 & = \\frac{1}{n}\\sum\\limits_{i=1}^ny_i - \\frac{1}{n} n \\hat\\mu &&(\\text{summation property of constant } \\hat\\mu)\\\\\n0 & = \\bar y - \\hat \\mu && (\\text{definition of mean }\\frac{1}{n}\\sum\\limits_{i=1}^ny_i = \\bar y)\\\\\n\\hat\\mu & = \\bar y && (+\\hat\\mu\\text{ to both sides})\n\\end{align}\n\\]\nSo, we see the method of moments estimates our true population mean \\(\\mu\\), with the sample mean \\(\\bar y\\). As a method of moments estimator, it is also asymptotically consistent.\n\n\n\nOLS as a Method of Moments Estimator\nOLS is a special case of the Method of Moments Estimator. Consider the bivariate regression model. The OLS estimator can be derived as a method of moments estimator, with 2 moments (expectation functions set equal to 0), one for each parameter (\\(\\beta_0, \\beta_1\\)):\n\\[\n\\begin{split}\n& E(y-\\beta_0 -\\beta_1x) = 0 \\\\\n& E(x(y - \\beta_0 - \\beta_1 x)) = 0\n\\end{split}\n\\]\nThe estimates of these moments would use the sample equivalents: \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\).\n\\[\n\\begin{split}\n& E(y-\\hat\\beta_0 -\\hat\\beta_1x) = 0 \\\\\n& E(x(y - \\hat\\beta_0 - \\hat\\beta_1 x)) = 0\n\\end{split}\n\\]\nRemember our OLS minimisation conditions:\n\\[\n\\begin{split}\n& \\sum\\limits_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\\n& \\sum\\limits_{i=1}^n x_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0\n\\end{split}\n\\]\nSince by definition, average/expectation is \\(E(x) = \\frac{1}{n} \\sum x_i\\), we can rewrite the OLS minimisation conditions as:\n\\[\n\\begin{split}\n& n \\times E(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i) = 0 \\\\\n& n \\times E(x_i(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i)) = 0\n\\end{split}\n\\]\nAnd since anything multiplied to a zero turns into zero, we can ignore the \\(n\\) in the first order condition, and only focus on the expected value part. Thus, our conditions are:\n\\[\n\\begin{split}\n& E(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i) = 0 \\\\\n& E(x_i(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i)) = 0\n\\end{split}\n\\]\nWhich as we can see, are the exact same minimisation conditions as the method of moments estimator. Thus, the OLS estimator is a special case of the Method of Moments estimator, and they produce the same coefficients. This is an important property for the instrumental variables method that will be covered later.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "3 Classic Least Squares Theory"
    ]
  },
  {
    "objectID": "4.html",
    "href": "4.html",
    "title": "Causal Frameworks",
    "section": "",
    "text": "In the past few chapters, we have focused on correlations.\nThis chapter introduces the main causal frameworks (potential outcomes, causal graphs), the main causal estimands used in causal inference, and the idea of selection bias and confounders. This chapter is the foundation in which the next set of methods for causal inference will be built on.\nUse the right sidebar for quick navigation. R-code for causal diagrams is provided at the bottom.\n\n\nPotential Outcomes Framework\n\nTreatment and Potential Outcomes\nIn causal inference, we are interested in how treatment \\(D\\) causes outcome variable \\(Y\\).\n\\(D\\) is our treatment variable. The indicator of treatment for each unit \\(i\\) is \\(D_i\\).\n\\[\nD_i = \\begin{cases}\n1 \\quad \\text{if unit } i \\text{ recieved the treatment} \\\\\n0 \\quad \\text{if unit } i \\text{ did not recieve the treatment}\n\\end{cases}\n\\]\n\n\n\n\n\n\nFurther information on Treatment Variables\n\n\n\n\n\nCausal variables/treatments must occur before the outcome. A variable cannot cause something to occur in the past.\nCausal variables/treatments must be able to be manipulated (in order to imagine a world where the treatment did not occur).\n\nFor example, \\(D\\) cannot be sex assigned at birth, ethnicity, etc.\nFor example, major global events (how did 9/11 cause the Arab spring?)\n\n\n\n\nImagine there are two hypothetical parallel worlds - one where unit \\(i\\) receives the treatment \\(D\\), and one where unit \\(i\\) does not receive the treatment \\(D\\). Everything else in these worlds is identical.\nPotential Outcomes for unit \\(i\\) are denoted:\n\\[\nY_{di}, Y_i(d) =\\begin{cases}\n& Y_{1i}, \\ Y_i(1) \\quad \\text{Outcome for unit } i \\text{ when } D_i = 1 \\\\\n& Y_{0i}, \\ Y_i(0) \\quad \\text{Outcome for unit } i \\text{ when } D_i = 0 \\\\\n\\end{cases}\n\\]\n\n\n\n\n\n\nExample of Potential Outcomes\n\n\n\n\n\nFor example, imagine we are interested in finding the effect of democracy \\(D\\) on GDP growth \\(Y\\). Potential outcome \\(Y_{1i}\\) is the potential GDP growth of country \\(i\\) if they were a democracy, and outcome \\(Y_{0i}\\) is the potential GDP growth of a country \\(i\\) if they were not a democracy.\n\n\n\n\n\n\nObserved Outcomes and “Missing Data”\nOf course, there is not two parallel worlds with 2 potential outcomes. In the real world, each unit \\(i\\) either receives treatment \\(D\\), or does not. We do not observe the other potential outcome.\n\\(Y_i\\) is the observed outcome for unit \\(i\\). This is given by formula:\n\\[\nY_i = D_i \\cdot Y_{1i} + (1-D_i) \\cdot Y_{0i}\n\\]\nIf we plug in \\(D_i = 0, 1\\) to the equation above, we get the observed outcomes:\n\\[\nY_i = \\begin{cases}\nY_{1i} \\quad \\text{if } D_i = 1 \\\\\nY_{0i} \\quad \\text{if } D_i = 0 \\\\\n\\end{cases}\n\\]\nBefore the treatment (A priori), both potential outcomes could be observed. After the treatment, one is observed, and the other is counterfactual. For any given experiment, only one will ever be seen, and the counterfactual will never be seen (missing data problem).\n\n\n\n\n\n\nNeyman Urn Model\n\n\n\n\n\nPotential Outcomes can be visualised with the Neyman Urn Model.\nBefore the treatment, we have a box (we cannot see) with both potential outcomes.\n\n\n\n\n\nThen, when we apply treatment, we stick our hand into the box that we cannot see, and pull out one observed outcome.\n\n\n\n\n\nWe are essentially sampling from potential outcomes to get observed outcomes.\n\n\n\nThis missing data problem is called the fundamental problem of causal inference.\n\n\n\nStable Unit Treatment Value Assumption\nThe above given observed and potential outcome frameworks depends on the Stable Unit Treatment Value Assumption (SUTVA).\n\\[\n\\begin{align}\nY_{(D_1, D_2, \\dots, D_N)i} & = Y_{(D_1', D_2', \\dots, D_N')i} \\\\\nY_{di} \\text{ under current randomisation} & = Y_{di} \\text{ under all other randomisations}\n\\end{align}\n\\]\nOr more intuitively, the potential outcomes of unit \\(i\\) only depends on their own treatment status, and no other unit’s treatment status. Thus, changing everyone else’s treatment status has no effects on unit \\(i\\)’s potential outcomes \\(Y_{di}\\). The treatment is also the same for everyone (treatment is stable and consistent)\n\n\n\n\n\n\nExamples of SUTVA Violations\n\n\n\n\n\n\nSpill-over effects: If we are testing a new curriculum, one student \\(j\\) getting the new curriculum may teach their friend \\(i\\) the new curriculum, thus affecting the potential outcomes of \\(i\\).\nContagion: If we are studying a disease, diseases can spread, so another unit \\(j\\) getting a disease affects the potential outcomes of unit \\(i\\).\nDilution: If we are studying vaccines - there is herd immunity - other people getting the vaccine also reduces our chances of getting the disease.\nVariable levels of treatment: If we are doing a drug trial, if some people got two doses, while others only got one dose. This is not a consistent treatment.\nTechnical errors: If someone who is supposed to be treated accidentally is not treated. This is not a consistent treatment.\n\n\n\n\nWhen SUTVA is violated, potential outcomes become very messy, and we no longer have the neat framework as before.\n\n\n\n\n\n\nCausal Estimands\n\nIndividual Treatment Effect\nRemember the potential outcomes from parallel worlds \\(Y_{1i}\\) and \\(Y_{0i}\\).\nSince these two parallel worlds are identical except for the fact one receives the treatment \\(D\\) and the other does not, the causal effect of \\(D\\) should be the difference between the potential outcomes of these two worlds. Thus, the individual treatment effect of a unit \\(i\\) is:\n\\[\n\\tau_i = Y_{1i} - Y_{0i}\n\\]\nThis is the specific treatment effect for a specific unit \\(i\\). This cannot be observed, because we do not see both potential outcomes for the same unit \\(i\\).\nThis is also very hard to estimate, as we cannot reliably fill in the missing potential outcome for any one unit \\(i\\). Thus, we almost never use individual treatment effects, and use group treatment effects.\n\n\n\nAverage Treatment Effect (ATE)\nATE is a group-level causal estimand.\n\n\n\n\n\n\nGroup-Level Causal Estimands\n\n\n\n\n\nConsider a population of units \\(i = 1, \\dots, N\\).\nThe population has potential outcomes represented in two (only partially observed) vectors:\n\\[\n\\begin{split}\n& Y_1 = (Y_{11}, Y_{12}, \\dots, Y_{1N}) \\\\\n& Y_0 = (Y_{01}, Y_{02}, \\dots, Y_{0N})\n\\end{split}\n\\]\nWe compare these two vectors of potential outcomes. The most common way to do this is to use their expected values.\n\n\n\nThe Average Treatment Effect is defined as:\n\\[\n\\begin{split}\n\\tau_{ATE} & = E(Y_{1i} - Y_{0i}) \\\\\n& = \\underbrace{\\frac{1}{N} \\sum\\limits_{i=1}^N (Y_{1i} - Y_{0i})}_{\\text{a formula for average}}\n\\end{split}\n\\]\nWe cannot calculate this with observed data - since we need all potential outcomes to do this. We can estimate this (covered throughout this course).\n\n\n\nAverage Treatment Effect on the Treated (ATT)\nAn alternative estimand to the ATE is the Average Treatment Effect on the Treated (ATT):\n\\[\n\\begin{split}\n\\tau_{ATT} & = E(Y_{1i} - Y_{0i} \\ | \\ D_i = 1) \\\\\n& = \\underbrace{\\frac{1}{N_1} \\sum\\limits_{i=1}^N D_i (Y_{1i} - Y_{0i}) \\quad  \\text{where } N_1 = \\sum\\limits_{i=1}^ND_i}_{\\text{a formula for the average only for treated units}}\n\\end{split}\n\\]\nThis is the causal effect of only units who have received the treatment. Note that frequently the ATT is not equal to the ATE, so be aware of which estimand you are trying to estimate/identify.\n\n\n\n\n\n\nATT vs. ATE\n\n\n\n\n\nWhen does \\(\\tau_{ATT} = \\tau_{ATE}\\)?\n\nWhen the expectation of the potential outcomes of both the treated and control are the same, then the two equal each other.\n\nThe opposite is also true: if the expectation of the potential outcomes of both the treated and control are different, then the two are not equal.\n\n\n\nThe opposite estimand is the Average Treatment effect on the Untreated (ATU), which only measures the causal effect of units who did not recieve the treatment.\nThis is not used very often, since it is kind of uninituive to think about treatment effects on individuals who did not recieve treatment. However, it can be useful in understanding identification assumptions.\n\n\n\nConditional Average Treatment Effect (CATE)\nThe conditional average treatment effect is any treatment effect where there is a condition on a characteristic/covariate:\n\\[\n\\tau_{CATE}(x) = E(Y_{1i} - Y_{0i} \\ | \\ \\underbrace{X_i = x)}_{\\text{condition}}\n\\]\nThis is the causal effect of only variables who meet the condition of the covariate specified. For example, you could find the conditional average treatment effect of only women (so the covariate which we are conditioning on is gender). You can also condition on multiple covariates.\nThis is often used for tailoring products/medicine/advertising to certain groups of people. It is also frequently used in identification strategies.\nThis estimand will go by other names, including the Local Average Treatment Effect (LATE).\n\n\n\n\n\n\nSelection Bias and Confounders\n\nNaive Estimator and Selection Bias\nA natural way to estimate the ATE is to use a naive estimator: find the average difference of observed outcomes. This is called the naive estimator:\n\\[\n\\hat\\tau_{naive} = \\underbrace{E(Y_i|D_i = 1)}_{\\text{for treated}} - \\underbrace{E(Y_i|D_i = 0)}_{\\text{for control}}\n\\]\nHowever, there is an issue - we can show this with algebra:\n\\[\n\\begin{align}\n\\hat\\tau_{naive} & = E(Y_i|D_i = 1) - E(Y_i|D_i = 0) \\\\\n& = \\underbrace{E(Y_{1i}|D_i = 1) - E(Y_{0i} | D_i = 0)}_{\\because \\text{ observed potential outcomes}} \\\\\n& = E(Y_{1i}|D_i = 1) - E(Y_{0i} | D_i = 0) + \\underbrace{E(Y_{0i}|D_i = 1) \\color{red}{- E(Y_{0i}|D_i = 1)}}_{\\because \\text{ this equals 0, so we can add it}} \\\\\n& = \\underbrace{E(Y_{1i}|D_i = 1) \\color{red}{- E(Y_{0i}|D_i = 1)}}_{\\tau_{ATT}} + \\underbrace{E(Y_{0i}|D_i = 1) - E(Y_{0i} | D_i = 0)}_{\\text{Selection Bias}} \\\\\n\\end{align}\n\\]\nWe can see that our naive estimator produces the \\(\\tau_{ATT}\\) plus an extra bit (called the selection bias). Thus, our naive estimator is biased, so we should be careful about using this naive estimator (correlation does not equal causation).\n\n\n\n\n\n\nNaive Estimator Biased for ATU\n\n\n\n\n\nThe proof above shows how the naive estimator is a biased estimator for the \\(\\tau_{ATT}\\). We can also prove it is a biased estimator of the ATU:\n\\[\n\\begin{split}\n\\hat\\tau_{naive} & = E(Y_i|D_i = 1) - E(Y_i|D_i = 0) \\\\\n& = \\underbrace{E(Y_{1i}|D_i = 1) - E(Y_{0i} | D_i = 0)}_{\\because \\text{ observed potential outcomes}} \\\\\n& = E(Y_{1i}|D_i = 1) - E(Y_{0i} | D_i = 0) + \\underbrace{E(Y_{1i}|D_i = 0) - E(Y_{1i}|D_i = 0)}_{\\because \\text{ this equals 0, so we can add it}} \\\\\n& = \\underbrace{E(Y_{1i}|D_i = 0)- E(Y_{0i}|D_i = 0)}_{\\tau_{ATU}} + \\underbrace{E(Y_{1i}|D_i = 1) - E(Y_{1i} | D_i = 0)}_{\\text{Selection Bias}} \\\\\n\\end{split}\n\\]\n\n\n\n\n\n\n\n\n\nNaive Estimator Biased for ATE\n\n\n\n\n\nThe proofs above shows how the naive estimator is a biased estimator for the \\(\\tau_{ATT}\\) and \\(\\tau_{ATU}\\). We can also prove it is a biased estimator of the ATE.\nLet us first start with the ATE. Let us call \\(Y_{1i} - Y_{0i} := \\tau_i\\) for notation simplicity:\n\\[\n\\begin{align}\n\\tau_{ATE} & = E(Y_{1i} - Y_{0i})  = E(\\tau_i)\\\\\n& = \\underbrace{E(\\tau_i|D_i = 1)Pr(D_i = 1) + E(\\tau_i|D_i = 0)Pr(D_i = 0)}_{\\because \\text{ weighted average of ATE and ATU by proportion}} \\\\\n& = E(\\tau_i|D_i = 1) \\underbrace{(1 -Pr(D_i = 0))}_{\\because \\text{ complement prob.}} + E(\\tau_i|D_i = 0)Pr(D_i = 0) \\\\\n\\end{align}\n\\]\nLet us call \\(Pr(D_i = 0) := \\pi\\) for notation simplicity. Now, continue:\n\\[\n\\begin{split}\n& = \\underbrace{E(\\Delta|D_i = 1) - \\pi E(\\Delta|D_i = 1)}_{\\because \\text{ distribute out}} + E(\\tau_i|D_i = 0)\\pi \\\\\n& = E(\\tau_i|D_i = 1) + \\underbrace{\\pi[E(\\tau_i|D_i = 0) - E(\\tau_i|D_i = 1)]}_{\\because \\ \\pi \\text{ factored out }} \\\\\n& = E(Y_{1i} |D_i = 1) - E(Y_{0i}|D_i = 1) + \\pi[E(\\tau_i|D_i = 0) - E(\\tau_i|D_i = 1)] \\\\\n\\end{split}\n\\]\nLet us call the part \\(\\pi[E(\\tau_i|D_i = 0) - E(\\tau_i|D_i = 1)] := \\Pi(\\tau_i)\\). Now, continue to simplify:\n\\[\n\\begin{split}\n& = E(Y_{1i} |D_i = 1) - E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) + \\underbrace{E(Y_{1i} |D_i = 0) - E(Y_{0i}|D_i = 0)}_{\\because \\text{ these two cancel out so we add 0}}  \\\\\n& = E(Y_{1i} |D_i = 1) - E(Y_{0i}|D_i = 0) + E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& = \\underbrace{E(Y_i|D_i = 1)}_{\\because \\text{ observed outcome}} - \\underbrace{E(Y_i|D_i = 0)}_{\\because \\text{ observed outcome}} + E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& = \\underbrace{E(Y_{i} |D_i = 1) - E(Y_{i}|D_i = 0)}_{\\hat\\tau_{naive}} + E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& = \\hat\\tau_{naive}+ E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i)\n\\end{split}\n\\]\nThus, we can see that \\(\\tau_{ATE}\\) is not equivalent to \\(\\hat\\tau_{naive}\\). Let us isolate \\(\\hat\\tau_{naive}\\) to identify the selection bias.\n\\[\n\\begin{split}\n& \\tau_{ATE} = \\hat\\tau_{naive}+ E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& -\\hat\\tau_{naive} = -\\tau{ATE} + E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& \\hat\\tau_{naive} = \\tau_{ATE} - E(Y_{1i} |D_i = 0) + E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& \\hat\\tau_{naive} = \\tau_{ATE} + \\underbrace{E(Y_{0i}|D_i = 1)- E(Y_{1i} |D_i = 0) + \\Pi(\\tau_i)}_{\\text{selection bias}}\n\\end{split}\n\\]\n\n\n\n\n\n\nConfounders\nTake the selection bias formula from above:\n\\[\n\\underbrace{E(Y_{0i}|D_i = 1)}_{Y_{0i}\\text{ (treated)}} - \\underbrace{E(Y_{0i} | D_i = 0)}_{Y_{0i}\\text{ (control)}}\n\\]\nIf selection bias is non-zero, this essentially states that the expected potential outcome before treatment \\(Y_{0i}\\) between the treatment and control groups is not equal.\nOr in other words, the treatment and control groups have some other variable causing differences even before treatment has begun. This implies that the differences between the treatment and control group may not be due to treatment \\(D\\), but due to the underlying differences before treatment even occurred.\nConfounders are variables that cause the differences between treatment and control groups before the treatment has started. Confounders correlate with both the treatment variable and the outcome. If a variable only correlates with \\(D\\) or \\(Y\\), then it is not a confounder. If must correlate with both \\(D\\) and \\(Y\\).\nThis is why correlation does not equal causation - if the treatment and control group are different before we start the experiment, we cannot say the difference between the two is purely a result of treatment \\(D\\).\n\n\n\nOmitted Variable Bias in Regression\nWe can demonstrate how confounders cause bias in regression. Suppose there is some confounding variable \\(z\\) that we have not included in a “short” regression. The actual, “true” regression of the population, would include this confounder \\(z\\)\n\\[\n\\underbrace{\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u}_{\\text{short regression}}\n\\qquad \\underbrace{\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf z\\boldsymbol\\delta + \\mathbf u}_{\\text{true regression with z} }\n\\]\nThe OLS estimate of the “short regression” excluding confounder \\(z\\) is:\n\\[\n\\boldsymbol{\\hat\\beta} = (\\mathbf X^T \\mathbf X)^{-1} \\mathbf X^T \\mathbf y\n\\]\nLet us now plug in the “true” model into where \\(\\mathbf y\\) is:\n\\[\n\\begin{align}\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} (\\color{blue}{\\mathbf X \\boldsymbol\\beta + \\mathbf z\\boldsymbol\\delta + \\mathbf u}\\color{black}) && (\\text{plug in } \\color{blue}{\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf z\\boldsymbol\\delta + \\mathbf u}\\color{black} )\\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf z\\boldsymbol\\delta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{multiply out})\\\\\n& = \\color{blue}{\\mathbf I}\\color{black}{\\boldsymbol\\beta} + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf z\\boldsymbol\\delta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{inverses } (\\color{blue}{\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X = \\mathbf I}\\color{black})\\\\\n& = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf z\\boldsymbol\\delta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{identity property of } \\mathbf I)\n\\end{align}\n\\]\nNow, let us find the expected value of \\(\\boldsymbol{\\hat\\beta}\\), which is conditional on \\(\\mathbf X, \\mathbf z\\), and simplify (using zero conditional mean):\n\\[\n\\begin{align}\nE(\\boldsymbol{\\hat\\beta}|\\mathbf X, \\mathbf z) & = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf z\\boldsymbol\\delta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} E(\\mathbf u | \\mathbf X, \\mathbf z) \\\\\n& = \\boldsymbol\\beta + (\\mathbf X^T \\mathbf X)^{-1} \\mathbf X^T \\mathbf z\\boldsymbol\\delta &&(\\because E(\\mathbf u | \\mathbf X, \\mathbf z) = 0)\n\\end{align}\n\\]\nNow, what if we had a regression of outcome variable being the confounder \\(z\\), on the explanatory variables \\(\\mathbf X\\), such that \\(\\mathbf z = \\mathbf X \\boldsymbol\\pi + \\mathbf v\\). Our OLS estimate would have the solution:\n\\[\n\\boldsymbol{\\hat\\pi} = (\\mathbf X^T\\mathbf X)^{-1}\\mathbf X^T \\mathbf z\n\\]\nNow, we can plug \\(\\boldsymbol{\\hat\\pi}\\) into our expected value of \\(\\boldsymbol{\\hat\\beta}\\):\n\\[\nE(\\boldsymbol{\\hat\\beta}|\\mathbf X, \\mathbf z)  = \\boldsymbol\\beta + \\boldsymbol{\\hat\\pi \\delta}\n\\]\nNow, using the law of iterated expectations, we get (assuming we are using an unbiased estimator for \\(\\boldsymbol{\\hat\\pi}\\) such that \\(E(\\boldsymbol{\\hat\\pi}) = \\boldsymbol\\pi\\)):\n\\[\n\\begin{align}\nE(\\boldsymbol{\\hat\\beta}) & = E(E(\\boldsymbol{\\hat\\beta}|\\mathbf X, \\mathbf z)) && (\\text{LIE: } E(X) = E(E(X|Y))\\\\\n& = E(\\boldsymbol\\beta + \\boldsymbol{\\hat\\pi \\delta})  && (\\text{LIE: } E(X) = E(E(X|Y)) \\\\\n& = \\boldsymbol\\beta + E(\\boldsymbol{\\hat\\pi}) \\boldsymbol \\delta && (\\boldsymbol\\beta, \\boldsymbol\\delta\\text{ are constants})\\\\\n& = \\boldsymbol\\beta + \\boldsymbol{\\pi \\delta} && (\\text{unbiased estimator } E(\\boldsymbol{\\hat\\pi}) = \\boldsymbol\\pi)\n\\end{align}\n\\]\nThus, we can see by not including confounder \\(z\\) in our “short regression”, the estimator is now biased by \\(\\boldsymbol{\\hat\\pi \\delta}\\).\n\n\n\nAssignment Mechanism\nThe Assignment Mechanism is the procedure that determines the treatment status of each unit. In causal inference, we want to restrict the assignment mechanism, in order to remove the effect of selection bias.\nThere are two types of studies that use different assignment mechanisms:\n\nRandomisation Experiments: The assignment mechanism is both known, and controlled by the researcher. Generally, the researcher chooses some type of randomisation.\nObservational Studies: The assignment mechanism is not known to, or not under the control of the researcher. This means that confounders may be driving selection into treatment and control, inducing bias.\n\nGenerally, the most credible studies are randomisation studies, since we can control interventions to parse out the effect of confounders. Observational studies generally rely on more assumptions that need to be met, and need to be defended for the study to be credible.\n\n\n\n\n\n\nDirected Acyclic Graphs\n\nComponents of the Graphs\nCausal Diagrams are a visual way to represent causal theories and frameworks, which allows us to visualise how variables interact with each other.\nEach Directed Acyclic Graph (DAG) has the following components:\n\nNodes: representing variables (which are also called vertices).\nDirected Edges: Arrows that encode one-way causal theories between variables. For example, we might believe \\(Z\\) causes \\(X\\), so we will draw an arrow from \\(Z\\) to \\(X\\). These connections are observable (solid) or unobservable (dashed).\n\n\n\n\n\n\n\nExample of Directed Acyclic Graphs\n\n\n\n\n\nBelow is an example of a directed acyclic graph:\n\n\n\n\n\nWhat does this diagram show?\n\nWe have two unobserved variables: \\(Q\\) and \\(Y\\)\nWe have three observed variables: \\(Z\\), \\(D\\), and \\(Y\\).\nWe can see the causal theories represented by arrows.\n\nWhat can we learn from this diagram?\n\n\\(Z \\rightarrow Y\\) is confounded by \\(W\\): \\(W\\) is affecting who gets treatment \\(Z\\), and causing \\(Y\\). Thus, \\(W\\) is affecting who gets selected into treatment \\(Z\\), and selecting your potential outcome \\(Y\\). Thus, this is an example of selection bias.\n\\(D \\rightarrow Y\\) is confounded by \\(Q\\).\n\\(Z \\rightarrow D\\) is not confounded, so we can estimate this causal effect.\n\nNote: All these conclusions are only true if our causal theory is correct (we have specified all the possible variables, and we have specified the correct causal relationships).\n\n\n\nFeatures of a Directed Acyclic Graph:\n\nThey must be acyclic: This means that they are not circular - \\(A\\) does not terminate back at \\(A\\).\nNon-Connections: The absence of relationships between variables.\n\n\n\n\nRepresenting Interventions\nTreatments (interventions by the researcher, for example) are when we determine one variable exogenously (such as by randomisation).\nOr in other words, one variable is determined randomly externally, not caused by any variables within the directed acyclic graphs.\nTreatments are represented by the do() operator. When the treatment is exogenous, we can break all the connections into that variable’s node.\nThis is because we are determining the value of the variable, not any other variables.\n\n\n\n\n\n\nExample of Interventions\n\n\n\n\n\n\n\n\n\n\nAn intervention here is on variable \\(D\\). That means the value of \\(D\\) is being chosen outside of this graph (by randomisation, or the researcher).\nThis allows us to delete the arrow between \\(Q \\rightarrow D\\) and \\(Z \\rightarrow D\\). This is because we are exogenously determining \\(D\\), so \\(Q\\) and \\(Z\\) are not determining the value of \\(D\\).\n\n\n\nWith exogenously determined variables, we can find the causal effect that variable is causing on another.\n\n\n\nBlocked Paths\nA set of nodes \\(\\{ \\mathbf S \\}\\) blocks a path \\(p\\) if either:\n\nIf the path \\(p\\) contains at least one arrow-emitting node included in the set of nodes \\(S\\), or\nThe path \\(p\\) contains at least one collision node (multiple arrows point into it) that is outside the set of nodes \\(S\\), and the collision node has no descendant within the set of nodes \\(S\\) (no arrows go out of it to another node).\n\nTake this directed acyclic graph:\n\n\n\n\n\nWe can see the following:\n\nThe path \\(D \\rightarrow P \\rightarrow Y\\) is blocked by set \\(\\{P\\}\\), because the node \\(P\\) is one arrow-emitting node that is in the path \\(D \\rightarrow P \\rightarrow Y\\).\nThe path \\(D \\leftarrow M \\rightarrow Y\\) is blocked by set \\(\\{M\\}\\), because the node \\(M\\) is one arrow-emitting node in the path \\(D \\leftarrow M \\rightarrow Y\\).\n\\(D \\leftarrow Z \\rightarrow M \\rightarrow Y\\) is blocked by \\(\\{M\\}\\), \\(\\{Z\\}\\), or \\(\\{M, Z\\}\\) - note \\(D\\) and \\(Y\\) do not emit arrows so they cannot block.\n\nBlocking paths is important, since in order to estimate \\(D \\rightarrow Y\\), we need to block any other path between \\(D\\) and \\(Y\\) that is not directly \\(D \\rightarrow Y\\).\n\n\n\n\n\n\nImplementation in R\nThis section will show how you can create DAGs in R. We will need the ggdag and dagitty packages.\n\nlibrary(ggdag)\nlibrary(dagitty)\n\n\n\n\n\n\n\nSimple DAGs with Dagify\n\n\n\n\n\nYou can create a very simple DAG with dagify as follows:\n\ndag_object &lt;- dagify(\n  Y ~ X + D, #Y is caused by X and D\n  D ~ X #D is caused by X\n)\n\nggdag(dag_object) + theme_dag()\n\nThis dag is not very customisable. This can be an issue if you want nodes to be in a specific location. See below for a more customisable DAG.\n\n\n\n\n\n\n\n\n\nCustom DAGs with Dagitty\n\n\n\n\n\nYou can create more complex DAGs with Dagitty. Dagitty allows us to position nodes in a coordinate system, which is useful in some purposes.\n\ndag_object &lt;- dagitty('dag {\n      D [pos = \"0, 1\"]\n      Y [pos = \"2, 1\"]\n      X [pos = \"1, 2\"]\n      \n      D -&gt; Y\n      D &lt;- X -&gt; Y\n  }')\n\nggdag(dag_object) + theme_dag()\n\nThe pos arguments have the coordinates of where you want to put each node.\nBelow are the path connections, where you can use -&gt; and &lt;- to indicate relationships.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "4 Causal Frameworks"
    ]
  },
  {
    "objectID": "5.html",
    "href": "5.html",
    "title": "Randomised Controlled Trials",
    "section": "",
    "text": "Up until now, we have focused on theory of statistics and causal inference. But now, we are ready to dive into methodology - different designs to measure and identify causal effects.\nThis chapter introduces the “gold standard” of causal inference: randomised controlled trials. This chapter also covers extensions, such as stratified experiments and survey experiments.\nUse the right sidebar for quick navigation.\n\n\nRandomisation\n\nRandomised Experiments\nExperiments are a research design where the assignment mechanism is controlled by the researcher.\nRandomised Experiments use randomisation as the assignment mechanism. Treatment values are assigned to \\(N\\) units at random, with both known and positive probabilities of being assigned to treatment and control groups.\nQuick notation for randomised experiments:\n\nWe have \\(N\\) total number of units in our experiment.\nA randomly subset of \\(N_1\\) units are assigned to treatment \\(D = 1\\).\nThe remaining \\(N_0 = N - N_1\\) are assigned to control.\n\n\n\n\n\n\n\nMore on Randomisation\n\n\n\n\n\n\\(N_1\\), the number of individuals assigned to treatment, does not necessarily need to be 50% (although this is quite a common number).\nAlso note that when you fix the number of units to be treated at \\(N_1\\), technically, not all units have an independent probability of being selected. This is because once you have assigned \\(N_1\\) individuals to treatment, we know the remaining individuals must be assigned to control. This usually is not a huge issue.\nYou can use bernoulli randomisation (simple randomisation) to avoid this issue. Bernoulli gives every individual a certain chance of being selected. This does mean that with different randomisation trials, we will get different numbers of treated individuals for each trial.\n\n\n\n\n\n\nIdentification Assumptions\nRandomisation implies that assignment probabilities do not depend on the potential outcomes. The potential outcome values do not affect our chances of being selected for treatment.\n\\[\nPr(D=1|Y_0, Y_1) = Pr(D=1)\n\\]\nOr in other words, treatment is independent of potential outcomes (also unconfounded or ignorability):\n\\[\n(Y_1, Y_0)  \\perp\\!\\!\\!\\!\\perp D\n\\]\nThis implies that \\(E(Y_{0i})\\) is the same between treatment and control groups, and \\(E(Y_{1i})\\) is also the same between treatment and control:\n\\[\n\\begin{split}\n& E(Y_{0i} | D_i = 1) = E(Y_{0i} | D_i = 0) = E(Y_{0i})\\\\\n& E(Y_{1i} | D_i = 1) = E(Y_{1i} | D_i = 0) = E(Y_{1i})\n\\end{split}\n\\tag{1}\\]\n\n\n\nProof of Identification\nLet us return to our naive estimator, and our problem of selection bias. Using the above property in Equation 1, we can simplify:\n\\[\n\\begin{align}\n& \\underbrace{E(Y_{1i}|D_i = 1)- E(Y_{0i}|D_i = 1)}_{\\tau_{ATT}} + \\underbrace{E(Y_{0i}|D_i = 1) - E(Y_{0i} | D_i = 0)}_{\\text{Selection Bias}} \\\\\n& = \\underbrace{E(Y_{1i}|D_i = 1)- E(Y_{0i}|D_i = 1)}_{\\tau_{ATT}} + \\underbrace{E(Y_{0i}) - E(Y_{0i})}_{\\text{Selection Bias}} && (\\because \\text{eq. (1)} \\ )\\\\\n& = \\underbrace{E(Y_{1i}|D_i = 1)- E(Y_{0i}|D_i = 1)}_{\\tau_{ATT}} + 0\n\\end{align}\n\\]\nThus, under randomisation, selection bias is equal to 0, and thus our comparison of observed outcomes is now an unbiased estimator of \\(\\tau_{ATT}\\). Now look at the formula for the ATT estimand. We can simplify as follows using Equation 1:\n\\[\n\\begin{align}\n\\tau_{ATT} & = E(Y_{1i} -Y_{0i}|D_i = 1)\\\\\n& = E(Y_{1i} |D_i = 1) - E(Y_{0i} | D_i = 1) \\\\\n& = E(Y_{1i} ) - E(Y_{0i}) && (\\because \\text{equation (1)} \\ )\\\\\n& = \\underbrace{E(Y_{1i} - Y_{0i})}_{\\tau_{ATE}}\n\\end{align}\n\\]\nAnd now we see that \\(\\tau_{ATT}\\) and \\(\\tau_{ATE}\\) are equivalent under randomisation, and we can identify the \\(\\tau_{ATE}\\) with our observed data.\n\n\n\n\n\n\nGraphical Identification\n\n\n\n\n\nLet us look at a direct acyclic graph:\n\n\n\n\n\nBecause we are randomly assigning treatment \\(D\\), we are exogenously determining \\(D\\). Thus, values of \\(D\\) are not being caused by \\(U\\), they are being caused by randomisation.\nThus, we can eliminate the arrow between \\(U \\rightarrow D\\). This allows us to estimate \\(D \\rightarrow Y\\) without any confounders.\n\n\n\n\n\n\nThe Balancing Property\nRandomisation balances all observed and unobserved pre-treatment characteristics between units between the treatment and control.\nThis is because not only is \\((Y_1, Y_0) \\perp\\!\\!\\!\\!\\perp D\\), but also any covariate \\(X\\) is also independent of treatment: \\(X \\perp\\!\\!\\!\\!\\perp D\\).\nThis means that if randomisation is successful, we should expect minimal differences between control and treatment groups for all pre-treatment characteristics values.\n\n\n\n\n\n\nDetails on the Balancing Property\n\n\n\n\n\nIn any one sample, we actually are likely to have some imbalances in \\(X\\) between control and treatment simply due to chance.\n\nYou could control for imbalanced covariates, but you do not have to (we will discuss this later).\n\nYou can adopt other randomisation procedures, such as stratified randomisation, to guarantee balance on \\(X\\).\n\n\n\nWe can test this assumption by finding the average \\(X\\) values for both control and treatment groups, and see if there are any statistical significant differences in \\(X\\) between control and treatment. This is typically done with a t-test or a regression:\n\\[\nX_i = \\alpha + \\gamma D_i + u_i \\quad \\text{test if } \\gamma \\text{ is significant}\n\\]\n\n\n\nComplications and Limitations\nRandomisation can be complicated by a few factors:\n\nMissing data (often due to individuals dropping out). We are concerned that there is some covariate that is causing some people to drop out, which re-introduces selection bias.\nMeasurement Problems: Hawthorne Effect - subjects know what you are studying, and will change their behaviour as a result.\nNon-Compliance: Some units assigned to treatment might not take the treatment, and some units assigned to control may take the treatment (this can often be dealt with by using an instrumental variable design).\n\nRandomisation does not help with external validity - the ability to extrapolate our results to external situations.\n\n\n\n\n\n\nMore on External Validity\n\n\n\n\n\nExternal validity asks if we can generalise our conclusions from our subjects, to other subjects outside our experiment. Can we extrapolate our estimates to to other populations?\nFor example, if we measured the effect of migration on tolerance for our subjects in India, can we say the same effect is true of someone in Japan, the US, or Europe?\nThis is important - if we cannot extrapolate, some results may be very niche.\nTo extrapolate to a greater population, our actual sample of observations in our experiment, should be representative of the greater population. This is often violated, as random sample for experiments is very very difficult.\nThis is called 𝑋-Validity: we can study this with data - to see how representative our population is compared to the population.\nNon-representative programme of treatment is another threat: Sometimes, treatments will differ between areas.\nFor example, if we are encouraging people to migrate to test how that changes their tolerance, how are the governmental/ngo/private agencies working with you affecting the process. Would less capable agencies create different effects?\nThis is called \\(C\\)-validity, and we cannot measure this with data, unless you redo your experiment in another context.\n\n\n\n\n\n\n\n\n\nCausal Estimation\n\nDifference in Means Estimator\nOur causal estimand is the Average Treatment Effect (ATE):\n\\[\n\\tau_{ATE} = E(Y_{1i}) - E(Y_{0i})\n\\]\nWe can estimate this using the difference-in-means estimator, by taking the sample mean \\(Y\\) of the treatment group, minus the sample mean \\(Y\\) of the control group:\n\\[\n\\hat\\tau_{ATE} = \\bar Y_1 - \\bar Y_0\n\\]\nThis is an unbiased estimator because selection bias is eliminated with randomisation. This is also an asymptotically consistent estimator due to the law of large numbers.\n\n\n\nOrdinary Least Squares Estimator\nWe can also estimate the \\(\\tau_{ATE}\\) with a bivariate regression:\n\\[\nY_i = \\hat\\gamma + \\hat\\tau D_i + \\hat\\epsilon_i\n\\]\nHere, \\(\\hat\\tau\\) is our estimator of the ATE. This gives the same estimate as the difference-in-means estimator.\n\n\n\n\n\n\nProof OLS is Equivalent to Difference-in-Means\n\n\n\n\n\nRemember that OLS is the best approximation of the conditional expectation function \\(E(y|x)\\).\nThus, we can write the regression as:\n\\[\nE(Y_i|D_i) = \\hat\\gamma + \\hat\\tau D_i + \\hat\\epsilon_i\n\\]\nNow, let us find the difference between treatment \\(E(Y_i|D_i =1)\\) and control \\(E(Y_i|D_i = 0)\\):\n\\[\n\\begin{split}\n& E(Y_i|D_i = 1) - E(Y_i|D_i = 0) \\\\\n= & \\ \\hat\\gamma + \\hat\\tau(1) - (\\hat\\gamma + \\hat\\tau(0)) \\\\\n= & \\ \\hat\\gamma + \\hat\\tau - \\hat\\gamma \\\\\n= & \\ \\hat\\tau\n\\end{split}\n\\]\nThus, the difference in means is equivalent to \\(\\hat\\tau\\) regression coefficient.\n\n\n\nFurthermore, \\(\\hat\\gamma\\) is equivalent to the average \\(Y\\) in the control group \\(\\bar Y_0\\).\nWe do not need to include covariates. This is because randomisation allows us to meet the asymptotic consistency condition of both randomisation and exogeneity.\nHowever, sometimes pre-treatment covariates are included. We should not include post-treatment covariates.\n\n\n\n\n\n\nIncluding Pre-Treatment Covariates\n\n\n\n\n\nThere are several reasons one might want to include pre-treatment covariates:\n\nCan increase precision (reduce standard error), by getting better predictions of \\(Y\\).\nCan control for observable imbalance that was observed in the balance tables. Many researchers will compare a model without and with an imbalanced covariate, to show that the covariate does not matter significantly.\nCan allow for estimation of heterogenous treatment effects by including interactions in the model.\n\nThere is one risk: it may introduce small-sample bias. This will be discussed later in the discussion of the fully-interacted estimator.\nWe should not include post-treatment covariates. Anything that is measured post-treatment could be measuring a treatment effect (something that results from the treatment). This may “model away” your treatment effect.\n\n\n\n\n\n\n\n\n\nStatistical Inference\n\nStandard Inference\nWe can use a t-test for statistical inference.\n\nEstimate the \\(\\hat\\tau_{ATE}\\) and robust standard error \\(\\widehat{rse}(\\hat\\tau_{ATE})\\).\nState hypotheses, normally \\(H_0 : \\tau_{ATE} = 0\\) and \\(H_1 \\tau_{ATE} ≠ 0\\).\nCalculate the t-test statistic \\(\\hat\\tau /\\widehat{rse}(\\hat\\tau)\\).\nRefer to the relevant t-distribution, and calculate the p-value.\n\nGenerally, we use a statistical significance level of \\(\\alpha = 0.05\\), so we reject the null if \\(|t|&gt;1.96\\).\nFor more complex randomisation schemes, you will need different standard errors. For example, if you use a cluster randomisation scheme, you might need clustered standard errors.\nWe can also use Nonparametric Bootstrap to create our sampling distribution for statistical inference, instead of relying on asymptotic normality of the standard t-test.\n\n\n\n\n\n\nBlock Bootstrap\n\n\n\n\n\nFor blocked experiments, you should randomly sample blocks, not units, to create your bootstrap re-samples.\nFor example, if your data is clustered in cities, you should re-sample by cities.\n\n\n\n\n\n\nRandomisation Inference\nConsider a new sharp null hypothesis, that all individual causal effects are zero (not just the average causal effect):\n\\[\nH_0^s : Y_1 = Y_0, \\quad H_A^s : Y_1 ≠ Y_0\n\\]\nAssuming \\(H_0\\) is true, we can actually fully construct the potential outcomes \\(Y_{0i}\\) and \\(Y_{1i}\\), since we know every unit has 0 individual treatment effect.\nOnce constructed, we can imagine what different treatments we would observe under different randomization schemes (given \\(H_0\\) is true). Thus, we can construct the sampling distribution, so we do not need to “imagine” a hypothetical sampling distribution.\nProcedure for conducting randomisation inference (also called permutation test or Fisher’s exact test) is as follows:\n\nCalculate the total number of randomisations possible. This is calculated as a permutation of \\(_NP_{N_1}\\) (\\(N\\) choose \\(N_1\\)).\nCalculate and store the value of \\(\\widehat{\\tau_j}\\) of each permuted dataset \\(j\\). Thus, we will have a distribution of \\(\\widehat{\\tau_j}\\).\nCalculate \\(p\\)-value as the proportion of \\(\\widehat{\\tau_j}\\) that are as or more extreme than the actually observed \\(\\hat\\tau\\).\n\n\n\n\n\n\n\nDetails on Randomisation Inference\n\n\n\n\n\nIf we have \\(N\\) total units, and \\(N_1\\) in the treatment group and \\(N_0\\) in the control group, we can calculate all possible randomisation permutations as follows:\n\\[\n\\begin{pmatrix} N \\\\ N_1 \\end{pmatrix} = \\frac{N!}{N_1 ! N_0 !}\n\\]\nThis is the total number of assignments possible given \\(N\\), \\(N_1\\), and \\(N_0\\).\nThen, we can calculate the \\(\\widehat{\\tau_j}\\) of every possible randomisation assignment. The figure below shows this:\n\n\n\n\n\nNow, plot all \\(\\widehat{\\tau_j}\\) in a distribution:\n\n\n\n\n\nLet us say our sample \\(\\hat\\tau = 6\\). We would simply find the area under the curve that is above \\(\\hat\\tau = 6\\), and below \\(-\\hat\\tau = -6\\).\n\n\n\n\n\n\n\n\n\nPros/Cons of Randomisation Inference\n\n\n\n\n\nRandomisation Inference is assumption free - we do not need asymptotic properties or hypothetical sampling distributions.\nSince we also do not need asymptotic property inferences, we can do inference with very small samples as well.\nDownsides: the randomisation inference only tests if the sharp null hypothesis is true, but sometimes, that might not be something we want to test.\n\n\n\n\n\n\n\n\n\nOther Randomisation Procedures\n\nStratified Randomisation\nStratified (also called blocked or conditional) randomisation are when randomisation occurs separately within levels of some covariates(s) \\(X\\). Generally, you separate your sample of \\(N\\) units into \\(J\\) subgroups.\nFor example, you could split people up into male or female, and random sample within each group, rather than everyone together.\n\n\n\n\n\n\nExample of Stratification\n\n\n\n\n\nLet us say you have 4 subjects, with pre-treatment potential outcomes of \\(Y_{0i} = \\{2, 2, 8, 8 \\}\\).\nIf you just randomly assign, then there is a 33% chance that you end up with the random assignment where \\(\\{8, 8\\}\\) are placed in one group, and \\(\\{2, 2\\}\\) are placed in another group.\nThis is a concern: our treatment and control groups would be very imbalanced in this situation, which violates our independence assumption.\nWith blocking, we could divide our sample into \\(J = 2\\) subgroups, having group 1 being \\(\\{2, 2\\}\\), and group 2 being \\(\\{8, 8\\}\\). Then, we randomly sample one from each group into the treatment. This way, we are guaranteed better balance.\n\n\n\nThis can prevent imbalances as normal randomisation can have a high probability (in certain situations) of creating imbalances.\n\n\n\n\n\n\nEstimation with Stratification\n\n\n\n\n\nTo estimate the ATE, you will need a weighted average of the ATE for each subgroup \\(j\\), with the weights being the proportion of units each group \\(j\\) accounts for:\n\\[\n\\tau_{ATE} = \\sum\\limits_{j=1}^J \\frac{N_j}{N} \\tau_j\n\\]\n\n\n\n\n\n\nCluster Randomisation\nCluster randomisation is when we randomly assign units (or have individuals naturally) in groups. Every unit within a group (called a cluster) will get the same treatment. We randomly sample the groups to get the treatment or control.\nFor example, we could randomise development treatment at the village level, or randomise treatment of a cirriculum at the school level.\nThe main reason for this is to prevent SUTVA violations.\nFor example, imagine you are testing the effects of a new curriculum. If you randomise by each student, students will talk to their friends, and treated individuals may teach control individuals about the new curriculum. But by randomising by school (either an entire school gets or does not get the new curriculum), this concern is not a huge issue.\n\n\n\n\n\n\nSurvey Experiments\n\nFraming/Endorsement Experiments\n\n\nPriming Experiments\n\n\nList Experiments\n\n\n\n\n\n Back to top",
    "crumbs": [
      "5 Randomised Controlled Trials"
    ]
  },
  {
    "objectID": "6.html",
    "href": "6.html",
    "title": "Selection on Observables",
    "section": "",
    "text": "In the last chapter, we discussed randomisation. Randomisation is great, but, it requires specific circumstances of the research having control over the assignment mechanism. However, in the social sciences, this rarely occurs.\nThis chapter introduces the selection on observables framework, which allows us to identify causal effects in an observational setting by controlling for observable pre-treatment covariates. We discuss the main estimators, including regression, matching, and weighting.\nUse the right sidebar for quick navigation. R-code provided at the bottom.\n\n\nIdentification\n\nBlocking Backdoor Paths\nWithout randomisation, we need some other way to account for pre-treatment covariates that may be confounding and causing selection bias. Controlling for a set of nodes/confounders \\(X\\) can identify the causal effect of \\(D \\rightarrow Y\\), if:\n\nNo node within set \\(X\\) is a descendant of \\(D\\) (no element within \\(X\\) results from \\(D\\)).\nThe nodes within set \\(X\\) block all back-door paths from \\(D \\rightarrow Y\\).\n\n\n\n\n\n\nIn the figure above, let us block the backdoor paths between \\(D \\rightarrow Y\\):\n\nBackdoor path \\(D \\rightarrow X \\rightarrow Y\\). To block this path, we must control for \\(X\\).\nBackdoor path \\(D \\rightarrow V \\rightarrow Y\\). We do not need to control for \\(V\\), since it is post-treatment (a descendant of \\(D\\)). In fact, \\(V\\) is a bad control (see below).\n\nThus, to identify \\(D \\rightarrow Y\\) here, we only need to control for \\(X\\), and no other variable.\n\n\n\n\n\n\nGood and Bad Controls\n\n\n\n\n\nGood controls block backdoor paths, which facilitate identification of the causal effect.\nBad controls are when we control for post-treatment variables. For example, \\(P\\) below is a bad control, since it is caused by \\(D\\), so it is post-treatment.\n\n\n\n\n\nYou also never want to control variables that only predict \\(D\\). These are bad because controlling for these removes variation in \\(D\\) that could be useful.\nNeutral controls are ones that don’t identify the causal effect, but improve efficiency. For example, \\(Q\\) below affects \\(Y\\), but there is no backdoor path. Controlling \\(Q\\) will not help identification, but can control noise in \\(Y\\) which may increase efficiency.\n\n\n\n\n\n\n\n\n\n\n\nIdentification Assumptions\nOnce we have determined the set of confounders \\(X\\) that we need to control to block all backdoor paths, the assumptions needed for identification of causal effects are:\n\nConditional Ignorability (also known as exogeneity or independence): Among units with identical confounder values \\(X_i\\), treatment \\(D_i\\) is as-if randomly assigned. Or in other words, potential outcomes are independent from treatment within each specific confounder value \\(X_i = x\\).\n\n\\[\n(Y_{0i}, Y_{1i}) \\perp\\!\\!\\!\\!\\perp D_i  \\ | \\ X_i = x, \\quad \\forall \\ x \\in \\mathcal X\n\\]\nThis implies that for any given value of all confounders \\(X_i = x\\), we know that potential outcomes \\(Y_{di}\\) are equivalent between treatment and control:\n\\[\n\\begin{split}\nE(Y_{1i}|X_i = x) = E(Y_{1i}|D_i = 1, X_i = x) = E(Y_{1i}|D_i = 0, X_i = x) \\\\\nE(Y_{0i}|X_i = x) = E(Y_{0i}|D_i = 1, X_i = x) = E(Y_{0i}|D_i = 0, X_i = x)\n\\end{split}\n\\tag{1}\\]\n\nCommon Support: for any unit \\(i\\) with value of \\(X_i\\), there is a non-zero probability that they could be assigned to both control \\(D_i = 0\\) or treatment \\(D_i = 1\\).\n\n\\[\n0 &lt; Pr(D_i = 1 \\ | X_i = x) &lt; 1 \\quad \\forall \\ x \\in \\mathcal X\n\\]\n\n\n\n\n\n\nExample of Identification Assumptions\n\n\n\n\n\nImagine we have a theory that being abducted \\(D\\) causes turning out to vote.\nBlattman (2009) finds that age is the primary way violent groups chose to abduct individuals: abduction parties released young children and older adults, but kept all adolescent and young males.\nThat means our theory is that age \\(X\\) affects selection into treatment \\(D\\). Young children and older adults are less likely to get abducted \\(D\\), while adolescent and young males are more likely \\(D\\).\n\n\n\n\n\n\nIdentification of the ATE\nWith our assumptions above, we can identify the ATE. We start with the conditional average treatment effect, conditional on some value of confounders \\(X_i = x\\). Note the properties shown in Equation 1 .\n\\[\n\\begin{align}\n\\tau_{CATE}(x) & = E(Y_{1i} - Y_{0i} \\ | \\ X_i = x) \\\\\n& = E(Y_{1i}|X_i = x) - E(Y_{0i}|X_i = x) && (\\text{property of expectation}) \\\\\n& = E(Y_{1i}|D_i = 1, X_i = x) - E(Y_{0i}|D_i = 0X_i = x) &&( \\because \\text{equation (1)} \\ ) \\\\\n& = \\underbrace{E(Y_i|D_i = 1, X_i = x)}_{\\because \\text{ observable}} - \\underbrace{E(Y_i|D_i = 0, X_i = x)}_{\\because \\text{ observable}}\n\\end{align}\n\\tag{2}\\]\nNow, let us discuss the ATE, and plug in the CATE from Equation 2 to identify it:\n\\[\n\\begin{align}\n\\tau_{ATE} & = E(Y_{1i} - Y_{0i}) \\\\\n& = \\int \\underbrace{E(Y_{1i} - Y_{0i} \\ | \\ X_i = x)}_{\\tau_{CATE}(x)} d \\ \\underbrace{Pr(X_i = x)}_{\\text{weight}} && (\\text{weighted average})\\\\\n& = \\int(\\underbrace{E(Y_i|D_i = 1, X_i) - E(Y_i|D_i = 0, X_i)}_{\\because \\text{ equation (2)}})d \\ Pr(X_i = x)\n\\end{align}\n\\tag{3}\\]\nThus \\(\\tau_{ATE}\\) is identified as the weighted average of all the CATEs, who themselves are difference-in-means of the observed \\(Y_i\\) at every possible value of \\(X_i = x\\).\nWe assumed that the pre-treatment covariate \\(X\\) is continuous. This is why we need an integral. However, we can simplify this if \\(X\\) is discrete:\n\\[\n\\tau_{ATE} = \\sum\\limits_{x \\in \\mathcal X} ( E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)) Pr(X_i = x)\n\\tag{4}\\]\n\n\n\nIdentification of the ATT\nWe can weaken conditional ignorability, and still identify the ATT. Only \\(Y_{0i}\\) needs to be independent of \\(D_i\\) for units with the same covariates \\(X_i\\). Or in other words, \\((Y_{0i}) \\perp\\!\\!\\!\\perp D_i | X_i = x\\). This implies:\n\\[\nE(Y_{0i}|X_i = x) = E(Y_{0i}|D_i = 0, X_i = x) = E(Y_{0i}|D_i = 1, X_i = x)\n\\tag{5}\\]\nStart with the conditional ATT, using weakened conditional ignorability from Equation 5 :\n\\[\n\\begin{split}\n\\tau_{CATT}(x) & = E(Y_{1i}-Y_{0i}|D_i = 1, X_i = x) \\\\\n& = E(Y_{1i}|D_i = 1, X_i = x) - E(Y_{0i}|D_i = 1, X_i = x) \\\\\n& = E(Y_{1i}|D_i = 1, X_i = x) - \\underbrace{E(Y_{0i}|D_i = 0, X_i = x)}_{\\because \\text{ equation (5)}} \\\\\n& = \\underbrace{E(Y_i|D_i=1, X_i = x)}_{\\because \\text{ observable}} - \\underbrace{E(Y_1|D_i = 0, X_i x)}_{\\because \\text{ observable}}\n\\end{split}\n\\tag{6}\\]\nNow, look at the ATT, and plug in CATT from Equation 6 to identify it.\n\\[\n\\begin{align}\n\\tau_{ATT} & = E(Y_{1i} - Y_{0i}|D_i = 1) \\\\\n& = \\int \\underbrace{E(Y_{1i} - Y_{0i}|D_i = 1, X_i = x)}_{\\tau_{CATT}(x)}d \\ \\underbrace{Pr(X_i = x|D_i = 1)}_{Pr(X_i = x) \\text{ within treated}} \\\\\n& = \\int (\\underbrace{E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)}_{\\because \\text{ equation (6)}})d \\ Pr(X_i = x|D_i = 1)\n\\end{align}\n\\]\nWe can simplify this if \\(X\\) is discrete:\n\\[\n\\tau_{ATT} = \\sum\\limits_{x \\in \\mathcal X} ( E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)) Pr(X_i = x | D_i = 1)\n\\]\nEven when all assumptions are met for identification of the ATE, the \\(\\tau_{ATE}\\) can be different than the \\(\\tau_{ATT}\\). This is because the weights \\(Pr(X_i = x|D_i = 1)\\) for the ATT are different than the ATE \\(Pr(X_i = x)\\).\n\n\n\n\n\n\nParametric Estimators\n\nOrdinary Least Squares Estimator\nOLS is a natural approach for controlling for confounders \\(X\\), since \\(\\hat\\beta_{OLS}\\) estimates partial out the effects of covariates. OLS is a good estimator of \\(\\tau_{ATE}\\) under 2 conditions:\n\nConstant treatment effect: \\(\\tau_i = Y_{1i} - Y_{0i}\\) for all units \\(i\\).\nLinearity: Potential outcomes are linear, and can be written as:\n\n\\[\nY_i(d) = \\beta_0 + d\\beta_1 + \\mathbf X_i \\gamma + \\epsilon_i \\quad \\text{for} \\quad d = 0, 1\n\\]\nWhy these conditions? Suppose we have the above linear potential outcomes. We can show:\n\\[\n\\begin{align}\n\\tau_i & = Y_{1i} - Y_{0i} && (\\text{definition of } \\tau_i) \\\\\n& = (\\beta_0 + (1)\\beta_1 + \\mathbf X_i \\gamma + \\epsilon_i) - (\\beta_0 + (0)\\beta_1 + \\mathbf X_i \\gamma + \\epsilon_i) && (\\text{plug in } Y_i(1), Y_i(0) \\ )\\\\\n& = (\\beta_0 + \\beta_1 + \\mathbf X_i \\gamma + \\epsilon_i) - (\\beta_0 + \\mathbf X_i \\gamma + \\epsilon_i) && (\\text{multiply}) \\\\\n& = \\beta_0 + \\beta_1 + \\mathbf X_i \\gamma + \\epsilon_i - \\beta_0 - \\mathbf X_i\\gamma - \\epsilon_i && (\\text{distribute negative sign})\\\\\n& = \\beta_1 && (\\text{cancel out terms})\n\\end{align}\n\\]\nWe also know that conditional ignorability implies zero-conditional mean. Thus \\(\\beta_1\\) is an unbiased and asymptotically consistent estimator of the ATE.\nYou should be cautious using OLS when assumption 2, linearity, is violated. OLS is the best linear estimator, but how far your data is from linearity will determine if the estimator is useful.\n\n\n\n\n\n\nNon-Linearity\n\n\n\n\n\nWhat if potential outcomes \\(Y_i(d)\\) is an unknown and non-linear function of \\(d\\) and \\(X_i\\).\nWe know the OLS is the best linear predictor of the conditional expectation function in terms of mean squared error. Thus, \\(\\beta_1\\) will provide the best linear approximation to the population regression function.\nThis does not mean it is good - just the best linear approximation.\n\n\n\nYou should not use OLS if you believe assumption 1, heterogeneity, is violated. The reasoning is explained below.\n\n\n\nOLS Bias under Heterogeneity\nWhat if there are heterogenous treatment effects (where \\(\\tau_i\\) is different between units)? Standard OLS in this case is no longer an unbiased estimator of the ATE.\nRecall the discrete identification of the ATE (in equation Equation 4 ) is a weighted average of CATEs:\n\\[\n\\hat\\tau_{ATE} = \\sum\\limits_{x \\in \\mathcal X} ( \\hat\\tau_{CATE}(x)) \\underbrace{Pr(X_i = x)}_{\\text{weight}} \\\\\n\\]\nOLS, when there are non constant treatment effects, can also be rewritten as a weighted average of CATEs:\n\\[\n\\hat\\beta_{OLS} = \\sum\\limits_{x \\in \\mathcal X} ( \\hat\\tau_{CATE}(x)) \\underbrace{ \\frac{Var(D_i|X_i = X)Pr(X_i = x)}{\\sum Var(D_i | X_i = x')Pr(X_i = x')} }_{\\text{weight}} \\\\\n\\]\nNotice how the weights are different. The weights in the OLS are the conditional variances of \\(D_i\\). This means that OLS is not an unbiased estimator of the ATE or ATT, but rather, a weighted average of the ATT and ATU.\nOLS, under heterogeneity, actually provides an unbiased estimator of the conditional variance weighted average treatment effect. This is not the same as the ATE or the ATT.\n\n\n\n\n\n\nConditional Variance Weighted Average Treatment Effect (CVW-ATE)\n\n\n\n\n\nThis estimand can also be described as a weighted average of the ATT (average treatment effect on the treated) and the ATU (average treatment effect on the untreated):\n\\[\n\\tau_{OLS} = w_1 \\cdot \\tau_{ATT} + w_0 \\cdot \\tau_{ATU}\n\\]\nWhere:\n\\[\n\\begin{split}\nw_1 & = \\frac{(1 - Pr(D=1)) Var(\\pi(X)|D = 0)}{Pr(D=1)Var(\\pi(X)|D=1) + (1-Pr(D=1)Var(\\pi(X)|D=0)} \\\\\nw_0 & = 1 - w_1\n\\end{split}\n\\]\nThe reason for this is because regression is prone to extrapolation beyond common support - i.e. it can “estimate” potential outcomes for units that are not observed. This can lead to bias.\nThis is in contrast to the subclassification estimator, which cannot be computed if there are missing observable outcomes for a substratum/category of \\(X\\).\nThe weights of \\(D_i(X_i = x)\\) can also be seen as propensity scores of \\(\\pi(x)(1 - \\pi(x)\\). Therefore:\n\nWeights are higher for groups with propensity scores close to 0.5.\nWeights are low for groups with propensity scores close to 0 or 1.\nOLS minimises estimation uncertainty by downweighting groups of \\(X_i\\) where group-specific ATEs are less precisely estimated.\n\n\n\n\n\n\n\nFully Interacted Estimator\nThe Fully-Interacted Estimator, a newly developed large-sample regression estimator (Lin 2013), solves the heterogeneity bias in the OLS estimator. The fully-interacted estimator takes the form:\n\\[\n\\widehat{Y_i} = \\hat\\alpha + D_i \\widehat{\\tau}_{int} + (\\mathbf X_i - \\mathbf {\\bar X}) \\hat\\beta +D_i (\\mathbf X_i - \\mathbf{\\bar X}) \\hat\\gamma\n\\]\n\nWhere \\(X_i\\) are covariate values sufficient to satisfy conditional independence.\nWhere \\(\\bar X\\) contains the sample means of all \\(X_i\\) covariates.\n\nThis estimator \\(\\hat\\tau_{int}\\) is technically biased when estimating \\(\\tau_{ATE}\\). However, the bias is arbitrarily small in large samples under conditional ignorability.\nThis estimator thus allows us to accurately estimate the ATE even under heterogenous treatment effects, assuming our sample size is sufficiently large.\n\n\n\n\n\n\nOther Solutions to the OLS Bias under Heterogeneity\n\n\n\n\n\nThere are a few other solutions to this issue of OLS bias under heterogeneity:\n\nDoubly-robust estimation uses a weighted average of regression and IPW estimators, which will be asymptotically consistent as long as the regression model is correctly specified.\nMatching as pre-processing uses matching to make treatment and control groups similar, then runs regression models to estimate causal effects.\n\n\n\n\n\n\n\n\n\n\nNonparametric Estimators\n\nSubclassification Estimator\nUsing the discrete identification of the ATE shown in Equation 4 , we can instead use the sample equivalents to get the subclassification estimator:\n\\[\n\\hat\\tau_{ATE} = \\sum\\limits_{j=1}^M \\underbrace{(\\bar Y_{1j} - \\bar Y_{0j})}_{\\tau_{CATE}(j)} \\underbrace{\\frac{n_j}{n}}_{\\text{weight}}\n\\]\n\nWhere \\(M\\) is the number of levels/categories of \\(X\\), and \\(j\\) is one specific level/category of \\(X\\).\nWhere \\(n_j\\) is the number of units in a level/category \\(j\\) of \\(X\\).\nWhere \\(\\bar Y_{dj}\\) is the mean outcome for units with \\(D_i = d\\) in level/category \\(j\\) of \\(X\\).\n\nFor subclassificaion to be possible, within each level \\(j\\) of covariate \\(X\\), there must be at least one unit in control \\(D=0\\) and treatment \\(D=1\\).\n\n\n\n\n\n\nIntuitive Procedure of Subclassification\n\n\n\n\n\nMore intuitively, the procedure is as follows:\n\nChoose one specific value for all covaraites \\(X\\). Find the average treatment effect within this specific value of \\(X\\).\nMultiply that average treatment effect by the number of observations that meet this specific value of \\(X\\) divided by the total number of units.\nDo this for every possible values of all covaraites \\(X\\), then sum up all the weighted average treatment effects to get the overall ATE.\n\n\n\n\n\n\n\n\n\n\nSubclassification with Multiple Confounders\n\n\n\n\n\nLet us say we have 2 confounders, \\(X_1\\) and \\(X_2\\). Both confounders are categorical with 3 categories.\nWe would need to create \\(M=9\\) levels of strata, for every possible combination of values of \\(X_1\\) and \\(X_2\\). Then, we would estimate the within-strata average treatment effect, and weight them.\nThis illustrates how with large amounts of confounders, you will need a huge number of stratum. This makes subclassification infeasible in many cases.\n\n\n\n\n\n\n\n\n\nSubclassification for the ATT\n\n\n\n\n\nWhen pre-treatment covariate \\(X\\) is discrete, the identification result of the ATT is:\n\\[\n\\tau_{ATT} = \\sum\\limits_{x \\in \\mathcal X} ( E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)) Pr(X_i = x | D_i = 1)\n\\]\nWe can calculate this within our give sample to get the subclassificaiton estimator:\n\\[\n\\hat\\tau_{ATT} = \\sum\\limits_{j=1}^M(\\bar Y_{1j} - \\bar Y_{0j}) \\frac{n_{1j}}{n_1}\n\\]\n\nWhere \\(M\\) is the number of strata (levels/categories of \\(X\\)).\nWhere \\(n_j\\) is the number of units in a level/category \\(j\\) of \\(X\\).\nWhere \\(n_{1j}\\) is the number of treated cells \\(D = 1\\) in a level/category \\(j\\) of \\(X\\).\nWhere \\(\\bar Y_{dj}\\) is the mean outcome for units with \\(D_i = d\\) in level/category \\(j\\) of \\(X\\).\n\n\n\n\n\n\n\nMatching Estimator\nWe have a missing data problem in causal inference: we do not know all the potential outcomes. Matching “estimates” missing potential outcomes of a unit.\nFor each observation in the treated group, matching finds an observation in the untreated group that have the most similar values of a set of pre-treatment covariates \\(X\\). Thus, we have pairs of treatment-control observations that act as counterfactuals. We can estimate the ATT as the average difference in observed outcomes within the pairs:\n\\[\n\\hat\\tau_{ATT} = \\frac{1}{n_1} \\sum\\limits_{i:D_i = 1}(Y_i - \\widetilde{Y_i})\n\\]\n\nWhere \\(n_1\\) is the number of units in the treatment group.\nWhere \\(Y_i\\) is the unit’s observed \\(Y\\) in the treatment group.\nWhere \\(\\tilde Y_i\\) is unit \\(i\\)’s closest neighbour in the untreated group.\n\n\n\n\n\n\n\nChoices during Matching\n\n\n\n\n\nWe have to make several choices when conducting matching.\n\nWhat covariates to match on. We generally want to select a set of pre-treatment covariates \\(X\\) such that these covariates ensure the conditional ignorability assumption is met.\nMatch with or without replacement. Matching with replacement means that once you have used one control unit to match to a treatment unit, you can still use that same control unit to match to another treatment unit. This has advantages since you can ensure better and closer matches. However, matching without replacement is also possible.\nHow many to match. You can decide to match multiple control units to one treatment unit, and use the average of the treatment units to approximate a true control unit. This may result in more accurate matches for treatment units that may not have a good single control unit to match to.\n\nWe can also choose to use more advanced matching methods, such as Mahalanobis Distance matching or Propensity Score matching, which are shown below. These are good for matching on more \\(X\\).\n\n\n\n\n\n\n\n\n\nMatching on Multiple Neighbours\n\n\n\n\n\nSometimes, a treatment unit may not have one close control unit to match to. Instead, we could use a combination of control units to match to the treatment unit, and use the average \\(Y\\) of those combination of control units to approximate a more accurate match.\nSuppose we use \\(M_i\\) number of close control units to match to a treatment unit \\(i\\). Then, the matching estimator would be defined as follows:\n\\[\n\\hat\\tau_{ATT} = \\frac{1}{n_1} \\sum\\limits_{i:D_i = 1}(Y_i - \\left(\\frac{1}{M_i} \\sum\\limits_{m=1}^{M_i} \\widetilde{Y_{i_m}}\\right))\n\\]\nWhere \\(\\widetilde{Y_{i_m}}\\) is the obsered outcome for the \\(m\\)th match of unit \\(i\\).\n\n\n\n\n\n\n\n\n\nWeaknesses of Matching\n\n\n\n\n\nMatching does not always create “perfect” matches. This means that the pairs matched together may not be identical to each other in terms of covariates \\(X\\) or potential outcomes.\nThe inability to find exact matches can cause bias, especially for the more covariates we match on (see below).\n\n\n\n\n\n\nMatching with Multiple Covariates\nConsider that we \\(k&gt;1\\) number of confounders \\(X\\). Now, we have to match observations in \\(k\\) variables, which implies we are in a multidimensional \\(\\mathbb R^k\\) space.\nThe most commonly used distance metric is Mahalanobis Distance - which measures the distance in \\(X_i\\) between units \\(i\\) and \\(j\\):\n\\[\nD_M (\\mathbf X_i, \\mathbf X_j) = \\sqrt{(\\mathbf X_i - \\mathbf X_j)^T \\boldsymbol\\Sigma_X^{-1} (\\mathbf X_i - \\mathbf X_j)}\n\\]\n\nWhere \\(\\boldsymbol \\Sigma_X\\) is the sample variance-covariance matrix of \\(\\mathbf X_i\\).\n\n\n\n\n\n\n\nEuclidean Distance\n\n\n\n\n\nEuclidean distance is another common distance metric:\n\\[\nD_E ( \\mathbf X_i, \\mathbf X_j) = \\sqrt{(x_{1i} - x_{1j})^2 + (x_{2i}-x_{2j})^2 + \\dots + (x_{ki}-x_{kj})^2}\n\\]\nEuclidean distance, while very simple, is not recommended. This is because Euclidean distance with non-normalised variables can get you very bizarre results that depend on the scale of the variables.\nThere are other distance metrics, but these are exceedingly rare in selection on observables.\n\n\n\nThere is one issue with matching in multi-dimensional spaces. It becomes very difficult to match every unit \\(i\\) on every covariate \\(X\\), even if we have a large number of observations.\n\n\n\n\n\n\nCurse of Dimensionality\n\n\n\n\n\nWhen we try to match on more than one \\(X\\) variable, we go from matching on a number line \\(\\mathbb R^1\\) to a \\(n\\)-dimensional space, \\(\\mathbb R^n\\).\nThe search space increases exponentially as you increase the number of dimensions.\n\nTake a look at the figure on the left. If we only match on a one dimensional plane (lets say the horizontal line between 0 and 1), we can see our red line covers approximately 30% of the horizontal line. But in 3 dimensions, our red box covers a significantly less proportion of the entire cube.\nThe figure on the right illustrates this. \\(d\\) represents the dimensions. We can see as the dimensions increase, the fraction of volume increases significantly slower relative to distance.\nThus, with a bigger space, the distance between two units increases, so you get worse matches.\n\n\n\nThis curse of dimensionality creates a bias problem - since we get non-exact matches. The more dimensions you add, the worse it becomes.\n\n\n\n\n\n\nMore on Bias\n\n\n\n\n\nThe poor matches caused by increased dimensionality inject error into our estimates of missing potential outcomes.\nThe bias term as you increase the number of dimensions \\(k\\), changes by \\(N^{(-1/k)}\\). This implies no \\(\\sqrt{n}\\) consistency for \\(k&gt;2\\).\nIf \\(N_0\\) (number of untreated units) is much larger than \\(N_1\\) (number of treated units), bias will typically be smaller.\nThere are ways to correct this bias, including Abadie and Imbens (2011) Bias Correction method.\nThere is a new method: Bias-corrected matching, which estimates bias ineherent to mathching estimators via regression, then subtracts it from the matching estimate to correct for it.\n\n\n\n\n\n\nPropensity Scores Matching\nPropensity Score matching is an alternative way to match over many dimensions. The propensity score is an unobserved property, defined as the probability of a unit \\(i\\) of receiving treatment:\n\\[\n\\pi(X_i) \\equiv Pr(D_i = 1|X_i)\n\\]\nWhen supposing the conditional ignorability and common support assumptions, the propensity score \\(\\pi(X_i)\\) has the balancing property: \\(D_i \\perp X_i \\ | \\ \\pi(X_i)\\). This implies that conditional ignorability holds on the propensity scores alone:\n\\[\n(Y_{1i}, Y_{0i}) \\perp\\!\\!\\!\\!\\perp D_i \\ | \\ \\pi(X_i)\n\\]\nThus, instead of conditioning on \\(X_i\\) as we did in selection on observables, we can instead condition on \\(\\pi (X_i)\\), and still identify the causal estimand.\nHowever, we do not actually observe \\(\\pi (X_i)\\). We estimate \\(\\pi (X_i)\\) with a binary response model, with outcome variable \\(D_i\\), and explanatory variables \\(X_i\\). This will get us a fitted probability \\(Pr(D_i = 1) = \\hat\\pi(X_i)\\).\nThen, once we have the propensity score estimates \\(\\hat\\pi(X_i)\\), we can do nearest neighbour matching with the propensity scores (in \\(\\mathbb R^1\\)). This will allow us to identify the \\(\\tau_{ATT}\\).\n\n\n\n\n\n\nBalance Checks\n\n\n\n\n\nThe accurate estimation of \\(\\tau_{ATT}\\) implies an accurate prediction of the propensity scores \\(\\pi(X_i)\\). We can test our matched treatment and control groups to see if the balancing property holds for covariates \\(X_i\\).\n\n\n\n\n\n\nGenetic Matching\n\n\n\n\n\n\nWeighting Estimator\n\nIdentification with Weighting\nWe know that the ATE can be written as a weighted average, as shown in Equation 4 . We can rewrite the \\(\\tau_{ATE}\\) as follows using observed potential outcomes outcomes and conditional ignorability ( Equation 1 ).\n\\[\n\\begin{split}\n& = \\sum\\limits_{x \\in \\mathcal X} \\underbrace{(E(Y_{1i}|D_i = 1, X_i = x)}_{\\because \\text{ observed}} - \\underbrace{E(Y_{0i}|D_i = 0, X_i = x)}_{\\because \\text{ observed}})Pr(X_i = x) \\\\\n& = \\sum\\limits_{x \\in \\mathcal X}  (\\underbrace{E(Y_{1i}|X_i = x)}_{\\because \\text{ eq. (1)}} - \\underbrace{E(Y_{0i}|X_i = x)}_{\\because \\text{ eq. (1)}})Pr(X_i = x) \\\\\n& = \\underbrace{E[E(Y_{1i}|X_i = x) - E(Y_{0i}|X_i = x)]}_{\\text{definition of weighted average}}\n\\end{split}\n\\]\nLet us do an algebra trick - multiply both terms within the CATE by 1 (in blue):\n\\[\n\\begin{split}\n& = E \\left [E(Y_{1i}|X_i=x) \\color{blue}{\\frac{\\pi(X_i)}{\\pi(X_i)}}\\color{black} - (E(Y_{0i}|X_i=x) \\color{blue}{\\frac{1-\\pi(X_i)}{1-\\pi(X_i)}} \\right] \\\\\n& \\color{black} = E \\left[ \\frac{E(Y_{1i}|X_i = x) \\pi(X_i)}{\\pi(X_i)} -  \\frac{E(Y_{0i}|X_i = x) (1-\\pi(X_i))}{1-\\pi(X_i)} \\right]\n\\end{split}\n\\]\nWe know that propensity score \\(\\pi(X_i) := E(D_i|X_i = x)\\). Thus, we can convert the above to:\n\\[\n\\begin{split}\n& = E \\left[ \\frac{E(Y_{1i}|X_i = x)E(D_i|X_i = x)}{\\pi(X_i)} - \\frac{E(Y_{0i}|X_i = x)(1-E(D_i|X_i = x))}{1-\\pi(X_i)}\\right] \\\\\n& = E \\left[ \\frac{E(Y_{1i}|X_i = x)E(D_i|X_i = x)}{\\pi(X_i)} - \\frac{E(Y_{0i}|X_i = x)E(1-D_i|X_i = x)}{1-\\pi(X_i)}\\right]\n\\end{split}\n\\]\n\\[\n\\begin{align}\n& = E \\left[ \\frac{E(Y_{1i}D_i|X_i = x)}{\\pi(X_i)} - \\frac{E(Y_{0i}(1-D_i)|X_i = x)}{1 - \\pi(X_i)}\\right] && (\\text{property of expectation})\\\\\n& = E \\left[ E \\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} |X_i = x\\right) - E \\left( \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)} | X_i = x \\right) \\right] && (\\text{property of expectation})\\\\\n& = E\\left[ E\\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} - \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)} |X_i = x \\right) \\right]  && (\\text{property of expectation})\n\\end{align}\n\\]\n\\[\n\\begin{align}\n& = E\\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} - \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)}\\right) && (\\text{LIE: } E(X) = E[E(X|Y)] \\ ) \\\\\n& = E\\left( \\frac{Y_{i}D_i}{\\pi(X_i)} - \\frac{Y_{i}(1-D_i)}{1-\\pi(X_i)}\\right) && (\\text{observered outcome}) \\\\\n& = E \\left( \\frac{\\color{blue}{Y_i} \\color{black}D_i(1-\\pi(X_i))-\\color{blue}{Y_i}\\color{black}(1-D_i)\\pi(X_i)}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{getting common denom.}) \\\\\n& = E\\left( Y_i \\frac{D_i(1-\\pi(X_i))-(1-D_i)\\pi(X_i)}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{factor out }Y_i) \\\\\n& = E\\left( Y_i \\frac{D_i - D_i\\pi(X_i)-(\\pi(X_i) -D_i\\pi(X_i))}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{distribute out}) \\\\\n&  = E\\left( Y_i \\frac{D_i \\color{blue}{- D_i\\pi(X_i)} \\color{black}-\\pi(X_i) \\color{blue}{+ D_i\\pi(X_i)}}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{distribute out negative})\\\\\n& = E\\left( Y_i \\frac{D_i -\\pi(X_i) }{\\pi(X_i)(1-\\pi(X_i))}\\right) && (\\text{cancel out})\\\\\n\\end{align}\n\\tag{7}\\]\nAnd thus, we have identified the ATE.\n\n\n\nInverse Probability Weighting Estimator\nAn alternative use of propensity scores is weighting. As shown above, under conditional ignorability and common support, we can identify the ATE as:\n\\[\n\\tau_{ATE} = E\\left[ Y_i \\times \\underbrace{\\frac{D_i - \\pi(X_i)}{\\pi(X_i) (1 - \\pi(X_i))}}_{\\text{weight}}\\right]\n\\]\nThe inverse probability weighting (IPW) estimator is the sample estimator:\n\\[\n\\begin{split}\n\\hat\\tau_{ATE} & = \\frac{1}{N} \\sum\\limits_{i=1}^N \\left(Y_i \\frac{D_i - \\hat\\pi(X_i)}{\\hat\\pi(X_i) (1 - \\hat\\pi(X_i))} \\right) \\\\\n& = \\frac{1}{N} \\sum\\limits_{i=1}^N \\left(\\frac{D_i Y_i}{\\hat\\pi(X_i)} - \\frac{(1-D_i) Y_i}{1 - \\hat\\pi(X_i)} \\right)\n\\end{split}\n\\]\n\nThe second equation is equivalent to the first, shown by Equation 7 .\n\nEssentially, those who are unlikely to be treated but do get treated get weighted more, and individuals who are likely to be treated but do not get treated get weighted more.\n\n\n\n\n\n\nWeighting Estimator for ATT\n\n\n\n\n\nThe identification of the ATT under both conditional ignorability and common support are:\n\\[\n\\tau_{ATT} = \\frac{1}{Pr(D = 1)} \\times E\\left[ Y_i \\times \\underbrace{\\frac{D_i - \\pi(X_i)}{(1 - \\pi(X_i))}}_{\\text{weight}}\\right]\n\\]\nThe sample IPW estimator would be:\n\\[\n\\begin{split}\n\\hat\\tau_{ATT} & = \\frac{1}{N_1}\\sum\\limits_{i=1}^N \\left( Y_i \\frac{D_i - \\hat\\pi(X_i)}{1 - \\hat\\pi(X_i)} \\right) \\\\\n& = \\frac{1}{N_1} \\sum\\limits_{i=1}^N \\left( D_iY_i - (1-D_i)Y_i \\frac{\\hat\\pi(X_i)}{1 - \\hat\\pi(X_i)} \\right)\n\\end{split}\n\\]\n\n\n\nThe IPW estimator is asymptotically consistent, but has very poor small sample properties. They are highly sensitive to extreme values of \\(\\hat\\pi(X_i)\\). This generates high variance (inefficiency), and can produce significant bias under model mispecification.\n\n\n\n\n\n\nFalsification Tests\n\nTesting Assumptions with Falsification\nThe stronger (bolder) our assumptions for identification, the less credible our results are. Selection on Observables involves a very strong and hard to verify assumption: conditional ignorability. Can we really be sure that we have controlled for all confounders \\(X_i\\) needed to satisfy conditional ignorability?\nPlacebo tests are a type of falsification test to show evidence against our assumptions. Suppose that we make the assumption of conditional ignorability \\((Y_{0i}, Y_{1i}) \\perp D_i | X_i\\). Suppose we are concerned about the presence of another confounder \\(U\\) that is not included in \\(X_i\\).\n\n\n\n\n\nThe presence of \\(U\\) will falsify our conditional ignorability assumption, and means we cannot identify the causal effect of \\(D \\rightarrow Y\\).\n\n\n\n\n\n\nFalsification vs. Validation\n\n\n\n\n\nFalsification is a principle of trying to criticise our own research, rather than defend it. Falsification is about testing if our assumptions are not met. Failing a test provides evidence that our assumption is not met.\n\nEx. Covariates are balanced - thus there is no evidence that our assumptions are not met. We are not saying that our assumption is correct, just that there is no evidence against it.\n\nValidation is the opposite - we test to see if there is evidence in favour of our assumptions.\n\nEx. Covariates are balanced - thus our assumptions are met.\n\n\n\n\nFor falsification tests, we should not just pay attention to statistical significance - we must also pay attention to the magnitude of the point estimation.\n\n\n\nPlacebo Outcome Test\nA placebo outcome test utilities another alternative outcome variable \\(Y'\\) that is caused by our hypothesised unobserved confounder \\(U\\):\n\n\n\n\n\nWe can see that if \\(U\\) does not exist, \\(D\\) should have zero effect on the new outcome \\(Y'\\). Thus, if \\(U\\) is present, we should find a relationship between \\(D\\) and \\(Y'\\).\n\\[\nY'_i = \\gamma + \\delta D_i + u_i\n\\]\n\nIf we find that there is an effect of \\(D\\) on the new outcome \\(Y'\\) (non-zero \\(\\delta\\)), that is evidence that \\(U\\) exists, and is evidence to reject our conditional ignorability assumption (falsifies our design - red flag!).\nIf you do not find an effect of \\(D\\) on new outcome \\(Y'\\) (\\(\\delta = 0\\)) you find no evidence of \\(U\\), and no evidence to reject our conditional ignorability assumption (fails to falsify our design).\n\nWe must be sure that \\(Y\\) is not related to \\(Y'\\) except through \\(D\\) and \\(U\\). If this is true, we just run our original research design but replace \\(Y\\) with \\(Y'\\).\n\n\n\nPlacebo Treatment Test\nA placebo treatment test involves some other treatment \\(D'\\), that was assigned at the same time\n\n\n\n\n\nWe can see that if \\(U\\) does not exist, the effect of \\(D'\\) should have no effect on \\(Y\\). If \\(U\\) does exist, there should be some effect of \\(D'\\) on \\(Y\\).\n\\[\nY_i = \\gamma + \\delta D'_i + u_i\n\\]\n\nIf we find that there is an effect of \\(D'\\) on \\(Y\\) (non-zero \\(\\delta\\)), that is evidence that \\(U\\) exists, and is evidence to reject our conditional ignorability assumption (falsifies our design - red flag!).\nIf you do not find an effect of \\(D'\\) on \\(Y\\) (\\(\\delta = 0\\)) you find no evidence of \\(U\\), and no evidence to reject our conditional ignorability assumption (fails to falsify our design).\n\nWe must be sure that \\(Y\\) is not related to \\(D'\\) except through \\(D\\) and \\(U\\). If this is true, we just run our original research design but replace \\(D\\) with \\(D'\\).\n\n\n\n\n\n\nPartial Identification\n\nDecomposing the ATE\nWith falsification, we were concerned with what assumptions we needed to be not-false in order to identify the ATE. However, we can take a different approach - what can we learn about the ATE without any assumptions?\nLet us decompose the ATE into parts:\n\\[\n\\begin{align}\n\\tau_{ATE}  = & E(Y_{1i} - Y_{0i}) \\\\\n& \\\\\n= & E(Y_{1i} - Y_{0i} | D_i = 1) Pr(D_i = 1) \\\\\n& \\quad - E(Y_{1i} - Y_{0i}|D_i = 0)Pr(D_i = 0)  && (\\text{def. of weighted avg.})\\\\\n& \\\\\n= & [ \\color{blue}{E(Y_i |D_i = 1)} \\color{black}- \\color{red}{E(Y_{0i}|D_i = 1)} \\color{black}] Pr(D_i = 1) \\\\\n& \\quad + [ \\color{red}{E(Y_{1i}|D_i = 0)} \\color{black} - \\color{blue}{E(Y_i|D_i = 0)} \\color{black}]Pr(D_i = 0) && (\\text{observed + unobserved})\n\\end{align}\n\\]\nSome of the quantities are observed (in blue), and some of the quantities are unobserved (in red). Previously, we made assumptions (conditional ignorability, common support) to fill the unobserved quantities. But, we can make actually any assumption as possible.\n\n\n\nNonparametric Bounds\nOne way to fill in our unobserved outcomes through the “best” and “worst” possible outcomes. This allows us to construct a plausible range of the ATE.\nFirst, let us construct the worst-case scenario - the lowest possible \\(\\tau\\).\n\n\\(E(Y_{0i}|D_i = 1) = Y_H\\). Units in the treated \\(D_i=1\\), their potential outcome \\(Y_{0i}\\) will be the highest \\(Y\\) possible, \\(Y_H\\).\n\\(E(Y_{1i}|D_i = 0) = Y_L\\). Units in the control \\(D_i=0\\), their unobserved potential outcome \\(Y_{1i}\\) will be the lowest \\(Y\\) possible, \\(Y_L\\).\n\nThus, the lowest possible \\(\\tau\\) (sharp lower bound) is:\n\\[\n\\begin{split}\n\\tau_L = & [ \\color{blue}{E(Y_i |D_i = 1)} \\color{black}  - \\color{red}{Y_H} \\color{black}] Pr(D_i = 1) \\\\\n& \\quad + [ \\color{red}{Y_L} \\color{black} - \\color{blue}{E(Y_i|D_i = 0)} \\color{black}]Pr(D_i = 0)\n\\end{split}\n\\]\nNow, let us construct the best-case scenario - the highest possible \\(\\tau\\).\n\n\\(E(Y_{0i}|D_i = 1) = Y_L\\). Units in the treated \\(D_i=1\\), their potential outcome \\(Y_{0i}\\) will be the lowest \\(Y\\) possible, \\(Y_L\\).\n\\(E(Y_{1i}|D_i = 0) = Y_H\\). Units in the control \\(D_i=0\\), their unobserved potential outcome \\(Y_{1i}\\) will be the highest \\(Y\\) possible, \\(Y_H\\).\n\nThus, the highest possible \\(\\tau\\) (sharp upper bound) is:\n\\[\n\\begin{split}\n\\tau_H = & [ \\color{blue}{E(Y_i |D_i = 1)} \\color{black}  - \\color{red}{Y_L} \\color{black}] Pr(D_i = 1) \\\\\n& \\quad + [ \\color{red}{Y_H} \\color{black} - \\color{blue}{E(Y_i|D_i = 0)} \\color{black}]Pr(D_i = 0)\n\\end{split}\n\\]\nWe know that the true \\(\\tau_{ATE} \\in [\\tau_L, \\tau_H]\\).\n\n\n\nMonotone Treatment Selection Assumption\nOur extreme case from above is not very useful. However, we can layer on assumptions to lower the possible \\(\\tau\\) values.\nOne assumption is the Monotone Treatment Selection (MTS) assumption. This assumption basically says that potential outcomes for units in treatment, are always higher than for those in the control.\n\\[\n\\begin{split}\n& E(Y_{0i}|D_i = 0) ≤ \\overbrace{E(Y_{0i}|D_i = 1)}^{\\text{unobserved}} \\\\\n& \\underbrace{E(Y_{1i} |D_i = 0)}_{\\text{unobserved}} ≤ E(Y_{1i} |D_i = 1)\n\\end{split}\n\\]\nThis is basically saying that selection bias is one-direction.\nThis implies a tighter sharp upper bound on \\(\\tau\\).\n\\[\n\\begin{align}\n\\tau_H = & [ \\underbrace{E(Y_i |D_i = 1)}_{\\text{observed}}  - \\color{red}{E(Y_{0i}|D_i = 0)} \\color{black}] Pr(D_i = 1) \\\\\n& \\quad + [ \\color{red}{E(Y_{1i}|D_i =1)} \\color{black} - \\underbrace{E(Y_i|D_i = 0)}_{\\text{observed}}]Pr(D_i = 0) \\\\\n= & E(Y_i|D_i = 1) - E(Y_i|D_i = 0) && (\\text{def. of weighted avg.})\\\\\n\\end{align}\n\\]\nThis indicates that the upper bound of plausible \\(\\tau_{ATE}\\) values is the naive estimator of differences in observed outcomes.\nWe can also make the reverse assumption, where selection bias is in the opposite direction. This means a tighter sharp lower bound \\(\\underline\\tau\\). These assumptions help narrow our possible \\(\\tau_{ATE}\\) values, and can allow us to test if our estimated \\(\\hat\\tau\\) is reasonable (within the plausible bounds).\n\n\n\n\n\n\nImplementation in R\nFor all methods, you will need the tidyverse package:\n\nlibrary(tidyverse)\nlibrary(MatchIt)\nlibrary(estimatr)\n\nSee how to perform each estimator in R:\n\n\n\n\n\n\nDistance Matching\n\n\n\n\n\nFirst, let us conduct nearest neighbour matching with Mahalanobis distance by using the matchit() function.\n\nmatch_object = MatchIt::matchit(D ~ X1 + X2 + X3,\n                                data = my_data,\n                                method = \"nearest\", #distance matching\n                                distance = \"mahalanobis\")\n\n# for output summary\nsummary(match_object)\n\nSecond, let us save the matched data with the match.data() function.\n\nmatch_data &lt;- MatchIt::match.data(match_object,\n                                  weights = 'nn_weights')\n\nThird, we can test if matching worked by using a balance table and a love plot:\n\n# balance table\ncobalt::bal.tab(D ~ X1 + X2 + X3, \n                data = match_data, # from the 2nd step\n                weights = \"nn_weights\",\n                disp = c(\"means\", \"sds\"))\n\n#love plot\ncobalt::love.plot(match_object,\n                  data = my_data, #original dataset\n                  stars = 'raw')\n\nFinally, we can estimate the treatment effect. There are two options - either using a weighted regression, or using the matching algorithm:\n\n# using weighted regression\nestimate &lt;- lm_robust(Y ~ D,\n                      data = match_data, #data from step 2\n                      weights = nn_weights)\nsummary(estimate)\n\n## using the Matching package:\nestimate = Matching::Match(Y = my_data$Y, #outcome\n                           Tr = my_data$D, #treatment\n                           X = my_data[,c(\"X1\", \"X2\", \"X3\")], #covariates\n                           M=1, #number of neighbours\n                           BiasAdjust = TRUE, #for biased adjustment\n                           Weight = 2)\nsummary(estimate)\n\nYou will have the estimates that you can use.\n\n\n\n\n\n\n\n\n\nPropensity Score Matching\n\n\n\n\n\nFirst, we want to estimate propensity scores with a logistic regression (or a random forest):\n\n#logistic model\npscore_model = glm(D ~ X1 + X2,\n                   data = my_data,\n                   family = \"binomial\")\n\n# estimate propensity scores\nmy_data$pscore_estimate = predict(pscore_model,\n                                  my_data,\n                                  type = \"response\")\n\nNow, let us match with propensity scores:\n\n# match\nmatch_object = MatchIt::matchit(D ~ pscore_estimate,\n                                data = my_data,\n                                method = \"nearest\",\n                                distance = \"Mahalanobis\")\n\n# save matched data\nmatch_data &lt;- MatchIt::match.data(match_object,\n                                  weights = 'pscore_weights')\n\nThird, we can test if matching worked with a balance table and a love plot:\n\n#balance table\ncobalt::bal.tab(D ~ X1 + X2 + X3,\n                data = match_data, #matched data from step 2\n                weights = \"pscore_weights\",\n                disp = c(\"means\", \"sds\"))\n\n#love plot\ncobalt::love.plot(match_object,\n                  data = my_data, #original dataset\n                  addl = ~ X1 + X2 + X3,\n                  stars = 'raw')\n\nFinally, let us do the estimation:\n\nestimate &lt;- lm_robust(Y ~ D,\n                      data = match_data, #from step 2\n                      weights = pscore_weights)\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nInverse Probability Weighting\n\n\n\n\n\nFirst, we want to estimate propoensity scores with a logistic regression (or a random forest):\n\n#logistic model\npscore_model = glm(D ~ X1 + X2,\n                   data = my_data,\n                   family = \"binomial\")\n\n# estimate propensity scores\nmy_data$pscore_estimate = predict(pscore_model,\n                                  type = \"response\")\n\nSecond, we calculate the inverse probability weights based on the formula from earlier:\n\nmy_data$ipweight = ifelse(my_data$D == 1, # condition\n                       1/my_data$pscore_estimate,\n                       1/(1-my_data$pscore_estimate))\n\nFinally, we can estimate the ATE, or ATT, or use a weighted regression for the ATE:\n\n# ATE estimator\nmean((my_data$D * my_data$Y) * my_data$ipweight - ((1 - my_data$D) * my_data$Y) * my_data$ipweight)\n\n# ATT estimator\nsum(my_data$D * my_data$Y - (1 - my_data$D) * my_data$Y * (my_data$pscore_estimate/(1 - my_data$pscore_estimate)))/sum(my_data$D)\n\n# ATE with weighted regression\nestimate &lt;- lm_robust(Y ~ D, \n                      data = my_data,\n                      weights = ipweight)\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nOLS Estimator\n\n\n\n\n\nFor the OLS estimator, we can use the lm_robust() function:\n\nestimate &lt;- lm_robust(Y ~ D + X1 + X2 + X3,\n                      data = my_data)\nsummary(estimate)\n\nWe can also use the fixest package and the feols() function:\n\nlibrary(fixest)\n\nestimate &lt;- feols(Y ~ D + X1 + X2 + X3,\n                  data = my_data,\n                  se = \"hetero\")\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nFully Interacted Estimator\n\n\n\n\n\nFor the fully interacted estimator, we can use the lm_lin() function.\n\nestimate &lt;- lm_lin(Y ~ D,\n                   covariates = ~ X1 + X2 + X3,\n                   data = my_data)\nsummary(estimate)\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "6 Selection on Observables"
    ]
  },
  {
    "objectID": "quant2.html",
    "href": "quant2.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "In the last chapter, we discussed the basics of statistics, and briefly introduced regression as a way to find correlations.\nThis chapter dives deep into multiple linear regression, the foundational model for all of statistics. We cover the specification of the model, estimation and statistical inference, as well as extensions.\nUse the right sidebar for quick navigation. R-code is provided at the bottom.\n\n\nBasics of the Model\n\nModel Specification\nLet us say we have some outcome variable \\(y\\), and several explanatory variables \\(x_1, x_2, \\dots, x_k\\). We have data on \\(n\\) number of observations \\(i = 1, \\dots n\\).\nThe linear regression model can be written as a conditional expectation \\(E(y|x)\\) function:\n\\[\nE(y_i |x_i) = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_kx_{ki}\n\\]\nThe linear model can also be specified for any specific outcome value \\(y_i\\) for unit \\(i\\):\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_kx_{ki} + u_i\n\\]\nWe can also specify the linear model in terms of linear algebra:\n\\[\n\\begin{pmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\\end{pmatrix} =\n\\begin{pmatrix}1 & x_{11} & \\dots & x_{k1} \\\\1 & x_{12} & \\dots & x_{k2} \\\\\\vdots & \\vdots & \\vdots & \\vdots \\\\1 & x_{1n} & \\dots & x_{kn}\\end{pmatrix}\n\\begin{pmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_k\\end{pmatrix}\n+ \\begin{pmatrix}u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n\\end{pmatrix}\n\\]\n\\[\n\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u\n\\]\n\n\n\n\n\n\nMore Info on Conditional Expectations\n\n\n\n\n\nImagine \\(y\\) is income, and \\(x\\) is age.\nAt age \\(x=20\\), not every 20 year old makes the same amount of income. There is some distribution, with some making more, and some making less. This is the distribution \\(y|x=20\\).\nWe can find the expected value of this distribution, \\(E(y|x=20)\\). This is a conditional expectation, and indicates the expected income of a 20 year old if we randomly chose one from the distribution.\nAt \\(x=30\\), the \\(E(y|x)\\) probably is different (30 year olds make more money). Thus, the linear model is essentially stating that the expected \\(y\\) depends on \\(x\\). Or in terms of this example, the expected income depends on the individual’s age.\n\n\n\n\n\n\n\n\n\nMore Info on the Error Term \\(u_i\\)\n\n\n\n\n\nThe \\(u_i\\) is called the error term. This indicates that not every value of \\(y_i\\) in our data will be exactly on the linear best-fit line.\nGraphically, it is the highlighted part:\n\n\n\n\n\nIn social science terms, the \\(u_i\\) is the effect of any other variable not included in our model on \\(y\\).\nFor example, if \\(x\\) is age, and \\(y\\) is income, we will have the following relationship:\n\\[\n\\text{income}_i = \\beta_0 + \\beta_1 \\text{age}_i + u_i\n\\]\nHowever, not every individual lies perfectly on this linear line. This is because there are other factors outside of age that affect \\(y\\) (income), and these other factors are bundled into the error term.\n\n\n\n\n\n\nEstimation Process\nTo estimate the population parameters \\(\\beta_0, \\dots, \\beta_k\\), we use our sample data, and try to find the values \\(\\widehat{\\beta_0}, \\dots, \\widehat{\\beta_k}\\) that minimise the square sum of residuals (SSR):\n\\[\n\\begin{align}\nSSR & = \\sum\\limits_{i=1}^n(y_i - \\hat y_i)^2 \\\\\n& = \\sum\\limits_{i=1}^n(y_i - \\color{blue}{(\\widehat{\\beta_0} + \\widehat{\\beta_1}x_{1i} + \\dots + \\widehat{\\beta_k}x_{ki})}\\color{black})^2 && (\\text{plug in } \\color{blue}{\\hat y = \\widehat{\\beta_0} + \\widehat{\\beta_1}x_{1i} + \\dots }\\color{black}) \\\\\n& = \\sum\\limits_{i=1}^n(y_i - \\widehat{\\beta_0} - \\widehat{\\beta_1}x_{1i} - \\dots - \\widehat{\\beta_k}x_{ki})^2 &&(\\text{distribute negative sign})\n\\end{align}\n\\]\nIn the linear algebra representation (where \\(\\mathbf b\\) is the vector of estimated parameters \\(\\widehat{\\beta_0}, \\dots, \\widehat{\\beta_k}\\)):\n\\[\n\\begin{align}\nS(\\hat{\\boldsymbol\\beta}) & = (\\mathbf y - \\hat{\\mathbf y})^\\mathsf{T} (\\mathbf y - \\hat{\\mathbf y})\\\\\n& = (\\mathbf y - \\color{blue}{\\mathbf X \\hat{\\boldsymbol\\beta}}\\color{black})^\\mathsf{T} (\\mathbf y - \\color{blue}{\\mathbf{X} \\hat{\\boldsymbol\\beta}}\\color{black}) && (\\text{plug in } \\color{blue}{\\hat{\\mathbf y}  = \\mathbf X \\hat{\\boldsymbol\\beta}}\\color{black}) \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\nIntuitive Visualisation of SSR\n\n\n\n\n\nThe residuals are the difference from our predicted best-fit line result \\(\\widehat{y_i}\\), and the actual value of \\(y_i\\) in the data. Below highlighted in red are the residuals.\n\n\n\n\n\nAfter we have the residual values, we simply square each of them, then sum all of them together. That is the sum of squared residuals.\n\n\n\nThis estimation is called the ordinary least squares (OLS) estimator. The solutions to the OLS estimator can be derived mathematically.\n\n\n\nDeriving OLS Estimates\nOLS wants to minimise the sum of squared residuals \\(S(\\hat{\\boldsymbol\\beta})\\) - the differences between the actual \\(\\mathbf y\\) and our predicted \\(\\hat{\\mathbf y}\\):\n\\[\n\\begin{align}\nS(\\hat{\\boldsymbol\\beta}) & = (\\mathbf y - \\hat{\\mathbf y})^\\mathsf{T} (\\mathbf y - \\hat{\\mathbf y})\\\\\n& = (\\mathbf y - \\color{blue}{\\mathbf X \\hat{\\boldsymbol\\beta}}\\color{black})^\\mathsf{T} (\\mathbf y - \\color{blue}{\\mathbf{X} \\hat{\\boldsymbol\\beta}}\\color{black}) && (\\text{plug in } \\color{blue}{\\hat{\\mathbf y}  = \\mathbf X \\hat{\\boldsymbol\\beta}}\\color{black}) \\\\\n& = \\mathbf y^\\mathsf{T} \\mathbf y - \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf y - \\mathbf y^\\mathsf{T} \\mathbf{X}\\hat{\\boldsymbol\\beta} + \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf{Xb} && (\\text{distribute out)} \\\\\n& = \\mathbf y^\\mathsf{T} \\mathbf y - \\color{blue}{2\\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf y}\\color{black} + \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf{X} \\hat{\\boldsymbol\\beta} &&(\\text{combine } \\color{blue}{- \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf y - \\mathbf y^\\mathsf{T} \\mathbf{X}\\hat{\\boldsymbol\\beta}}\\color{black})\n\\end{align}\n\\]\nNow, let us find the first order condition:\n\\[\n\\frac{\\partial S(\\hat{\\boldsymbol\\beta})}{\\partial \\hat{\\boldsymbol\\beta}} = -2\\mathbf X^\\mathsf{T} \\mathbf y + 2 \\mathbf X^\\mathsf{T} \\mathbf{X} \\hat{\\boldsymbol\\beta} = 0\n\\]\nWhen assuming \\(\\mathbf X^\\mathsf{T} \\mathbf X\\) is invertable (which is true if \\(\\mathbf X\\) is full rank), we can isolate \\(\\hat{\\beta}\\) to find the solution to OLS:\n\\[\n\\begin{align}\n-2\\mathbf X^T\\mathbf y + 2 \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol{\\hat{\\beta}} & = 0 \\\\\n2 \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol{\\hat\\beta} & = 2\\mathbf X^\\mathsf{T} \\mathbf y && (+ 2\\mathbf X^\\mathsf{T} \\mathbf y \\text{ to both sides}) \\\\\n\\boldsymbol{\\hat\\beta} & = (2\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} 2 \\mathbf X^\\mathsf{T} \\mathbf y && (\\times (2\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\text{ to both sides})\\\\\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y &&(\\text{cancel out } 2^{-1}\\times 2)\n\\end{align}\n\\]\nThose are our coefficient solutions to OLS. With the estimated parameters \\(\\widehat{\\beta_0}, \\dots, \\widehat{\\beta_k}\\), we now have a best-fit line, called the fitted values.\nFor more detailed analysis of the OLS estimator, see the causal inference section.\n\n\n\n\n\n\nInterpretation\n\nInterpretation of Parameters\nI define \\(\\widehat{\\beta_j} \\in \\{\\widehat{\\beta_1}, \\dots, \\widehat{\\beta_k}\\}\\), multiplied to \\(x_j \\in \\{x_1, \\dots, x_k\\}\\). \\(\\widehat{\\beta_0}\\) is the intercept.\n\n\n\n\n\n\n\n\n\nContinuous \\(x_j\\)\nBinary \\(x_j\\)\n\n\nContinuous \\(y\\)\nFor every one unit increase in \\(x_j\\), there is an expected \\(\\widehat{\\beta_j}\\) unit change in \\(y\\).\nWhen all explanatory variables equal 0, the expected value of \\(y\\) is \\(\\widehat{\\beta_0}\\).\nThere is a \\(\\widehat{\\beta_j}\\) unit difference in \\(y\\) between category \\(x_j = 1\\) and category \\(x_j = 0\\).\nFor category \\(x_j = 0\\), the expected value of \\(y\\) is \\(\\widehat{\\beta_0}\\) (when all other explanatory variables equal 0).\n\n\nBinary \\(y\\)\nFor every one unit increase in \\(x_j\\), there is an expected \\(\\widehat{\\beta_j} \\times 100\\) percentage point change in the probability of a unit being in category \\(y=1\\).\nWhen all explanatory variables equal 0, the expected probability of a unit being in category \\(y=1\\) is \\(\\widehat{\\beta_0} \\times 100\\)\nThere is a \\(\\widehat{\\beta_j}\\times 100\\) percentage point difference in the probability of a unit being in category \\(y=1\\) between category \\(x_j = 1\\) and category \\(x_j = 0\\).\nFor category \\(x_j = 0\\), the expected probability of a unit being in category \\(y=1\\) is \\(\\widehat{\\beta_0} \\times 100\\) (when all other explanatory variables equal 0).\n\n\n\nIf you have multiple explanatory variables, always add: while holding all other explanatory variables not \\(x_j\\) constant.\n\n\n\nStandardised Interpretations\nSometimes, a \\(\\beta_j\\) increase in \\(y\\) for every one unit increase in \\(x\\) is not particularly useful for us to interpret. For example, if \\(y\\) is democracy, what does a 5 unit increase in democracy actually mean?\nWe can add more relevant detail by expressing the change of \\(y\\) and \\(x\\) in terms of their standard deviations. Or in other words, we want to find the change in \\(\\frac{\\hat y_i}{\\sigma_y}\\) for every one standard deviation \\(\\sigma_x\\) increase in \\(x\\). For simplicity, let us use a simple linear regression \\(E(y_i|x_i) = \\beta_0 + \\beta_1 x_i\\):\n\\[\n\\begin{align}\n& E \\left(\\frac{y_i}{\\sigma_y} | x_i = x + \\sigma_x \\right ) - E \\left(\\frac{y_i}{\\sigma_y} | x_i = x \\right ) \\\\\n& = \\frac{E(y_i|x_i = x+ \\sigma_x)}{\\sigma_y} - \\frac{E(y_i|x_i = x)}{\\sigma_y} &&(\\text{property of expectation}) \\\\\n& = \\frac{E(y_i|x_i = x+ \\sigma_x) - E(y_i|x_i = x)}{\\sigma_y} && (\\text{combine into 1 fraction})\\\\\n& = \\frac{\\beta_0 + \\beta_1(x+\\sigma_x) - [\\beta_0 + \\beta_1(x)]}{\\sigma_y} && (\\text{plug in regression models})\\\\\n& = \\frac{\\beta_0 + \\beta_1x + \\beta_1\\sigma_x - \\beta_0 -\\beta_1x}{\\sigma_y} && (\\text{distribute out})\\\\\n& = \\frac{\\beta_1\\sigma_x}{\\sigma_y} && (\\text{cancel and simplify})\n\\end{align}\n\\]\nThus, for a one standard deviation \\(\\sigma_x\\) increase in \\(x_j\\), there is an expected \\(\\frac{\\beta_j\\sigma_x}{\\sigma_y}\\)-standard deviation change in \\(y\\).\n\n\n\nResidual Standard Deviation\nResiduals are the distance of the actual value \\(y_i\\) of observation \\(i\\), compared to the predicted \\(\\widehat{y_i}\\) from our fitted values/best-fit line. They can be obtained after we fit our model:\n\\[\n\\begin{align}\n\\mathbf{\\hat u} & = \\mathbf y - \\mathbf{\\hat y} \\\\\n& = \\mathbf y - \\color{blue}{\\mathbf{X} \\hat{\\boldsymbol\\beta}} && \\color{black}(\\text{plug in } \\color{blue}{\\hat{\\mathbf y} = \\mathbf{X} \\hat{\\boldsymbol\\beta}} \\color{black}) \\\\\n& = \\mathbf y - \\mathbf X \\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y} && \\color{black}(\\text{plug in } \\color{blue}{\\boldsymbol{\\hat\\beta}  = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y}\\color{black})\n\\end{align}\n\\]\nThe residual standard deviation \\(\\hat\\sigma\\) measures the spread/variance of our residuals - so, how far away the actual values \\(y_i\\) are from our predicted values \\(\\widehat{y_i}\\) in general for all observations.\nThe residual variance is estimated with the formula below (with the residual standard deviation being the square root):\n\\[\n\\hat\\sigma^2 = \\frac{\\sum_{i=1}^n \\hat u_i^2}{n-k-1} = \\frac{\\mathbf{\\hat u}^\\mathsf{T} \\mathbf{\\hat u}}{n-k-1}\n\\]\n\n\n\n\n\n\nVisualisation of Residual Standard Deviation\n\n\n\n\n\nBelow is a figure illustrating different residual standard deviations, with the same best-fit line.\n\n\n\n\n\n\n\n\nSmaller \\(\\hat\\sigma\\) mean the actual values are, on average, close to our predicted values, and larger \\(\\hat\\sigma\\) mean the actual values are, on average, further away from our predicted values.\n\n\n\nR-Squared\nOur fitted values equation takes the following form:\n\\[\n\\begin{align}\n\\hat{\\mathbf y} & = \\mathbf X \\hat{\\boldsymbol\\beta}  \\\\\n\\hat{\\mathbf y} & = \\mathbf X\\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y} && \\color{black}(\\text{plug in } \\color{blue}{\\boldsymbol{\\hat\\beta} = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y}\\color{black}) \\\\\n\\hat{\\mathbf y} &= \\color{blue}{\\mathbf P}\\color{black}{\\mathbf y}  && (\\text{plug in } \\color{blue}{\\mathbf P : = \\mathbf X(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}} \\color{black})\n\\end{align}\n\\]\nWe can see that \\(\\mathbf P\\) is a matrix that turns \\(\\mathbf y \\rightarrow \\hat{\\mathbf y}\\). Matrix \\(\\mathbf P\\) is our linear model that projects the true values \\(\\mathbf y\\) into a space of our regressors \\(\\mathbf X\\).\nOne thing we might be interested in is how well our model \\(\\mathbf{Py}\\) explains the actual \\(\\mathbf y\\). One way we can do this is the scalar product of two vectors. We know that the scalar/dot product calculates the project/shadow of one vector on another. Thus, the scalar product \\(\\mathbf y^\\mathsf{T}\\mathbf{Py}\\) describes the shadow the actual \\(y\\) casts on our projected model.\nHowever, this value will change based on the scale of our \\(y\\) variable. Thus, we will divide it by \\(\\mathbf y^\\mathsf{T}\\mathbf y\\), which is the “maximum” shadow possible (perfect shadow). This ratio is called \\(R^2\\).\n\\[\nR^2 = \\frac{\\mathbf y^\\mathsf{T}\\mathbf{Py}}{\\mathbf y^\\mathsf{T}\\mathbf y}\n\\]\nR-Squared (\\(R^2\\)) measures the proportion of variation in \\(y\\) that is explained by our explanatory variables. R-Squared is always between 0 and 1 (or 0-100 as a percentage). Higher values indicate our model better explains the variation in \\(y\\).\nInterpreting R-squared: The Model explains \\(R^2 \\times 100\\) percent of the variation in \\(y\\).\n\n\n\n\n\n\nStatistical Inference\n\nHomoscedasticity and Heteroscedasticity\nHomoscedasticity is defined as:\n\\[\nVar(\\mathbf u | \\mathbf X) = \\sigma^2 \\mathbf I_n = \\begin{pmatrix}\n\\sigma^2 & 0 & \\dots & 0 \\\\\n0 & \\sigma^2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & 0 \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{pmatrix}\n\\]\nOr in other words, no matter the values of any explanatory variable, the error term variance is constant.\nIf this is false, then we have heteroscedasticity.\n\n\n\n\n\n\nIntuitive Visualisation of Homoscedasticity\n\n\n\n\n\nAn easy way to identify homoscedasticity is to look at a residual plot (just the plot of all \\(\\widehat{u_i}\\)):\n\nNotice how the homoscedasticity residuals seem to have the same up-down variance, no matter the value of \\(x\\).\nThe heteroscedasticity residuals have a clear pattern - the up-down variance is smaller when \\(x\\) is smaller, and the up-down variance is larger when \\(x\\) is larger.\nEssentially, if you see a pattern in the residual plot, it is likely heteroscedasticity.\n\n\n\nIf you have homoscedasticity, you should use normal OLS standard errors.\nIf you have heteroscedasticity, you should use robust OLS standard errors. You should also use robust standard errors if you are not sure which errors to use.\n\n\n\nDeriving Standard Errors\nWe will only derive homoscedastic (normal) standard errors. The robust standard error derivation is beyond the scope of this lesson (just trust the computer that it will calculate it properly).\nLet us assume homoscedasticity. We want to find the variance of our estimator, \\(Var(\\boldsymbol{\\hat\\beta} | \\mathbf X)\\). Let us start off with our OLS solution. We can simplify as follows:\n\\[\n\\begin{align}\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}(\\mathbf X \\boldsymbol\\beta + \\mathbf u) && (\\text{plug in } \\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u) \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&(\\text{multiply out})\\\\\n& = \\color{blue}{\\mathbf I}\\color{black}{\\boldsymbol\\beta} + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&( \\ \\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X = \\mathbf I}\\color{black})\\\\\n& = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{identity property of } \\mathbf I)\n\\end{align}\n\\]\n\\[\nVar(\\boldsymbol{\\hat\\beta} | \\mathbf X) = Var(\\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u \\ | \\ \\mathbf X)\n\\]\n\\(\\boldsymbol\\beta\\) is a vector of fixed constants. \\((\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u\\) can be imagined as a matrix of fixed constants, since we are conditioning the above variance on \\(\\mathbf X\\) (so for each \\(\\mathbf X\\), the statement is fixed).\n\n\n\n\n\n\nMathematical Lemma\n\n\n\n\n\nIf \\(\\mathbf u\\) is an \\(n\\) dimensional vector of random variables, \\(\\mathbf c\\) is an \\(m\\) dimensional vector, and \\(\\mathbf B\\) is an \\(n \\times m\\) dimensional matrix with fixed constants, then the following is true:\n\\[\nVar(\\mathbf c + \\mathbf{Bu}) = \\mathbf B Var(\\mathbf u)\\mathbf B^T\n\\]\nI will not prove this lemma here, but it is provable.\n\n\n\nWith the Lemma above, and with the definition of homoscedasticity, we can simplify:\n\\[\n\\begin{align}\nVar(\\boldsymbol{\\hat\\beta} | \\mathbf X) & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} Var(\\mathbf u | \\mathbf X) [(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}]^{-1} && (\\text{lemma})\\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} Var(\\mathbf u | \\mathbf X) \\color{blue}{\\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1}} && \\color{black}( \\ \\color{blue}{[(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}]^{-1} = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1}}\\color{black})\\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\color{blue}{\\sigma^2 \\mathbf I_n}\\color{black}{ \\mathbf X} (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\color{blue}{Var(\\mathbf u | \\mathbf X) = \\sigma^2 \\mathbf I_n}\\color{black}) \\\\\n& =  \\color{red}{\\sigma^2} \\color{black} (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf I_n \\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\text{move scalar } \\color{red}{\\sigma^2}\\color{black})\\\\\n& =  \\sigma^2 (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}  \\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\text{identity property of } \\mathbf I_n)\\\\\n& =  \\sigma^2 (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\text{inverses } \\mathbf X^\\mathsf{T}  \\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\text{ cancel})\n\\end{align}\n\\]\nHowever, we do not actually know what \\(\\sigma^2\\) is. We can estimate it with \\(\\hat\\sigma^2\\) (discussed here).\nThe standard errors are the square root of the variance. Thus, our standard errors for any coefficient estimate \\(\\hat\\beta_j\\) are:\n\\[\nse(\\hat\\beta_j) = \\hat\\sigma \\sqrt{(\\mathbf X^T \\mathbf X)^{-1}_{jj}}\n\\]\n\n\n\nT-Tests\nIn regression, our typical hypotheses are:\n\n\\(H_0 : \\beta_j = 0\\) (i.e. there is no relationship between \\(x_j\\) and \\(y\\)).\n\\(H_1:\\beta_j ≠ 0\\) (i.e. there is a relationship between \\(x_j\\) and \\(y\\)).\n\nUsing the standard error (see above), we calculate the \\(t\\)-statistic, and using the \\(t\\)-statistic, we calculate a p-value.\n\n\n\n\n\n\nDetails of Running a Hypothesis Test\n\n\n\n\n\nFirst, we calculate the t-test statistic:\n\\[\nt = \\frac{\\widehat{\\beta_1} - H_0}{\\widehat{se}(\\widehat{\\beta_1})}\n\\]\n\nWhere \\(H_0\\) is typically 0, but if you do decide to alter the null hypothesis, you would plug it in.\n\nNow, we consult a t-distribution of \\(n-k-1\\) degrees of freedom. We use a t-distribution because the standard error calculation used in OLS is slightly imprecise.\n\nNote: we can only do this step if we believe the central limit theorem is met (that our errors are asymptotically normal). We need a large enough sample size.\n\nWe start from the middle of the t-distribution, and move t-test-statstic number of standard deviations from both sides of the middle.\nThen, we find the probability of getting a t-test statistic even further from the middle than the one we got. The area highlighted in the figure below showcases this. In the figure, the t-test statistic is 2.228.\n\n\n\n\n\nThe area highlighted, divided by the entire area under the curve, is the p-value.\n\n\n\nThe p-value we get is the probability of getting a test statistic equally or more extreme than the one we got, given the null hypothesis is true.\n\nIf \\(p&lt;0.05\\), we believe the probability of a null hypothesis is low enough, such that we reject the null hypothesis (that there is no relationship between \\(x\\) and \\(y\\)), and conclude our alternate hypothesis (that there is a relationship between \\(x\\) and \\(y\\)).\nIf \\(p &gt; 0.05\\), we cannot reject the null hypothesis, and cannot reject that there is no relationship between \\(x\\) and \\(y\\).\n\nNOTE: this is not causality - we are only looking at the relationship. Causality needs to be established with an adequate research design.\n\n\n\nConfidence Intervals\nThe 95% confidence intervals of coefficients have the following bounds:\n\\[\n\\widehat{\\beta_j} - 1.96 \\widehat{se}(\\widehat{\\beta_j}), \\ \\ \\widehat{\\beta_j} + 1.96 \\widehat{se}(\\widehat{\\beta_j})\n\\]\n\nThe 1.96 is an approximation assuming a normal distribution. The actual confidence intervals (calculated by computers) will use a t-distribution of \\(n-k-1\\), which will result in a slightly different multiplicative factor.\n\nThe confidence interval means that under repeated sampling and estimating \\(\\widehat{\\beta_j}\\), 95% of the confidence intervals that we construct will include the true \\(\\beta_j\\) value in the population.\nIf the confidence interval contains 0, we cannot conclude a relationship between \\(x_j\\) and \\(y\\), as 0 is a plausible value of \\(\\beta_j\\). These results will always match those of the t-test.\n\n\n\nF-Tests\nF-tests are used to test more than one coefficient at a time. The hypotheses will be:\n\n\\(M_0 : y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_g x_g + u_i\\) (the smaller null model).\n\\(M_a : y = \\beta_0 + \\beta_1x_1 + \\dots + \\beta_g x_g + \\dots + \\beta_kx_k + u_i\\) (the bigger model with additional variables).\n\n\n\n\n\n\n\nDetails of the F-test\n\n\n\n\n\nF-tests compare the \\(R^2\\) of the two models through the F-statistic:\n\\[\nF = \\frac{(SSR_0 - SSR_a) / (k_a - k_0)}{SSR_a /(n - k_a - 1)}\n\\]\nWe then consult a F-distribution with \\(k_a - k_0\\) and \\(n-k_a - 1\\) degrees of freedom, obtaining a p-value (in the same way as the t-test).\n\n\n\nThe p-value we get is the probability of getting a test statistic equally or more extreme than the one we got, given the null hypothesis is true.\n\nIf \\(p&lt;0.05\\), the we believe the probability of the null hypothesis is low enough, such that we reject the null hypothesis (that \\(M_0\\) is the better model), and conclude our alternate hypothesis (that \\(M_a\\) is a better model). This also means the extra coefficients in \\(M_a\\) are jointly statistically significant.\nIf \\(p &gt; 0.05\\), we cannot reject the null hypothesis, and cannot reject that \\(M_0\\) is a better model. Thus, the extra coefficients in \\(M_a\\) are jointly not statistically significant.\n\n\n\n\nPredictive Inference\nWe can predict using the linear regression by plugging in explanatory variable values, and finding the predicted \\(\\widehat{y_i}\\).\n\\[\n\\begin{align}\n\\hat{\\mathbf y} & = \\mathbf X \\hat{\\boldsymbol\\beta}  \\\\\n\\hat{\\mathbf y} & = \\mathbf X\\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y} && \\color{black}(\\text{plug in } \\color{blue}{\\boldsymbol{\\hat\\beta} = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y}\\color{black}) \\\\\n\\end{align}\n\\]\nWe also have confidence intervals for every predicted \\(\\widehat{y_i}\\). These intervals are calculated with the residual standard deviation (covered previously):\n\\[\n\\widehat{y_i} - 1.96 \\hat\\sigma, \\ \\widehat{y_i} + 1.96 \\hat\\sigma\n\\]\n\n\n\n\n\n\nExtensions\n\nCategorical Explanatory Variables\nTake an explanatory variable \\(x\\), which has \\(n\\) number of categories \\(1, \\dots, n\\). To include \\(x\\) in our regression, we would create \\(n-1\\) dummy variables, to create the following regression model:\n\\[\nE(y_i|x_i) = \\beta_0 + \\beta_1x_{1i} + \\dots + \\beta_k x_{n-1 \\ i}\n\\]\n\nCategories \\(1, \\dots, n-1\\) get there own binary variable \\(x_1, \\dots, x_{n-1}\\).\nCategory \\(n\\) (the reference category) does not get its own variable. We can change which category we wish to be the reference.\n\nInterpretation is as follows (category \\(j\\) is any one of category \\(1, \\dots, n-1\\)).\n\n\\(\\beta_j\\) is the difference in expected \\(y\\) between category \\(j\\) and the reference category.\n\\(\\beta_0\\) is the expected \\(y\\) of the reference category.\nThus, category \\(j\\) has an expected \\(y\\) of \\(\\beta_0 + \\beta_j\\).\n\n\n\n\n\n\n\nExample of a Categorical Explanatory Variable\n\n\n\n\n\nLet us say that \\(x\\) is the variable development level of a country, with 3 categories: low (L), medium (M), and high (H). \\(y\\) will be the crime rate of the country.\nLet us set low development (L) as our reference category. Our regression will be:\n\\[\nE(y|x) = \\beta_0 + \\beta_1x_M + \\beta_2 x_H\n\\]\nNow let us interpret the coefficients:\n\n\\(\\beta_0\\) is the expected crime rate for a country of low (L) development.\n\\(\\beta_1\\) is the difference in expected crime rate between a medium (M) developed country and a low (L) developed country (since low is the reference category).\n\\(\\beta_2\\) is the difference in expected crime rate between a high (H) developed country and a low (L) developed country (since low is the reference category).\n\nThe expected/predicted \\(y\\) (crime rate) for each category is:\n\nLow (L): \\(\\beta_0\\)\nMedium (M): \\(\\beta_0 + \\beta_1\\)\nHigh (H): \\(\\beta_0 + \\beta_2\\).\n\n\n\n\nEach coefficient \\(\\beta_j\\)’s statistical significance is a difference-in-means significance test, not the significance of the categorical variable as a whole. To find if the entire categorical variable is significant, you should use a F-test.\n\n\n\nFixed Effects\nWhen we have hierarchical or panel data, we need to control for differences between clusters. We essentially include the cluster variable as a categorical variable in our regression.\n\n\n\n\n\n\nHierarchical/Clustered Data\n\n\n\n\n\nHierarchical data is data where the basic units of analysis \\(i\\) are clustered, grouped, or nested into clusters.\nFor example, let us say we are measuring how income affects voter turnout in european countries. We have observations from France, Switzerland, Germany, and many other countries. However, these observations can be grouped by the country they came from.\nWhy is this grouping important? This is because there may be something in common between observations within the same cluster. For example, Switzerland might just have higher voter turnout in general due to something about Swiss institutions or culture.\nThis means that observations aren’t random - i.e. we know that if we select from switzerland, it is likely to have higher turnout - observations from the same country are correlated. Thus, we need some way to account for this clustering of observations. We will explore this below.\n\n\n\n\n\n\n\n\n\nPanel Data\n\n\n\n\n\nPanel data is data that can be clustered in two ways - by unit, and by time. For example, let us say we have a dataset on all countries and their GDP between 1960-2020.\n\nWe will have clusters based on country: Germany will have an observation in 1960, in 1961, …, to 2020. Same for every other country. These observations are grouped by the unit (country in this case).\nWe will also have clusters based on time: We will have all GDP observations for all countries in 1960, in 1961, etc. These observations are grouped by the time (year in this case).\n\n\n\n\nLet us say we have \\(m\\) number of clusters \\(i = 1, \\dots, m\\). Within each cluster, we will have units \\(t = 1, \\dots, n\\). Our cluster fixed effects model will take the form:\n\\[\n\\begin{split}\ny_{it} & = \\alpha_i + \\beta_1x_1 + \\dots + \\beta_kx_k + u_{it} \\\\\n& \\text{where } \\alpha_i = \\beta_{00} + \\underbrace{\\beta_{02}D_{i2} + \\beta_{03}D_{i3} + \\dots + \\beta_{0m}D_{im}}_{\\text{unique intercepts for each cluster}}\n\\end{split}\n\\]\n\nWhere \\(D_{i2}, D_{i3}, \\dots, D_{im}\\) are dummy variables for clusters \\(2, \\dots, m\\). Cluster 1 is the reference category.\n\\(y_{it}\\) indicates the \\(y\\) value of the \\(t\\)th individual in the \\(i\\)th cluster.\n\nFor panel data, we use two-way fixed effects, which is basically just two fixed effects for different clustering. Let us say we have \\(i = 1, \\dots, m\\) units with \\(t = 1, \\dots, n\\) different numbers of time periods. Our two way fixed effects model takes the form:\n\\[\n\\begin{split}\ny_{it} & = \\alpha_i + \\gamma_t + \\beta_1x_1 + \\dots + \\beta_kx_k + u_{it} \\\\\n& \\text{where } \\alpha_i =  \\alpha_{00} + \\underbrace{\\alpha_{02}D_{i2} + \\alpha_{03}D_{i3} + \\dots + \\alpha_{0m}D_{im}}_{\\text{unique intercepts for each unit}} \\\\\n& \\text{where } \\gamma_t =  \\gamma_{00} + \\underbrace{\\gamma_{02}T_{i2} + \\gamma_{03}D_{t3} + \\dots + \\gamma_{0n}T_{in}}_{\\text{unique intercepts for each time}} \\\\\n\\end{split}\n\\]\n\nWhere \\(D_{i2}, D_{i3}, \\dots, D_{im}\\) are dummy variables for units \\(2, \\dots, m\\)., and \\(T_{i2}, T_{i3}, \\dots, T_{in}\\) are dummy variables for time periods \\(2, \\dots, n\\).\n\\(y_{it}\\) indicates the observation of unit \\(i\\) in time period \\(t\\).\n\n\n\n\n\n\n\nIntuitive Explanation of Fixed Effects\n\n\n\n\n\nFor one-way fixed effects, we essentially add a unique intercept term for every cluster, accounting for the average differences in \\(y\\) between each category.\n\n\\(\\beta_{00}\\) is the intercept for the reference category 1.\n\\(\\beta_{00} + \\beta_{0i}\\) is the intercept for the \\(i\\)th category.\n\nFor two-way fixed effects, we add a unique intercept term for every year and country, accounting for the average differences in \\(y\\) between each country, and the average differences in \\(y\\) between each year.\n\n\n\n\n\n\nInteraction Effects\nAn interaction between two variables means they are multiplied in the regression equation:\n\\[\ny_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} + \\beta_3 x_{1i} x_{2i}\n\\]\nInterpretation of the relationship between \\(x_1\\) and \\(y\\) is as follows:\n\n\n\n\n\n\n\n\n\nBinary \\(x_2\\)\nContinuous \\(x_2\\)\n\n\nBinary \\(x_1\\)\nWhen \\(x_2 = 0\\), the effect of \\(x_1\\) (going from 0 to 1) on \\(y\\) is \\(\\widehat{\\beta_1}\\).\nWhen \\(x_2 = 1\\), the effect of \\(x_1\\) (going from 0 to 1) on \\(y\\) is \\(\\widehat{\\beta_1} + \\widehat{ \\beta_3}\\).\nThe effect of \\(x_1\\) (going from 0 to 1) on \\(y\\) is \\(\\widehat{\\beta_1} + \\widehat{\\beta_3} x_2\\).\n\n\nContinuous \\(x_1\\)\nWhen \\(x_2 = 0\\), for every increase in one unit of \\(x_1\\), there is an expected \\(\\widehat{\\beta_1}\\) unit change in \\(y\\).\nWhen \\(x_2 = 1\\), for every increase in one unit of \\(x_1\\), there is an expected \\(\\widehat{\\beta_1}+ \\widehat{\\beta_3}\\) change in \\(y\\).\nFor every increase of one unit in \\(x_1\\), there is an expected \\(\\widehat{\\beta_1} + \\widehat{\\beta_3} x_2\\) change in \\(y\\).\n\n\n\n\n\n\n\n\n\nProof of Interpretations of Interactions\n\n\n\n\n\nWe can solve for the change of \\(x_1\\) on \\(y\\) using a partial derivative of \\(y\\) in respect to \\(x_1\\):\n\\[\n\\begin{split}\n\\frac{\\partial \\widehat{y_i}}{\\partial x_{1i}} & = \\frac{\\partial}{\\partial x_{1i}} \\left[ \\widehat{\\beta_0} + \\widehat{\\beta_1}x_{1i} + \\widehat{\\beta_2}x_{2i} + \\widehat{\\beta_3}x_{1i}x_{2i}\\right] \\\\\n\\frac{\\partial \\widehat{y_i}}{\\partial x_{1i}} & = \\widehat{\\beta_1} + \\widehat{\\beta_3}x_2\n\\end{split}\n\\]\nThis gives us the effect of \\(x_1\\) on \\(y\\).\n\n\n\n\\(\\widehat{\\beta_0}\\) is still the expected \\(y\\) when all explanatory variables equal 0.\nThe coefficient of the interaction \\(\\widehat{\\beta_3}\\), when statistically significant, indicates a statistically significant interaction effect. If it is not statistically significant, then the interaction effect is not statistically significant (and can be dropped).\n\n\n\nPolynomial Transformations\nSometimes the relationship between two variables is not a straight line - we can add more flexibility with polynomials. The most common form of polynomial transformation is the quadratic transformation:\n\\[\ny_i = \\beta_0 + \\beta_1x_{i} + \\beta_2 x_{i}^2 + u_i\n\\]\nOur estimated \\(\\widehat{\\beta_0}\\) remains the expected value of \\(y\\) when all explanatory variables equal 0.\nUnfortunately, the \\(\\widehat{\\beta_1}\\) and \\(\\widehat{\\beta_2}\\) coefficients are not directly interpretable.\n\n\\(\\widehat{\\beta_2}\\)’s sign can tell us if the best-fit parabola opens upward or downward.\nThe significance of \\(\\widehat{\\beta_2}\\) also indicates if the quadratic term is statistically significant. If it is not, we can remove the transformation.\n\nWe can interpret two things about the quadratic transformation:\n\nFor every one unit increase in \\(x\\), there is an expected \\(\\widehat{\\beta_1} + 2 \\widehat{\\beta_2}x\\) unit increase in \\(y\\).\nThe minimum/maximum point in the best-fit parabola occurs at \\(x_i = - \\widehat{\\beta_1}/2 \\widehat{\\beta_2}\\)\n\n\n\n\n\n\n\nProof of Polynomial Interpretations\n\n\n\n\n\nWe can derive the change in \\(y\\) given a one unit increase in \\(x\\) by finding the partial derivative of \\(y\\) in respect to \\(x\\):\n\\[\n\\begin{split}\n\\frac{\\partial \\widehat{y_i}}{\\partial x} & = \\frac{\\partial}{\\partial x} \\left[ \\widehat{\\beta_0} + \\widehat{\\beta_1}x_i + \\widehat{\\beta_2}x_i^2 \\right] \\\\\n\\frac{\\partial \\widehat{y_i}}{\\partial x} & = \\widehat{\\beta_1} + 2 \\widehat{\\beta_2}x_i\n\\end{split}\n\\]\nWe can also solve for the \\(x_i\\) that results in the minimum/maximum of the best-fit parabola by setting the partial derivative equal to 0:\n\\[\n\\begin{split}\n0 & = \\widehat{\\beta_1} + 2 \\widehat{\\beta_2}x_i \\\\\nx_i & = -\\widehat{\\beta_1}/2 \\widehat{\\beta_2}\n\\end{split}\n\\]\n\n\n\nWe can go beyond quadratic - as long as we always include lower degree terms in our model:\n\nCubic: \\(y_i = \\beta_0 + \\beta_1x_{i} + \\beta_2 x_{i}^2 + \\beta_3 x_i^3 + u_i\\)\nQuartic: \\(y_i = \\beta_0 + \\beta_1x_{i} + \\beta_2 x_{i}^2 + \\beta_3 x_i^3 + \\beta_4 x_i^4 + u_i\\)\n\n\n\n\nLogarithmic Transformations\nLogarithmic transformations are often used to change skewed variables into normally distributed variables.\n\n\n\n\n\n\nLogging a Skewed Variable\n\n\n\n\n\nMany monetary variables are heavily skewed. Natural logging these variables can turn them into normal distributions. This is useful, since skewed variables tend to have heteroscedasticity, and by making them normal, we can use the smaller normal standard errors.\nFor example, take this variable called expenses with a significant right skew:\n\n\n\n\n\nIf we take the log of this variable, we get the following distribution that is almost normal:\n\n\n\n\n\n\n\n\nWe have 3 types of logarithmic transformations:\n\n\n\n\n\n\n\n\n\n\\(x\\)\n\\(\\log (x)\\)\n\n\n\\(y\\)\nLinear Model:\n\\(y = \\beta_0 + \\beta_1 x + u\\)\nLinear-Log Model:\n\\(y = \\beta_0 + \\beta_1 \\log x + u\\)\n\n\n\\(\\log (y)\\)\nLog-Linear Model:\n\\(\\log(y) = \\beta_0 + \\beta_1 x + u\\)\nLog-Log Model:\n\\(\\log y = \\beta_0 + \\beta_1 \\log x + u\\)\n\n\n\n\nInterpreting the models:\n\n\n\n\n\n\n\n\n\n\\(x\\)\n\\(\\log (x)\\)\n\n\n\\(y\\)\nLinear Model:\nWhen \\(x\\) increases by one unit, there is an expected \\(\\widehat{\\beta_1}\\) unit change in \\(y\\).\nLinear-Log Model:\nWhen \\(x\\) increases by 10%, there is an expected \\(0.096 \\widehat{\\beta_1}\\) unit change in \\(y\\).\n\n\n\\(\\log (y)\\)\nLog-Linear Model:\nFor every one unit increase in \\(x\\), \\(y\\) is multiplied by \\(e^{\\widehat{\\beta_1}}\\).\nLog-Log Model:\nMultiplying \\(x\\) by \\(e\\) will multiply the expected value of \\(y\\) by \\(e^{\\widehat{\\beta_1}}\\).\n\n\n\n\n\n\n\n\n\nProof of Interpretations for Log Transformations\n\n\n\n\n\nProof of Linear-Log Model:\n\\[\n\\begin{split}\n& E(y_i|x_i = x) = \\beta_0 + \\beta_1 \\log x \\\\\n& E(y_i | x_i = e^A x) = \\beta_0 + \\beta_1 \\log(e^A x) \\\\\n& = \\beta_0 + \\beta_1 (\\log(e^A) + \\log x) \\\\\n& = \\beta_0 + \\beta_1 (A + \\log x) \\\\\n& = \\beta_0 + \\beta_1A + \\beta_1 \\log x\n\\end{split}\n\\]\n\\[\n\\begin{split}\nE(y_i|x_i = \\alpha x) - E(y_i|x_i = x) & = \\beta_0 + \\beta_1 A + \\beta_1 \\log (x) - (\\beta_0 + \\beta_1 \\log x) \\\\\n& = \\beta_1 A\n\\end{split}\n\\]\n\nWhen \\(A = 0.095\\), then \\(e^A = 1.1\\). Thus, a 1.1 times increase of \\(x\\) results in a \\(0.095 \\widehat{\\beta_1}\\) change in \\(y\\).\n\n\nProof of Log-Linear Model:\n\\[\n\\begin{split}\nE(\\log y_i | x_i = x) =  \\log y_i & = \\beta_0 + \\beta_1 x \\\\\ny_i & = e^{\\beta_0 + \\beta_1 x} \\\\\ny_i & = e^{\\beta_0}e^{\\beta_1 x} \\\\\nE(\\log y_i|x_i = x+1) = \\log y_i & = \\beta_0 + \\beta_1(x+1) \\\\\ny_i & = e^{\\beta_0 + \\beta_1 + \\beta_1 x} \\\\\ny_i & = e^{\\beta_0}e^{\\beta_1}e^{\\beta_1x}\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\frac{E(\\log y_i|x_i = x+1)}{E(\\log y_i | x_i = x)} & = \\frac{e^{\\beta_0}e^{\\beta_1}e^{\\beta_1x}}{e^{\\beta_0}e^{\\beta_1x}} \\\\\n& = e^{\\beta_1}\n\\end{split}\n\\]\n\nThus, when \\(x\\) increases by one, there is a multiplicative increase of \\(e^{\\beta_1}\\).\n\n\nProof of Log-Log model:\n\\[\n\\begin{split}\nE(\\log y_i | x_i = x) =  \\log y_i & = \\beta_0 + \\beta_1 \\log x \\\\\ny_i & = e^{\\beta_0 + \\beta_1 \\log x} \\\\\ny_i & = e^{\\beta_0}e^{\\beta_1 \\log x} \\\\\nE(\\log y_i|x_i = ex) = \\log y_i & = \\beta_0 + \\beta_1 \\log (ex) \\\\\ny_i & = e^{\\beta_0 + \\beta_1 \\log e + \\beta_1 \\log x} \\\\\ny_i & = e^{\\beta_0}e^{\\beta_1}e^{\\beta_1 \\log x}\n\\end{split}\n\\]\n\\[\n\\begin{split}\n\\frac{E(\\log y_i|x_i = ex)}{E(\\log y_i | x_i = x)} & = \\frac{e^{\\beta_0}e^{\\beta_1}e^{\\beta_1 \\log x}}{e^{\\beta_0}e^{\\beta_1 \\log x}} \\\\\n& = e^{\\beta_1}\n\\end{split}\n\\]\n\nThus, when \\(x\\) is multiplied by \\(e\\), there is a multiplicative increase of \\(e^{\\beta_1}\\).\n\n\n\n\n\n\n\n\n\n\nImplementation in R\nYou will need package fixest.\n\nlibrary(fixest)\n\nRegression with normal standard errors can be done with the lm() function:\n\nmodel &lt;- lm(y ~ x1 + x2 + x3, data = mydata)\nsummary(model)\n\nRegression with robust standard errors can be done with the feols() function:\n\nmodel &lt;- feols(y ~ x1 + x2 + x3, data = mydata, se = \"hetero\")\nsummary(model)\n\nOutput will include coefficients, standard errors, p-values, and more.\n\n\n\n\n\n\nBinary and Categorical Variables\n\n\n\n\n\nYou can include binary and categorical variables by using the as.factor() function:\n\nfeols(y ~ x1 + as.factor(x2) + x3, data = mydata, se = \"hetero\")\n\nYou can do the same for \\(y\\) or \\(x\\). Just remember, \\(y\\) cannot be a categorical variable (use multinomial logsitic regression instead).\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n\n\n\n\nYou can include one-way fixed effects by adding a | after your regression formula in feols():\n\nmodel &lt;- feols(y ~ x1 + x2 + x3 | cluster,\n               data = mydata, se = \"hetero\")\nsummary(model)\n\nYou can add two-way fixed effects as follows:\n\nmodel &lt;- feols(y ~ x1 + x2 + x3 | unit + year,\n               data = mydata, se = \"hetero\")\nsummary(model)\n\n\n\n\n\n\n\n\n\n\nInteraction Effects\n\n\n\n\n\nTwo interact two variables, use * between them. This will automatically include both the interaction term, and the two variables by themselves.\n\nfeols(y ~ x1 + x2*x3, data = mydata, se = \"hetero\")\n\nIf for some reason, you only want the interaction term, but not the variables by themselves, you can use a colon : between the two variables:\n\nfeols(y ~ x1 + x2:x3, data = mydata, se = \"hetero\")\n\n\n\n\n\n\n\n\n\n\nPolynomial Transformations\n\n\n\n\n\nTo conduct a polynomial transformation, you can use the I() function. The second argument is the degree of the polynomial:\n\nfeols(y ~ x1 + I(x2, 3), data = mydata, se = \"hetero\") #cubic for x2\n\n\n\n\n\n\n\n\n\n\nLogarithmic Transformations\n\n\n\n\n\nThe best way to do a logarithmic transformation is to create a new variable that is the log of the variable you want to transform using the log() function, before you even start the regression:\n\nmydata$x1_log &lt;- log(mydata$x1)\n\n\n\n\n\n\n\n\n\n\nConfidence Intervals\n\n\n\n\n\nTo find the confidence intervals for coefficients, first estimate the model with lm() or feols() as shown previously, then use the confint() command:\n\nconfint(model)\n\n\n\n\n\n\n\n\n\n\nF-Tests\n\n\n\n\n\nTo run a f-test, use the anova() command, and input your two different models, with the null model going first.\n\nanova(model1, model2)\n\nNote: F-tests only work with models that are run with homoscedastic standard errors. Robust standard errors will not work.\n\n\n\n\n\n\n\n\n\nLaTeX Regression Tables\n\n\n\n\n\nYou can use the texreg package to make nice regression tables automatically.\n\nlibrary(texreg)\n\nThe syntax for texreg() is as follows:\n\ntexreg(l = list(model1, model2, model3),\n       custom.model.names = c(\"model 1\", \"model 2\", \"model 3\"),\n       custom.coef.names = c(\"intercept\", \"x1\", \"x2\"),\n       digits = 3)\n\nYou can replace texreg() with screenreg() if you want a nicer regression table in the R-console.\nNote: you must have the same amount of model names as total models in your texreg, and you must have the same amount of coeficient names as the total amount of coefficients in all of your models.\n\n\n\n\n\n\n\n\n\nPrediction\n\n\n\n\n\nWe can use the predict() function to generate fitted value predictions in R:\n\nmy_predictions &lt;- predict(model, newdata = my_new_data)\n\nmy_new_data is a dataframe with a bunch of explanatory variable values (for every explanatory variable) for a collection of observations, that you wish to predict \\(\\hat y\\) for.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "2 Multiple Linear Regression"
    ]
  },
  {
    "objectID": "quant6.html",
    "href": "quant6.html",
    "title": "Selection on Observables",
    "section": "",
    "text": "In the last chapter, we discussed randomisation. Randomisation is great, but, it requires specific circumstances of the research having control over the assignment mechanism. However, in the social sciences, this rarely occurs.\nThis chapter introduces the selection on observables framework, which allows us to identify causal effects in an observational setting by controlling for observable pre-treatment covariates. We discuss the main estimators, including regression, matching, and weighting.\nUse the right sidebar for quick navigation. R-code provided at the bottom.\n\n\nIdentification\n\nBlocking Backdoor Paths\nWithout randomisation, we need some other way to account for pre-treatment covariates that may be confounding and causing selection bias. Controlling for a set of nodes/confounders \\(X\\) can identify the causal effect of \\(D \\rightarrow Y\\), if:\n\nNo node within set \\(X\\) is a descendant of \\(D\\) (no element within \\(X\\) results from \\(D\\)).\nThe nodes within set \\(X\\) block all back-door paths from \\(D \\rightarrow Y\\).\n\n\n\n\n\n\nIn the figure above, let us block the backdoor paths between \\(D \\rightarrow Y\\):\n\nBackdoor path \\(D \\rightarrow X \\rightarrow Y\\). To block this path, we must control for \\(X\\).\nBackdoor path \\(D \\rightarrow V \\rightarrow Y\\). We do not need to control for \\(V\\), since it is post-treatment (a descendant of \\(D\\)). In fact, \\(V\\) is a bad control (see below).\n\nThus, to identify \\(D \\rightarrow Y\\) here, we only need to control for \\(X\\), and no other variable.\n\n\n\n\n\n\nGood and Bad Controls\n\n\n\n\n\nGood controls block backdoor paths, which facilitate identification of the causal effect.\nBad controls are when we control for post-treatment variables. For example, \\(P\\) below is a bad control, since it is caused by \\(D\\), so it is post-treatment.\n\n\n\n\n\nYou also never want to control variables that only predict \\(D\\). These are bad because controlling for these removes variation in \\(D\\) that could be useful.\nNeutral controls are ones that don’t identify the causal effect, but improve efficiency. For example, \\(Q\\) below affects \\(Y\\), but there is no backdoor path. Controlling \\(Q\\) will not help identification, but can control noise in \\(Y\\) which may increase efficiency.\n\n\n\n\n\n\n\n\n\n\n\nIdentification Assumptions\nOnce we have determined the set of confounders \\(X\\) that we need to control to block all backdoor paths, the assumptions needed for identification of causal effects are:\n\nConditional Ignorability (also known as exogeneity or independence): Among units with identical confounder values \\(X_i\\), treatment \\(D_i\\) is as-if randomly assigned. Or in other words, potential outcomes are independent from treatment within each specific confounder value \\(X_i = x\\).\n\n\\[\n(Y_{0i}, Y_{1i}) \\perp\\!\\!\\!\\!\\perp D_i  \\ | \\ X_i = x, \\quad \\forall \\ x \\in \\mathcal X\n\\]\nThis implies that for any given value of all confounders \\(X_i = x\\), we know that potential outcomes \\(Y_{di}\\) are equivalent between treatment and control:\n\\[\n\\begin{split}\nE(Y_{1i}|X_i = x) = E(Y_{1i}|D_i = 1, X_i = x) = E(Y_{1i}|D_i = 0, X_i = x) \\\\\nE(Y_{0i}|X_i = x) = E(Y_{0i}|D_i = 1, X_i = x) = E(Y_{0i}|D_i = 0, X_i = x)\n\\end{split}\n\\tag{1}\\]\n\nCommon Support: for any unit \\(i\\) with value of \\(X_i\\), there is a non-zero probability that they could be assigned to both control \\(D_i = 0\\) or treatment \\(D_i = 1\\).\n\n\\[\n0 &lt; Pr(D_i = 1 \\ | X_i = x) &lt; 1 \\quad \\forall \\ x \\in \\mathcal X\n\\]\n\n\n\n\n\n\nExample of Identification Assumptions\n\n\n\n\n\nImagine we have a theory that being abducted \\(D\\) causes turning out to vote.\nBlattman (2009) finds that age is the primary way violent groups chose to abduct individuals: abduction parties released young children and older adults, but kept all adolescent and young males.\nThat means our theory is that age \\(X\\) affects selection into treatment \\(D\\). Young children and older adults are less likely to get abducted \\(D\\), while adolescent and young males are more likely \\(D\\).\n\n\n\n\n\n\nIdentification of the ATE\nWith our assumptions above, we can identify the ATE. We start with the conditional average treatment effect, conditional on some value of confounders \\(X_i = x\\). Note the properties shown in Equation 1 .\n\\[\n\\begin{align}\n\\tau_{CATE}(x) & = E(Y_{1i} - Y_{0i} \\ | \\ X_i = x) \\\\\n& = E(Y_{1i}|X_i = x) - E(Y_{0i}|X_i = x) && (\\text{property of expectation}) \\\\\n& = E(Y_{1i}|D_i = 1, X_i = x) - E(Y_{0i}|D_i = 0X_i = x) &&( \\because \\text{equation (1)} \\ ) \\\\\n& = \\underbrace{E(Y_i|D_i = 1, X_i = x)}_{\\because \\text{ observable}} - \\underbrace{E(Y_i|D_i = 0, X_i = x)}_{\\because \\text{ observable}}\n\\end{align}\n\\tag{2}\\]\nNow, let us discuss the ATE, and plug in the CATE from Equation 2 to identify it:\n\\[\n\\begin{align}\n\\tau_{ATE} & = E(Y_{1i} - Y_{0i}) \\\\\n& = \\int \\underbrace{E(Y_{1i} - Y_{0i} \\ | \\ X_i = x)}_{\\tau_{CATE}(x)} d \\ \\underbrace{Pr(X_i = x)}_{\\text{weight}} && (\\text{weighted average})\\\\\n& = \\int(\\underbrace{E(Y_i|D_i = 1, X_i) - E(Y_i|D_i = 0, X_i)}_{\\because \\text{ equation (2)}})d \\ Pr(X_i = x)\n\\end{align}\n\\tag{3}\\]\nThus \\(\\tau_{ATE}\\) is identified as the weighted average of all the CATEs, who themselves are difference-in-means of the observed \\(Y_i\\) at every possible value of \\(X_i = x\\).\nWe assumed that the pre-treatment covariate \\(X\\) is continuous. This is why we need an integral. However, we can simplify this if \\(X\\) is discrete:\n\\[\n\\tau_{ATE} = \\sum\\limits_{x \\in \\mathcal X} ( E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)) Pr(X_i = x)\n\\tag{4}\\]\n\n\n\nIdentification of the ATT\nWe can weaken conditional ignorability, and still identify the ATT. Only \\(Y_{0i}\\) needs to be independent of \\(D_i\\) for units with the same covariates \\(X_i\\). Or in other words, \\((Y_{0i}) \\perp\\!\\!\\!\\perp D_i | X_i = x\\). This implies:\n\\[\nE(Y_{0i}|X_i = x) = E(Y_{0i}|D_i = 0, X_i = x) = E(Y_{0i}|D_i = 1, X_i = x)\n\\tag{5}\\]\nStart with the conditional ATT, using weakened conditional ignorability from Equation 5 :\n\\[\n\\begin{split}\n\\tau_{CATT}(x) & = E(Y_{1i}-Y_{0i}|D_i = 1, X_i = x) \\\\\n& = E(Y_{1i}|D_i = 1, X_i = x) - E(Y_{0i}|D_i = 1, X_i = x) \\\\\n& = E(Y_{1i}|D_i = 1, X_i = x) - \\underbrace{E(Y_{0i}|D_i = 0, X_i = x)}_{\\because \\text{ equation (5)}} \\\\\n& = \\underbrace{E(Y_i|D_i=1, X_i = x)}_{\\because \\text{ observable}} - \\underbrace{E(Y_1|D_i = 0, X_i x)}_{\\because \\text{ observable}}\n\\end{split}\n\\tag{6}\\]\nNow, look at the ATT, and plug in CATT from Equation 6 to identify it.\n\\[\n\\begin{align}\n\\tau_{ATT} & = E(Y_{1i} - Y_{0i}|D_i = 1) \\\\\n& = \\int \\underbrace{E(Y_{1i} - Y_{0i}|D_i = 1, X_i = x)}_{\\tau_{CATT}(x)}d \\ \\underbrace{Pr(X_i = x|D_i = 1)}_{Pr(X_i = x) \\text{ within treated}} \\\\\n& = \\int (\\underbrace{E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)}_{\\because \\text{ equation (6)}})d \\ Pr(X_i = x|D_i = 1)\n\\end{align}\n\\]\nWe can simplify this if \\(X\\) is discrete:\n\\[\n\\tau_{ATT} = \\sum\\limits_{x \\in \\mathcal X} ( E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)) Pr(X_i = x | D_i = 1)\n\\]\nEven when all assumptions are met for identification of the ATE, the \\(\\tau_{ATE}\\) can be different than the \\(\\tau_{ATT}\\). This is because the weights \\(Pr(X_i = x|D_i = 1)\\) for the ATT are different than the ATE \\(Pr(X_i = x)\\).\n\n\n\n\n\n\nParametric Estimators\n\nOrdinary Least Squares Estimator\nOLS is a natural approach for controlling for confounders \\(X\\), since \\(\\hat\\beta_{OLS}\\) estimates partial out the effects of covariates. OLS is a good estimator of \\(\\tau_{ATE}\\) under 2 conditions:\n\nConstant treatment effect: \\(\\tau_i = Y_{1i} - Y_{0i}\\) for all units \\(i\\).\nLinearity: Potential outcomes are linear, and can be written as:\n\n\\[\nY_i(d) = \\beta_0 + d\\beta_1 + \\mathbf X_i \\gamma + \\epsilon_i \\quad \\text{for} \\quad d = 0, 1\n\\]\nWhy these conditions? Suppose we have the above linear potential outcomes. We can show:\n\\[\n\\begin{align}\n\\tau_i & = Y_{1i} - Y_{0i} && (\\text{definition of } \\tau_i) \\\\\n& = (\\beta_0 + (1)\\beta_1 + \\mathbf X_i \\gamma + \\epsilon_i) - (\\beta_0 + (0)\\beta_1 + \\mathbf X_i \\gamma + \\epsilon_i) && (\\text{plug in } Y_i(1), Y_i(0) \\ )\\\\\n& = (\\beta_0 + \\beta_1 + \\mathbf X_i \\gamma + \\epsilon_i) - (\\beta_0 + \\mathbf X_i \\gamma + \\epsilon_i) && (\\text{multiply}) \\\\\n& = \\beta_0 + \\beta_1 + \\mathbf X_i \\gamma + \\epsilon_i - \\beta_0 - \\mathbf X_i\\gamma - \\epsilon_i && (\\text{distribute negative sign})\\\\\n& = \\beta_1 && (\\text{cancel out terms})\n\\end{align}\n\\]\nWe also know that conditional ignorability implies zero-conditional mean. Thus \\(\\beta_1\\) is an unbiased and asymptotically consistent estimator of the ATE.\nYou should be cautious using OLS when assumption 2, linearity, is violated. OLS is the best linear estimator, but how far your data is from linearity will determine if the estimator is useful.\n\n\n\n\n\n\nNon-Linearity\n\n\n\n\n\nWhat if potential outcomes \\(Y_i(d)\\) is an unknown and non-linear function of \\(d\\) and \\(X_i\\).\nWe know the OLS is the best linear predictor of the conditional expectation function in terms of mean squared error. Thus, \\(\\beta_1\\) will provide the best linear approximation to the population regression function.\nThis does not mean it is good - just the best linear approximation.\n\n\n\nYou should not use OLS if you believe assumption 1, heterogeneity, is violated. The reasoning is explained below.\n\n\n\nOLS Bias under Heterogeneity\nWhat if there are heterogenous treatment effects (where \\(\\tau_i\\) is different between units)? Standard OLS in this case is no longer an unbiased estimator of the ATE.\nRecall the discrete identification of the ATE (in equation Equation 4 ) is a weighted average of CATEs:\n\\[\n\\hat\\tau_{ATE} = \\sum\\limits_{x \\in \\mathcal X} ( \\hat\\tau_{CATE}(x)) \\underbrace{Pr(X_i = x)}_{\\text{weight}} \\\\\n\\]\nOLS, when there are non constant treatment effects, can also be rewritten as a weighted average of CATEs:\n\\[\n\\hat\\beta_{OLS} = \\sum\\limits_{x \\in \\mathcal X} ( \\hat\\tau_{CATE}(x)) \\underbrace{ \\frac{Var(D_i|X_i = X)Pr(X_i = x)}{\\sum Var(D_i | X_i = x')Pr(X_i = x')} }_{\\text{weight}} \\\\\n\\]\nNotice how the weights are different. The weights in the OLS are the conditional variances of \\(D_i\\). This means that OLS is not an unbiased estimator of the ATE or ATT, but rather, a weighted average of the ATT and ATU.\nOLS, under heterogeneity, actually provides an unbiased estimator of the conditional variance weighted average treatment effect. This is not the same as the ATE or the ATT.\n\n\n\n\n\n\nConditional Variance Weighted Average Treatment Effect (CVW-ATE)\n\n\n\n\n\nThis estimand can also be described as a weighted average of the ATT (average treatment effect on the treated) and the ATU (average treatment effect on the untreated):\n\\[\n\\tau_{OLS} = w_1 \\cdot \\tau_{ATT} + w_0 \\cdot \\tau_{ATU}\n\\]\nWhere:\n\\[\n\\begin{split}\nw_1 & = \\frac{(1 - Pr(D=1)) Var(\\pi(X)|D = 0)}{Pr(D=1)Var(\\pi(X)|D=1) + (1-Pr(D=1)Var(\\pi(X)|D=0)} \\\\\nw_0 & = 1 - w_1\n\\end{split}\n\\]\nThe reason for this is because regression is prone to extrapolation beyond common support - i.e. it can “estimate” potential outcomes for units that are not observed. This can lead to bias.\nThis is in contrast to the subclassification estimator, which cannot be computed if there are missing observable outcomes for a substratum/category of \\(X\\).\nThe weights of \\(D_i(X_i = x)\\) can also be seen as propensity scores of \\(\\pi(x)(1 - \\pi(x)\\). Therefore:\n\nWeights are higher for groups with propensity scores close to 0.5.\nWeights are low for groups with propensity scores close to 0 or 1.\nOLS minimises estimation uncertainty by downweighting groups of \\(X_i\\) where group-specific ATEs are less precisely estimated.\n\n\n\n\n\n\n\nFully Interacted Estimator\nThe Fully-Interacted Estimator, a newly developed large-sample regression estimator (Lin 2013), solves the heterogeneity bias in the OLS estimator. The fully-interacted estimator takes the form:\n\\[\n\\widehat{Y_i} = \\hat\\alpha + D_i \\widehat{\\tau}_{int} + (\\mathbf X_i - \\mathbf {\\bar X}) \\hat\\beta +D_i (\\mathbf X_i - \\mathbf{\\bar X}) \\hat\\gamma\n\\]\n\nWhere \\(X_i\\) are covariate values sufficient to satisfy conditional independence.\nWhere \\(\\bar X\\) contains the sample means of all \\(X_i\\) covariates.\n\nThis estimator \\(\\hat\\tau_{int}\\) is technically biased when estimating \\(\\tau_{ATE}\\). However, the bias is arbitrarily small in large samples under conditional ignorability.\nThis estimator thus allows us to accurately estimate the ATE even under heterogenous treatment effects, assuming our sample size is sufficiently large.\n\n\n\n\n\n\nOther Solutions to the OLS Bias under Heterogeneity\n\n\n\n\n\nThere are a few other solutions to this issue of OLS bias under heterogeneity:\n\nDoubly-robust estimation uses a weighted average of regression and IPW estimators, which will be asymptotically consistent as long as the regression model is correctly specified.\nMatching as pre-processing uses matching to make treatment and control groups similar, then runs regression models to estimate causal effects.\n\n\n\n\n\n\n\n\n\n\nNonparametric Estimators\n\nSubclassification Estimator\nUsing the discrete identification of the ATE shown in Equation 4 , we can instead use the sample equivalents to get the subclassification estimator:\n\\[\n\\hat\\tau_{ATE} = \\sum\\limits_{j=1}^M \\underbrace{(\\bar Y_{1j} - \\bar Y_{0j})}_{\\tau_{CATE}(j)} \\underbrace{\\frac{n_j}{n}}_{\\text{weight}}\n\\]\n\nWhere \\(M\\) is the number of levels/categories of \\(X\\), and \\(j\\) is one specific level/category of \\(X\\).\nWhere \\(n_j\\) is the number of units in a level/category \\(j\\) of \\(X\\).\nWhere \\(\\bar Y_{dj}\\) is the mean outcome for units with \\(D_i = d\\) in level/category \\(j\\) of \\(X\\).\n\nFor subclassificaion to be possible, within each level \\(j\\) of covariate \\(X\\), there must be at least one unit in control \\(D=0\\) and treatment \\(D=1\\).\n\n\n\n\n\n\nIntuitive Procedure of Subclassification\n\n\n\n\n\nMore intuitively, the procedure is as follows:\n\nChoose one specific value for all covaraites \\(X\\). Find the average treatment effect within this specific value of \\(X\\).\nMultiply that average treatment effect by the number of observations that meet this specific value of \\(X\\) divided by the total number of units.\nDo this for every possible values of all covaraites \\(X\\), then sum up all the weighted average treatment effects to get the overall ATE.\n\n\n\n\n\n\n\n\n\n\nSubclassification with Multiple Confounders\n\n\n\n\n\nLet us say we have 2 confounders, \\(X_1\\) and \\(X_2\\). Both confounders are categorical with 3 categories.\nWe would need to create \\(M=9\\) levels of strata, for every possible combination of values of \\(X_1\\) and \\(X_2\\). Then, we would estimate the within-strata average treatment effect, and weight them.\nThis illustrates how with large amounts of confounders, you will need a huge number of stratum. This makes subclassification infeasible in many cases.\n\n\n\n\n\n\n\n\n\nSubclassification for the ATT\n\n\n\n\n\nWhen pre-treatment covariate \\(X\\) is discrete, the identification result of the ATT is:\n\\[\n\\tau_{ATT} = \\sum\\limits_{x \\in \\mathcal X} ( E(Y_i|D_i = 1, X_i = x) - E(Y_i|D_i = 0, X_i = x)) Pr(X_i = x | D_i = 1)\n\\]\nWe can calculate this within our give sample to get the subclassificaiton estimator:\n\\[\n\\hat\\tau_{ATT} = \\sum\\limits_{j=1}^M(\\bar Y_{1j} - \\bar Y_{0j}) \\frac{n_{1j}}{n_1}\n\\]\n\nWhere \\(M\\) is the number of strata (levels/categories of \\(X\\)).\nWhere \\(n_j\\) is the number of units in a level/category \\(j\\) of \\(X\\).\nWhere \\(n_{1j}\\) is the number of treated cells \\(D = 1\\) in a level/category \\(j\\) of \\(X\\).\nWhere \\(\\bar Y_{dj}\\) is the mean outcome for units with \\(D_i = d\\) in level/category \\(j\\) of \\(X\\).\n\n\n\n\n\n\n\nMatching Estimator\nWe have a missing data problem in causal inference: we do not know all the potential outcomes. Matching “estimates” missing potential outcomes of a unit.\nFor each observation in the treated group, matching finds an observation in the untreated group that have the most similar values of a set of pre-treatment covariates \\(X\\). Thus, we have pairs of treatment-control observations that act as counterfactuals. We can estimate the ATT as the average difference in observed outcomes within the pairs:\n\\[\n\\hat\\tau_{ATT} = \\frac{1}{n_1} \\sum\\limits_{i:D_i = 1}(Y_i - \\widetilde{Y_i})\n\\]\n\nWhere \\(n_1\\) is the number of units in the treatment group.\nWhere \\(Y_i\\) is the unit’s observed \\(Y\\) in the treatment group.\nWhere \\(\\tilde Y_i\\) is unit \\(i\\)’s closest neighbour in the untreated group.\n\n\n\n\n\n\n\nChoices during Matching\n\n\n\n\n\nWe have to make several choices when conducting matching.\n\nWhat covariates to match on. We generally want to select a set of pre-treatment covariates \\(X\\) such that these covariates ensure the conditional ignorability assumption is met.\nMatch with or without replacement. Matching with replacement means that once you have used one control unit to match to a treatment unit, you can still use that same control unit to match to another treatment unit. This has advantages since you can ensure better and closer matches. However, matching without replacement is also possible.\nHow many to match. You can decide to match multiple control units to one treatment unit, and use the average of the treatment units to approximate a true control unit. This may result in more accurate matches for treatment units that may not have a good single control unit to match to.\n\nWe can also choose to use more advanced matching methods, such as Mahalanobis Distance matching or Propensity Score matching, which are shown below. These are good for matching on more \\(X\\).\n\n\n\n\n\n\n\n\n\nMatching on Multiple Neighbours\n\n\n\n\n\nSometimes, a treatment unit may not have one close control unit to match to. Instead, we could use a combination of control units to match to the treatment unit, and use the average \\(Y\\) of those combination of control units to approximate a more accurate match.\nSuppose we use \\(M_i\\) number of close control units to match to a treatment unit \\(i\\). Then, the matching estimator would be defined as follows:\n\\[\n\\hat\\tau_{ATT} = \\frac{1}{n_1} \\sum\\limits_{i:D_i = 1}(Y_i - \\left(\\frac{1}{M_i} \\sum\\limits_{m=1}^{M_i} \\widetilde{Y_{i_m}}\\right))\n\\]\nWhere \\(\\widetilde{Y_{i_m}}\\) is the obsered outcome for the \\(m\\)th match of unit \\(i\\).\n\n\n\n\n\n\n\n\n\nWeaknesses of Matching\n\n\n\n\n\nMatching does not always create “perfect” matches. This means that the pairs matched together may not be identical to each other in terms of covariates \\(X\\) or potential outcomes.\nThe inability to find exact matches can cause bias, especially for the more covariates we match on (see below).\n\n\n\n\n\n\nMatching with Multiple Covariates\nConsider that we \\(k&gt;1\\) number of confounders \\(X\\). Now, we have to match observations in \\(k\\) variables, which implies we are in a multidimensional \\(\\mathbb R^k\\) space.\nThe most commonly used distance metric is Mahalanobis Distance - which measures the distance in \\(X_i\\) between units \\(i\\) and \\(j\\):\n\\[\nD_M (\\mathbf X_i, \\mathbf X_j) = \\sqrt{(\\mathbf X_i - \\mathbf X_j)^T \\boldsymbol\\Sigma_X^{-1} (\\mathbf X_i - \\mathbf X_j)}\n\\]\n\nWhere \\(\\boldsymbol \\Sigma_X\\) is the sample variance-covariance matrix of \\(\\mathbf X_i\\).\n\n\n\n\n\n\n\nEuclidean Distance\n\n\n\n\n\nEuclidean distance is another common distance metric:\n\\[\nD_E ( \\mathbf X_i, \\mathbf X_j) = \\sqrt{(x_{1i} - x_{1j})^2 + (x_{2i}-x_{2j})^2 + \\dots + (x_{ki}-x_{kj})^2}\n\\]\nEuclidean distance, while very simple, is not recommended. This is because Euclidean distance with non-normalised variables can get you very bizarre results that depend on the scale of the variables.\nThere are other distance metrics, but these are exceedingly rare in selection on observables.\n\n\n\nThere is one issue with matching in multi-dimensional spaces. It becomes very difficult to match every unit \\(i\\) on every covariate \\(X\\), even if we have a large number of observations.\n\n\n\n\n\n\nCurse of Dimensionality\n\n\n\n\n\nWhen we try to match on more than one \\(X\\) variable, we go from matching on a number line \\(\\mathbb R^1\\) to a \\(n\\)-dimensional space, \\(\\mathbb R^n\\).\nThe search space increases exponentially as you increase the number of dimensions.\n\nTake a look at the figure on the left. If we only match on a one dimensional plane (lets say the horizontal line between 0 and 1), we can see our red line covers approximately 30% of the horizontal line. But in 3 dimensions, our red box covers a significantly less proportion of the entire cube.\nThe figure on the right illustrates this. \\(d\\) represents the dimensions. We can see as the dimensions increase, the fraction of volume increases significantly slower relative to distance.\nThus, with a bigger space, the distance between two units increases, so you get worse matches.\n\n\n\nThis curse of dimensionality creates a bias problem - since we get non-exact matches. The more dimensions you add, the worse it becomes.\n\n\n\n\n\n\nMore on Bias\n\n\n\n\n\nThe poor matches caused by increased dimensionality inject error into our estimates of missing potential outcomes.\nThe bias term as you increase the number of dimensions \\(k\\), changes by \\(N^{(-1/k)}\\). This implies no \\(\\sqrt{n}\\) consistency for \\(k&gt;2\\).\nIf \\(N_0\\) (number of untreated units) is much larger than \\(N_1\\) (number of treated units), bias will typically be smaller.\nThere are ways to correct this bias, including Abadie and Imbens (2011) Bias Correction method.\nThere is a new method: Bias-corrected matching, which estimates bias ineherent to mathching estimators via regression, then subtracts it from the matching estimate to correct for it.\n\n\n\n\n\n\nPropensity Scores Matching\nPropensity Score matching is an alternative way to match over many dimensions. The propensity score is an unobserved property, defined as the probability of a unit \\(i\\) of receiving treatment:\n\\[\n\\pi(X_i) \\equiv Pr(D_i = 1|X_i)\n\\]\nWhen supposing the conditional ignorability and common support assumptions, the propensity score \\(\\pi(X_i)\\) has the balancing property: \\(D_i \\perp X_i \\ | \\ \\pi(X_i)\\). This implies that conditional ignorability holds on the propensity scores alone:\n\\[\n(Y_{1i}, Y_{0i}) \\perp\\!\\!\\!\\!\\perp D_i \\ | \\ \\pi(X_i)\n\\]\nThus, instead of conditioning on \\(X_i\\) as we did in selection on observables, we can instead condition on \\(\\pi (X_i)\\), and still identify the causal estimand.\nHowever, we do not actually observe \\(\\pi (X_i)\\). We estimate \\(\\pi (X_i)\\) with a binary response model, with outcome variable \\(D_i\\), and explanatory variables \\(X_i\\). This will get us a fitted probability \\(Pr(D_i = 1) = \\hat\\pi(X_i)\\).\nThen, once we have the propensity score estimates \\(\\hat\\pi(X_i)\\), we can do nearest neighbour matching with the propensity scores (in \\(\\mathbb R^1\\)). This will allow us to identify the \\(\\tau_{ATT}\\).\n\n\n\n\n\n\nBalance Checks\n\n\n\n\n\nThe accurate estimation of \\(\\tau_{ATT}\\) implies an accurate prediction of the propensity scores \\(\\pi(X_i)\\). We can test our matched treatment and control groups to see if the balancing property holds for covariates \\(X_i\\).\n\n\n\n\n\n\nGenetic Matching\n\n\n\n\n\n\nWeighting Estimator\n\nIdentification with Weighting\nWe know that the ATE can be written as a weighted average, as shown in Equation 4 . We can rewrite the \\(\\tau_{ATE}\\) as follows using observed potential outcomes outcomes and conditional ignorability ( Equation 1 ).\n\\[\n\\begin{split}\n& = \\sum\\limits_{x \\in \\mathcal X} \\underbrace{(E(Y_{1i}|D_i = 1, X_i = x)}_{\\because \\text{ observed}} - \\underbrace{E(Y_{0i}|D_i = 0, X_i = x)}_{\\because \\text{ observed}})Pr(X_i = x) \\\\\n& = \\sum\\limits_{x \\in \\mathcal X}  (\\underbrace{E(Y_{1i}|X_i = x)}_{\\because \\text{ eq. (1)}} - \\underbrace{E(Y_{0i}|X_i = x)}_{\\because \\text{ eq. (1)}})Pr(X_i = x) \\\\\n& = \\underbrace{E[E(Y_{1i}|X_i = x) - E(Y_{0i}|X_i = x)]}_{\\text{definition of weighted average}}\n\\end{split}\n\\]\nLet us do an algebra trick - multiply both terms within the CATE by 1 (in blue):\n\\[\n\\begin{split}\n& = E \\left [E(Y_{1i}|X_i=x) \\color{blue}{\\frac{\\pi(X_i)}{\\pi(X_i)}}\\color{black} - (E(Y_{0i}|X_i=x) \\color{blue}{\\frac{1-\\pi(X_i)}{1-\\pi(X_i)}} \\right] \\\\\n& \\color{black} = E \\left[ \\frac{E(Y_{1i}|X_i = x) \\pi(X_i)}{\\pi(X_i)} -  \\frac{E(Y_{0i}|X_i = x) (1-\\pi(X_i))}{1-\\pi(X_i)} \\right]\n\\end{split}\n\\]\nWe know that propensity score \\(\\pi(X_i) := E(D_i|X_i = x)\\). Thus, we can convert the above to:\n\\[\n\\begin{split}\n& = E \\left[ \\frac{E(Y_{1i}|X_i = x)E(D_i|X_i = x)}{\\pi(X_i)} - \\frac{E(Y_{0i}|X_i = x)(1-E(D_i|X_i = x))}{1-\\pi(X_i)}\\right] \\\\\n& = E \\left[ \\frac{E(Y_{1i}|X_i = x)E(D_i|X_i = x)}{\\pi(X_i)} - \\frac{E(Y_{0i}|X_i = x)E(1-D_i|X_i = x)}{1-\\pi(X_i)}\\right]\n\\end{split}\n\\]\n\\[\n\\begin{align}\n& = E \\left[ \\frac{E(Y_{1i}D_i|X_i = x)}{\\pi(X_i)} - \\frac{E(Y_{0i}(1-D_i)|X_i = x)}{1 - \\pi(X_i)}\\right] && (\\text{property of expectation})\\\\\n& = E \\left[ E \\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} |X_i = x\\right) - E \\left( \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)} | X_i = x \\right) \\right] && (\\text{property of expectation})\\\\\n& = E\\left[ E\\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} - \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)} |X_i = x \\right) \\right]  && (\\text{property of expectation})\n\\end{align}\n\\]\n\\[\n\\begin{align}\n& = E\\left( \\frac{Y_{1i}D_i}{\\pi(X_i)} - \\frac{Y_{0i}(1-D_i)}{1-\\pi(X_i)}\\right) && (\\text{LIE: } E(X) = E[E(X|Y)] \\ ) \\\\\n& = E\\left( \\frac{Y_{i}D_i}{\\pi(X_i)} - \\frac{Y_{i}(1-D_i)}{1-\\pi(X_i)}\\right) && (\\text{observered outcome}) \\\\\n& = E \\left( \\frac{\\color{blue}{Y_i} \\color{black}D_i(1-\\pi(X_i))-\\color{blue}{Y_i}\\color{black}(1-D_i)\\pi(X_i)}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{getting common denom.}) \\\\\n& = E\\left( Y_i \\frac{D_i(1-\\pi(X_i))-(1-D_i)\\pi(X_i)}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{factor out }Y_i) \\\\\n& = E\\left( Y_i \\frac{D_i - D_i\\pi(X_i)-(\\pi(X_i) -D_i\\pi(X_i))}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{distribute out}) \\\\\n&  = E\\left( Y_i \\frac{D_i \\color{blue}{- D_i\\pi(X_i)} \\color{black}-\\pi(X_i) \\color{blue}{+ D_i\\pi(X_i)}}{\\pi(X_i)(1-\\pi(X_i))}\\right) &&(\\text{distribute out negative})\\\\\n& = E\\left( Y_i \\frac{D_i -\\pi(X_i) }{\\pi(X_i)(1-\\pi(X_i))}\\right) && (\\text{cancel out})\\\\\n\\end{align}\n\\tag{7}\\]\nAnd thus, we have identified the ATE.\n\n\n\nInverse Probability Weighting Estimator\nAn alternative use of propensity scores is weighting. As shown above, under conditional ignorability and common support, we can identify the ATE as:\n\\[\n\\tau_{ATE} = E\\left[ Y_i \\times \\underbrace{\\frac{D_i - \\pi(X_i)}{\\pi(X_i) (1 - \\pi(X_i))}}_{\\text{weight}}\\right]\n\\]\nThe inverse probability weighting (IPW) estimator is the sample estimator:\n\\[\n\\begin{split}\n\\hat\\tau_{ATE} & = \\frac{1}{N} \\sum\\limits_{i=1}^N \\left(Y_i \\frac{D_i - \\hat\\pi(X_i)}{\\hat\\pi(X_i) (1 - \\hat\\pi(X_i))} \\right) \\\\\n& = \\frac{1}{N} \\sum\\limits_{i=1}^N \\left(\\frac{D_i Y_i}{\\hat\\pi(X_i)} - \\frac{(1-D_i) Y_i}{1 - \\hat\\pi(X_i)} \\right)\n\\end{split}\n\\]\n\nThe second equation is equivalent to the first, shown by Equation 7 .\n\nEssentially, those who are unlikely to be treated but do get treated get weighted more, and individuals who are likely to be treated but do not get treated get weighted more.\n\n\n\n\n\n\nWeighting Estimator for ATT\n\n\n\n\n\nThe identification of the ATT under both conditional ignorability and common support are:\n\\[\n\\tau_{ATT} = \\frac{1}{Pr(D = 1)} \\times E\\left[ Y_i \\times \\underbrace{\\frac{D_i - \\pi(X_i)}{(1 - \\pi(X_i))}}_{\\text{weight}}\\right]\n\\]\nThe sample IPW estimator would be:\n\\[\n\\begin{split}\n\\hat\\tau_{ATT} & = \\frac{1}{N_1}\\sum\\limits_{i=1}^N \\left( Y_i \\frac{D_i - \\hat\\pi(X_i)}{1 - \\hat\\pi(X_i)} \\right) \\\\\n& = \\frac{1}{N_1} \\sum\\limits_{i=1}^N \\left( D_iY_i - (1-D_i)Y_i \\frac{\\hat\\pi(X_i)}{1 - \\hat\\pi(X_i)} \\right)\n\\end{split}\n\\]\n\n\n\nThe IPW estimator is asymptotically consistent, but has very poor small sample properties. They are highly sensitive to extreme values of \\(\\hat\\pi(X_i)\\). This generates high variance (inefficiency), and can produce significant bias under model mispecification.\n\n\n\n\n\n\nFalsification Tests\n\nTesting Assumptions with Falsification\nThe stronger (bolder) our assumptions for identification, the less credible our results are. Selection on Observables involves a very strong and hard to verify assumption: conditional ignorability. Can we really be sure that we have controlled for all confounders \\(X_i\\) needed to satisfy conditional ignorability?\nPlacebo tests are a type of falsification test to show evidence against our assumptions. Suppose that we make the assumption of conditional ignorability \\((Y_{0i}, Y_{1i}) \\perp D_i | X_i\\). Suppose we are concerned about the presence of another confounder \\(U\\) that is not included in \\(X_i\\).\n\n\n\n\n\nThe presence of \\(U\\) will falsify our conditional ignorability assumption, and means we cannot identify the causal effect of \\(D \\rightarrow Y\\).\n\n\n\n\n\n\nFalsification vs. Validation\n\n\n\n\n\nFalsification is a principle of trying to criticise our own research, rather than defend it. Falsification is about testing if our assumptions are not met. Failing a test provides evidence that our assumption is not met.\n\nEx. Covariates are balanced - thus there is no evidence that our assumptions are not met. We are not saying that our assumption is correct, just that there is no evidence against it.\n\nValidation is the opposite - we test to see if there is evidence in favour of our assumptions.\n\nEx. Covariates are balanced - thus our assumptions are met.\n\n\n\n\nFor falsification tests, we should not just pay attention to statistical significance - we must also pay attention to the magnitude of the point estimation.\n\n\n\nPlacebo Outcome Test\nA placebo outcome test utilities another alternative outcome variable \\(Y'\\) that is caused by our hypothesised unobserved confounder \\(U\\):\n\n\n\n\n\nWe can see that if \\(U\\) does not exist, \\(D\\) should have zero effect on the new outcome \\(Y'\\). Thus, if \\(U\\) is present, we should find a relationship between \\(D\\) and \\(Y'\\).\n\\[\nY'_i = \\gamma + \\delta D_i + u_i\n\\]\n\nIf we find that there is an effect of \\(D\\) on the new outcome \\(Y'\\) (non-zero \\(\\delta\\)), that is evidence that \\(U\\) exists, and is evidence to reject our conditional ignorability assumption (falsifies our design - red flag!).\nIf you do not find an effect of \\(D\\) on new outcome \\(Y'\\) (\\(\\delta = 0\\)) you find no evidence of \\(U\\), and no evidence to reject our conditional ignorability assumption (fails to falsify our design).\n\nWe must be sure that \\(Y\\) is not related to \\(Y'\\) except through \\(D\\) and \\(U\\). If this is true, we just run our original research design but replace \\(Y\\) with \\(Y'\\).\n\n\n\nPlacebo Treatment Test\nA placebo treatment test involves some other treatment \\(D'\\), that was assigned at the same time\n\n\n\n\n\nWe can see that if \\(U\\) does not exist, the effect of \\(D'\\) should have no effect on \\(Y\\). If \\(U\\) does exist, there should be some effect of \\(D'\\) on \\(Y\\).\n\\[\nY_i = \\gamma + \\delta D'_i + u_i\n\\]\n\nIf we find that there is an effect of \\(D'\\) on \\(Y\\) (non-zero \\(\\delta\\)), that is evidence that \\(U\\) exists, and is evidence to reject our conditional ignorability assumption (falsifies our design - red flag!).\nIf you do not find an effect of \\(D'\\) on \\(Y\\) (\\(\\delta = 0\\)) you find no evidence of \\(U\\), and no evidence to reject our conditional ignorability assumption (fails to falsify our design).\n\nWe must be sure that \\(Y\\) is not related to \\(D'\\) except through \\(D\\) and \\(U\\). If this is true, we just run our original research design but replace \\(D\\) with \\(D'\\).\n\n\n\n\n\n\nPartial Identification\n\nDecomposing the ATE\nWith falsification, we were concerned with what assumptions we needed to be not-false in order to identify the ATE. However, we can take a different approach - what can we learn about the ATE without any assumptions?\nLet us decompose the ATE into parts:\n\\[\n\\begin{align}\n\\tau_{ATE}  = & E(Y_{1i} - Y_{0i}) \\\\\n& \\\\\n= & E(Y_{1i} - Y_{0i} | D_i = 1) Pr(D_i = 1) \\\\\n& \\quad - E(Y_{1i} - Y_{0i}|D_i = 0)Pr(D_i = 0)  && (\\text{def. of weighted avg.})\\\\\n& \\\\\n= & [ \\color{blue}{E(Y_i |D_i = 1)} \\color{black}- \\color{red}{E(Y_{0i}|D_i = 1)} \\color{black}] Pr(D_i = 1) \\\\\n& \\quad + [ \\color{red}{E(Y_{1i}|D_i = 0)} \\color{black} - \\color{blue}{E(Y_i|D_i = 0)} \\color{black}]Pr(D_i = 0) && (\\text{observed + unobserved})\n\\end{align}\n\\]\nSome of the quantities are observed (in blue), and some of the quantities are unobserved (in red). Previously, we made assumptions (conditional ignorability, common support) to fill the unobserved quantities. But, we can make actually any assumption as possible.\n\n\n\nNonparametric Bounds\nOne way to fill in our unobserved outcomes through the “best” and “worst” possible outcomes. This allows us to construct a plausible range of the ATE.\nFirst, let us construct the worst-case scenario - the lowest possible \\(\\tau\\).\n\n\\(E(Y_{0i}|D_i = 1) = Y_H\\). Units in the treated \\(D_i=1\\), their potential outcome \\(Y_{0i}\\) will be the highest \\(Y\\) possible, \\(Y_H\\).\n\\(E(Y_{1i}|D_i = 0) = Y_L\\). Units in the control \\(D_i=0\\), their unobserved potential outcome \\(Y_{1i}\\) will be the lowest \\(Y\\) possible, \\(Y_L\\).\n\nThus, the lowest possible \\(\\tau\\) (sharp lower bound) is:\n\\[\n\\begin{split}\n\\tau_L = & [ \\color{blue}{E(Y_i |D_i = 1)} \\color{black}  - \\color{red}{Y_H} \\color{black}] Pr(D_i = 1) \\\\\n& \\quad + [ \\color{red}{Y_L} \\color{black} - \\color{blue}{E(Y_i|D_i = 0)} \\color{black}]Pr(D_i = 0)\n\\end{split}\n\\]\nNow, let us construct the best-case scenario - the highest possible \\(\\tau\\).\n\n\\(E(Y_{0i}|D_i = 1) = Y_L\\). Units in the treated \\(D_i=1\\), their potential outcome \\(Y_{0i}\\) will be the lowest \\(Y\\) possible, \\(Y_L\\).\n\\(E(Y_{1i}|D_i = 0) = Y_H\\). Units in the control \\(D_i=0\\), their unobserved potential outcome \\(Y_{1i}\\) will be the highest \\(Y\\) possible, \\(Y_H\\).\n\nThus, the highest possible \\(\\tau\\) (sharp upper bound) is:\n\\[\n\\begin{split}\n\\tau_H = & [ \\color{blue}{E(Y_i |D_i = 1)} \\color{black}  - \\color{red}{Y_L} \\color{black}] Pr(D_i = 1) \\\\\n& \\quad + [ \\color{red}{Y_H} \\color{black} - \\color{blue}{E(Y_i|D_i = 0)} \\color{black}]Pr(D_i = 0)\n\\end{split}\n\\]\nWe know that the true \\(\\tau_{ATE} \\in [\\tau_L, \\tau_H]\\).\n\n\n\nMonotone Treatment Selection Assumption\nOur extreme case from above is not very useful. However, we can layer on assumptions to lower the possible \\(\\tau\\) values.\nOne assumption is the Monotone Treatment Selection (MTS) assumption. This assumption basically says that potential outcomes for units in treatment, are always higher than for those in the control.\n\\[\n\\begin{split}\n& E(Y_{0i}|D_i = 0) ≤ \\overbrace{E(Y_{0i}|D_i = 1)}^{\\text{unobserved}} \\\\\n& \\underbrace{E(Y_{1i} |D_i = 0)}_{\\text{unobserved}} ≤ E(Y_{1i} |D_i = 1)\n\\end{split}\n\\]\nThis is basically saying that selection bias is one-direction.\nThis implies a tighter sharp upper bound on \\(\\tau\\).\n\\[\n\\begin{align}\n\\tau_H = & [ \\underbrace{E(Y_i |D_i = 1)}_{\\text{observed}}  - \\color{red}{E(Y_{0i}|D_i = 0)} \\color{black}] Pr(D_i = 1) \\\\\n& \\quad + [ \\color{red}{E(Y_{1i}|D_i =1)} \\color{black} - \\underbrace{E(Y_i|D_i = 0)}_{\\text{observed}}]Pr(D_i = 0) \\\\\n= & E(Y_i|D_i = 1) - E(Y_i|D_i = 0) && (\\text{def. of weighted avg.})\\\\\n\\end{align}\n\\]\nThis indicates that the upper bound of plausible \\(\\tau_{ATE}\\) values is the naive estimator of differences in observed outcomes.\nWe can also make the reverse assumption, where selection bias is in the opposite direction. This means a tighter sharp lower bound \\(\\underline\\tau\\). These assumptions help narrow our possible \\(\\tau_{ATE}\\) values, and can allow us to test if our estimated \\(\\hat\\tau\\) is reasonable (within the plausible bounds).\n\n\n\n\n\n\nImplementation in R\nFor all methods, you will need the tidyverse package:\n\nlibrary(tidyverse)\nlibrary(MatchIt)\nlibrary(estimatr)\n\nSee how to perform each estimator in R:\n\n\n\n\n\n\nDistance Matching\n\n\n\n\n\nFirst, let us conduct nearest neighbour matching with Mahalanobis distance by using the matchit() function.\n\nmatch_object = MatchIt::matchit(D ~ X1 + X2 + X3,\n                                data = my_data,\n                                method = \"nearest\", #distance matching\n                                distance = \"mahalanobis\")\n\n# for output summary\nsummary(match_object)\n\nSecond, let us save the matched data with the match.data() function.\n\nmatch_data &lt;- MatchIt::match.data(match_object,\n                                  weights = 'nn_weights')\n\nThird, we can test if matching worked by using a balance table and a love plot:\n\n# balance table\ncobalt::bal.tab(D ~ X1 + X2 + X3, \n                data = match_data, # from the 2nd step\n                weights = \"nn_weights\",\n                disp = c(\"means\", \"sds\"))\n\n#love plot\ncobalt::love.plot(match_object,\n                  data = my_data, #original dataset\n                  stars = 'raw')\n\nFinally, we can estimate the treatment effect. There are two options - either using a weighted regression, or using the matching algorithm:\n\n# using weighted regression\nestimate &lt;- lm_robust(Y ~ D,\n                      data = match_data, #data from step 2\n                      weights = nn_weights)\nsummary(estimate)\n\n## using the Matching package:\nestimate = Matching::Match(Y = my_data$Y, #outcome\n                           Tr = my_data$D, #treatment\n                           X = my_data[,c(\"X1\", \"X2\", \"X3\")], #covariates\n                           M=1, #number of neighbours\n                           BiasAdjust = TRUE, #for biased adjustment\n                           Weight = 2)\nsummary(estimate)\n\nYou will have the estimates that you can use.\n\n\n\n\n\n\n\n\n\nPropensity Score Matching\n\n\n\n\n\nFirst, we want to estimate propensity scores with a logistic regression (or a random forest):\n\n#logistic model\npscore_model = glm(D ~ X1 + X2,\n                   data = my_data,\n                   family = \"binomial\")\n\n# estimate propensity scores\nmy_data$pscore_estimate = predict(pscore_model,\n                                  my_data,\n                                  type = \"response\")\n\nNow, let us match with propensity scores:\n\n# match\nmatch_object = MatchIt::matchit(D ~ pscore_estimate,\n                                data = my_data,\n                                method = \"nearest\",\n                                distance = \"Mahalanobis\")\n\n# save matched data\nmatch_data &lt;- MatchIt::match.data(match_object,\n                                  weights = 'pscore_weights')\n\nThird, we can test if matching worked with a balance table and a love plot:\n\n#balance table\ncobalt::bal.tab(D ~ X1 + X2 + X3,\n                data = match_data, #matched data from step 2\n                weights = \"pscore_weights\",\n                disp = c(\"means\", \"sds\"))\n\n#love plot\ncobalt::love.plot(match_object,\n                  data = my_data, #original dataset\n                  addl = ~ X1 + X2 + X3,\n                  stars = 'raw')\n\nFinally, let us do the estimation:\n\nestimate &lt;- lm_robust(Y ~ D,\n                      data = match_data, #from step 2\n                      weights = pscore_weights)\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nInverse Probability Weighting\n\n\n\n\n\nFirst, we want to estimate propoensity scores with a logistic regression (or a random forest):\n\n#logistic model\npscore_model = glm(D ~ X1 + X2,\n                   data = my_data,\n                   family = \"binomial\")\n\n# estimate propensity scores\nmy_data$pscore_estimate = predict(pscore_model,\n                                  type = \"response\")\n\nSecond, we calculate the inverse probability weights based on the formula from earlier:\n\nmy_data$ipweight = ifelse(my_data$D == 1, # condition\n                       1/my_data$pscore_estimate,\n                       1/(1-my_data$pscore_estimate))\n\nFinally, we can estimate the ATE, or ATT, or use a weighted regression for the ATE:\n\n# ATE estimator\nmean((my_data$D * my_data$Y) * my_data$ipweight - ((1 - my_data$D) * my_data$Y) * my_data$ipweight)\n\n# ATT estimator\nsum(my_data$D * my_data$Y - (1 - my_data$D) * my_data$Y * (my_data$pscore_estimate/(1 - my_data$pscore_estimate)))/sum(my_data$D)\n\n# ATE with weighted regression\nestimate &lt;- lm_robust(Y ~ D, \n                      data = my_data,\n                      weights = ipweight)\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nOLS Estimator\n\n\n\n\n\nFor the OLS estimator, we can use the lm_robust() function:\n\nestimate &lt;- lm_robust(Y ~ D + X1 + X2 + X3,\n                      data = my_data)\nsummary(estimate)\n\nWe can also use the fixest package and the feols() function:\n\nlibrary(fixest)\n\nestimate &lt;- feols(Y ~ D + X1 + X2 + X3,\n                  data = my_data,\n                  se = \"hetero\")\nsummary(estimate)\n\n\n\n\n\n\n\n\n\n\nFully Interacted Estimator\n\n\n\n\n\nFor the fully interacted estimator, we can use the lm_lin() function.\n\nestimate &lt;- lm_lin(Y ~ D,\n                   covariates = ~ X1 + X2 + X3,\n                   data = my_data)\nsummary(estimate)\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "6 Selection on Observables"
    ]
  },
  {
    "objectID": "quant5.html",
    "href": "quant5.html",
    "title": "Randomised Controlled Trials",
    "section": "",
    "text": "Up until now, we have focused on theory of statistics and causal inference. But now, we are ready to dive into methodology - different designs to measure and identify causal effects.\nThis chapter introduces the “gold standard” of causal inference: randomised controlled trials. This chapter also covers extensions, such as stratified experiments and survey experiments.\nUse the right sidebar for quick navigation.\n\n\nRandomisation\n\nRandomised Experiments\nExperiments are a research design where the assignment mechanism is controlled by the researcher.\nRandomised Experiments use randomisation as the assignment mechanism. Treatment values are assigned to \\(N\\) units at random, with both known and positive probabilities of being assigned to treatment and control groups.\nQuick notation for randomised experiments:\n\nWe have \\(N\\) total number of units in our experiment.\nA randomly subset of \\(N_1\\) units are assigned to treatment \\(D = 1\\).\nThe remaining \\(N_0 = N - N_1\\) are assigned to control.\n\n\n\n\n\n\n\nMore on Randomisation\n\n\n\n\n\n\\(N_1\\), the number of individuals assigned to treatment, does not necessarily need to be 50% (although this is quite a common number).\nAlso note that when you fix the number of units to be treated at \\(N_1\\), technically, not all units have an independent probability of being selected. This is because once you have assigned \\(N_1\\) individuals to treatment, we know the remaining individuals must be assigned to control. This usually is not a huge issue.\nYou can use bernoulli randomisation (simple randomisation) to avoid this issue. Bernoulli gives every individual a certain chance of being selected. This does mean that with different randomisation trials, we will get different numbers of treated individuals for each trial.\n\n\n\n\n\n\nIdentification Assumptions\nRandomisation implies that assignment probabilities do not depend on the potential outcomes. The potential outcome values do not affect our chances of being selected for treatment.\n\\[\nPr(D=1|Y_0, Y_1) = Pr(D=1)\n\\]\nOr in other words, treatment is independent of potential outcomes (also unconfounded or ignorability):\n\\[\n(Y_1, Y_0)  \\perp\\!\\!\\!\\!\\perp D\n\\]\nThis implies that \\(E(Y_{0i})\\) is the same between treatment and control groups, and \\(E(Y_{1i})\\) is also the same between treatment and control:\n\\[\n\\begin{split}\n& E(Y_{0i} | D_i = 1) = E(Y_{0i} | D_i = 0) = E(Y_{0i})\\\\\n& E(Y_{1i} | D_i = 1) = E(Y_{1i} | D_i = 0) = E(Y_{1i})\n\\end{split}\n\\tag{1}\\]\n\n\n\nProof of Identification\nLet us return to our naive estimator, and our problem of selection bias. Using the above property in Equation 1, we can simplify:\n\\[\n\\begin{align}\n& \\underbrace{E(Y_{1i}|D_i = 1)- E(Y_{0i}|D_i = 1)}_{\\tau_{ATT}} + \\underbrace{E(Y_{0i}|D_i = 1) - E(Y_{0i} | D_i = 0)}_{\\text{Selection Bias}} \\\\\n& = \\underbrace{E(Y_{1i}|D_i = 1)- E(Y_{0i}|D_i = 1)}_{\\tau_{ATT}} + \\underbrace{E(Y_{0i}) - E(Y_{0i})}_{\\text{Selection Bias}} && (\\because \\text{eq. (1)} \\ )\\\\\n& = \\underbrace{E(Y_{1i}|D_i = 1)- E(Y_{0i}|D_i = 1)}_{\\tau_{ATT}} + 0\n\\end{align}\n\\]\nThus, under randomisation, selection bias is equal to 0, and thus our comparison of observed outcomes is now an unbiased estimator of \\(\\tau_{ATT}\\). Now look at the formula for the ATT estimand. We can simplify as follows using Equation 1:\n\\[\n\\begin{align}\n\\tau_{ATT} & = E(Y_{1i} -Y_{0i}|D_i = 1)\\\\\n& = E(Y_{1i} |D_i = 1) - E(Y_{0i} | D_i = 1) \\\\\n& = E(Y_{1i} ) - E(Y_{0i}) && (\\because \\text{equation (1)} \\ )\\\\\n& = \\underbrace{E(Y_{1i} - Y_{0i})}_{\\tau_{ATE}}\n\\end{align}\n\\]\nAnd now we see that \\(\\tau_{ATT}\\) and \\(\\tau_{ATE}\\) are equivalent under randomisation, and we can identify the \\(\\tau_{ATE}\\) with our observed data.\n\n\n\n\n\n\nGraphical Identification\n\n\n\n\n\nLet us look at a direct acyclic graph:\n\n\n\n\n\nBecause we are randomly assigning treatment \\(D\\), we are exogenously determining \\(D\\). Thus, values of \\(D\\) are not being caused by \\(U\\), they are being caused by randomisation.\nThus, we can eliminate the arrow between \\(U \\rightarrow D\\). This allows us to estimate \\(D \\rightarrow Y\\) without any confounders.\n\n\n\n\n\n\nThe Balancing Property\nRandomisation balances all observed and unobserved pre-treatment characteristics between units between the treatment and control.\nThis is because not only is \\((Y_1, Y_0) \\perp\\!\\!\\!\\!\\perp D\\), but also any covariate \\(X\\) is also independent of treatment: \\(X \\perp\\!\\!\\!\\!\\perp D\\).\nThis means that if randomisation is successful, we should expect minimal differences between control and treatment groups for all pre-treatment characteristics values.\n\n\n\n\n\n\nDetails on the Balancing Property\n\n\n\n\n\nIn any one sample, we actually are likely to have some imbalances in \\(X\\) between control and treatment simply due to chance.\n\nYou could control for imbalanced covariates, but you do not have to (we will discuss this later).\n\nYou can adopt other randomisation procedures, such as stratified randomisation, to guarantee balance on \\(X\\).\n\n\n\nWe can test this assumption by finding the average \\(X\\) values for both control and treatment groups, and see if there are any statistical significant differences in \\(X\\) between control and treatment. This is typically done with a t-test or a regression:\n\\[\nX_i = \\alpha + \\gamma D_i + u_i \\quad \\text{test if } \\gamma \\text{ is significant}\n\\]\n\n\n\nComplications and Limitations\nRandomisation can be complicated by a few factors:\n\nMissing data (often due to individuals dropping out). We are concerned that there is some covariate that is causing some people to drop out, which re-introduces selection bias.\nMeasurement Problems: Hawthorne Effect - subjects know what you are studying, and will change their behaviour as a result.\nNon-Compliance: Some units assigned to treatment might not take the treatment, and some units assigned to control may take the treatment (this can often be dealt with by using an instrumental variable design).\n\nRandomisation does not help with external validity - the ability to extrapolate our results to external situations.\n\n\n\n\n\n\nMore on External Validity\n\n\n\n\n\nExternal validity asks if we can generalise our conclusions from our subjects, to other subjects outside our experiment. Can we extrapolate our estimates to to other populations?\nFor example, if we measured the effect of migration on tolerance for our subjects in India, can we say the same effect is true of someone in Japan, the US, or Europe?\nThis is important - if we cannot extrapolate, some results may be very niche.\nTo extrapolate to a greater population, our actual sample of observations in our experiment, should be representative of the greater population. This is often violated, as random sample for experiments is very very difficult.\nThis is called 𝑋-Validity: we can study this with data - to see how representative our population is compared to the population.\nNon-representative programme of treatment is another threat: Sometimes, treatments will differ between areas.\nFor example, if we are encouraging people to migrate to test how that changes their tolerance, how are the governmental/ngo/private agencies working with you affecting the process. Would less capable agencies create different effects?\nThis is called \\(C\\)-validity, and we cannot measure this with data, unless you redo your experiment in another context.\n\n\n\n\n\n\n\n\n\nCausal Estimation\n\nDifference in Means Estimator\nOur causal estimand is the Average Treatment Effect (ATE):\n\\[\n\\tau_{ATE} = E(Y_{1i}) - E(Y_{0i})\n\\]\nWe can estimate this using the difference-in-means estimator, by taking the sample mean \\(Y\\) of the treatment group, minus the sample mean \\(Y\\) of the control group:\n\\[\n\\hat\\tau_{ATE} = \\bar Y_1 - \\bar Y_0\n\\]\nThis is an unbiased estimator because selection bias is eliminated with randomisation. This is also an asymptotically consistent estimator due to the law of large numbers.\n\n\n\nOrdinary Least Squares Estimator\nWe can also estimate the \\(\\tau_{ATE}\\) with a bivariate regression:\n\\[\nY_i = \\hat\\gamma + \\hat\\tau D_i + \\hat\\epsilon_i\n\\]\nHere, \\(\\hat\\tau\\) is our estimator of the ATE. This gives the same estimate as the difference-in-means estimator.\n\n\n\n\n\n\nProof OLS is Equivalent to Difference-in-Means\n\n\n\n\n\nRemember that OLS is the best approximation of the conditional expectation function \\(E(y|x)\\).\nThus, we can write the regression as:\n\\[\nE(Y_i|D_i) = \\hat\\gamma + \\hat\\tau D_i + \\hat\\epsilon_i\n\\]\nNow, let us find the difference between treatment \\(E(Y_i|D_i =1)\\) and control \\(E(Y_i|D_i = 0)\\):\n\\[\n\\begin{split}\n& E(Y_i|D_i = 1) - E(Y_i|D_i = 0) \\\\\n= & \\ \\hat\\gamma + \\hat\\tau(1) - (\\hat\\gamma + \\hat\\tau(0)) \\\\\n= & \\ \\hat\\gamma + \\hat\\tau - \\hat\\gamma \\\\\n= & \\ \\hat\\tau\n\\end{split}\n\\]\nThus, the difference in means is equivalent to \\(\\hat\\tau\\) regression coefficient.\n\n\n\nFurthermore, \\(\\hat\\gamma\\) is equivalent to the average \\(Y\\) in the control group \\(\\bar Y_0\\).\nWe do not need to include covariates. This is because randomisation allows us to meet the asymptotic consistency condition of both randomisation and exogeneity.\nHowever, sometimes pre-treatment covariates are included. We should not include post-treatment covariates.\n\n\n\n\n\n\nIncluding Pre-Treatment Covariates\n\n\n\n\n\nThere are several reasons one might want to include pre-treatment covariates:\n\nCan increase precision (reduce standard error), by getting better predictions of \\(Y\\).\nCan control for observable imbalance that was observed in the balance tables. Many researchers will compare a model without and with an imbalanced covariate, to show that the covariate does not matter significantly.\nCan allow for estimation of heterogenous treatment effects by including interactions in the model.\n\nThere is one risk: it may introduce small-sample bias. This will be discussed later in the discussion of the fully-interacted estimator.\nWe should not include post-treatment covariates. Anything that is measured post-treatment could be measuring a treatment effect (something that results from the treatment). This may “model away” your treatment effect.\n\n\n\n\n\n\n\n\n\nStatistical Inference\n\nStandard Inference\nWe can use a t-test for statistical inference.\n\nEstimate the \\(\\hat\\tau_{ATE}\\) and robust standard error \\(\\widehat{rse}(\\hat\\tau_{ATE})\\).\nState hypotheses, normally \\(H_0 : \\tau_{ATE} = 0\\) and \\(H_1 \\tau_{ATE} ≠ 0\\).\nCalculate the t-test statistic \\(\\hat\\tau /\\widehat{rse}(\\hat\\tau)\\).\nRefer to the relevant t-distribution, and calculate the p-value.\n\nGenerally, we use a statistical significance level of \\(\\alpha = 0.05\\), so we reject the null if \\(|t|&gt;1.96\\).\nFor more complex randomisation schemes, you will need different standard errors. For example, if you use a cluster randomisation scheme, you might need clustered standard errors.\nWe can also use Nonparametric Bootstrap to create our sampling distribution for statistical inference, instead of relying on asymptotic normality of the standard t-test.\n\n\n\n\n\n\nBlock Bootstrap\n\n\n\n\n\nFor blocked experiments, you should randomly sample blocks, not units, to create your bootstrap re-samples.\nFor example, if your data is clustered in cities, you should re-sample by cities.\n\n\n\n\n\n\nRandomisation Inference\nConsider a new sharp null hypothesis, that all individual causal effects are zero (not just the average causal effect):\n\\[\nH_0^s : Y_1 = Y_0, \\quad H_A^s : Y_1 ≠ Y_0\n\\]\nAssuming \\(H_0\\) is true, we can actually fully construct the potential outcomes \\(Y_{0i}\\) and \\(Y_{1i}\\), since we know every unit has 0 individual treatment effect.\nOnce constructed, we can imagine what different treatments we would observe under different randomization schemes (given \\(H_0\\) is true). Thus, we can construct the sampling distribution, so we do not need to “imagine” a hypothetical sampling distribution.\nProcedure for conducting randomisation inference (also called permutation test or Fisher’s exact test) is as follows:\n\nCalculate the total number of randomisations possible. This is calculated as a permutation of \\(_NP_{N_1}\\) (\\(N\\) choose \\(N_1\\)).\nCalculate and store the value of \\(\\widehat{\\tau_j}\\) of each permuted dataset \\(j\\). Thus, we will have a distribution of \\(\\widehat{\\tau_j}\\).\nCalculate \\(p\\)-value as the proportion of \\(\\widehat{\\tau_j}\\) that are as or more extreme than the actually observed \\(\\hat\\tau\\).\n\n\n\n\n\n\n\nDetails on Randomisation Inference\n\n\n\n\n\nIf we have \\(N\\) total units, and \\(N_1\\) in the treatment group and \\(N_0\\) in the control group, we can calculate all possible randomisation permutations as follows:\n\\[\n\\begin{pmatrix} N \\\\ N_1 \\end{pmatrix} = \\frac{N!}{N_1 ! N_0 !}\n\\]\nThis is the total number of assignments possible given \\(N\\), \\(N_1\\), and \\(N_0\\).\nThen, we can calculate the \\(\\widehat{\\tau_j}\\) of every possible randomisation assignment. The figure below shows this:\n\n\n\n\n\nNow, plot all \\(\\widehat{\\tau_j}\\) in a distribution:\n\n\n\n\n\nLet us say our sample \\(\\hat\\tau = 6\\). We would simply find the area under the curve that is above \\(\\hat\\tau = 6\\), and below \\(-\\hat\\tau = -6\\).\n\n\n\n\n\n\n\n\n\nPros/Cons of Randomisation Inference\n\n\n\n\n\nRandomisation Inference is assumption free - we do not need asymptotic properties or hypothetical sampling distributions.\nSince we also do not need asymptotic property inferences, we can do inference with very small samples as well.\nDownsides: the randomisation inference only tests if the sharp null hypothesis is true, but sometimes, that might not be something we want to test.\n\n\n\n\n\n\n\n\n\nOther Randomisation Procedures\n\nStratified Randomisation\nStratified (also called blocked or conditional) randomisation are when randomisation occurs separately within levels of some covariates(s) \\(X\\). Generally, you separate your sample of \\(N\\) units into \\(J\\) subgroups.\nFor example, you could split people up into male or female, and random sample within each group, rather than everyone together.\n\n\n\n\n\n\nExample of Stratification\n\n\n\n\n\nLet us say you have 4 subjects, with pre-treatment potential outcomes of \\(Y_{0i} = \\{2, 2, 8, 8 \\}\\).\nIf you just randomly assign, then there is a 33% chance that you end up with the random assignment where \\(\\{8, 8\\}\\) are placed in one group, and \\(\\{2, 2\\}\\) are placed in another group.\nThis is a concern: our treatment and control groups would be very imbalanced in this situation, which violates our independence assumption.\nWith blocking, we could divide our sample into \\(J = 2\\) subgroups, having group 1 being \\(\\{2, 2\\}\\), and group 2 being \\(\\{8, 8\\}\\). Then, we randomly sample one from each group into the treatment. This way, we are guaranteed better balance.\n\n\n\nThis can prevent imbalances as normal randomisation can have a high probability (in certain situations) of creating imbalances.\n\n\n\n\n\n\nEstimation with Stratification\n\n\n\n\n\nTo estimate the ATE, you will need a weighted average of the ATE for each subgroup \\(j\\), with the weights being the proportion of units each group \\(j\\) accounts for:\n\\[\n\\tau_{ATE} = \\sum\\limits_{j=1}^J \\frac{N_j}{N} \\tau_j\n\\]\n\n\n\n\n\n\nCluster Randomisation\nCluster randomisation is when we randomly assign units (or have individuals naturally) in groups. Every unit within a group (called a cluster) will get the same treatment. We randomly sample the groups to get the treatment or control.\nFor example, we could randomise development treatment at the village level, or randomise treatment of a cirriculum at the school level.\nThe main reason for this is to prevent SUTVA violations.\nFor example, imagine you are testing the effects of a new curriculum. If you randomise by each student, students will talk to their friends, and treated individuals may teach control individuals about the new curriculum. But by randomising by school (either an entire school gets or does not get the new curriculum), this concern is not a huge issue.\n\n\n\n\n\n\nSurvey Experiments\n\nFraming/Endorsement Experiments\n\n\nPriming Experiments\n\n\nList Experiments\n\n\n\n\n\n Back to top",
    "crumbs": [
      "5 Randomised Controlled Trials"
    ]
  },
  {
    "objectID": "quant4.html",
    "href": "quant4.html",
    "title": "Causal Frameworks",
    "section": "",
    "text": "In the past few chapters, we have focused on correlations.\nThis chapter introduces the main causal frameworks (potential outcomes, causal graphs), the main causal estimands used in causal inference, and the idea of selection bias and confounders. This chapter is the foundation in which the next set of methods for causal inference will be built on.\nUse the right sidebar for quick navigation. R-code for causal diagrams is provided at the bottom.\n\n\nPotential Outcomes Framework\n\nTreatment and Potential Outcomes\nIn causal inference, we are interested in how treatment \\(D\\) causes outcome variable \\(Y\\).\n\\(D\\) is our treatment variable. The indicator of treatment for each unit \\(i\\) is \\(D_i\\).\n\\[\nD_i = \\begin{cases}\n1 \\quad \\text{if unit } i \\text{ recieved the treatment} \\\\\n0 \\quad \\text{if unit } i \\text{ did not recieve the treatment}\n\\end{cases}\n\\]\n\n\n\n\n\n\nFurther information on Treatment Variables\n\n\n\n\n\nCausal variables/treatments must occur before the outcome. A variable cannot cause something to occur in the past.\nCausal variables/treatments must be able to be manipulated (in order to imagine a world where the treatment did not occur).\n\nFor example, \\(D\\) cannot be sex assigned at birth, ethnicity, etc.\nFor example, major global events (how did 9/11 cause the Arab spring?)\n\n\n\n\nImagine there are two hypothetical parallel worlds - one where unit \\(i\\) receives the treatment \\(D\\), and one where unit \\(i\\) does not receive the treatment \\(D\\). Everything else in these worlds is identical.\nPotential Outcomes for unit \\(i\\) are denoted:\n\\[\nY_{di}, Y_i(d) =\\begin{cases}\n& Y_{1i}, \\ Y_i(1) \\quad \\text{Outcome for unit } i \\text{ when } D_i = 1 \\\\\n& Y_{0i}, \\ Y_i(0) \\quad \\text{Outcome for unit } i \\text{ when } D_i = 0 \\\\\n\\end{cases}\n\\]\n\n\n\n\n\n\nExample of Potential Outcomes\n\n\n\n\n\nFor example, imagine we are interested in finding the effect of democracy \\(D\\) on GDP growth \\(Y\\). Potential outcome \\(Y_{1i}\\) is the potential GDP growth of country \\(i\\) if they were a democracy, and outcome \\(Y_{0i}\\) is the potential GDP growth of a country \\(i\\) if they were not a democracy.\n\n\n\n\n\n\nObserved Outcomes and “Missing Data”\nOf course, there is not two parallel worlds with 2 potential outcomes. In the real world, each unit \\(i\\) either receives treatment \\(D\\), or does not. We do not observe the other potential outcome.\n\\(Y_i\\) is the observed outcome for unit \\(i\\). This is given by formula:\n\\[\nY_i = D_i \\cdot Y_{1i} + (1-D_i) \\cdot Y_{0i}\n\\]\nIf we plug in \\(D_i = 0, 1\\) to the equation above, we get the observed outcomes:\n\\[\nY_i = \\begin{cases}\nY_{1i} \\quad \\text{if } D_i = 1 \\\\\nY_{0i} \\quad \\text{if } D_i = 0 \\\\\n\\end{cases}\n\\]\nBefore the treatment (A priori), both potential outcomes could be observed. After the treatment, one is observed, and the other is counterfactual. For any given experiment, only one will ever be seen, and the counterfactual will never be seen (missing data problem).\n\n\n\n\n\n\nNeyman Urn Model\n\n\n\n\n\nPotential Outcomes can be visualised with the Neyman Urn Model.\nBefore the treatment, we have a box (we cannot see) with both potential outcomes.\n\n\n\n\n\nThen, when we apply treatment, we stick our hand into the box that we cannot see, and pull out one observed outcome.\n\n\n\n\n\nWe are essentially sampling from potential outcomes to get observed outcomes.\n\n\n\nThis missing data problem is called the fundamental problem of causal inference.\n\n\n\nStable Unit Treatment Value Assumption\nThe above given observed and potential outcome frameworks depends on the Stable Unit Treatment Value Assumption (SUTVA).\n\\[\n\\begin{align}\nY_{(D_1, D_2, \\dots, D_N)i} & = Y_{(D_1', D_2', \\dots, D_N')i} \\\\\nY_{di} \\text{ under current randomisation} & = Y_{di} \\text{ under all other randomisations}\n\\end{align}\n\\]\nOr more intuitively, the potential outcomes of unit \\(i\\) only depends on their own treatment status, and no other unit’s treatment status. Thus, changing everyone else’s treatment status has no effects on unit \\(i\\)’s potential outcomes \\(Y_{di}\\). The treatment is also the same for everyone (treatment is stable and consistent)\n\n\n\n\n\n\nExamples of SUTVA Violations\n\n\n\n\n\n\nSpill-over effects: If we are testing a new curriculum, one student \\(j\\) getting the new curriculum may teach their friend \\(i\\) the new curriculum, thus affecting the potential outcomes of \\(i\\).\nContagion: If we are studying a disease, diseases can spread, so another unit \\(j\\) getting a disease affects the potential outcomes of unit \\(i\\).\nDilution: If we are studying vaccines - there is herd immunity - other people getting the vaccine also reduces our chances of getting the disease.\nVariable levels of treatment: If we are doing a drug trial, if some people got two doses, while others only got one dose. This is not a consistent treatment.\nTechnical errors: If someone who is supposed to be treated accidentally is not treated. This is not a consistent treatment.\n\n\n\n\nWhen SUTVA is violated, potential outcomes become very messy, and we no longer have the neat framework as before.\n\n\n\n\n\n\nCausal Estimands\n\nIndividual Treatment Effect\nRemember the potential outcomes from parallel worlds \\(Y_{1i}\\) and \\(Y_{0i}\\).\nSince these two parallel worlds are identical except for the fact one receives the treatment \\(D\\) and the other does not, the causal effect of \\(D\\) should be the difference between the potential outcomes of these two worlds. Thus, the individual treatment effect of a unit \\(i\\) is:\n\\[\n\\tau_i = Y_{1i} - Y_{0i}\n\\]\nThis is the specific treatment effect for a specific unit \\(i\\). This cannot be observed, because we do not see both potential outcomes for the same unit \\(i\\).\nThis is also very hard to estimate, as we cannot reliably fill in the missing potential outcome for any one unit \\(i\\). Thus, we almost never use individual treatment effects, and use group treatment effects.\n\n\n\nAverage Treatment Effect (ATE)\nATE is a group-level causal estimand.\n\n\n\n\n\n\nGroup-Level Causal Estimands\n\n\n\n\n\nConsider a population of units \\(i = 1, \\dots, N\\).\nThe population has potential outcomes represented in two (only partially observed) vectors:\n\\[\n\\begin{split}\n& Y_1 = (Y_{11}, Y_{12}, \\dots, Y_{1N}) \\\\\n& Y_0 = (Y_{01}, Y_{02}, \\dots, Y_{0N})\n\\end{split}\n\\]\nWe compare these two vectors of potential outcomes. The most common way to do this is to use their expected values.\n\n\n\nThe Average Treatment Effect is defined as:\n\\[\n\\begin{split}\n\\tau_{ATE} & = E(Y_{1i} - Y_{0i}) \\\\\n& = \\underbrace{\\frac{1}{N} \\sum\\limits_{i=1}^N (Y_{1i} - Y_{0i})}_{\\text{a formula for average}}\n\\end{split}\n\\]\nWe cannot calculate this with observed data - since we need all potential outcomes to do this. We can estimate this (covered throughout this course).\n\n\n\nAverage Treatment Effect on the Treated (ATT)\nAn alternative estimand to the ATE is the Average Treatment Effect on the Treated (ATT):\n\\[\n\\begin{split}\n\\tau_{ATT} & = E(Y_{1i} - Y_{0i} \\ | \\ D_i = 1) \\\\\n& = \\underbrace{\\frac{1}{N_1} \\sum\\limits_{i=1}^N D_i (Y_{1i} - Y_{0i}) \\quad  \\text{where } N_1 = \\sum\\limits_{i=1}^ND_i}_{\\text{a formula for the average only for treated units}}\n\\end{split}\n\\]\nThis is the causal effect of only units who have received the treatment. Note that frequently the ATT is not equal to the ATE, so be aware of which estimand you are trying to estimate/identify.\n\n\n\n\n\n\nATT vs. ATE\n\n\n\n\n\nWhen does \\(\\tau_{ATT} = \\tau_{ATE}\\)?\n\nWhen the expectation of the potential outcomes of both the treated and control are the same, then the two equal each other.\n\nThe opposite is also true: if the expectation of the potential outcomes of both the treated and control are different, then the two are not equal.\n\n\n\nThe opposite estimand is the Average Treatment effect on the Untreated (ATU), which only measures the causal effect of units who did not recieve the treatment.\nThis is not used very often, since it is kind of uninituive to think about treatment effects on individuals who did not recieve treatment. However, it can be useful in understanding identification assumptions.\n\n\n\nConditional Average Treatment Effect (CATE)\nThe conditional average treatment effect is any treatment effect where there is a condition on a characteristic/covariate:\n\\[\n\\tau_{CATE}(x) = E(Y_{1i} - Y_{0i} \\ | \\ \\underbrace{X_i = x)}_{\\text{condition}}\n\\]\nThis is the causal effect of only variables who meet the condition of the covariate specified. For example, you could find the conditional average treatment effect of only women (so the covariate which we are conditioning on is gender). You can also condition on multiple covariates.\nThis is often used for tailoring products/medicine/advertising to certain groups of people. It is also frequently used in identification strategies.\nThis estimand will go by other names, including the Local Average Treatment Effect (LATE).\n\n\n\n\n\n\nSelection Bias and Confounders\n\nNaive Estimator and Selection Bias\nA natural way to estimate the ATE is to use a naive estimator: find the average difference of observed outcomes. This is called the naive estimator:\n\\[\n\\hat\\tau_{naive} = \\underbrace{E(Y_i|D_i = 1)}_{\\text{for treated}} - \\underbrace{E(Y_i|D_i = 0)}_{\\text{for control}}\n\\]\nHowever, there is an issue - we can show this with algebra:\n\\[\n\\begin{align}\n\\hat\\tau_{naive} & = E(Y_i|D_i = 1) - E(Y_i|D_i = 0) \\\\\n& = \\underbrace{E(Y_{1i}|D_i = 1) - E(Y_{0i} | D_i = 0)}_{\\because \\text{ observed potential outcomes}} \\\\\n& = E(Y_{1i}|D_i = 1) - E(Y_{0i} | D_i = 0) + \\underbrace{E(Y_{0i}|D_i = 1) \\color{red}{- E(Y_{0i}|D_i = 1)}}_{\\because \\text{ this equals 0, so we can add it}} \\\\\n& = \\underbrace{E(Y_{1i}|D_i = 1) \\color{red}{- E(Y_{0i}|D_i = 1)}}_{\\tau_{ATT}} + \\underbrace{E(Y_{0i}|D_i = 1) - E(Y_{0i} | D_i = 0)}_{\\text{Selection Bias}} \\\\\n\\end{align}\n\\]\nWe can see that our naive estimator produces the \\(\\tau_{ATT}\\) plus an extra bit (called the selection bias). Thus, our naive estimator is biased, so we should be careful about using this naive estimator (correlation does not equal causation).\n\n\n\n\n\n\nNaive Estimator Biased for ATU\n\n\n\n\n\nThe proof above shows how the naive estimator is a biased estimator for the \\(\\tau_{ATT}\\). We can also prove it is a biased estimator of the ATU:\n\\[\n\\begin{split}\n\\hat\\tau_{naive} & = E(Y_i|D_i = 1) - E(Y_i|D_i = 0) \\\\\n& = \\underbrace{E(Y_{1i}|D_i = 1) - E(Y_{0i} | D_i = 0)}_{\\because \\text{ observed potential outcomes}} \\\\\n& = E(Y_{1i}|D_i = 1) - E(Y_{0i} | D_i = 0) + \\underbrace{E(Y_{1i}|D_i = 0) - E(Y_{1i}|D_i = 0)}_{\\because \\text{ this equals 0, so we can add it}} \\\\\n& = \\underbrace{E(Y_{1i}|D_i = 0)- E(Y_{0i}|D_i = 0)}_{\\tau_{ATU}} + \\underbrace{E(Y_{1i}|D_i = 1) - E(Y_{1i} | D_i = 0)}_{\\text{Selection Bias}} \\\\\n\\end{split}\n\\]\n\n\n\n\n\n\n\n\n\nNaive Estimator Biased for ATE\n\n\n\n\n\nThe proofs above shows how the naive estimator is a biased estimator for the \\(\\tau_{ATT}\\) and \\(\\tau_{ATU}\\). We can also prove it is a biased estimator of the ATE.\nLet us first start with the ATE. Let us call \\(Y_{1i} - Y_{0i} := \\tau_i\\) for notation simplicity:\n\\[\n\\begin{align}\n\\tau_{ATE} & = E(Y_{1i} - Y_{0i})  = E(\\tau_i)\\\\\n& = \\underbrace{E(\\tau_i|D_i = 1)Pr(D_i = 1) + E(\\tau_i|D_i = 0)Pr(D_i = 0)}_{\\because \\text{ weighted average of ATE and ATU by proportion}} \\\\\n& = E(\\tau_i|D_i = 1) \\underbrace{(1 -Pr(D_i = 0))}_{\\because \\text{ complement prob.}} + E(\\tau_i|D_i = 0)Pr(D_i = 0) \\\\\n\\end{align}\n\\]\nLet us call \\(Pr(D_i = 0) := \\pi\\) for notation simplicity. Now, continue:\n\\[\n\\begin{split}\n& = \\underbrace{E(\\Delta|D_i = 1) - \\pi E(\\Delta|D_i = 1)}_{\\because \\text{ distribute out}} + E(\\tau_i|D_i = 0)\\pi \\\\\n& = E(\\tau_i|D_i = 1) + \\underbrace{\\pi[E(\\tau_i|D_i = 0) - E(\\tau_i|D_i = 1)]}_{\\because \\ \\pi \\text{ factored out }} \\\\\n& = E(Y_{1i} |D_i = 1) - E(Y_{0i}|D_i = 1) + \\pi[E(\\tau_i|D_i = 0) - E(\\tau_i|D_i = 1)] \\\\\n\\end{split}\n\\]\nLet us call the part \\(\\pi[E(\\tau_i|D_i = 0) - E(\\tau_i|D_i = 1)] := \\Pi(\\tau_i)\\). Now, continue to simplify:\n\\[\n\\begin{split}\n& = E(Y_{1i} |D_i = 1) - E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) + \\underbrace{E(Y_{1i} |D_i = 0) - E(Y_{0i}|D_i = 0)}_{\\because \\text{ these two cancel out so we add 0}}  \\\\\n& = E(Y_{1i} |D_i = 1) - E(Y_{0i}|D_i = 0) + E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& = \\underbrace{E(Y_i|D_i = 1)}_{\\because \\text{ observed outcome}} - \\underbrace{E(Y_i|D_i = 0)}_{\\because \\text{ observed outcome}} + E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& = \\underbrace{E(Y_{i} |D_i = 1) - E(Y_{i}|D_i = 0)}_{\\hat\\tau_{naive}} + E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& = \\hat\\tau_{naive}+ E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i)\n\\end{split}\n\\]\nThus, we can see that \\(\\tau_{ATE}\\) is not equivalent to \\(\\hat\\tau_{naive}\\). Let us isolate \\(\\hat\\tau_{naive}\\) to identify the selection bias.\n\\[\n\\begin{split}\n& \\tau_{ATE} = \\hat\\tau_{naive}+ E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& -\\hat\\tau_{naive} = -\\tau{ATE} + E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& \\hat\\tau_{naive} = \\tau_{ATE} - E(Y_{1i} |D_i = 0) + E(Y_{0i}|D_i = 1) + \\Pi(\\tau_i) \\\\\n& \\hat\\tau_{naive} = \\tau_{ATE} + \\underbrace{E(Y_{0i}|D_i = 1)- E(Y_{1i} |D_i = 0) + \\Pi(\\tau_i)}_{\\text{selection bias}}\n\\end{split}\n\\]\n\n\n\n\n\n\nConfounders\nTake the selection bias formula from above:\n\\[\n\\underbrace{E(Y_{0i}|D_i = 1)}_{Y_{0i}\\text{ (treated)}} - \\underbrace{E(Y_{0i} | D_i = 0)}_{Y_{0i}\\text{ (control)}}\n\\]\nIf selection bias is non-zero, this essentially states that the expected potential outcome before treatment \\(Y_{0i}\\) between the treatment and control groups is not equal.\nOr in other words, the treatment and control groups have some other variable causing differences even before treatment has begun. This implies that the differences between the treatment and control group may not be due to treatment \\(D\\), but due to the underlying differences before treatment even occurred.\nConfounders are variables that cause the differences between treatment and control groups before the treatment has started. Confounders correlate with both the treatment variable and the outcome. If a variable only correlates with \\(D\\) or \\(Y\\), then it is not a confounder. If must correlate with both \\(D\\) and \\(Y\\).\nThis is why correlation does not equal causation - if the treatment and control group are different before we start the experiment, we cannot say the difference between the two is purely a result of treatment \\(D\\).\n\n\n\nOmitted Variable Bias in Regression\nWe can demonstrate how confounders cause bias in regression. Suppose there is some confounding variable \\(z\\) that we have not included in a “short” regression. The actual, “true” regression of the population, would include this confounder \\(z\\)\n\\[\n\\underbrace{\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u}_{\\text{short regression}}\n\\qquad \\underbrace{\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf z\\boldsymbol\\delta + \\mathbf u}_{\\text{true regression with z} }\n\\]\nThe OLS estimate of the “short regression” excluding confounder \\(z\\) is:\n\\[\n\\boldsymbol{\\hat\\beta} = (\\mathbf X^T \\mathbf X)^{-1} \\mathbf X^T \\mathbf y\n\\]\nLet us now plug in the “true” model into where \\(\\mathbf y\\) is:\n\\[\n\\begin{align}\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} (\\color{blue}{\\mathbf X \\boldsymbol\\beta + \\mathbf z\\boldsymbol\\delta + \\mathbf u}\\color{black}) && (\\text{plug in } \\color{blue}{\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf z\\boldsymbol\\delta + \\mathbf u}\\color{black} )\\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf z\\boldsymbol\\delta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{multiply out})\\\\\n& = \\color{blue}{\\mathbf I}\\color{black}{\\boldsymbol\\beta} + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf z\\boldsymbol\\delta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{inverses } (\\color{blue}{\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X = \\mathbf I}\\color{black})\\\\\n& = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf z\\boldsymbol\\delta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{identity property of } \\mathbf I)\n\\end{align}\n\\]\nNow, let us find the expected value of \\(\\boldsymbol{\\hat\\beta}\\), which is conditional on \\(\\mathbf X, \\mathbf z\\), and simplify (using zero conditional mean):\n\\[\n\\begin{align}\nE(\\boldsymbol{\\hat\\beta}|\\mathbf X, \\mathbf z) & = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf z\\boldsymbol\\delta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} E(\\mathbf u | \\mathbf X, \\mathbf z) \\\\\n& = \\boldsymbol\\beta + (\\mathbf X^T \\mathbf X)^{-1} \\mathbf X^T \\mathbf z\\boldsymbol\\delta &&(\\because E(\\mathbf u | \\mathbf X, \\mathbf z) = 0)\n\\end{align}\n\\]\nNow, what if we had a regression of outcome variable being the confounder \\(z\\), on the explanatory variables \\(\\mathbf X\\), such that \\(\\mathbf z = \\mathbf X \\boldsymbol\\pi + \\mathbf v\\). Our OLS estimate would have the solution:\n\\[\n\\boldsymbol{\\hat\\pi} = (\\mathbf X^T\\mathbf X)^{-1}\\mathbf X^T \\mathbf z\n\\]\nNow, we can plug \\(\\boldsymbol{\\hat\\pi}\\) into our expected value of \\(\\boldsymbol{\\hat\\beta}\\):\n\\[\nE(\\boldsymbol{\\hat\\beta}|\\mathbf X, \\mathbf z)  = \\boldsymbol\\beta + \\boldsymbol{\\hat\\pi \\delta}\n\\]\nNow, using the law of iterated expectations, we get (assuming we are using an unbiased estimator for \\(\\boldsymbol{\\hat\\pi}\\) such that \\(E(\\boldsymbol{\\hat\\pi}) = \\boldsymbol\\pi\\)):\n\\[\n\\begin{align}\nE(\\boldsymbol{\\hat\\beta}) & = E(E(\\boldsymbol{\\hat\\beta}|\\mathbf X, \\mathbf z)) && (\\text{LIE: } E(X) = E(E(X|Y))\\\\\n& = E(\\boldsymbol\\beta + \\boldsymbol{\\hat\\pi \\delta})  && (\\text{LIE: } E(X) = E(E(X|Y)) \\\\\n& = \\boldsymbol\\beta + E(\\boldsymbol{\\hat\\pi}) \\boldsymbol \\delta && (\\boldsymbol\\beta, \\boldsymbol\\delta\\text{ are constants})\\\\\n& = \\boldsymbol\\beta + \\boldsymbol{\\pi \\delta} && (\\text{unbiased estimator } E(\\boldsymbol{\\hat\\pi}) = \\boldsymbol\\pi)\n\\end{align}\n\\]\nThus, we can see by not including confounder \\(z\\) in our “short regression”, the estimator is now biased by \\(\\boldsymbol{\\hat\\pi \\delta}\\).\n\n\n\nAssignment Mechanism\nThe Assignment Mechanism is the procedure that determines the treatment status of each unit. In causal inference, we want to restrict the assignment mechanism, in order to remove the effect of selection bias.\nThere are two types of studies that use different assignment mechanisms:\n\nRandomisation Experiments: The assignment mechanism is both known, and controlled by the researcher. Generally, the researcher chooses some type of randomisation.\nObservational Studies: The assignment mechanism is not known to, or not under the control of the researcher. This means that confounders may be driving selection into treatment and control, inducing bias.\n\nGenerally, the most credible studies are randomisation studies, since we can control interventions to parse out the effect of confounders. Observational studies generally rely on more assumptions that need to be met, and need to be defended for the study to be credible.\n\n\n\n\n\n\nDirected Acyclic Graphs\n\nComponents of the Graphs\nCausal Diagrams are a visual way to represent causal theories and frameworks, which allows us to visualise how variables interact with each other.\nEach Directed Acyclic Graph (DAG) has the following components:\n\nNodes: representing variables (which are also called vertices).\nDirected Edges: Arrows that encode one-way causal theories between variables. For example, we might believe \\(Z\\) causes \\(X\\), so we will draw an arrow from \\(Z\\) to \\(X\\). These connections are observable (solid) or unobservable (dashed).\n\n\n\n\n\n\n\nExample of Directed Acyclic Graphs\n\n\n\n\n\nBelow is an example of a directed acyclic graph:\n\n\n\n\n\nWhat does this diagram show?\n\nWe have two unobserved variables: \\(Q\\) and \\(Y\\)\nWe have three observed variables: \\(Z\\), \\(D\\), and \\(Y\\).\nWe can see the causal theories represented by arrows.\n\nWhat can we learn from this diagram?\n\n\\(Z \\rightarrow Y\\) is confounded by \\(W\\): \\(W\\) is affecting who gets treatment \\(Z\\), and causing \\(Y\\). Thus, \\(W\\) is affecting who gets selected into treatment \\(Z\\), and selecting your potential outcome \\(Y\\). Thus, this is an example of selection bias.\n\\(D \\rightarrow Y\\) is confounded by \\(Q\\).\n\\(Z \\rightarrow D\\) is not confounded, so we can estimate this causal effect.\n\nNote: All these conclusions are only true if our causal theory is correct (we have specified all the possible variables, and we have specified the correct causal relationships).\n\n\n\nFeatures of a Directed Acyclic Graph:\n\nThey must be acyclic: This means that they are not circular - \\(A\\) does not terminate back at \\(A\\).\nNon-Connections: The absence of relationships between variables.\n\n\n\n\nRepresenting Interventions\nTreatments (interventions by the researcher, for example) are when we determine one variable exogenously (such as by randomisation).\nOr in other words, one variable is determined randomly externally, not caused by any variables within the directed acyclic graphs.\nTreatments are represented by the do() operator. When the treatment is exogenous, we can break all the connections into that variable’s node.\nThis is because we are determining the value of the variable, not any other variables.\n\n\n\n\n\n\nExample of Interventions\n\n\n\n\n\n\n\n\n\n\nAn intervention here is on variable \\(D\\). That means the value of \\(D\\) is being chosen outside of this graph (by randomisation, or the researcher).\nThis allows us to delete the arrow between \\(Q \\rightarrow D\\) and \\(Z \\rightarrow D\\). This is because we are exogenously determining \\(D\\), so \\(Q\\) and \\(Z\\) are not determining the value of \\(D\\).\n\n\n\nWith exogenously determined variables, we can find the causal effect that variable is causing on another.\n\n\n\nBlocked Paths\nA set of nodes \\(\\{ \\mathbf S \\}\\) blocks a path \\(p\\) if either:\n\nIf the path \\(p\\) contains at least one arrow-emitting node included in the set of nodes \\(S\\), or\nThe path \\(p\\) contains at least one collision node (multiple arrows point into it) that is outside the set of nodes \\(S\\), and the collision node has no descendant within the set of nodes \\(S\\) (no arrows go out of it to another node).\n\nTake this directed acyclic graph:\n\n\n\n\n\nWe can see the following:\n\nThe path \\(D \\rightarrow P \\rightarrow Y\\) is blocked by set \\(\\{P\\}\\), because the node \\(P\\) is one arrow-emitting node that is in the path \\(D \\rightarrow P \\rightarrow Y\\).\nThe path \\(D \\leftarrow M \\rightarrow Y\\) is blocked by set \\(\\{M\\}\\), because the node \\(M\\) is one arrow-emitting node in the path \\(D \\leftarrow M \\rightarrow Y\\).\n\\(D \\leftarrow Z \\rightarrow M \\rightarrow Y\\) is blocked by \\(\\{M\\}\\), \\(\\{Z\\}\\), or \\(\\{M, Z\\}\\) - note \\(D\\) and \\(Y\\) do not emit arrows so they cannot block.\n\nBlocking paths is important, since in order to estimate \\(D \\rightarrow Y\\), we need to block any other path between \\(D\\) and \\(Y\\) that is not directly \\(D \\rightarrow Y\\).\n\n\n\n\n\n\nImplementation in R\nThis section will show how you can create DAGs in R. We will need the ggdag and dagitty packages.\n\nlibrary(ggdag)\nlibrary(dagitty)\n\n\n\n\n\n\n\nSimple DAGs with Dagify\n\n\n\n\n\nYou can create a very simple DAG with dagify as follows:\n\ndag_object &lt;- dagify(\n  Y ~ X + D, #Y is caused by X and D\n  D ~ X #D is caused by X\n)\n\nggdag(dag_object) + theme_dag()\n\nThis dag is not very customisable. This can be an issue if you want nodes to be in a specific location. See below for a more customisable DAG.\n\n\n\n\n\n\n\n\n\nCustom DAGs with Dagitty\n\n\n\n\n\nYou can create more complex DAGs with Dagitty. Dagitty allows us to position nodes in a coordinate system, which is useful in some purposes.\n\ndag_object &lt;- dagitty('dag {\n      D [pos = \"0, 1\"]\n      Y [pos = \"2, 1\"]\n      X [pos = \"1, 2\"]\n      \n      D -&gt; Y\n      D &lt;- X -&gt; Y\n  }')\n\nggdag(dag_object) + theme_dag()\n\nThe pos arguments have the coordinates of where you want to put each node.\nBelow are the path connections, where you can use -&gt; and &lt;- to indicate relationships.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "4 Causal Frameworks"
    ]
  },
  {
    "objectID": "quant3.html",
    "href": "quant3.html",
    "title": "Classic Least Squares Theory",
    "section": "",
    "text": "Last chapter, we discussed the multiple linear regression model, and how it can help us measure relationships between explanatory and outcome variables.\nThis chapter introduces some key theory regarding the ordinary least squares estimator behind linear regression. Topics covered includes properties of estimators, the OLS estimator, and the Method of Moments estimator.\nUse the right sidebar for quick navigation.\n\n\nEstimators\n\nEstimands and Estimators\nAn estimand is the true value of some true parameter \\(\\theta\\) in the population we are trying to measure.\nWe often do not have data on the population. We typically have a sample from the population, and use an estimator (procedure) to produce a sample estimate \\(\\hat\\theta\\).\nHowever, because of sampling variability (not all random samples will be identical), each sample \\(n\\) will have a different estimate \\(\\hat\\theta_n\\).\nIf we keep taking \\(N\\) number of samples, we will have \\(N\\) number of estimates \\(\\hat\\theta_1, \\hat\\theta_2, \\dots, \\hat\\theta_N\\). Thus, any specific estimate \\(\\theta_n\\) from sample \\(n\\) can be thought of as a random draw from the sampling distribution \\(\\hat\\theta_1, \\hat\\theta_2, \\dots, \\hat\\theta_N\\).\n\n\n\n\n\n\nExample of a Sampling Distribution\n\n\n\n\n\nLet us say we want to find the mean salary of all individuals in the UK. The true value of the mean salary for every individual is \\(\\theta\\).\nHowever, asking all 60 million people is nearly impossible. So, we take a randomly sample of 1000 individuals, and then find the sample mean. Our estimator is thus the sample mean estimator.\nOur first sample of 1000 individuals yields an estimate \\(\\hat\\theta_1\\). If we take another sample, we will get slightly different people in this sample, and get another estimate \\(\\hat\\theta_2\\). We keep taking samples, and get more and more estimates \\(\\hat\\theta_3, \\hat\\theta_4, \\dots, \\hat\\theta_n\\).\nWe plot all of these samples into a distribution as follows:\n\n\n\n\n\nThis indicates the potential estimates we can get. If we were to conduct only one sample, we would essentially be selecting a random \\(\\hat\\theta_i\\) value from this distribution.\n\n\n\nThe sampling distribution of an estimator is the key property of estimators. The two parameters of interest from this sampling distribution are its expectation and variance.\n\n\n\nUnbiased Estimators\nAn estimator of a parameter is unbiased, if its estimates \\(\\hat\\theta_n\\) have an expectation equal to the true population value of the parameter:\n\\[\nE(\\hat\\theta_n) = \\theta\n\\]\nOr in other words, if we repeatedly sample and use the estimator, on average, the estimates will be equal to the true population value.\nWe want an unbiased estimator, because if \\(E(\\hat\\theta_n) = \\theta\\), that means our “best guess” of the estimator value is the true parameter value \\(\\theta\\). That means any one estimate \\(\\hat\\theta_n\\) is on average, correct.\nIf our estimator is biased, we can quantify bias with the following formula:\n\\[\nBias(\\hat\\theta_n) = E(\\hat\\theta_n)-\\theta\n\\]\n\n\n\nVariance and Efficiency\nUnbiasedness is not the only desirable property of estimators - we also care about the variance. After all, if we have two unbiased estimators, the one with less variance will be on average, closer to the true population value, for any one estimate \\(\\hat\\theta\\).\n\n\n\n\n\n\nExample of the Importance of Variance\n\n\n\n\n\nFor example, let us say the true population parameter is \\(\\theta = 0\\). We will have two estimators: estimator \\(A\\) and estimator \\(B\\):\n\nEstimator \\(A\\), after two samples (for simplicity), produces estimates -1 and 1.\nEstimator \\(B\\), after two samples, produces estimates -100 and 100.\n\nBoth estimators are unbiased \\(E(\\hat\\theta_n) = 0\\). However, clearly, estimator \\(A\\) is, on average, closer to \\(\\theta =0\\) than estimator \\(B\\). This is because while both estimators are unbiased, estimator \\(A\\) has a smaller variance than estimator \\(B\\) - that is on average, estimator \\(A\\)’s estimators are more closely “packed around” the expectation of the estimator.\n\n\n\nThe variance of an estimator can be quantified as:\n\\[\nVar(\\hat\\theta_n) = E[(\\hat\\theta_n - E(\\hat\\theta_n))^2]\n\\]\nAn efficient estimator is one that, on average, has the closest estimated value \\(\\hat\\theta_n\\) to the true population parameter. If two estimators are both unbiased, the one with lower variance is more efficient. Efficiency can be quantified as the estimator with the lowest mean squared error:\n\\[\nMSE(\\hat\\theta_n) = E[(\\hat\\theta_n - \\theta)^2] =Var(\\hat\\theta_n) + Bias(\\hat\\theta_n)^2\n\\]\nWe generally want an efficient estimator, since we know it will be giving us the closest guess to the true population parameter \\(\\theta\\).\n\n\n\n\n\n\nEfficient but Biased\n\n\n\n\n\nInterestingly, it is possible for a biased estimator to be more efficient than an unbiased estimator.\nThis is particularly the case when the biased estimator has a slight bias but small variance, while the unbiased estimator has a giant variance. In this case, the biased estimator is producing estimates \\(\\hat\\theta\\) that on average, are closer to the true population parameter \\(\\theta\\).\n\n\n\n\n\n\nAsymptotically Consistent Estimators\nAsymptotic properties are properties of estimators as the sample size \\(n\\) approaches infinity.\nAn estimator is consistent, if as we increase sample size towards infinity, the estimate will become more and more concentrated around the true population value \\(\\theta\\). At \\(n = ∞\\), our sampling distribution collapses to just one value, the true population value \\(\\theta\\). Mathematically:\n\\[\nPr(|\\hat\\theta_n - \\theta|&gt; \\epsilon) \\rightarrow 0, \\text { as } n \\rightarrow ∞\n\\]\nOr in other words, the probability that the distance between an estimate \\(\\hat\\theta_n\\) and the true population value \\(\\theta\\) will be higher than a small close-to-zero value \\(\\epsilon\\) will be 0, since our estimates \\(\\hat\\theta_n\\) will converge at the \\(\\theta\\).\nThis is a useful property, since even if our estimator is biased, if it is asymptotically consistent, we know that with large enough sample sizes, that bias becomes infinitely small and negligible.\n\n\n\n\n\n\nBiased but Consistent\n\n\n\n\n\nAn estimator can be both biased, but consistent. In smaller sample sizes, the estimator might not be on average correct, but over a large enough sample size, it will become “unbiased”.\nFor example, in the figure below, we can see that this estimator is biased at small values of \\(n\\), but as \\(n\\) increases, it becomes more consistent, collapsing its distribution around the true \\(\\theta\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaw of Large Numbers and Consistency\n\n\n\n\n\nThe law of large numbers states that the sample average of a random sample, is a consistent estimator of the population mean.\nFor example, let us say we have a random variable \\(x\\). We take a random sample of \\(n\\) units, so our sample is \\((x_1, \\dots, x_n)\\).\n\nLet us define \\(\\bar x_n\\) as our sample average.\nLet us define \\(\\mu\\) as the true population mean of variable \\(x\\).\n\nThe law of large numbers states that:\n\\[\nplim( \\bar x_n) = \\mu\n\\]\n\nWhere \\(plim\\) states that as \\(n\\) approaches infinity, the probability distribution of \\(\\bar x_n\\) collapses around \\(\\mu\\).\n\n\nWhy is this the case? This sample mean estimator is calculated simply through the formula for mean:\n\\[\n\\bar x_n = \\frac{1}{n}\\sum\\limits_{i=1}^n x_i\n\\]\nLet us define the variance of our sample of \\(x_1, \\dots, x_n\\) as \\(Var(x_i) = \\sigma^2\\). We can now find the variance of our sampling distribution of estimator \\(\\bar x_n\\):\n\\[\n\\begin{split}\nVar(\\bar x_n) & = Var\\left( \\frac{1}{n}\\sum\\limits_{i=1}^n x_i \\right) \\\\\n& = \\frac{1}{n^2} Var \\left(\\sum\\limits_{i=1}^n x_i\\right) \\\\\n& = \\frac{1}{n^2} \\sum\\limits_{i=1}^n Var(x_i) \\\\\n& = \\frac{1}{n^2} \\sigma^2 \\\\\n& = \\frac{\\sigma^2}{n}\n\\end{split}\n\\]\nAnd as sample size \\(n\\) increases to infinity, we get:\n\\[\n\\lim\\limits_{n \\rightarrow ∞} Var(\\bar x_n) = \\lim\\limits_{n \\rightarrow ∞} \\frac{\\sigma^2}{n} = 0\n\\]\nThus, the variance of our estimator \\(\\bar x_n\\) shrinks to zero, so as sample size increases to infinity \\(n\\), the sampling distribution of estimator \\(\\bar x_n\\) collapses around the true population mean.\n\n\n\n\n\n\nAsymptotic Normality\nAnother asymptotic property of estimators, as sample size \\(n\\) approaches infinity, is that the sampling distribution approaches a normal distribution.\nThe central limit theorem establishes asymptotic normality of estimators. Let us say we have \\(N\\) number of random variables \\(\\hat\\theta_1, \\dots, \\hat\\theta_N\\) (estimates are realisations of random variables). The central limit theorem states that:\n\\[\nPr(w_n &lt; w) \\rightarrow \\Phi(w) \\quad \\text{as } n \\rightarrow ∞\n\\]\n\nWhere \\(w_n\\) is a transformed version of the random variable \\(\\hat\\theta_n\\), defined as \\(w_n = \\frac{\\bar\\theta_n - \\mu}{\\sigma / \\sqrt{n}}\\).\nWhere \\(Pr(w_n &lt; w)\\) is the cumulative density function of the random variable \\(w_n\\).\nWhere \\(\\Phi(w)\\) is the cumulative density function (cdf) of the standard normal distribution \\(\\mathcal N(0, 1)\\).\n\nThe importance of CLM comes from the fact that as we increase sample size, our sampling distribution becomes more and more normally distributed. This property is essentially for carrying out statistical inference and significance tests, as they generally assume that our estimator is normally distributed.\n\n\n\nNonparametric Bootstrap\nMost traditional statistical tests rely on asymptotic normality. However, asymptotic normality can only be satisfied if we have a large enough sample size. When we are dealing with small samples, we cannot invoke central limit theorem.\nNonparametric Bootstrap, instead of assuming some sampling distribution, is a method to simulate the sampling distribution. This is done by re-sampling from the sample with replacement. The procedure is as follows:\n\nYou take the sample you observe (with sample size \\(n\\)), and randomly re-sample \\(n\\) observations from that sample with replacement (so allowing observations to repeat in our re-sample).\nContinue to do this over and over again to get \\(B\\) number of re-samples.\nFor each re-sample \\(b\\), you should calculate the \\(\\widehat{\\theta_b}\\). Plot all of the sample \\(\\widehat{\\theta_b}\\) in a distribution.\n\nYou can also estimate the standard error of \\(\\hat\\theta\\) using the standard deviation of the distribution. However, do not use these standard errors for confidence intervals or tests unless you are confident the sampling distribution is approximately normal.\nNonparametric Bootstrap is also used in some more complex estimators where it is very difficult to calculate or estimate the standard errors.\n\n\n\n\n\n\nOrdinary Least Squares Estimator\n\nDeriving the Estimator\nOur linear regression model, and the fitted values \\(\\hat{\\mathbf{y}}\\), take the following form:\n\\[\n\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u, \\qquad \\hat{\\mathbf y} = \\mathbf X \\hat{\\boldsymbol\\beta}\n\\]\nOLS wants to minimise the sum of squared residuals \\(S(\\hat{\\boldsymbol\\beta})\\) - the differences between the actual \\(\\mathbf y\\) and our predicted \\(\\hat{\\mathbf y}\\):\n\\[\n\\begin{align}\nS(\\hat{\\boldsymbol\\beta}) & = (\\mathbf y - \\hat{\\mathbf y})^\\mathsf{T} (\\mathbf y - \\hat{\\mathbf y})\\\\\n& = (\\mathbf y - \\color{blue}{\\mathbf X \\hat{\\boldsymbol\\beta}}\\color{black})^\\mathsf{T} (\\mathbf y - \\color{blue}{\\mathbf{X} \\hat{\\boldsymbol\\beta}}\\color{black}) && (\\text{plug in } \\color{blue}{\\hat{\\mathbf y}  = \\mathbf X \\hat{\\boldsymbol\\beta}}\\color{black}) \\\\\n& = \\mathbf y^\\mathsf{T} \\mathbf y - \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf y - \\mathbf y^\\mathsf{T} \\mathbf{X}\\hat{\\boldsymbol\\beta} + \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf{Xb} && (\\text{distribute out)} \\\\\n& = \\mathbf y^\\mathsf{T} \\mathbf y - \\color{blue}{2\\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf y}\\color{black} + \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf{X} \\hat{\\boldsymbol\\beta} &&(\\text{combine } \\color{blue}{- \\hat{\\boldsymbol\\beta}^\\mathsf{T} \\mathbf X^\\mathsf{T} \\mathbf y - \\mathbf y^\\mathsf{T} \\mathbf{X}\\hat{\\boldsymbol\\beta}}\\color{black})\n\\end{align}\n\\]\nNow, let us find the first order condition:\n\\[\n\\frac{\\partial S(\\hat{\\boldsymbol\\beta})}{\\partial \\hat{\\boldsymbol\\beta}} = -2\\mathbf X^\\mathsf{T} \\mathbf y + 2 \\mathbf X^\\mathsf{T} \\mathbf{X} \\hat{\\boldsymbol\\beta} = 0\n\\]\nWhen assuming \\(\\mathbf X^\\mathsf{T} \\mathbf X\\) is invertable (which is true if \\(\\mathbf X\\) is full rank), we can isolate \\(\\hat{\\beta}\\) to find the solution to OLS:\n\\[\n\\begin{align}\n-2\\mathbf X^T\\mathbf y + 2 \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol{\\hat{\\beta}} & = 0 \\\\\n2 \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol{\\hat\\beta} & = 2\\mathbf X^\\mathsf{T} \\mathbf y && (+ 2\\mathbf X^\\mathsf{T} \\mathbf y \\text{ to both sides}) \\\\\n\\boldsymbol{\\hat\\beta} & = (2\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} 2 \\mathbf X^\\mathsf{T} \\mathbf y && (\\times (2\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\text{ to both sides})\\\\\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y &&(\\text{cancel out } 2^{-1}\\times 2)\n\\end{align}\n\\]\nThose are our coefficient solutions to OLS.\n\n\n\nRegression Anatomy Theorem\nTake our multiple linear regression: \\(y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + ... + \\beta_k x_{ki} + u_i\\).\nLet us say we are interested in \\(x_1\\). Let us make \\(x_1\\) the outcome variable of a regression with explanatory variables \\(x_2, ..., x_k\\):\n\\[\nx_{1i} = \\gamma_0 + \\gamma_1 x_{2i} + ... + \\gamma_{k-1}x_{ki} + \\widetilde{r_{1i}}\n\\]\nThe error term \\(\\widetilde{r_{1i}}\\) is the part of \\(x_1\\) that cannot be explained by \\(x_2, ..., x_k\\).\nNow, take the regression of with outcome variable \\(y\\), with all explanatory variables except \\(x_1\\):\n\\[\ny_i = \\delta_0 + \\delta_1 x_{2i} + ... + \\delta_{k-1} x_{ki} + \\widetilde{y_i}\n\\]\nThe error term \\(\\widetilde{y_i}\\) is the part of \\(y_i\\) that cannot be explained by \\(x_2, ..., x_k\\). That implies \\(x_1\\) must be the one explaining \\(\\widetilde{y_i}\\). But, \\(x_1\\) may also correlated with \\(x_2, ..., x_k\\), and those correlated parts are already picked up in the regression coefficients of \\(x_2, ..., x_k\\). Thus, \\(\\widetilde{y_i}\\) must be explained by the part of \\(x_1\\) that is uncorrelated with \\(x_2, ..., x_k\\), which we derived earlier as \\(\\widetilde{r_{1i}}\\).\nThus, we can create another regression with explanatory variable \\(\\widetilde{x_{1i}}\\) and outcome variable \\(\\widetilde{y_i}\\):\n\\[\n\\widetilde{y_i} = \\alpha_0 + \\alpha_1 \\widetilde{r_{1i}} + u_i\n\\]\nWe plug \\(\\widetilde{y_i}\\) back into our regression of \\(y_i\\) with explanatory variables \\(x_2 ..., x_k\\):\n\\[\n\\begin{align}\ny_i & = \\delta_0 + \\delta_1 x_{2i} + ... + \\delta_{k-1} x_{ki} + \\widetilde{y_i} \\\\\ny_i & = \\delta_0 + \\delta_1 x_{2i} + ... + \\delta_{k-1} x_{ki} + \\alpha_0 + \\alpha_1 \\widetilde{r_{1i}} + u_i && (\\text{plug in } \\widetilde{y_i} = \\alpha_0 + \\alpha_1 \\widetilde{r_{1i}} + u_i)\\\\\ny_i  & = \\underbrace{(\\delta_0 + \\alpha_0)}_{\\beta_0} + \\underbrace{\\alpha_1 \\widetilde{r_{1i}}}_{\\beta_1 x_{1i}} + \\underbrace{\\delta_1x_{2i}}_{\\beta_2 x_{2i}} + ... + \\underbrace{\\delta_{k-1} x_{ki}}_{\\beta_kx_{ki}} + \\underbrace{u_i}_{u_i} && (\\text{rearrange})\n\\end{align}\n\\]\nThis new regression mirrors the original multiple linear regression. Importantly, we see the estimate of \\(\\alpha_1\\) will be the same as \\(\\beta_1\\) in the original regression. This coefficient explains the expected change in \\(y\\), given an increase in the part of \\(x_1\\) uncorrelated with \\(x_2, ..., x_k\\).\nSo essentially, we have partialed out the effect of the other explanatory variables, and only focus on the effect on \\(y\\) of the uncorrelated part of \\(x_1\\) (which is \\(\\widetilde{r_{1i}}\\)). This is what controlling for confounders is.\n\n\n\nOLS as an Unbiased Estimator\nOLS is an unbiased estimator of the relationship between any \\(x_j\\) and \\(y\\) under 4 conditions:\n\nLinearity in parameters: the model of the population (data generating process) can be modelled as \\(\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u\\).\nRandom Sampling: the observations in our sample are randomly sampled.\nNo Perfect Multicolinearity: There is no exact linear relationships between the regressors. This ensures that \\(\\mathbf X^\\mathsf{T} \\mathbf X\\) is invertible, which is required for the derivation of OLS.\nZero Conditional Mean: \\(E(\\mathbf u|\\mathbf X) = 0\\). This implies that no \\(x_j\\) is correlated with \\(\\mathbf u\\) (exogeneity), and no function of multiple regressors is correlated with \\(\\mathbf u\\).\n\nLet us prove OLS is unbiased - i.e. \\(E(\\hat{\\boldsymbol\\beta}) = \\boldsymbol\\beta\\). Let us manipulate our OLS solution:\n\\[\n\\begin{align}\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}\\color{blue}{(\\mathbf X \\boldsymbol\\beta + \\mathbf u)} && \\color{black}(\\text{plug in } \\color{blue}{\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u}\\color{black}) \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&(\\text{multiply out})\\\\\n& = \\color{blue}{\\mathbf I}\\color{black}{\\boldsymbol\\beta} + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&( \\ \\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X = \\mathbf I}\\color{black})\\\\\n& = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{identity property of } \\mathbf I)\n\\end{align}\n\\]\nNow, let us take the expectation of \\(\\boldsymbol{\\hat\\beta}\\) conditional on \\(\\mathbf X\\). Remember condition 4, \\(E(\\mathbf u | \\mathbf X) = 0\\):\n\\[\n\\begin{align}\nE(\\boldsymbol{\\hat\\beta}|\\mathbf X) & = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} E(\\mathbf u | \\mathbf X) &&(\\mathbf u \\text{ conditional on value of } \\mathbf X) \\\\\nE(\\boldsymbol{\\hat\\beta}|\\mathbf X) & = \\boldsymbol\\beta &&(E(\\mathbf u | \\mathbf X) = 0)\n\\end{align}\n\\]\nNow, we can use the law of iterated expectations (LIE) to conclude this proof:\n\\[\n\\begin{align}\nE(\\boldsymbol{\\hat\\beta}) & = E(E(\\boldsymbol{\\hat\\beta}|\\mathbf X)) && (\\text{LIE: E(X) = E(E(X|Y))})\\\\\n& = E(\\boldsymbol\\beta) && (\\text{LIE: E(X) = E(E(X|Y))})\\\\\n& = \\boldsymbol\\beta && (\\text{expecation of a constant})\n\\end{align}\n\\]\nThus, OLS is unbiased under the 4 conditions above.\n\n\n\nGauss-Markov Theorem\nThe Gauss-Markov Theorem states that the OLS estimator is the best linear unbiased estimator (BLUE) - the unbiased linear estimator with the lowest variance, under 5 conditions:\n\nLinearity (see unbiasedness conditions)\nRandom Sampling (…)\nNo Perfect Multicollinearity (…)\nZero-Conditional Mean (…)\nHomoscedasticity (the new condition).\n\nHomoscedasticity is when no matter the values of any explanatory variable, the error term variance is constant at \\(\\sigma^2\\). The error term variance does not change based on the values of the explanatory variables:\n\\[\nVar(\\mathbf u | \\mathbf X) = \\sigma^2 \\mathbf I_n = \\begin{pmatrix}\n\\sigma^2 & 0 & \\dots & 0 \\\\\n0 & \\sigma^2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & 0 \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\nVisualisation of Homoscedasticity\n\n\n\n\n\nAn easy way to identify homoscedasticity is to look at a residual plot (just the plot of all \\(\\widehat{u_i}\\)):\n\n\n\n\n\nNotice how the homoscedasticity residuals seem to have the same up-down variance, no matter the value of \\(x\\).\nThe heteroscedasticity residuals have a clear pattern - the up-down variance is smaller when \\(x\\) is smaller, and the up-down variance is larger when \\(x\\) is larger.\nEssentially, if you see a pattern in the residual plot, it is likely heteroscedasticity.\n\n\n\nThe Gauss-Markov Theorem is one of the main reasons we focus so heavily on the OLS estimator. If we believe our data-generating structure to be linear, then OLS is the best unbiased estimator we can use, since it has the lowest variance.\n\n\n\nDeriving Variance\nLet us assume homoscedasticity. We want to find the variance of our estimator, \\(Var(\\boldsymbol{\\hat\\beta} | \\mathbf X)\\). Let us start off with our OLS solution. We can simplify as follows:\n\\[\n\\begin{align}\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}\\color{blue}{(\\mathbf X \\boldsymbol\\beta + \\mathbf u)} && \\color{black}(\\text{plug in } \\color{blue}{\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u}\\color{black}) \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&(\\text{multiply out})\\\\\n& = \\color{blue}{\\mathbf I}\\color{black}{\\boldsymbol\\beta} + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&( \\ \\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X = \\mathbf I}\\color{black})\\\\\n& = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{identity property of } \\mathbf I)\n\\end{align}\n\\]\n\\[\nVar(\\boldsymbol{\\hat\\beta} | \\mathbf X) = Var(\\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u \\ | \\ \\mathbf X)\n\\]\n\\(\\boldsymbol\\beta\\) is a vector of fixed constants. \\((\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u\\) can be imagined as a matrix of fixed constants, since we are conditioning the above variance on \\(\\mathbf X\\) (so for each \\(\\mathbf X\\), the statement is fixed).\n\n\n\n\n\n\nMathematical Lemma\n\n\n\n\n\nIf \\(\\mathbf u\\) is an \\(n\\) dimensional vector of random variables, \\(\\mathbf c\\) is an \\(m\\) dimensional vector, and \\(\\mathbf B\\) is an \\(n \\times m\\) dimensional matrix with fixed constants, then the following is true:\n\\[\nVar(\\mathbf c + \\mathbf{Bu}) = \\mathbf B Var(\\mathbf u)\\mathbf B^\\mathsf{T}\n\\]\nI will not prove this lemma here, but it is provable.\n\n\n\nWith the Lemma above, and with the definition of homoscedasticity, we can simplify:\n\\[\n\\begin{align}\nVar(\\boldsymbol{\\hat\\beta} | \\mathbf X) & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} Var(\\mathbf u | \\mathbf X) [(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}]^{-1} && (\\text{lemma})\\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} Var(\\mathbf u | \\mathbf X) \\color{blue}{\\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1}} && \\color{black}( \\ \\color{blue}{[(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}]^{-1} = \\mathbf X(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1}}\\color{black})\\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\color{blue}{\\sigma^2 \\mathbf I_n}\\color{black}{ \\mathbf X} (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\color{blue}{Var(\\mathbf u | \\mathbf X) = \\sigma^2 \\mathbf I_n}\\color{black}) \\\\\n& =  \\color{red}{\\sigma^2} \\color{black} (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf I_n \\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\text{move scalar } \\color{red}{\\sigma^2}\\color{black})\\\\\n& =  \\sigma^2 (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}  \\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\text{identity property of } \\mathbf I_n)\\\\\n& =  \\sigma^2 (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} && (\\text{inverses } \\mathbf X^\\mathsf{T}  \\mathbf X (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\text{ cancel})\n\\end{align}\n\\]\nHowever, we do not actually know what \\(\\sigma^2\\) is. We can estimate it with \\(\\hat\\sigma^2\\) (discussed here).\nWe can use these standard errors (square root of variance) for hypothesis testing, if we believe homoscedasticity is met. If not, we will need to use robust standard errors, which we will not derive here. In modern econometrics, it has become more common to use robust standard errors by default, unless we can definitively prove homoscedasticity is met.\n\n\n\nAsymptotic Consistency of OLS\nOLS is an asymptotically consistent estimator of the relationship between any \\(x_j\\) and \\(y\\) under 4 conditions. These conditions are identical to the unbiasedness conditions EXCEPT condition 4, which is weakened from the original unbiasedness condition.\n\nLinearity (see unbiasedness)\nRandom Sampling (…)\nNo Perfect Multicolinearity (…)\nZero Mean and Exogeneity: \\(E(u_i) = 0\\), and \\(Cov(x_i, u_i) = 0\\), which implies \\(E(\\mathbf x_i u_i) = 0\\). This means that no regressor should be correlated with \\(\\mathbf u\\). This is weaker than Zero-Conditional mean, since it means a function of regressors can be correlated with \\(\\mathbf u\\).\n\nWe need condition 3 to ensure \\(\\mathbf X^\\mathsf{T} \\mathbf X\\) is invertible, in order to have OLS estimates. Once we have OLS estimates (derivation above), we can manipulate it as following:\n\\[\n\\begin{align}\n\\boldsymbol{\\hat\\beta} & = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf y \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T}\\color{blue}{(\\mathbf X \\boldsymbol\\beta + \\mathbf u)} && \\color{black}(\\text{plug in } \\color{blue}{\\mathbf y = \\mathbf X \\boldsymbol\\beta + \\mathbf u}\\color{black}) \\\\\n& = (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&(\\text{multiply out})\\\\\n& = \\color{blue}{\\mathbf I}\\color{black}{\\boldsymbol\\beta} + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u &&( \\ \\color{blue}{(\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf X = \\mathbf I}\\color{black})\\\\\n& = \\boldsymbol\\beta + (\\mathbf X^\\mathsf{T} \\mathbf X)^{-1} \\mathbf X^\\mathsf{T} \\mathbf u && (\\text{identity property of } \\mathbf I)\n\\end{align}\n\\]\n\n\n\n\n\n\nVector Notation\n\n\n\n\n\nThe following statements are true:\n\\[\n\\begin{split}\n& \\mathbf X^\\mathsf{T} \\mathbf X = \\sum\\limits_{i=1}^n \\mathbf x_i \\mathbf x_i^\\mathsf{T} \\\\\n& \\mathbf X^\\mathsf{T} \\mathbf  u = \\sum\\limits_{i=1}^n \\mathbf x_i u_i\n\\end{split}\n\\]\n\n\n\nUsing vector notation, law of large numbers, and zero-mean and exogeneity condition, we can simplify the above to:\n\\[\n\\begin{align}\n\\boldsymbol{\\hat\\beta} & = \\boldsymbol\\beta + \\left( \\sum\\limits_{i=1}^n \\mathbf x_i \\mathbf x_i^\\mathsf{T} \\right)^{-1} \\left( \\sum\\limits_{i=1}^n \\mathbf x_i \\mathbf u \\right) && (\\text{vector notation})\\\\\n\\boldsymbol{\\hat\\beta} & = \\boldsymbol\\beta + \\left( \\frac{1}{n}\\sum\\limits_{i=1}^n \\mathbf x_i \\mathbf x_i^\\mathsf{T} \\right)^{-1} \\left( \\frac{1}{n}\\sum\\limits_{i=1}^n \\mathbf x_i \\mathbf u \\right) && ( \\ \\left(\\frac{1}{n} \\right)^{-1} \\text{and } \\frac{1}{n} \\text{ cancel out}) \\\\\n\\text{plim} \\boldsymbol{\\hat\\beta} & = \\boldsymbol\\beta + \\left( \\text{plim} \\frac{1}{n}\\sum\\limits_{i=1}^n \\mathbf x_i \\mathbf x_i^\\mathsf{T} \\right)^{-1} \\left( \\text{plim} \\frac{1}{n}\\sum\\limits_{i=1}^n \\mathbf x_i u_i \\right) && (\\text{apply plim}) \\\\\n\\text{plim} \\boldsymbol{\\hat\\beta} & = \\boldsymbol\\beta + (E(\\mathbf x_i \\mathbf x_i^\\mathsf{T}))^{-1}E(\\mathbf x_i  u_i) && (\\text{law of large numbers})\\\\\n\\text{plim} \\boldsymbol{\\hat\\beta} & = \\boldsymbol\\beta && (E(\\mathbf x_i u_i) = 0)\n\\end{align}\n\\]\nThus, OLS is asymptotically consistent under the 4 conditions above.\n\n\n\nOLS as a Conditional Expectation Function\n\n\n\n\n\n\nConditional Expectation Functions\n\n\n\n\n\nA conditional expectation function (CEF) says that the value of \\(E(y)\\) depends on the value of \\(x\\). We notate a conditional expectation function as \\(E(y|x)\\). As we noted earlier, the linear regression model can be a conditional expectation function of \\(E(y|x)\\).\nA best linear approximation of a conditional expectation function can take the following form:\n\\[\nE(y_i|x_i) = b_0 + b_1x_i\n\\]\nWith parameters \\(b_0, b_1\\) that minimise the mean squared errors (MSE).\n\\[\n\\begin{split}\nMSE & = E(y_i - E(y_i|x_i))^2 \\\\\n& = \\frac{1}{n}\\sum\\limits_{i=1}^n( y_i - E(y_i|x_i))^2\n\\end{split}\n\\]\n\n\n\nOLS is a best-linear approximation of the conditional expectation function. Suppose we have the conditional expectation function, and its mean squared errors:\n\\[\n\\begin{align}\nE(y_i|x_i) & = b_0 + b_1x_i \\\\\nMSE & = E(y_i - E(y_i|x_i))^2 \\\\\n& =  E(y_i - \\beta_0 - \\beta_1x_i)^2\n\\end{align}\n\\]\nThe first order conditions are (using chain rule and partial derivatives):\n\\[\n\\begin{split}\n& E(y_i - b_0 - b_1x_i) = 0 \\\\\n& E(x_i(y_i - b_0 - b_1x_i) = 0\n\\end{split}\n\\]\nNow, recall our OLS minimisation conditions (simple linear regression):\n\\[\n\\begin{split}\n& \\sum\\limits_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\\n& \\sum\\limits_{i=1}^n x_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0\n\\end{split}\n\\]\nSince by definition, average/expectation is \\(E(x) = \\frac{1}{n} \\sum x_i\\), we can rewrite as:\n\\[\n\\begin{split}\n& n \\times E(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i) = 0 \\\\\n& n \\times E(x_i(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i)) = 0\n\\end{split}\n\\]\nAnd since anything multiplied to a zero turns into zero, we can ignore the \\(n\\) in the first order condition. Thus, our conditions are:\n\\[\n\\begin{split}\n& E(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i) = 0 \\\\\n& E(x_i(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i)) = 0\n\\end{split}\n\\]\nWhich as we can see, are the exact same minimisation conditions as the conditional expectation function. Thus, OLS is the best approximation of the conditional expectation function.\nThis property is very useful for causal inference, as it means OLS calculates the expected \\(y\\), which allows us to find causal effects by comparing the expected \\(y\\) of the treatment and control groups (assuming the OLS estimator is unbiased).\n\n\n\n\n\n\nMethod of Moments Estimator\n\nMethod of Moments\nThe Method of Moments Estimator is another estimator of the true value of populations in the parameter. The estimator defines key population moments of interest - which are the population parameters written in terms of expected value functions set equal to 0.\nThen, the Method of Moments uses the sample equivalents of the population moments to estimate the population parameter. For example, to estimate the population mean, the Method of Moments uses the sample mean.\nIn order to define a method of moments for a set of parameters \\(\\theta_1, \\dots, \\theta_k\\), we need to specify at least one population moment per parameter. Or in other words, we must have more than \\(k\\) population moments.\nOur population moments can be defined as the expected value of some function \\(m(\\theta; y)\\) that consists of both the variable \\(y\\) and our unknown parameter \\(\\theta\\). The expectation of the function \\(m(\\theta; y)\\) should equal 0.\n\\[\nE(m(\\theta; y)) = 0\n\\]\nOur sample moments will be the sample analogues of \\(\\theta\\) and \\(y\\), which are \\(\\hat\\theta\\) and \\(y_i\\):\n\\[\n\\frac{1}{n}\\sum\\limits_{i=1}^n m(\\hat\\theta; y_i) = 0\n\\]\nMethod of moments estimators are asymptotically consistent, because of the law of large numbers.\n\n\n\nPopulation Mean Estimator\nLet us say that we have some random variable \\(y\\), with a true population mean \\(\\mu\\). We want to estimate \\(\\mu\\), but we only have a sample of the population.\nHow can we define \\(\\mu\\) in a moment of the form: \\(E(m(\\mu, y)) = 0\\)? Well, we know \\(\\mu\\) is the expectation of \\(y\\), so \\(\\mu = E(y)\\). Since they are equal, \\(\\mu - E(y) = 0\\). Thus, we can define the mean as a moment of the following condition:\n\\[\nE(y - \\mu) = 0\n\\]\nThe method of moments estimator uses the sample equivalent of the population moment. The sample equivalent of \\(\\mu\\), is the sample mean \\(\\bar y\\):\n\\[\nE(y_i - \\hat\\mu) = \\frac{1}{n}\\sum\\limits_{i=1}^n (y_i - \\hat\\mu) = 0\n\\]\nWith this equation, we can then solve for \\(\\hat\\mu\\):\n\\[\n\\begin{align}\n0 & = \\frac{1}{n}\\sum\\limits_{i=1}^n (y_i - \\hat\\mu) \\\\\n0 & = \\frac{1}{n}\\sum\\limits_{i=1}^ny_i - \\frac{1}{n}\\sum\\limits_{i=1}^n \\hat\\mu  && (\\text{multiply out})\\\\\n0 & = \\frac{1}{n}\\sum\\limits_{i=1}^ny_i - \\frac{1}{n} n \\hat\\mu &&(\\text{summation property of constant } \\hat\\mu)\\\\\n0 & = \\bar y - \\hat \\mu && (\\text{definition of mean }\\frac{1}{n}\\sum\\limits_{i=1}^ny_i = \\bar y)\\\\\n\\hat\\mu & = \\bar y && (+\\hat\\mu\\text{ to both sides})\n\\end{align}\n\\]\nSo, we see the method of moments estimates our true population mean \\(\\mu\\), with the sample mean \\(\\bar y\\). As a method of moments estimator, it is also asymptotically consistent.\n\n\n\nOLS as a Method of Moments Estimator\nOLS is a special case of the Method of Moments Estimator. Consider the bivariate regression model. The OLS estimator can be derived as a method of moments estimator, with 2 moments (expectation functions set equal to 0), one for each parameter (\\(\\beta_0, \\beta_1\\)):\n\\[\n\\begin{split}\n& E(y-\\beta_0 -\\beta_1x) = 0 \\\\\n& E(x(y - \\beta_0 - \\beta_1 x)) = 0\n\\end{split}\n\\]\nThe estimates of these moments would use the sample equivalents: \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\).\n\\[\n\\begin{split}\n& E(y-\\hat\\beta_0 -\\hat\\beta_1x) = 0 \\\\\n& E(x(y - \\hat\\beta_0 - \\hat\\beta_1 x)) = 0\n\\end{split}\n\\]\nRemember our OLS minimisation conditions:\n\\[\n\\begin{split}\n& \\sum\\limits_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\\n& \\sum\\limits_{i=1}^n x_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0\n\\end{split}\n\\]\nSince by definition, average/expectation is \\(E(x) = \\frac{1}{n} \\sum x_i\\), we can rewrite the OLS minimisation conditions as:\n\\[\n\\begin{split}\n& n \\times E(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i) = 0 \\\\\n& n \\times E(x_i(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i)) = 0\n\\end{split}\n\\]\nAnd since anything multiplied to a zero turns into zero, we can ignore the \\(n\\) in the first order condition, and only focus on the expected value part. Thus, our conditions are:\n\\[\n\\begin{split}\n& E(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i) = 0 \\\\\n& E(x_i(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i)) = 0\n\\end{split}\n\\]\nWhich as we can see, are the exact same minimisation conditions as the method of moments estimator. Thus, the OLS estimator is a special case of the Method of Moments estimator, and they produce the same coefficients. This is an important property for the instrumental variables method that will be covered later.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "3 Classic Least Squares Theory"
    ]
  }
]