[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics for Political Science",
    "section": "",
    "text": "Preface\nThis is a collection of resources and notes on statistics for political science that I have collected throughout my undergraduate and postgraduate degrees.\nThe first part of this collection is regarding statistical theory of inference and estimators. We start with the some basic statistics in the form of distributions and random variables, as well as the basics of statistical inference. Then, we cover a series of estimators and the theory behind them. Finally, we cover some concepts in subfields of statistics, including causal identification and multivariate statistics.\nThe second part of the collection is more applied - showing how methods can be implemented for prediction and causal inference. We introduce different types of models, and how we can implement them with R code. If you are already comfortable with statistical theory, you can probably skip ahead to these applied chapters, and refer to the statistical theory chapters as references.\nFor the content in this guide, I assume some experience with basic statistics, such as a basic idea of sampling and statistical inference. I will review many of these concepts in the first two chapters. I also assume strong mathematical fundamentals, including calculus, linear algebra, and basic probability. There is some supplementary material provided in the appendix.\nI am continuously adding more to this collection as a learn more. Some parts may be incomplete, and you may see some changes in existing parts.\n\n\n\nNotation\nI will use a variety of notation in this guide. I will do my best to remain consistent with the notation throughout all chapters.\nFor common statistical operators, I denote them as following:\n\nProbability \\(\\P(\\cdot)\\).\nExpectation \\(\\E(\\cdot)\\).\nVariance \\(\\V(\\cdot)\\).\nCovariance \\(Cov(\\cdot, \\cdot)\\).\n\nRandom variables will typically be denoted by an uppercase letter, for example \\(X\\). A realisation of this variable (for example, in a sample), will be denoted with lowercase \\(x\\).\nI will use matrices and vectors frequently in this guide. Matrices are denoted with a bold capital letter \\(\\b X\\), and vectors are denoted with a bold lowercase letter \\(\\b x\\). Transposes will be denoted \\(\\b X^\\top\\).\nTo be very clear regarding probability density functions and cumulative density functions, I will always specify the variable in question as a subscript, for example, \\(f_Y(y)\\) and \\(F_Y(y)\\).\nFor indexes related to random variables and samples, I have made a few notational choices.\n\nFor observation of a sample/population, I will use index \\(t\\) to refer to a specific observation (Ex. \\(Y_t\\)), no matter if it is cross-sectional or time series data. This choice is consistent with Amemiya’s notation.\nFor number of parameters in a model, I will use \\(p\\) (ex. \\(X_p\\)), which is the standard in statistics. However, econometricians will often use \\(k\\) to refer to the number of parameters.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "random.html",
    "href": "random.html",
    "title": "1  Random Variables",
    "section": "",
    "text": "1.1 Random Variables\nA random variable has a set of potential outcomes \\(\\Omega\\), called the sample space. For example, if we are flipping a coin, the potential outcomes are either heads or tails, so \\(\\Omega = \\{H, T\\}\\). Only one outcome \\(\\omega\\) can be realised from this random variable at a time.\nThere are two types of random variables: discrete and continuous random variables.\nEvery outcome \\(\\omega \\in \\Omega\\) has some probability of being realised. For example, the probability of getting heads in a coin flip is 50%. We can express the probability of a certain outcome \\(\\omega\\) with a probability density function (pdf).\nFor discrete variables, we just plug in the outcome \\(y\\) we want the probability of. For continuous random variables, while the PDF does give you the probability of outcome \\(y\\), this isn’t as useful in context.\nFor example, let us say the random variable of how long will it take for me to get to school tomorrow? Do we really care about the probability that it will take exactly 5.372 minutes? No. What we care about is a range.\nThus when calculating probabilities for continuous random variables, we find the probability that random variable \\(Y\\) is somewhere between values \\(a\\) and \\(b\\).\n\\[\n\\P(Y \\in [a, b]) = \\int\\limits_a^bf_Y(y)dy\n\\tag{1.1}\\]\nAnother way to describe distributions is the cumulative distribution function (CDF). The CDF \\(F_Y(y)\\) finds the probability of an outcome equal or less to some value \\(y\\).\n\\[\nF(y) = \\P(Y≤y) = \\ \\underbrace{\\sum\\limits_{-∞}^y f_Y(y)}_{\\text{discrete}} \\ = \\ \\underbrace{\\int\\limits_{-∞}^yf_Y(y)dy}_{\\text{continuous}}\n\\tag{1.2}\\]",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "random.html#random-variables",
    "href": "random.html#random-variables",
    "title": "1  Random Variables",
    "section": "",
    "text": "Discrete Random VariablesContinuous Random Variables\n\n\nDiscrete random variables have a set of finite, distinct potential outcomes \\(\\Omega\\).\nRolling a die is a discrete random variable - you can only get outcomes 1, 2, 3, 4, 5, or 6. You cannot get outcome 3.5. There is only a discrete set of outcomes possible.\nFlipping a coin is a discrete random variable - you can only get outcomes heads (1) or tails (0). You cannot get 1.5.\n\n\nContinuous random variables are have an infinite number of outcomes \\(\\Omega\\) within a range.\nThe random variable how long will it take for me to get to school tomorrow is conintuous The outcome could be 5 minutes, could be 6 minutes, and could also be 5.461 minutes.\nTemperature is a random variable - it could be 5 degrees, 8 degrees, or 3.5738 degrees.\n\n\n\n\n\nDefinition 1.1 (Probability Density Function) For a random variable \\(Y\\), the probability density function \\(f_Y(y)\\) gives us the probability of a certain value \\(y\\) will be the outcome:\n\\[\nf_Y(y) = \\P(Y = y)\n\\]",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "random.html#expectation-and-moments",
    "href": "random.html#expectation-and-moments",
    "title": "1  Random Variables",
    "section": "1.2 Expectation and Moments",
    "text": "1.2 Expectation and Moments\nThe expectation of a random variable \\(Y\\), notated \\(\\E Y\\) or \\(\\mu_Y\\), is a measure for the centre or average of a random value. As the name implies, if we randomly draw an outcome \\(y\\) from a random variable \\(Y\\), the average outcome that we get will be \\(\\E Y\\).\n\nDefinition 1.2 (Expectation) For discrete random variables, the expectation is defined as the sum of every possible outcome \\(y_t\\) multiplied with their probability of occuring \\(f_Y(y_t)\\) (as given by the PDF):\n\\[\n\\E Y = \\mu_Y =  \\sum\\limits_t y_tf_Y(y_t)\n\\]\nFor continuous random variables, the expectation is given by\n\\[\n\\E Y = \\int\\limits_{-∞}^∞y f_Y(y)dy\n\\]\n\n\nExpected values also can be manipulated algebraically, as they are considered linear. This will show up in many proofs:\n\nTheorem 1.1 (Linearity of Expectations) Expectations are linear. This means that they can be added:\n\\[\n\\E (X+Y) = \\E X + \\E Y\n\\]\nAnd they can be multiplied with constants:\n\\[\n\\E(aX) = a  \\E X\n\\]\nThe expectation of a constant is the constant itself:\n\\[\n\\E (a) = a\n\\]\n\n\nExpectations can be used to define the raw moments of a random variable (also called raw moment). The \\(k\\)th moment of a random variable is defined as\n\\[\n\\mu_k' = \\E X^k\n\\]\nThe first raw moment \\(\\mu'_1\\) is the expected value of the random variable.\nThe central moment of an expected variable is defined as\n\\[\n\\bar\\mu_k = \\E[(X - \\E X)^k]\n\\tag{1.3}\\]",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "random.html#variance",
    "href": "random.html#variance",
    "title": "1  Random Variables",
    "section": "1.3 Variance",
    "text": "1.3 Variance\nWhile expectation measures the average/centre of a random variable, we might also want to know something about how spread out the outcomes of a random variable are.\n\nDefinition 1.3 (Variance) Variance is a measure of the spread of a random variable \\(Y\\), notated \\(\\V Y\\) or \\(\\sigma^2_Y\\). Mathematically speaking, it is the expected distance between each value of \\(Y\\) and \\(\\E Y\\) squared:\n\\[\n\\V Y = \\sigma^2_Y = \\E [(Y - \\E Y)^2]\n\\]\n\n\nVariance is also the second central moment \\(\\bar\\mu_2\\), as explained in eq. 1.3. Higher variance indicates a more spread out distribution, and lower variance indicates a less spread out distribution.\nVariance also has a useful algebraic property that we will see in many proofs:\n\nTheorem 1.2 (Algebraic Property of Variance) If \\(c\\) and \\(b\\) are constants, and \\(X\\) is a random variable, the following is true:\n\\[\n\\V(c + bX) = b^2 \\V(X)\n\\]\nWe can generalise this to linear algebra. If \\(\\b u\\) is an \\(n\\)-dimensional vector of random variables, and \\(\\b c\\) is a \\(m\\)-dimensional vector, and \\(\\b B\\) is an \\(m \\times n\\) matrix with fixed constants, then:\n\\[\n\\V(\\b c+ \\b{Bu}) = \\b B \\V(\\b u) \\b B^\\top\n\\]\n\n\nProof: From variance given in definition 1.3, we can expand \\(\\V(\\b c + \\b{BU})\\) as follows:\n\\[\n= \\E \\left[ (\\b c + \\b{Bu} - \\E(\\b c + \\b{Bu})) (\\b c + \\b{Bu} - \\E(\\b c + \\b{Bu}))^\\top \\right]\n\\tag{1.4}\\]\nWe know through the properties of expectation given in theorem 1.1, that the expectation of fixed constant vector \\(\\b c\\) and fixed constant matrix \\(\\b B\\) is themselves. Thus, we observe:\n\\[\n\\begin{align}\n\\b c + \\b{Bu} - \\E(\\b c + \\b{Bu}) & = \\b c + \\b{Bu} - (\\b c  - \\b B \\E\\b u) \\\\\n& = \\b c + \\b{Bu} - \\b c  + \\b B \\E\\b u \\\\\n& = \\b{Bu} + \\b B \\E \\b u \\\\\n& = \\b B(\\b u - \\E \\b u)\n\\end{align}\n\\tag{1.5}\\]\nSimilarly, we can use the same properties as above, as well as the algebraic properties of matrix transposes, to observe:\n\\[\n\\begin{align}\n(\\b c + \\b{Bu} - \\E(\\b c + \\b{Bu})^\\top & = (\\b B(\\b u - \\E \\b u))^\\top \\\\\n& = (\\b u - \\E \\b u)^\\top \\b B^\\top\n\\end{align}\n\\tag{1.6}\\]\nPlugging in eq. 1.5 and eq. 1.6 into eq. 1.4, we get:\n\\[\n\\V(\\b c + \\b{Bu}) = \\E[\\b B(\\b u - \\E \\b u)(\\b u - \\E \\b u)^\\top \\b B^\\top]\n\\]\nWe know from theorem 1.1 that the expectation of a fixed constant matrix \\(\\b B\\) is itself, so we can pull out \\(\\b B\\) and \\(\\b B^\\top\\) from the expectation to get\n\\[\n\\V(\\b c + \\b{Bu}) = \\b B \\E[(\\b u - \\E \\b u)(\\b u - \\E \\b u)^\\top ] \\b B^\\top\n\\]\nAnd by variance defined in definition 1.3, we can see that the expectation in the above equation is just \\(\\V (u)\\), so we can substitute that in to get\n\\[\n\\V(\\b c + \\b{Bu}) =  \\b B \\V (\\b u) \\b B^\\top\n\\]\nThus proving this theorem regarding variance.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "random.html#the-normal-distribution",
    "href": "random.html#the-normal-distribution",
    "title": "1  Random Variables",
    "section": "1.4 The Normal Distribution",
    "text": "1.4 The Normal Distribution\nThe normal distribution is a continuous random variable which has a probability density function.\n\nDefinition 1.4 (PDF of a Normal Distirbution) The PDF of a normal distribution is given by:\n\\[\nf_Y(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_Y}}e^{-\\frac{1}{2\\sigma^2}(y - \\mu_Y)^2}\n\\]\nWhere \\(\\mu_Y\\) is the expected value of the distribution and \\(\\sigma^2\\) is the variance.\n\n\nWe can see that the PDF \\(f_Y\\) is dependent on two parameters: the expected value \\(\\mu_Y\\), and the variance \\(\\sigma^2_Y\\). We often notate/define a normal distribution just by these parameters:\n\\[\nY \\sim \\mathcal N(\\mu_Y, \\sigma^2_Y)\n\\tag{1.7}\\]\nBelow illustrates how the PDF changes with different \\(\\mu_Y\\) and \\(\\sigma^2_Y\\) values:\n\n\n\n\n\nThe normal distribution can be altered algebraically while remaining a normal distribution.\n\nWe can add a constant \\(c\\) to all outcomes \\(\\omega \\in \\Omega\\) in our normal distribution \\(Y \\sim \\mathcal N(\\mu_Y, \\sigma^2_Y)\\), and obtain a new normal distribution \\(Z \\sim \\mathcal N(\\mu_Y + c, \\ \\sigma^2_Y)\\).\nWe can multiply a constant \\(c\\) to all outcomes \\(\\omega \\in \\Omega\\) in our normal distribution \\(Y \\sim \\mathcal N(\\mu_Y, \\sigma^2_Y)\\), and obtain a new normal distribution \\(Z \\sim \\mathcal N(c\\mu_Y, (c\\sigma_Y)^2)\\).\n\nThus, with any normal distribution \\(X\\), we can manipulate them to obtain another normal distribution \\(Z\\) with a mean of 0 and a variance of 1. This process is called standardisation:\n\\[\n\\mathrm{if } \\  X \\sim \\mathcal N(\\mu_X, \\sigma^2_X), \\quad Z = \\frac{X - \\mu_X}{\\sigma_X} \\sim \\mathcal N(0, 1)\n\\tag{1.8}\\]\nThis new normal distribution \\(Z \\sim \\mathcal N(0, 1)\\) is called the standard normal distribution. It is frequently used in statistics, so we have special symbols denoting its PDF, \\(\\varphi\\):\n\\[\n\\varphi(z) = f_Z(z) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}z^2}\n\\tag{1.9}\\]\nWe often also notate the CDF of a standard normal as \\(\\Phi(z) = F_Z(z)\\).",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "random.html#chi-squared-and-t-distribution",
    "href": "random.html#chi-squared-and-t-distribution",
    "title": "1  Random Variables",
    "section": "1.5 Chi-Squared and T-Distribution",
    "text": "1.5 Chi-Squared and T-Distribution\nLet us say that we have \\(p\\) number of random variables \\(Z_1, \\dots, Z_p\\), with all of these random variables being a standard normal \\(Z_t \\sim \\mathcal N(0, 1)\\). Now, the sum of their squares\n\\[\nW = Z_1^2 + Z_2^2 + \\dots + Z_p^2 = \\sum\\limits_{t=1}^pZ_t^2\n\\]\nis distributed according the the chi-squared \\(\\chi^2\\) distribution with \\(p\\) degrees of freedom:\n\\[\nW \\sim \\chi^2_p\n\\]\nDegrees of freedom \\(p\\) is the only parameter of the chi-squared distribution, and determines its shape and PDF. The figure below shows how the PDF changes as we change \\(p\\) (notated \\(k\\) below):\n\n\n\n\n\nNow, suppose two new random variables \\(Z \\sim \\mathcal N(0, 1)\\), and \\(W \\sim \\chi^2_p\\). Now the new random variable \\(T\\) defined as\n\\[\nT = \\frac{Z}{\\sqrt{W/p}}\n\\]\nis distributed according to the t-distribution \\(T \\sim t_p\\) with \\(p\\) degrees of freedom. The only parameter of the t-distribution is the degrees of freedom.\nThe shape of the t-distribution mirrors that of a standard normal distribution, but with slightly thicker tails and a smaller peak. As the degrees of freedom increases, the more and more the t-distribution mirrors a standard normal distribution. The PDF is illustrated below.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "random.html#bernoulli-and-binomial-distribution",
    "href": "random.html#bernoulli-and-binomial-distribution",
    "title": "1  Random Variables",
    "section": "1.6 Bernoulli and Binomial Distribution",
    "text": "1.6 Bernoulli and Binomial Distribution\nThe Bernoulli distribution is a discrete binary random variable, with only two potential outcomes \\(\\Omega = \\{0, 1\\}\\). Generally, we consider a value of 0 to be a failure, and a value of 1 to be a success.\nThe Bernoulli distribution only has one parameter, \\(p\\), the probability of a success. This also implies that the probability of a failure is \\(1-p\\). The PDF is\n\\[\nf_Y(y) = \\begin{cases}1-p & \\text{if } y=0 \\\\ p & \\text{if } y = 1\\end{cases}\n\\]\nFor example, flipping a coin can be a bernoulli distribution, if we consider heads to be a success and tails to be a failure. In this case, the probability of heads (success) is \\(p = 0.5\\).\nThe expected value of the bernoulli distribution can be calculated as seen in definition 1.2.\n\\[\n\\E Y  = \\sum\\limits_t y_t f_Y(y_t) = 0(1-p) + 1(p) = p\n\\tag{1.10}\\]\nAnd the variance of the bernoulli distribution is \\(\\V Y = p(1-p)\\).\nThe bernoulli distribution is a special case of the binomial distribution. The binomial distribution measures the probability of every number of successes after \\(n\\) number of trials.\nFor example, if \\(n=4\\), that would mean that if we consider flipping heads to be a sucesss and flipped a coin 4 times, the binomial distribution would give us the probability of 1 heads, 2 heads, 3 heads, and 4 heads. The PDF of a binomial distribution is:\n\\[\nf_Y(y) = \\frac{n!}{y!(n-y)!}p^y(1-p)^{n-y}\n\\]\nWe denote a binomial distribution as \\(X \\sim B(n, p)\\), where \\(n\\) is the number of trials, and \\(p\\) is the probability of a success in any trial. This also implies that our Bernoulli distribution (which is just a \\(n=1\\) trial binomial distribution) can be defined as \\(X \\sim B(1, p)\\).",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "random.html#joint-distributions",
    "href": "random.html#joint-distributions",
    "title": "1  Random Variables",
    "section": "1.7 Joint Distributions",
    "text": "1.7 Joint Distributions\nLet us say we have two random variables \\(X\\) and \\(Y\\). What is the probability that we get both some value \\(x\\) and \\(y\\)? This probability is given by the joint probability density function.\n\nDefinition 1.5 (Joint Probability Density Function) The joint PDF \\(f_{XY}\\) gives us the probability that random variable \\(X = x\\) and \\(Y = y\\):\n\\[\n\\P(X =x,\\ Y= y) = f_{XY}(x, y)\n\\]\n\n\nAs in eq. 1.1 for continuous random variables, we are not interested in the probability of a specific outcome \\(x\\) or \\(y\\), but rather, the probability of a range of outcomes \\(a ≤ X≤ b\\) and \\(c ≤ Y≤ d\\). This is given by:\n\\[\n\\P(X \\in [a,b], \\ Y \\in [c, d]) = \\int\\limits_a^b \\int\\limits_c^d f_{XY}(x,y)dy \\ dx\n\\]\nIf we are ever given only the joint probability distribution function \\(f_{XY}\\), and not the individual probability distribution functions \\(f_X\\) and \\(f_Y\\), we can still deduce the individual PDFs through the marginal distribution:\n\\[\nf_X(x) = \\int\\limits_{-∞}^∞ f_{XY}(x,y)dy, \\quad f_Y(y) = \\int\\limits_{-∞}^∞f_{XY}(x,y)dx\n\\tag{1.11}\\]\nThe discussion of multiple random variables leads to a discussion abou the relationship between two random variables.\n\nDefinition 1.6 (Independence) Two random variables \\(X\\) and \\(Y\\) are considered to be independent of each other \\(X \\ind Y\\) if the realisation of \\(X=x\\) has no impact on the probabilities of \\(Y= y\\). If two random variables are independent, then\n\\[\n\\P(X = x, \\ Y= y) = \\P(X= x) \\P(Y=y) \\quad \\text{if } X \\ind Y\n\\]\nThis implies that if \\(X \\ind Y\\), then the joint PDFs is also the product of the two independent PDFs:\n\\[\nf_{XY}(x, y) = f_X(x) f_Y(y)\n\\]\n\n\nWe might not only care about the probabilities of a specific \\(x\\) and \\(y\\) in a joint distribution. We might also care about how two random variables are associated with each other - which can be measured by covariance or correlation.\n\nDefinition 1.7 (Covariance) Covariance is defined as\n\\[\nCov(X, Y) = \\E[(X - \\E X)(Y - \\E Y)\n\\]\nWhere positive covariance indicates a positive relationship, negative covariance indicates a negative relationship, and 0 covariance indicates no relationship (independence).\n\n\nHowever, covariance is sensitive to measurement scale - i.e. if \\(X\\) was height, if we switched from metres to centimetres, our covariance would change values despite measuring the exact same concept.\nThus, correlation is a way to standardise and remove the effect of scale:\n\\[\nCorr(X, Y) = \\frac{Cov(X, Y)}{\\sqrt{\\V X \\V Y}}\n\\]\nSince covariance and correlation measure the relationship/association between two variables, that implies that if \\(Cov(X,Y) ≠ 0\\), then \\(X\\) and \\(Y\\) are not independent, and if \\(Cov(X, Y) = 0\\), then \\(X \\ind Y\\).",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "random.html#conditional-distributions",
    "href": "random.html#conditional-distributions",
    "title": "1  Random Variables",
    "section": "1.8 Conditional Distributions",
    "text": "1.8 Conditional Distributions\nLet us say we have two random variables \\(X\\) and \\(Y\\), however, \\(X\\) and \\(Y\\) are correlated, and not independent. This means that when \\(X\\) changes, \\(Y\\) changes as well.\nWhat this implies is that knowing \\(X\\) will give us some more indication on the distribution of \\(Y\\) - after all, if they are positively correlated, as \\(X\\) increases in value, we should expect \\(\\E Y\\) to also increase in value.\nThe conditional distribution of a random variable \\(Y\\) on \\(X\\), notated \\(Y|X\\), is the distribution of \\(Y\\), given a specific value \\(x\\) of \\(X\\). The conditional PDF is given by\n\\[\nf_{Y|X}(y |x) = \\frac{f_{XY}(x, y)}{f_X(x)}\n\\tag{1.12}\\]\nWe can also define the conditional distribution PDF in relation to the joint distributions:\n\\[\nf_{XY}(x,y) = f_X(x)f_{Y|X}(y|x) = f_Y(y) f_{X|Y}(x|y)\n\\tag{1.13}\\]\nSince conditional distirbutions are distributions, they have the same properties of normal distributions, and are able to be described by expectation and variance.\n\nDefinition 1.8 (Conditional Expectation) This conditional distribution also has some expected value \\(\\E(Y|X)\\). This is called a conditional expectation, and is given by\n\\[\n\\E(Y|X=x) = \\int\\limits_{-∞}^∞ y f_{Y|X}(y|x)dy\n\\]\n\nThe conditional expectation have a very interesting property, the law of iterated expectations, which will be used extensively for proofs later on.\n\nTheorem 1.3 (The Law of Iterated Expectations) For any two random variables \\(X\\) and \\(Y\\),\n\\[\n\\E[\\E(Y|X)] = \\E(Y)\n\\]\n\n\nProof: Let us start with the left side of the quation. Using definition 1.8, we can rewrite \\(\\E(Y|X)\\), and plug into the left side to get\n\\[\n\\E[\\E(Y|X)] = \\E\\left[ \\int y f_{Y|X}(y|x)dy\\right]\n\\]\nUsing definition 1.2, we can rewrite \\(\\E(\\cdot)\\), and find\n\\[\n= \\int \\left[ \\int y f_{Y|X}(y|x)dy\\right]f_X(x)dx\n\\]\nUsing eq. 1.13 and the properties of integrals, we can rewrite the above as\n\\[\n=\\int\\int yf_{XY}(x,y)dxdy\n\\]\nNow using the properties of integrals, we can get:\n\\[\n=\\int y \\left[\\int f_{XY}(x,y)dx\\right]dy\n\\]\nAnd using marginal distributions from eq. 1.11, and the definition of expectation given in definition 1.2, we get\n\\[\n= \\int y f_Y(y)dy = \\E Y\n\\]\nThus, we have proven the law of iterated expectations.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "inference.html",
    "href": "inference.html",
    "title": "2  Statistical Inference",
    "section": "",
    "text": "2.1 Data Generating Process\nLet us say we are interested in studying some variable \\(Y\\) in the population. To study \\(Y\\), we can imagine how \\(Y\\) was “generated” in the population through a model of the data generating process - how we believe the real values of \\(Y\\) in the population came to be.\nFor example, let us say we are interested in the variable height \\(Y\\) in the population (let us say in the UK). Let us (boldly, and for simplification) assume any individual \\(t\\)’s height is completely random and selected from a normal distribution. Then we can model the data generating process for any individual’s \\(Y_t\\) value as\n\\[\nY_t \\sim \\mathcal N(\\mu_Y, \\ \\sigma^2_Y)\n\\]\nThe parameters of this model are \\(\\b\\theta = (\\mu_Y, \\sigma^2_Y)\\), that determine the value of \\(Y_t\\). If we can find the values of \\(\\b\\theta\\), then we can explain how variable height \\(Y\\) works in the population.\nObviously, most variables \\(Y\\) in the real world are not completely random. For example, if \\(Y\\) is income, we might assume that another variable education \\(X\\) is associated with income \\(Y\\). In this scenario, we can model the data generating process as\n\\[\nY_t \\sim \\mathcal N( X\\beta, \\sigma^2_Y)\n\\]\nWhere \\(X\\beta\\) is now the expectation of the normal distribution. In this model, our parameters \\(\\theta\\) are now \\(\\b\\theta = (\\beta, \\sigma^2_Y)\\). This model states that as education \\(X\\) increases by 1, the expected \\(Y\\) value changes by \\(\\beta\\). This parameter \\(\\beta\\), if we can estimate it, will tell us the relationship between \\(X\\) and \\(Y\\).\nWe can make our data generating process more complex. Perhaps, we believe that multiple variables \\(X_1, X_2, \\dots, X_p\\) are correlated with \\(Y\\). We can model this as\n\\[\nY_t \\sim \\mathcal N(\\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p, \\ \\sigma^2_Y)\n\\]\nWhere now the parameters are \\(\\b\\theta = (\\beta_1, \\beta_2, \\dots, \\beta_p, \\sigma^2_Y)\\), and finding the values of these parameters will help us understand \\(Y\\) and its relationship with \\(X_1, \\dots, X_p\\).\nA lot of natural phenomena are normally distributed. However, we are not limited to the normal distribution: any distribution is possible. We can also change the functional form of the relationships (maybe instead of \\(\\beta X\\), the mean is described by \\(\\beta X^2\\)).\nWe choose the way we model our DGP based on what we know about \\(Y\\) from our own intuition and prior research. Once we have a model of the DGP, we will know that the value of \\(Y_i\\) depends on some parameters \\(\\b\\theta\\), for which we do not know the value of. The next step of statistics will be trying to figure out the values of \\(\\b\\theta\\).",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "inference.html#data-generating-process",
    "href": "inference.html#data-generating-process",
    "title": "2  Statistical Inference",
    "section": "",
    "text": "Definition 2.1 (Data Generating Process) The data generating process is the process in the real world that “generates” the values of variable \\(Y\\) for every individual \\(t = 1, 2, \\dots, n\\) in the population. We can model the data generating process of each individual’s \\(Y_i\\) value as some function of a vector of parameters \\(\\b\\theta\\).\n\\[\nY_t = f(\\b\\theta)\n\\]",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "inference.html#sampling-and-estimators",
    "href": "inference.html#sampling-and-estimators",
    "title": "2  Statistical Inference",
    "section": "2.2 Sampling and Estimators",
    "text": "2.2 Sampling and Estimators\nLet us say we model the DGP in the population as \\(Y_t = f(\\b\\theta)\\). We know that if we can figure out the values of parameters \\(\\b\\theta\\), then we can understand how \\(Y\\) works in our population. The true values of \\(\\b\\theta\\) in the population is called the estimand.\n\nDefinition 2.2 (Estimand) The estimand is \\(\\theta\\), the true population value of some parameter. This could be the true mean \\(\\mu_Y\\), the true variance \\(\\sigma^2_Y\\), or some true relationship \\(\\beta\\) between two variables from the population.\n\n\nHowever, a vector of estimands \\(\\b\\theta\\) is often not intuitive or easy to calculate. For example, let us say we are interested in people’s heights \\(Y\\) in the UK. For simplicity, assume a DGP of \\(Y_i \\sim \\mathcal N(\\mu_Y, \\sigma^2_Y)\\). We want to find \\(\\theta = \\mu_Y\\), the average height of all people in the UK. How can we do this?\nOf course, we could ask all 70 million people in the UK, and find the average. But clearly that would take an enormous amount of effort and resources to ask 70 million people. In some future applications we will discuss, the population is hypothetical, so asking the whole population is not even an option with unlimited resources.\nThe answer to this issue is sampling. We randomly take a subset of the population (let us say 1,000 people out of the 70 million), and then calculate the sample average height \\(\\bar Y\\).\nIf we believe that our sample is representative/similar to the population, then we might be able to say something about the population average height \\(\\mu_Y\\) from our sample average \\(\\bar Y\\). This procedure is called the estimator, and produces an estimate \\(\\hat\\theta_n\\) of our true population.\n\nDefinition 2.3 (Estimator and Estimates) The estimator is a procedure/process to turn sample data, into an estimate \\(\\hat\\theta_n\\) of our true population estimand \\(\\theta\\). For example, taking a sample and calculating the mean \\(\\bar Y\\) is a estimator of the true mean \\(\\mu_Y\\).\n\n\nA lot of statistics is about trying to find a good estimator in order to accurately (to the best of our ability) estimate the true population estimand \\(\\theta\\). In the next few sections, we will explore the properties of estimators, and determine how we determine what a good estimator is.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "inference.html#sampling-distributions",
    "href": "inference.html#sampling-distributions",
    "title": "2  Statistical Inference",
    "section": "2.3 Sampling Distributions",
    "text": "2.3 Sampling Distributions\nWe have taken a sample from the population, and using an estimator, produced a sample estimate \\(\\hat\\theta_n\\). However, there is an issue with our sample estimate \\(\\hat\\theta_n\\) that we have gotten: if we took another random sample, we would get different individuals in our sample due to randomness, which would result in a slightly different sample estimate \\(\\hat\\theta_m\\).\nNow, let us run a hypothetical thought experiment. We take 1 sample, and calculate our estimate \\(\\hat\\theta_1\\). We take a second sample, and calculate our estimate \\(\\hat\\theta_2\\), which is slightly different than the first. We then keep taking more and more samples (close to infinity number of samples), until we have estimates \\(\\hat\\theta_1, \\hat\\theta_2, \\hat\\theta_3, \\dots, \\hat\\theta_N\\).\nNow, we can plot our different sample estimates \\(\\hat\\theta_1, \\dots, \\hat\\theta_n\\) on a distribution. For example, if we return back to the average height in the UK example, each sample produces a different sample average height. We can plot them as follows:\n\n\n\n\n\nThis is called a sampling distribution, and illustrates the possible estimates \\(\\hat\\theta_n\\) we could get under repeated sampling. The actual form of the distribution (expected value, variance, shape) are determined by the estimator.\nNow, let us stop thinking in this hypothetical thought experiment, and return to the real world. In the real world, we are only going to take one random sample. But which sample in real-life did we get when sampling? Which \\(\\hat\\theta_n\\) is our specific sample estimate from this big distribution of potential sample estimates?\nWell, we can actually think of our individual sample estimate \\(\\hat\\theta_n\\) as a random variable, whose probabilities is based on this hypothetical sampling distribution. When we actually draw a sample and calculate \\(\\hat\\theta_n\\), we are essentially randomly selecting one of \\(\\hat\\theta_1, \\hat\\theta_2, \\dots, \\hat\\theta_N\\) from the sampling distribution.\nThis idea of our sample estimate \\(\\hat\\theta_n\\) being a random variable with a distribution defined by the sampling distribution also implies that the other properties of random variable distributions mentioned in the last chapter apply to sampling distributions. Sampling distributions have a mean, variance, and other properties that we will explore.\nSince sampling distributions are determined by our estimator (and its properties), we will often interchangeably switch between the terminology of estimators and sampling distributions.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "inference.html#finite-sample-properties",
    "href": "inference.html#finite-sample-properties",
    "title": "2  Statistical Inference",
    "section": "2.4 Finite Sample Properties",
    "text": "2.4 Finite Sample Properties\nAs we discussed above, our sample estimates \\(\\hat\\theta_n\\) are a random draw from a sampling distribution. We also mentioned that the shape, form, and parameters of the sampling distribution are a direct result of our estimator.\nWe will first look at finite sample properties - which apply to estimators/sampling distributions no matter the sample size of each sample. These properties of the sampling distributions/estimators include:\n\nUnbiasednessVarianceEfficiency\n\n\nWe know that our sampling distribution has some expectation, like any random variable. That means our estimates \\(\\hat\\theta_n\\) have some expected value. One property relating to the expectation of our estimator/sampling distribution is unbiasedness.\n\nDefinition 2.4 (Unbiased Estimator) An estimator is considered unbiased, if its estimates \\(\\hat\\theta_n\\) have an expectation that is equal to the value of the true population parameter.\n\\[\n\\E \\hat\\theta_n = \\theta\n\\]\n\n\nOr in other words, if we repeatedly sample and use an unbiased estimator to calculate our estimates \\(\\hat\\theta_n\\), on average, the estimates will be equal to the true population value \\(\\theta\\) of interest. In terms of the sampling distirbution, this means that the expectation (centre) of the sampling distribution is equal to the true population value \\(\\theta\\).\nWe want an unbiased estimator, because if \\(\\E\\hat\\theta_n = \\theta\\), then we know that our “expected” value of our estimator, our best guess of the estimator’s value, is indeed, the correct true population parameter \\(\\theta\\).\n\n\nJust like with random variables, we do not just care about the expected value of the estimator. We also care about its variance - how spread out/precise the individual estimates \\(\\hat\\theta_n\\) of an estimator are.\nFor example, let us say we have some true population parameter \\(\\theta = 0\\), and two estimators \\(A\\) and \\(B\\). \\(A\\) produces sample estimates -1 and 1, while \\(B\\) produces sample estimates -100 and 100.\nBoth estimators are unbiased, but, clearly, estimator \\(A\\)’s individual sample estimates are on average, much closer to the true \\(\\theta = 0\\). Thus, we care about the variance of the estimator.\n\nDefinition 2.5 (Variance of an Estimator) The variance of an estimator, which is also the variance of the sampling distribution, can be quantified as\n\\[\n\\V \\hat\\theta_n = \\E[(\\hat\\theta_n - \\E \\hat\\theta_n)^2]\n\\]\n\n\nIn an ideal world, we want an estimator that has the least variance of any unbiased estimator. This way, we know the estimator is on average correct, and that each individual estimate \\(\\hat\\theta_n\\) the estimator produces is not very far from the true population value of \\(\\theta\\).\n\n\nEfficiency is a measure of how close any specific realisation of the estimator \\(\\hat\\theta_n\\) is to the true population parameter \\(\\theta\\).\nWe typically measure efficiency with the Mean Squared Error (MSE) of an estimator:\n\\[\nMSE(\\hat\\theta_n) = \\E((\\hat\\theta_n - \\theta)^2) = \\V\\hat\\theta_n + \\underbrace{(\\E \\hat\\theta_n - \\theta)}_{\\text{bias}}\n\\]\nWhen judging between two estimators of \\(\\theta\\), we typically prefer the estimator with the smaller MSE. Estimator \\(\\hat\\theta_n^{(1)}\\) is considered to be more efficient that estimator \\(\\hat\\theta_n^{(2)}\\) if\n\\[\nMSE(\\hat\\theta_n^{(1)}) &lt; MSE(\\hat\\theta_n^{(2)})\n\\]\nIf both estimators are unbiased, the estimator that has the smaller variance is considered the more efficient one.\n\n\n\nAn estimator that is both unbiased, and has the least variance of any unbiased estimator, is called the best or most efficient unbiased estimator.\nWe typically want the most efficient unbiased estimators, as they are on average estimating the true population parameter \\(\\theta\\), and each individual estimate is the smallest distance from the true population parameter \\(\\theta\\).",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "inference.html#asymptotic-properties",
    "href": "inference.html#asymptotic-properties",
    "title": "2  Statistical Inference",
    "section": "2.5 Asymptotic Properties",
    "text": "2.5 Asymptotic Properties\nIn the previous section, we discussed properties that apply to estimators/sampling distributions no matter the sample size \\(n\\) (the size of each individual sample). That is what we call finite sample properties.\nHowever, we also are interested in asymptotic properties (also called large sample properties). Asymptotic properties are what happens to our estimators/sampling distributions when we increase sample size towards infinity \\(n \\rightarrow ∞\\).\nOne of the most important asymptotic properties are if our estimator is consistent/unbiased in asymptotic sample sizes.\n\nDefinition 2.6 (Asymptotic Consistency) An estimator is asymptotically consistent (or asymptotically unbiased), if as we increase sample size towards infinity, the estimate will become more and more concentrated around the true population value \\(\\theta\\). At \\(n=∞\\), we should expect our sample distribution to “collapse”, with only one potential outcome, the actual value of \\(\\theta\\) in the population.\n\\[\n\\P (|\\hat\\theta_n - \\theta| &gt; \\epsilon) \\rightarrow 0, \\quad \\mathrm{as} \\ n \\rightarrow ∞\n\\]\nWhat the above formula essentially states is that the difference between our estimate \\(\\hat\\theta_n\\) and the actual population \\(\\theta\\) is greater than some arbitrarily small value \\(\\epsilon\\) with 0 probability asymptotically.\n\n\nNote that an estimator can be biased in finite samples, but consistent/unbiased in asymptotic samples. This implies that as we increase our sample size, the bias of this type of estimator will decrease.\nOne of the principles establishing asymptotic consistency is the law of large numbers, which shows that the sample average estimator is asymptotically consistent.\n\nTheorem 2.1 (Law of Large Numbers) The law of large numbers says that the sample average of a random sample is a asymptotically consistent estimator of the population mean. In other words, if we have a sample of \\(x_1, \\dots, x_n\\), the sample mean \\(\\bar x_n\\) has the property\n\\[\n\\mathrm{plim}(\\bar x_n) = \\mu_X\n\\]\nWhere \\(\\mathrm{plim}\\) stats that as \\(n \\rightarrow ∞\\), the probability distribution of the sample estimator \\(\\bar x_n\\) collapses around the true mean \\(\\mu_X\\).\n\n\nProof: Let us restate the sample mean estimator by its formula (definition of average):\n\\[\n\\bar x_n = \\frac{1}{n}\\sum\\limits_{t=1}^n x_t\n\\]\nNow, let us find the variance of this sample estimator \\(\\bar x_n\\).\n\\[\n\\V \\bar x_n = \\V \\left( \\frac{1}{n}\\sum\\limits_{t=1}^n x_t \\right)\n\\]\nUsing the properties of variance in theorem 1.2, we establish that the above is\n\\[\n\\V \\bar x_n= \\left(\\frac{1}{n}\\right)^2 \\V \\left(\\sum\\limits_{t=1}^n x_t \\right)\n\\]\nLet us define the variance of our sample \\(x_1 \\dots, x_n\\) as \\(\\V x_t = \\sigma^2\\). We get\n\\[\n\\V \\bar x_n = \\frac{1}{n^2} n\\sigma^2 = \\frac{1}{n}\\sigma^2\n\\tag{2.1}\\]\nAnd if sample size \\(n \\rightarrow ∞\\), we have a limit:\n\\[\n\\lim\\limits_{n \\rightarrow ∞} \\V \\bar x_n = \\lim\\limits_{n \\rightarrow ∞} \\frac{1}{n}\\sigma^2 = 0\n\\]\nThus, we can see the variance becomes 0, and the estimator’s sampling distribution collapses to a single value. This law will be used frequently in proving asymptotic properties, since many different parameters (such as variance and covariance for example) involve expectations/averages.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "inference.html#central-limit-theorem",
    "href": "inference.html#central-limit-theorem",
    "title": "2  Statistical Inference",
    "section": "2.6 Central Limit Theorem",
    "text": "2.6 Central Limit Theorem\nAside from the expected value and variance of a sampling distribution, we also want to know what shape/form the sampling distribution is taking.\nAfter all, if we can define a PDF for the sampling distribution, we can figure out the probabilities of getting certain \\(\\hat\\theta_n\\) estimates, which is a key part of hypothesis testing (shown later in the chapter).\nThe central limit theory provides a way for us to know the form of the sampling distribution.\n\nTheorem 2.2 (Central Limit Theorem) Let \\(Z_1, \\dots, Z_n\\) be independent and identically distributed random variables that when realised, produce a sample \\(z_1, \\dots, z_n\\). Let us say the true mean \\(\\E Z_i = \\mu_Z\\), and the true variance \\(\\sigma^2_Z = \\V Z_i\\).\nConsider the sample average \\(\\bar z_n\\), and its variance \\(\\V \\bar z_n = \\sigma^2_Z/n\\) (from eq. 2.1). Now, let us define a new variable \\(W_n\\), which is \\(\\bar z_n\\) standardised (from eq. 1.8). The central limit theorem states that \\(w_n\\) is normally distributed as sample size \\(n \\rightarrow ∞\\):\n\\[\nW_n  = \\frac{\\bar z_n - \\mu}{\\sigma_Z / \\sqrt{n}} \\sim \\mathcal N(0, 1) \\quad \\mathrm{as} \\ n \\rightarrow ∞\n\\]\nThis property holds no matter the original distribution of \\(Z_i\\) or \\(\\bar z_n\\).\n\n\nThe central limit theorem says that as our sample size becomes larger and larger, our sample means estimator \\(\\bar z_n\\)’s standardised version \\(W_n\\) will be standardly normally distributed.\nIf we recall previously, the standard normal distribution has a probability density function of:\n\\[\n\\varphi(z) = f_Z(z) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}z^2}\n\\]\nThis means that if we have a sufficiently large sample size such that we can invoke the central limit theorem, we can calculate the exact probability of our sample estimate \\(\\hat\\theta_n\\) being the outcome of the random variable of the sampling distribution, by using the probability density function defined in definition 1.1.\nThis ability to calculate the probability of our realised sample estimate \\(\\hat\\theta_n\\) allows us to conduct statistical inference with hypothesis testing, which we will introduce below.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "inference.html#hypothesis-testing",
    "href": "inference.html#hypothesis-testing",
    "title": "2  Statistical Inference",
    "section": "2.7 Hypothesis Testing",
    "text": "2.7 Hypothesis Testing\nScience is about disproving a status-quo belief about a parameter \\(\\theta\\), and proving a new theory. We call the status-quo belief the null hypothesis \\(H_0\\), and our new theory the alternate hypothesis \\(H_1\\).\nFor example, your status quo theory might be that there is no relationship between education and income \\(\\theta = 0\\). Your alternate hypothesis is that there is a relatinoship between education and income \\(\\theta ≠ 0\\).\nHow can we “disprove” the status quo theory and conclude our alternate hypothesis is correct? What we essentially do is first assume that the null hypothesis \\(H_0\\) is true. We then gather a sample, and create a sample estimate \\(\\hat\\theta_n\\). We then calculate the probability of getting a sample estimate \\(\\hat\\theta_n\\), assuming that the null hypothesis is true.\nIf the probability of getting our specific sample estimate \\(\\hat\\theta_n\\) is very low, we typically conclude that assuming the status quo theory \\(H_0\\) is incorrect, and our alternative hypothesis \\(H_1\\), our \\(\\hat\\theta_n\\) would actually be more plausible.\nTo run a hypothesis test of this sort, we take a series of steps. First, we define our null and alternate hypotheses:\n\nNull hypothesis \\(H_0 : \\theta = \\theta_0\\).\nAlternate hypothesis \\(H_1 : \\theta ≠ \\theta_0\\)\n\nSecond, we have to calculate our test statistic. The actual test statistic (t-statistic, z-statistic, wald statistic) used depends on our estimator/sampling distribution. We will specify which test statistic to use for every estimator when we introduce specific estimators. However, every test statistic is calculated in the same way:\n\\[\n\\mathrm{test \\ statistic} = \\frac{\\hat\\theta_n - \\theta_0}{se(\\hat\\theta_n)}\n\\]\nWhere \\(\\hat\\theta_n\\) is our sample estimate, \\(\\theta_0\\) is the null hypothesis belief of the true value of \\(\\theta\\), and \\(se(\\hat\\theta_n)\\) is the standard deviation of our sampling distribution (square root of variance). The test statistic is measuring the distance in standard errors between \\(\\hat\\theta_n\\) and \\(\\theta_0\\).\nThen, once we have a test statistic, we go to a standard normal distribution (or t-distribution or \\(\\chi^2\\) distribution, depending on the test). We start from the expectation of this distribution, then go test-statistic-number-of-units to both sides of the expectation. We then highlight the areas further from the expectation on both sides. The figure below illustrates this with a test statistic of 2.228.\n\n\n\n\n\nNow, we calculate the probability of the highlighted areas using the probability density function of the distribution we are using. This step is typically done with a calculator/computer since integrals are complex to compute.\nThe probability we get is called the p-value. The p-value describes the probability of getting a test statistic as or more extreme than the one we got, assuming the null hypothesis \\(H_0: \\theta = \\theta_0\\) is true.\nIf the p-value is below 0.05 (5%), we believe that it is unlikely that the null hypothesis is true, so we reject the null hypothesis \\(H_0\\) and conclude our alternate hypothesis \\(H_1\\) is true. If the p-value is above 0.05 (5%), we have no evidence to reject the null hypothesis \\(H_0\\), so we still remain with the status quo \\(H_0\\).\nNote that you do not need to choose a 5% significance level - although that is the standard in many social science field and computer software.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "inference.html#nonparametric-bootstrap",
    "href": "inference.html#nonparametric-bootstrap",
    "title": "2  Statistical Inference",
    "section": "2.8 Nonparametric Bootstrap",
    "text": "2.8 Nonparametric Bootstrap\nIn theorem 2.2, we discussed how the central limit theorem allows us to know that in large sample sizes, our standardised sampling distribution will be normally distributed. However, what if we have a small sample size? After all, it is not easy to collect a large sample.\nBootstrap methods are a method to “simulate” the sampling distribution. Essentially, we start with some sample of size \\(n\\) with values \\(z_1, \\dots, z_n\\). This original sample gets us some estimate \\(\\hat\\theta_n\\).\nNow, to simulate a sampling distribution, what we do is to sample with replacement from our original sample \\(z_1, \\dots, z_n\\). This will create a new bootstrap sample \\(b_1\\) of size \\(n\\).\nWhen we sample with replacement, we allow for observations to be sampled multiple times into our bootstrap sample. For example, our new bootstrap sample might take the form \\(z_1, z_2, z_2, z_2, z_3, \\dots, z_n\\).\nUsing this bootstrap sample \\(b_1\\), we can calculate a sample estimate \\(\\hat\\theta_1\\). Now, let us repeat the process, and create multiple bootstrap samples \\(b_1, b_2, \\dots, b_B\\). For each, we calculate \\(\\hat\\theta_b\\), such that by the end of this process, we have \\(\\hat\\theta_1, \\hat\\theta_2, \\dots, \\hat\\theta_B\\).\nNow, we have basically simulated a sampling distribution. The benefit of this method is that we don’t need to rely on some fancy statistical theories like the central limit theorem to conclude the shape of our sampling distribution.\nThe standard error from bootstraping is:\n\\[\nse_B(\\hat\\theta) = \\sqrt{\\frac{1}{B-1} \\sum\\limits_{b=1}^B (\\hat\\theta_b - \\E \\hat\\theta_b})\n\\]\nThe downside of bootstrap is that it is fully reliant on our original sample being reflective of the population. If our original sample has some issues that misrepresent the population, the bootstrapped sampling distribution may be flawed.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "ols.html",
    "href": "ols.html",
    "title": "4  Least Squares Theory",
    "section": "",
    "text": "4.1 The Classical Linear Model\nA class of estimators called least squares estimators is another way to estimate population parameters \\(\\theta\\) besides MLE. However, this class of estimators can only be applied in one specific model: the classical linear model.\nLet us say there are individuals \\(t = 1, 2, \\dots, n\\) in the population. Each individual’s \\(Y\\) value is determined by a set of random variables \\(Y_1, \\dots, Y_n\\). Any individual random variable \\(Y_t\\) is data generating process defined as \\(Y_t \\sim \\set D(\\mu_Y, \\sigma^2_Y)\\), where \\(\\set D\\) represents any distributional form, \\(\\mu_Y\\) is the mean of the random variable, and \\(\\sigma^2_Y\\) is the variance of the random variable.\nThe classical linear model is a specification that the mean \\(\\mu_Y\\) of \\(Y_t\\) is linearly determined by a set of explanatory variables \\(\\set X = \\{X_1, \\dots, X_p\\}\\):\n\\[\n\\E(Y_t|\\set X_t) = \\beta_0 + \\beta_1 X_{1t} + \\beta_2 X_{2t} + \\dots + \\beta_p X_{pt}\n\\]\nWhere \\(\\beta_0, \\dots, \\beta_p\\) are a set of population parameters (that need to be estimated) that determine how \\(\\mu_Y\\) changes in respect to explanatory variables \\(\\set X\\).\nYou will frequently see the linear model represented in another form:\n\\[\nY_t = \\underbrace{\\beta_0 + \\beta_1 X_{1t} + \\dots + \\beta_p X_{pt}}_{\\E(Y_t | \\set X_t)} + \\eps_t, \\quad \\eps_t \\sim \\set D(\\mu_{\\eps_t} = 0, \\sigma^2_{\\eps_t})\n\\]\nWhere the error term \\(\\eps_t\\) represents the variance/randomness in our data generating process, and the rest of the model represents \\(\\mu_y\\).\nWe know that this data generating process applies for random variables \\(Y_1, Y_2, \\dots, Y_n\\). To represent all random variables together, we use the matrix representation of the linear model:\n\\[\n\\b y = \\b{X\\beta} + \\b\\eps \\iff \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\\end{pmatrix} =\n\\begin{pmatrix} 1 & x_{11} & \\dots & x_{1p} \\\\\n1 & x_{21} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\dots & \\vdots \\\\\n1 & x_{n1} & \\dots & x_{np}\\end{pmatrix}\n\\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p\\end{pmatrix} +\n\\begin{pmatrix} \\eps_1 \\\\ \\eps_2 \\\\ \\vdots \\\\ \\eps_n \\end{pmatrix}\n\\]\nThe classical linear model has a set of five assumptions:",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#the-classical-linear-model",
    "href": "ols.html#the-classical-linear-model",
    "title": "4  Least Squares Theory",
    "section": "",
    "text": "Linearityi.i.d.Linear IndependenceExogeneitySpherical Errors\n\n\n\nDefinition 4.1 (Linearity in Parameters) A model is linear in parameters if it can be written in the form \\(\\b y = \\b{X\\beta} + \\b \\eps\\). Or in other words, all parameters \\(\\beta_0, \\dots, \\beta_p\\) must be either added or subtracted from each other.\n\nIt is important to note that linearity in parameters is not equal to linearity in the classical sense. For example, the model\n\\[\nY_t = \\beta_0 + \\beta_1X_t + \\beta_2 X_t^2 + \\beta_3X_t^3 + \\eps_t\n\\]\nwould be considered linear in parameters because all parameters \\(\\beta_0, \\dots, \\beta_p\\) are added together, not multiplied. It does not matter if the \\(X\\) are linear or not.\n\n\n\nDefinition 4.2 (Independent and Identically Distributed) A series of random variables \\(Y_1, Y_2, \\dots, Y_n\\) is considered to be independent and identically distributed if each random variable is independent (definition 1.6), and each random variable has the exact same distribution.\n\nThis assumption is often not met in its purest form - since when we randomly sample \\(Y_t\\) without replacement, technically the next observation \\(Y_{t+1}\\) will have slightly different distribution, since we took one observation out already.\nThis assumption is also frequently violated in time-series data, where \\(Y_t\\) at time \\(t\\) is likely to have an effect on \\(Y_{t+1}\\) at time \\(t+1\\).\n\n\n\nDefinition 4.3 (Linear Independence) Also called non-perfect multicollinearity, this assumption means that no explanatory variables \\(X_1, \\dots, X_p\\) can be perfectly correlated with any other explanatory variable, or perfectly correlated with any linear combination of other explanatory variables.\n\nThis assumption is required for estimation to be possible with the ordinary least squares estimator, as it requires matrix \\(\\b X\\) to be full rank which allows \\(\\b X^\\top \\b X\\) to be invertable.\n\n\n\nDefinition 4.4 (Strict Exogeneity) Strict Exogeneity is \\(\\E(\\b\\eps | \\b X) = 0\\). This implies that \\(\\E(\\b X^\\top \\b\\eps) = 0\\), which means all regressors \\(X_1, \\dots, X_p\\) should be uncorrelated with the error terms \\(\\eps\\), and any linear combination of \\(X_1, \\dots, X_p\\) should be uncorrelated with the error term.\n\nThere is also a weaker form of exogeneity, called weak exogeneity:\n\nDefinition 4.5 (Weak Exogeneity) Weak exogeneity is defined as \\(\\E(\\b x_t \\eps_t) = 0\\). Weak exogeneity only requires that regressors \\(X_1, \\dots, X_p\\) individually are uncorrelated with the error term. Weak exogeneity allows for combinations of \\(X_1, \\dots, X_p\\) to to be correlated with \\(\\eps\\), which is not allowed under strict exogeneity.\n\n\n\n\nDefinition 4.6 (Spherical Errors) The spherical errors assumption states that the covariance matrix of errors \\(\\eps_t\\) takes the form:\n\\[\n\\V(\\b\\eps|\\b X) = \\sigma^2 \\b I_n = \\begin{pmatrix}\n\\sigma^2 & 0 & 0 & \\dots \\\\\n0 & \\sigma^2 & 0 & \\dots  \\\\\n0 & 0& \\sigma^2 & \\vdots \\\\\n\\vdots & \\vdots & \\dots & \\ddots\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\nVariance-Covariance Matrix of Errors\n\n\n\n\n\nThe variance-covariance matrix of errors \\(\\eps_t\\) takes the form:\n\\[\n\\V(\\b\\eps | \\b X) = \\b\\Omega = \\begin{pmatrix}\n\\V \\eps_1 & Cov(\\eps_1, \\eps_2) & Cov(\\eps_1, \\eps_3) & \\dots \\\\\nCov(\\eps_2, \\eps_1) & \\V \\eps_2 & Cov(\\eps_2, \\eps_3) & \\dots \\\\\nCov(\\eps_3, \\eps_1) & Cov(\\eps_3, \\eps_2) & \\V\\eps_3 & \\vdots \\\\\n\\vdots & \\vdots & \\dots & \\ddots\n\\end{pmatrix}\n\\]\n\n\n\nSpherical errors implies two things:\n\nNo Autocorrelation means that the covariance of any two error terms \\(Cov(\\eps_i, \\eps_j) = 0\\). This is reflected in spherical errors with all the 0’s in the non-diagonal positions.\nHomoscedasticity means that every \\(\\eps_t\\) for any observation \\(t\\) has the same variance \\(\\sigma^2\\). The variance of the error \\(\\eps_t\\) does not depend on the values of \\(X_1, \\dots, X_p\\). If this condition is violated, as in each observation \\(t\\) has their own \\(\\sigma^2_t\\), we call this homoscedasticity.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#ordinary-least-squares",
    "href": "ols.html#ordinary-least-squares",
    "title": "4  Least Squares Theory",
    "section": "4.2 Ordinary Least Squares",
    "text": "4.2 Ordinary Least Squares\nOLS is an estimation process that finds the values \\(\\hat{\\b\\beta}\\) by the \\(\\hat{\\b\\beta}\\) values that minimise the sum of squared residuals (SSR) (also called the sum of squared errors), the difference between \\(\\b y\\) and predicted \\(\\hat{\\b y}\\) squared, where \\(\\hat{\\b y}\\) is defined as \\(\\hat{\\b y} = \\b X \\hat{\\b\\beta}\\).\n\nDefinition 4.7 (Sum of Squared Residuals) We will define the SSR by function \\(S(\\hat{\\b\\beta})\\):\n\\[\nS(\\hat{\\b\\beta}) = \\sum\\limits_{i=1}^n (Y_i - \\hat Y_i)^2 \\ = \\ (\\b y - \\hat{\\b y})^\\top (\\b y - \\hat{\\b y})\n\\]\n\n\n\n\n\n\n\nWhy Sum of Squared Residuals?\n\n\n\n\n\nOne common question is why are we squaring the residuals (difference between \\(\\b y\\) and \\(\\hat{\\b y}\\)). The main reason is that squaring gets rid of negative and positive residuals, which might cancel each other out. We do not care about the direction of residuals/errors, only the magnitude.\nThe reason we choose to square the residuals, and not to use absolute value, is because of a variety of unique properties of OLS (unbiasedness, variance, efficeincy) that we will explore throughout this chapter.\n\n\n\nWe know that predicted \\(\\hat{\\b y} = \\b X \\hat{\\b \\beta}\\). Thus, let us plug that into \\(S(\\hat{\\b\\beta})\\), and simplify to get:\n\\[\n\\begin{align}\nS(\\hat{\\b\\beta}) & = (\\b y - \\b X \\hat{\\b\\beta})^\\top (\\b y - \\b X \\hat{\\b\\beta}) \\\\\n& = \\b y^\\top \\b y - \\hat{\\b\\beta}^\\top \\b X^\\top \\b y - \\b y^\\top \\b X \\hat{\\b\\beta} +  \\hat{\\b\\beta}^\\top \\b{X^\\top X} \\hat{\\b\\beta} \\\\\n& =  \\b y^\\top \\b y - 2 \\hat{\\b\\beta}^\\top \\b{X^\\top y} + \\hat{\\b\\beta}^\\top \\b{X^\\top X} \\hat{\\b\\beta}\n\\end{align}\n\\]\nNow, we want to maximise in respect to \\(\\hat{\\b\\beta}\\), so let us take the gradient of function \\(S\\) in respect to \\(\\hat{\\b\\beta}\\), and set it equal to 0. Then, we can solve for \\(\\hat{\\b\\beta}\\) to get our parameter estimates:\n\\[\n\\begin{align}\n\\frac{\\partial S}{\\partial \\hat{\\b\\beta}} = -2 \\b{X^\\top y} + 2 & \\b{X^\\top X} \\hat{\\b\\beta} = 0 \\\\\n2 \\b{X^\\top X}\\hat{\\b\\beta} & = 2 \\b{X^\\top y} \\\\\n\\hat{\\b\\beta} & = (2 \\b{X^\\top X})^{-1}2 \\b{X^\\top y} \\\\\n\\hat{\\b\\beta} & =  (2^{-1})2(\\b{X^\\top X})^{-1}\\b{X^\\top y} \\\\\n\\hat{\\b\\beta} & =  (\\b{X^\\top X})^{-1}\\b{X^\\top y}\n\\end{align}\n\\]\n\nDefinition 4.8 (OLS Estimate) The OLS estimate of parameters \\(\\b\\beta\\) is:\n\\[\n\\hat{\\b\\beta}_{\\mathrm{OLS}} =  (\\b{X^\\top X})^{-1}\\b{X^\\top y}\n\\]\n\n\n\n\n\n\n\nNon-Matrix Derivation for Simple Linear Regression\n\n\n\n\n\nFor simple linear regression, our sum of squared errors (definition 4.7) is:\n\\[\nS(\\hat\\beta_0, \\hat\\beta_1) = \\sum\\limits_{t=1}^n (Y_t - \\hat\\beta_0 - \\hat\\beta_1 X_t)^2\n\\]\nOur first order conditions by taking the partial derivative in respect to \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are:\n\\[\n\\begin{align}\n& \\frac{\\partial S}{\\partial \\hat\\beta_0} = \\sum\\limits_{i=1}^n (Y_t -\\hat\\beta_0 = \\hat\\beta X_t) = 0 \\\\\n& \\frac{\\partial S}{\\partial \\hat\\beta_1} = \\sum\\limits_{i=1}^n X_t (Y_t -\\hat\\beta_0 = \\hat\\beta X_t) = 0 \\\\\n\\end{align}\n\\]\nAnd the final solutions for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) after solving this system of equations is:\n\\[\n\\begin{align}\n& \\hat\\beta_0 = \\bar Y - \\hat\\beta_1 \\bar X \\\\\n& \\hat\\beta_1 = \\frac{\\sum (X_t -\\bar X)(Y_t -\\bar Y)}{\\sum (X_t - \\bar X)^2} = \\frac{Cov(X, Y)}{\\V Y}\n\\end{align}\n\\]\n\n\n\nOLS has some unique properties, that we will explore in the tabs below.\n\nP and M MatrixOLS ProjectionR-SquaredRegression Anatomy\n\n\nRecall our predictions \\(\\hat{\\b y} = \\b X \\hat{\\b\\beta}\\). Using our OLS solution, we can find\n\\[\n\\hat{\\b y} = \\b X (\\b{X^\\top X})^{-1} \\b{X^\\top y} = \\color{red}{\\b P}\\color{black}{\\b y}\n\\]\n\nDefinition 4.9 (Projection Matrix) Let us define the projection matrix \\(\\b P\\) as:\n\\[\n\\color{red}{\\b P}\\color{black} := \\b X(\\b{X^\\top X})^{-1} \\b X^\\top\n\\]\nMatrix \\(\\color{red}{\\b P}\\) is symmetrical as in \\(\\b P^\\top = \\b P\\), and is idempotent as in \\(\\b{PP} = \\b P\\).\n\nWe can see that projection matrix \\(\\color{red}{\\b P}\\) performs a linear mapping of \\(\\b y \\rightarrow \\hat{\\b y}\\), hence why it is called the “projection” matrix.\n\nNow, let us look at our residuals \\(\\hat{\\b \\eps} = \\b y - \\hat{\\b y}\\). By plogging in \\(\\b y = \\b{Py}\\), we can get:\n\\[\n\\hat{\\b\\eps} = \\b y - \\color{red}{\\b P} \\color{black}{\\b y} = (\\b I - \\color{red}{\\b P}\\color{black})\\b y = \\color{purple}{\\b M} \\color{black}{\\b y}\n\\tag{4.1}\\]\n\nDefinition 4.10 (Residual Maker Matrix) Let us define the residual maker matrix \\(\\b M\\) as:\n\\[\n\\color{purple}{\\b M}\\color{black} := \\b I - \\b X(\\b{X^\\top X})^{-1} \\b X^\\top = \\b I - \\color{red}{\\b P}\n\\]\nMatrix \\(\\color{purple}{\\b M}\\) is symmetrical as in \\(\\b M^\\top = \\b M\\), and is idempotent as in \\(\\b{MM} = \\b M\\). Residual maker \\(\\b M\\) is also orthogonal to \\(\\b P\\) and \\(\\b X\\), implying \\(\\b{PX} = 0\\) and \\(\\b{MX} = 0\\).\n\nWe can see that residual maker matrix \\(\\color{purple}{\\b M}\\) performs a linear mapping of \\(\\b y \\rightarrow \\hat{\\b\\eps}\\), thus “making” the residuals \\(\\hat{\\b\\eps}\\).\n\n\nFrom the previous tab, we see projection matrix \\(\\color{red}{\\b P}\\) performs a linear mapping of \\(\\b y \\rightarrow \\hat{\\b y}\\), hence why it is called the “projection” matrix. We can see that residual maker matrix \\(\\color{purple}{\\b M}\\) performs a linear mapping of \\(\\b y \\rightarrow \\hat{\\b\\eps}\\), thus “making” the residuals \\(\\hat{\\b\\eps}\\).\nWe know that our predictsion \\(\\hat{\\b y}\\) are a linear combination of our explanatory variable vectors \\(X_1, \\dots, X_p\\), since \\(\\hat{\\b y} = \\b X \\hat{\\b\\beta}\\). This means that projection matrix \\(\\color{red}{\\b P}\\) projects \\(\\b y\\) into \\(\\hat{\\b y}\\) which is in the space spanned by our \\(\\b X\\) (called the column space of \\(\\b X\\)).\nThis is visualised in the figure below, where observed vector \\(\\b y\\) is projected into the blue space of regressors \\(\\b X\\) to create vector \\(\\hat{\\b y}\\):\n\n\n\n\n\nWe can see as well that residual maker \\(\\color{purple}{\\b M}\\) projects vector \\(\\b y\\) into vector \\(\\b\\eps\\), which is in the space orthogonal/perpendicular to the column space of \\(\\b X\\). This is why our condition of strict exogeneity (definition 4.4) is required - there should be no correlation between \\(\\b X\\) and \\(\\b \\eps\\) as they are orthogonal by design.\n\n\nThis idea of projection from the last tabe means we can measure the fit of our model with the correlation/overlap between original vector \\(\\b y\\) and our predicted values vector \\(\\hat{\\b y} = \\b{Py}\\).\n\nDefinition 4.11 (R-Squared) \\(R^2\\) is a metric to measure the fit of our model.\n\\[\nR^2 = \\frac{\\overbrace{\\b{y^\\top Py}}^{\\text{model}}}{\\underbrace{\\b{y^\\top y}}_{\\V Y}} \\ = \\ 1-\\frac{\\overbrace{\\sum(Y_t - \\hat Y_t)^2}^{\\text{SSR}}}{\\underbrace{\\sum (Y_t - \\bar Y)^2}_{\\V Y}}\n\\]\n\\(R^2\\) is always between 0 and 1, and measures the proportion of variance our model with explanatory variables \\(X_1, \\dots, X_p\\) explains the variation in \\(Y\\).\n\nThe first definition of \\(R^2\\) works because a dot-product of vectors measures their overlap/shadow on each other - so \\(\\b{y^\\top Py}\\) measures the overlap between our actual \\(\\b y\\) and our model \\(\\b{Py}\\). This is divided by the maximum possible shadow of \\(\\b y\\) on itself, to ensure a \\(R^2\\) between 0 and 1.\nThe second definition of \\(R^2\\) works because we know the SSR is the part of \\(Y\\) that is not explained by our model (the error). Thus, SSR divided by the variance in \\(Y\\) is the porportion of the variance in \\(Y\\) not explained by our model. Thus, the remaining part must be explained by our model, so we do 1 minus SSR/Variance.\n\n\nA partitioned regression model is when we split up our matrices/vectors in our classical linear model \\(\\b y = \\b{X\\beta} + \\b\\eps\\). Let us say we only care about some of the explanatory variables in our data generating process. We can split matrix \\(\\b X\\) into two: \\(\\b X_1\\) contains the explanatory variables we care about, and \\(\\b X_2\\) contains the other explanatory variables. Similarly, we divide \\(\\b\\beta\\). We get:\n\\[\n\\b y = \\b X_1 \\b\\beta_1 + \\b X_2 \\b\\beta_2 + \\b\\eps\n\\]\nRecall the residual maker matrix \\(\\b M\\) (definition 4.10), and recall how \\(\\b M\\) is orthogonal to \\(\\b X\\), meaning \\(\\b{MX} = 0\\). Now, let us consider only the residual maker matrix of the second part of our partitioned model \\(\\b M_2\\), which means \\(\\b M_2 \\b X_2 = 0\\). Let us take our partitioned regression model, and pre-multiply \\(\\b M_2\\) to both sides:\n\\[\n\\begin{align}\n\\b M_2 \\b y & = \\b M_2(\\b X_1 \\b\\beta_1 + \\b X_2 \\b\\beta_2 + \\b\\eps) \\\\\n\\b M_2 \\b y & = \\b M_2 \\b X_1 \\b\\beta_1 + \\b M_2 \\b X_2 \\b\\beta_2 + \\b M_2 \\b\\eps\n\\end{align}\n\\]\nNow recall that \\(\\b M_2 \\b X_2 = 0\\). That means we can simplify the above to\n\\[\n\\b M_2 \\b y = \\b M_2 \\b X_1 \\b\\beta_1  + \\b M_2 \\b\\eps\n\\]\nNow, let us define \\(\\tilde{\\b y} := \\b M_2 \\b y\\), \\(\\tilde{\\b X_1} := \\b M_2 \\b X_1\\), and error \\(\\tilde{\\b\\eps} = \\b M_2 \\b\\eps\\). We can rewrite as\n\\[\n\\tilde{\\b y} = \\tilde{\\b X_1}\\b\\beta_1 + \\tilde{\\b \\eps}\n\\]\nUsing definition 4.8, we know the OLS estimate of \\(\\hat{\\b \\beta_1}\\) is:\n\\[\n\\hat{\\b\\beta}_1 = (\\tilde{\\b X_1^\\top} \\tilde{\\b X_1}) \\tilde{\\b X_1^\\top} \\tilde{\\b y}\n\\]\n\\(\\hat{\\b\\beta}_1\\) is equal to our normal OLS estimates without partitioning the model. Notice how we have \\(\\tilde{\\b X_1} := \\b M_2 \\b X_1\\) in our formula. Well, we know \\(\\b M_2 \\b X_2 = 0\\). That means that any part of \\(\\b X_1\\) that was correlated to \\(\\b X_2\\) became 0, after it was multiplied by \\(\\b M_2\\). Thus, \\(\\tilde{\\b X_1}\\) is the part of \\(\\b X_1\\) that is uncorrelated with \\(\\b X_2\\).\n\nTheorem 4.1 (Regression Anatomy Theorem) Our individual parameter estimates \\(\\hat\\beta_j \\in \\{\\hat\\beta_1, \\dots, \\hat\\beta_p \\}\\) are the relationship between \\(Y\\) and the part of \\(X_j \\in \\{X_1, \\dots, X_p\\}\\) uncorrelated with the other explanatory variables.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#geometry-and-projection",
    "href": "ols.html#geometry-and-projection",
    "title": "5  Least Squares Theory",
    "section": "5.3 Geometry and Projection",
    "text": "5.3 Geometry and Projection\nWe have derived our OLS estimation solution for \\(\\hat{\\b\\beta}\\). But what does this solution mean? What is OLS doing? First, let us define two matrices:\n\nDefinition 5.11 (Projection Matrix) Let us define the projection matrix \\(\\b P\\) as:\n\\[\n\\color{red}{\\b P}\\color{black} := \\b X(\\b{X^\\top X})^{-1} \\b X^\\top\n\\]\nMatrix \\(\\color{red}{\\b P}\\) is symmetrical as in \\(\\b P^\\top = \\b P\\), and is idempotent as in \\(\\b{PP} = \\b P\\).\n\n\nDefinition 5.12 (Residual Maker Matrix) Let us define the residual maker matrix \\(\\b M\\) as:\n\\[\n\\color{purple}{\\b M}\\color{black} := \\b I - \\b X(\\b{X^\\top X})^{-1} \\b X^\\top = \\b I - \\color{red}{\\b P}\n\\]\nMatrix \\(\\color{purple}{\\b M}\\) is symmetrical as in \\(\\b M^\\top = \\b M\\), and is idempotent as in \\(\\b{MM} = \\b M\\). Residual maker \\(\\b M\\) is also orthogonal to \\(\\b P\\) and \\(\\b X\\), implying \\(\\b{PX} = 0\\) and \\(\\b{MX} = 0\\).\n\n\n\n\n\n\n\nProof \\(M\\) and \\(P\\) are Idempotent\n\n\n\n\n\nWe can prove \\(\\color{red}{\\b P}\\) is an idempotent matrix (A similar proof is applicable to \\(\\color{purple}{\\b M}\\).):\n\\[\n\\begin{align}\n\\b{PP} & = \\b X(\\b{X^\\top X})^{-1} \\underbrace{\\b X^\\top \\b X(\\b{X^\\top X})^{-1}}_{\\text{inverses cancel}} \\b X^\\top \\\\\n& = \\b X(\\b{X^\\top X})^{-1} \\b X^\\top = \\b P\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nProof \\(M\\) and \\(P\\) are Orthogonal\n\n\n\n\n\nWe can show \\(\\b M\\) and \\(\\b P\\) are orthogonal, i.e. \\(\\b{P^\\top&lt;} = 0\\). First, recall that \\(\\b P\\) is a symmetrical matrix, meaning, \\(\\b P^\\top = \\b P\\). Thus,\n\\[\n\\b{P^\\top M} = \\b{PM}\n\\]\nNow, let us substitute the definition of \\(\\b M\\) from (definition 5.12), to get :\n\\[\n\\begin{align}\n\\b{P^\\top M} & = \\b P( \\b I - \\b P) \\\\\n& = \\b P - \\b{PP}\n\\end{align}\n\\]\nAnd since \\(\\b P\\) is idempotent, i.e. \\(\\b{PP} = \\b P\\), then we know that\n\\[\n\\b{P^\\top M} = \\b P - \\b P = 0\n\\]\nThus proving \\(\\b P\\) and \\(\\b M\\) are orthogonal.\n\n\n\nNow, recall our predictions \\(\\hat{\\b y} = \\b X \\hat{\\b\\beta}\\). Using our OLS solution, and the plugging in the definition of projection matrix \\(\\color{red}{\\b P}\\) (definition 5.11), we can find\n\\[\n\\hat{\\b y} = \\b X (\\b{X^\\top X})^{-1} \\b{X^\\top y} = \\color{red}{\\b P}\\color{black}{\\b y}\n\\]\nNow, let us look at our residuals \\(\\hat{\\b \\eps} = \\b y - \\hat{\\b y}\\). By plogging in \\(\\b y = \\b{Py}\\) from above, and substituting the definition of \\(\\color{purple}{\\b M}\\) (definition 5.12), we can get:\n\\[\n\\hat{\\b\\eps} = \\b y - \\color{red}{\\b P} \\color{black}{\\b y} = (\\b I - \\color{red}{\\b P}\\color{black})\\b y = \\color{purple}{\\b M} \\color{black}{\\b y}\n\\tag{5.1}\\]\nWe can see that projection matrix \\(\\color{red}{\\b P}\\) performs a linear mapping of \\(\\b y \\rightarrow \\hat{\\b y}\\), hence why it is called the “projection” matrix. We can see that residual maker matrix \\(\\color{purple}{\\b M}\\) performs a linear mapping of \\(\\b y \\rightarrow \\hat{\\b\\eps}\\), thus “making” the residuals \\(\\hat{\\b\\eps}\\).\nWe know that our predictsion \\(\\hat{\\b y}\\) are a linear combination of our explanatory variable vectors \\(X_1, \\dots, X_p\\), since \\(\\hat{\\b y} = \\b X \\hat{\\b\\beta}\\). This means that projection matrix \\(\\color{red}{\\b P}\\) projects \\(\\b y\\) into \\(\\hat{\\b y}\\) which is in the space spanned by our \\(\\b X\\) (called the column space of \\(\\b X\\)).\nThis is visualised in the figure below, where observed vector \\(\\b y\\) is projected into the blue space of regressors \\(\\b X\\) to create vector \\(\\hat{\\b y}\\):\n\n\n\n\n\nWe can see as well that residual maker \\(\\color{purple}{\\b M}\\) projects vector \\(\\b y\\) into vector \\(\\b\\eps\\), which is in the space orthogonal/perpendicular to the column space of \\(\\b X\\). This is why our condition of strict exogeneity (definition 5.4) is required - there should be no correlation between \\(\\b X\\) and \\(\\b \\eps\\) as they are orthogonal by design.\nThis idea of projection means we can measure the fit of our model with the correlation/overlap between original vector \\(\\b y\\) and our predicted values vector \\(\\hat{\\b y} = \\b{Py}\\).\n\nDefinition 5.13 (R-Squared) \\(R^2\\) is a metric to measure the fit of our model. It is defined as:\n\\[\nR^2 = \\frac{\\overbrace{\\b{y^\\top Py}}^{\\text{model}}}{\\underbrace{\\b{y^\\top y}}_{\\V Y}} \\ = \\ 1-\\frac{\\overbrace{\\sum(Y_t - \\hat Y_t)^2}^{\\text{SSR}}}{\\underbrace{\\sum (Y_t - \\bar Y)^2}_{\\V Y}}\n\\]\n\\(R^2\\) is always between 0 and 1, and measures the proportion of variance our model with explanatory variables \\(X_1, \\dots, X_p\\) explains the variation in \\(Y\\).",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#regression-anatomy",
    "href": "ols.html#regression-anatomy",
    "title": "5  Least Squares Theory",
    "section": "5.3 Regression Anatomy",
    "text": "5.3 Regression Anatomy",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#unbiasedness-of-ols",
    "href": "ols.html#unbiasedness-of-ols",
    "title": "5  Least Squares Theory",
    "section": "5.3 Unbiasedness of OLS",
    "text": "5.3 Unbiasedness of OLS\nWe know that an estimator has two finite sample properties: unbiasedness (definition 2.4) and variance (definition 2.5). Let us focus on unbiasedness for now.\n\nTheorem 5.2 (OLS is an Unbiased Estimator) The Ordinary Least Squares estimator is an unbiased estimator under the assumptions of linearity, i.i.d., no perfect multicollinearity, and strict exogeneity.\n\\[\n\\E \\hat{\\b\\beta} = \\b\\beta\n\\]\nNote that the assumption of spherical errors is not needed for the unbiasedness of OLS.\n\n\nProof: Let us start with our OLS solution (definition 5.8), and plug in our original model \\(\\b y = \\b{X\\beta} + \\b\\eps\\) into where \\(\\b y\\) is in the OLS solution:\n\\[\n\\begin{align}\n\\hat{\\b\\beta} & = (\\b{X^\\top X})^{-1} \\b{X^\\top y} \\\\\n& = (\\b{X^\\top X})^{-1} \\b X^\\top (\\b{X\\beta} + \\b\\eps) \\\\\n& = \\underbrace{(\\b{X^\\top X})^{-1}\\b{X^\\top X}}_{\\text{inverses cancel}}\\b\\beta + (\\b{X^\\top X})^{-1} \\b{X^\\top \\eps} \\\\\n& = \\b\\beta + (\\b{X^\\top X})^{-1}\\b{X^\\top \\eps}\n\\end{align}\n\\tag{5.3}\\]\nNow, let us take the expectation of \\(\\hat{\\b\\beta}\\) conditional on \\(\\b X\\) (remember that \\(\\b X\\) and \\(\\b \\beta\\) are fixed constants, so they are not affected by the expectation). We can use the strict exogeneity assumption (definition 5.4) to simplify:\n\\[\n\\E(\\hat{\\b\\beta}|\\b X) = \\b\\beta + (\\b{X^\\top X})^{-1} \\underbrace{\\E (\\b \\eps | \\b X)}_{= \\ 0} = \\b\\beta\n\\]\nNow, we know \\(\\E(\\hat{\\b\\beta} | \\b X)\\). We can deduce \\(\\E(\\hat{\\b\\beta})\\) using the law of iterated expectations (theorem 1.3), and plugging in \\(\\E(\\hat{\\b\\beta} | \\b X) = \\b\\beta\\):\n\\[\n\\E(\\hat{\\b\\beta}) = \\E[\\E(\\hat{\\b\\beta}|\\b X)] = \\E[\\b\\beta] = \\b\\beta\n\\]\nThe final step is because the expectation of a constant \\(\\b\\beta\\) (the fixed true population value) is the constant itself. Thus, we have proven OLS is an unbiased.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#variance-of-ols",
    "href": "ols.html#variance-of-ols",
    "title": "5  Least Squares Theory",
    "section": "5.4 Variance of OLS",
    "text": "5.4 Variance of OLS",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#gauss-markov-theorem",
    "href": "ols.html#gauss-markov-theorem",
    "title": "4  Least Squares Theory",
    "section": "4.4 Gauss-Markov Theorem",
    "text": "4.4 Gauss-Markov Theorem\nYou might ask, what is so special about Ordinary Least Squares, and why should we use this estimator? The answer lies in the Gauss-Markov Theorem.\n\nTheorem 4.5 (Gauss-Markov) If all of the assumptions of linearity, i.i.d., no perfect multicollinearity, strict exogeneity, and spherical errors are all met, then the Ordinary Least Squares estimator is the best linear unbiased estimator (BLUE) - the unbiased linear estimator with the lowest variance of any other unbiased estimator.\nFormally, if \\(\\hat{\\b\\beta}\\) is the OLS estimator, and \\(\\tilde{\\b\\beta}\\) is any other linear unbiased estimator, then\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) ≤ \\V(\\tilde{\\b\\beta} | \\b X)\n\\]\n\n\nAny linear estimator \\(\\tilde{\\b\\beta}\\) must be in the form \\(\\tilde{\\b\\beta} = \\b{Cy}\\), where \\(\\b C\\) is some linear mapping. For example, using projection matrix \\(\\b P\\) (definition 4.9), OLS can be written as \\(\\hat{\\b\\beta} = \\b{Py}\\). Before we prove the Gauss-Markov theorem, we need a lemma about any unbiased linear estimator.\n\nLemma 4.1 For any linear estimator \\(\\tilde{\\b\\beta} = \\b{Cy}\\) to be unbiased, \\(\\b{CX} = \\b I\\).\n\n\n\n\n\n\n\nProof of Lemma 3.1\n\n\n\n\n\nProof: Let us start off with our linear estimator \\(\\tilde{\\b\\beta} = \\b{Cy}\\), and plug in the true linear model \\(\\b y = \\b{X\\beta} + \\b\\eps\\) into our linear estimator:\n\\[\n\\tilde{\\b\\beta} = \\b C (\\b{X\\beta} + \\b\\eps) = \\b{CX\\beta} + \\b{C\\eps}\n\\]\nNow, let us find the expected value of this estimator conditional on \\(\\b X\\). Remember that the expected values of constants (like \\(\\b C\\), \\(\\b \\beta\\), and \\(\\b X\\) since we are conditioning on \\(\\b X\\)) are the constants themselves.\n\\[\n\\begin{align}\n\\E(\\tilde{\\b\\beta}|\\b X) & = \\E(\\b{CX\\beta} + \\b{C\\eps}) \\\\\n& = \\b{CX\\beta} + \\b C \\E(\\b\\eps| \\b X)\n\\end{align}\n\\]\nFrom the strict exogeneity assumption (definition 4.4), we know \\(\\E(\\b\\eps | \\b X) = 0\\), so we can simplify to\n\\[\n\\E(\\tilde{\\b\\beta}|\\b X) = \\b{CX\\beta}\n\\]\nAnd using the law of iterated expectations (theorem 1.3), we can find \\(\\E\\tilde{\\b\\beta}\\):\n\\[\n\\E\\tilde{\\b\\beta} = \\E[\\E(\\tilde{\\b\\beta}|\\b X)] = \\E[\\b{CX\\beta}] = \\b{CX\\beta}\n\\]\nFor unbiasedness (definition 2.4), we know \\(\\E\\tilde{\\b\\beta} = \\b\\beta\\). The only way \\(\\b{CX\\beta}\\) will equal \\(\\b\\beta\\) is if \\(\\b{CX} = \\b I\\). Thus, for any linear unbiased estimator, the lemma \\(\\b{CX} = \\b I\\) must hold.\n\n\n\nWith this lemma, now let us prove Gauss-Markov. First, let us calculate the variance of unbiased linear estimator \\(\\tilde{\\b\\beta}\\):\n\\[\n\\begin{align}\n\\V(\\tilde{\\b\\beta} | \\b X) & = \\V(\\b{Cy} | \\b X) \\\\\n& = \\V(\\b C( \\b{X\\beta} + \\b \\eps)| \\b X) \\\\\n& = \\V(\\b{CX\\beta} + \\b{C\\eps} | \\b X)\n\\end{align}\n\\]\nAnd since we know from Lemma 4.1 that \\(\\b{CX = I}\\), we can get\n\\[\n\\V(\\tilde{\\b\\beta} | \\b X) = \\V(\\b\\beta + \\b{C\\eps} | \\b X)\n\\]\nWe know that \\(\\b\\beta\\) is a vector of fixed constants (the true population values). We also know \\(\\b C\\) is some fixed constant matrix (that depends on \\(\\b X\\), but we are conditioning on \\(\\b X\\)). Thus, we can use theorem 1.2 to rewrite the above as\n\\[\n\\V(\\tilde{\\b\\beta} | \\b X) = \\b C\\V(\\b\\eps | \\b X) \\b C^\\top\n\\]\nNow, according to the assumption of spherical errors (definition 4.6), we know that \\(\\V(\\b \\eps| \\b X) = \\sigma^2 \\b I_n\\). Thus, let us plug that into our equation to get\n\\[\n\\begin{align}\n\\V(\\tilde{\\b\\beta} | \\b X) & = \\b C \\sigma^2 \\b I_n \\b C^\\top \\\\\n& = \\sigma^2 \\b{CC^\\top}\n\\end{align}\n\\tag{4.3}\\]\nNow we have the variance of estimator \\(\\tilde{\\b\\beta}\\). To prove Gauss-Markov, we need to show that the variance of \\(\\tilde{\\b\\beta}\\) is greater than the variance of \\(\\hat{\\b\\beta}\\). For this to be true,\n\\[\n\\V(\\tilde{\\b\\beta}|\\b X) - \\V(\\tilde{\\b\\beta}| \\b X) ≥ 0\n\\]\nWe can plug in the variance of \\(\\tilde{\\b\\beta}\\) from eq. 4.3, and the variance of OLS \\(\\hat{\\b\\beta}\\) from theorem 4.3:\n\\[\n\\begin{align}\n\\sigma^2 \\b{CC^\\top} - \\sigma^2 (\\b{X^\\top X})^{-1} & ≥ 0 \\\\\n\\sigma^2 (\\b{CC^\\top} - (\\b{X^\\top X})^{-1}) & ≥ 0\n\\end{align}\n\\]\nWe know from Lemma 4.1 that \\(\\b{CX} = \\b I\\), which through the properties of tranposes, also implies that \\(\\b{X^\\top C^\\top} = (\\b{CX})^\\top = \\b I\\). Multipling by \\(\\b I\\) doesn’t change anything, so we can insert a \\(\\b{CX}\\) and \\(\\b{X^\\top C^\\top}\\) into our equation above to get\n\\[\n\\sigma^2 (\\b{CC^\\top} - \\b{CX} (\\b{X^\\top X})^{-1}\\b{X^\\top C^\\top}) ≥ 0\n\\]\nFactoring out \\(\\b C\\) and \\(\\b C^\\top\\), and remembering our residual maker \\(\\b M\\) (definition 4.10),\n\\[\n\\begin{align}\n\\sigma^2 \\b C(\\b I - \\b X(\\b{X^\\top X}^{-1}\\b X^\\top) \\b C^\\top & ≥ 0 \\\\\n\\sigma^2 \\b{CMC} & ≥ 0\n\\end{align}\n\\]\nWe know \\(\\sigma^2\\), the variance of the error term, must be positive. \\(\\b{CMC}\\) is also a positive semi-definite matrix (behaves like a positive number). The proof is provided below.\n\n\n\n\n\n\nProof \\(CMC\\) is Positive Semi-Definite\n\n\n\n\n\nTo show \\(\\b {CMC}\\) is positive semi-definite, the following must be true for every vector \\(\\b z\\):\n\\[\n\\b{z^\\top CMC^\\top z} ≥ 0\n\\]\nRemember that from definition 4.10 that \\(\\b M\\) is symmetric and idempotent. This implies that \\(\\b M = \\b{MM} = \\b M^\\top\\). Thus, plugging this in, we get\n\\[\n\\underbrace{\\b{z^\\top CM}}_{\\b w^\\top} \\underbrace{\\b{M^\\top C^\\top z}}_{\\b w} = \\b{w^\\top w} = \\sum\\limits_{i=1}^n w_i^2 ≥ 0\n\\]\nWhich is true since the square of any number cannot be negative. Thus, \\(\\b{CMC}\\) is positive semi-definite, and behaves like a positive number.\n\n\n\nThis property means that OLS produces the best estimates for any linear model, which makes it very popular in statistics (especially considering many statistical models are linear).",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#asymptotic-consistency",
    "href": "ols.html#asymptotic-consistency",
    "title": "5  Least Squares Theory",
    "section": "5.6 Asymptotic Consistency",
    "text": "5.6 Asymptotic Consistency\nBefore we introduce asymptotic properties of the OLS estimator, we have to introduce a new assumption: Weak exogeneity. Previously, we discussed strict exogeneity (definition 5.4). Weak exgoeneity is a weaker version of this assumption.\n\nDefinition 5.12 (Weak Exogeneity) Weak exogeneity is defined as \\(\\E(\\b x_t \\eps_t) = 0\\). Weak exogeneity only requires that regressors \\(X_1, \\dots, X_p\\) individually are uncorrelated with the error term. Weak exogeneity allows for combinations of \\(X_1, \\dots, X_p\\) to to be correlated with \\(\\eps\\), which is not allowed under strict exogeneity\n\n\n\n\n\n\n\nRelationship between Weak and Strict Exogeneity\n\n\n\n\n\nIf you meet strict exogeneity, you automatically meet the assumption of weak exogeneity. However, the opposite is not true - weak exogeneity does not imply strict exogeneity.\n\n\n\n\nTheorem 5.5 (Consistency of OLS) The Ordinary Least Squares estimator is asymptotically consistent (definition 2.6), under the assumptions of linearity, i.i.d., no perfect multicollinearity, weak exogeneity, and spherical errors.\nMathematically, this means\n\\[\n\\mathrm{plim}\\hat{\\b\\beta} = \\b\\beta\n\\]\n\n\nProof: Let us perform the same steps as in eq. 5.3, and start off where we left off. We can rewrite our matrix notation in the form of vectors, scalars, and summation:\n\\[\n\\begin{align}\n\\hat{\\b\\beta} & = \\b\\beta + (\\b{X^\\top X})^{-1}\\b{X^\\top\\eps} \\\\\n& = \\b\\beta + \\left(\\sum\\limits_{t=1}^n \\b x_t \\b x_t^\\top\\right)^{-1} \\left(\\sum\\limits_{t=1}^n\\b x_t \\eps_t \\right)\n\\end{align}\n\\]\nNow, let us do a little algebra trick as follows:\n\\[\n\\hat{\\b\\beta} = \\b\\beta + \\left(\\frac{1}{n}\\sum\\limits_{t=1}^n \\b x_t \\b x_t^\\top\\right)^{-1} \\left(\\frac{1}{n}\\sum\\limits_{t=1}^n\\b x_t \\eps_t \\right)\n\\]\nThe reason we can do this is because the first \\(\\frac{1}{n}\\) is inversed as \\(\\frac{1}{n}^{-1}\\), so this cancels out the second one, maintaining the equality of our equation.\nNow, we want to prove \\(\\mathrm{plim}\\hat{\\b\\beta} =\\b\\beta\\), so let us take the probability limit of both sides.\n\\[\n\\mathrm{plim}\\hat{\\b\\beta} = \\mathrm{plim} \\b\\beta + \\left( \\mathrm{plim} \\frac{1}{n}\\sum\\limits_{t=1}^n \\b x_t \\b x_t^\\top \\right)^{-1} \\left( \\mathrm{plim}\\frac{1}{n}\\sum\\limits_{t=1}^n\\b x_t \\eps_t \\right)\n\\]\nWe know that the probability limit of a constant is itself, so \\(\\mathrm{plim} \\b\\beta = \\b\\beta\\), since \\(\\b\\beta\\) is a constant of true population parameters. Look at the other two terms on the right. They take the form of sample averages \\(\\frac{1}{n}\\sum\\). Using the law of of large numbers (theorem 2.1), we can simplify to:\n\\[\n\\mathrm{plim}\\hat{\\b\\beta} = \\b\\beta + (\\E(\\b x_t \\b x_t^\\top))^{-1} \\underbrace{\\E(\\b x_t \\eps_t)}_{=0} = \\b\\beta\n\\]\nAnd we know \\(\\E(\\b x_t \\eps_t) = 0\\) because of the condition of weak exogeneity (definition 5.12). Thus, we have proved that OLS is asymptotically consistent.\nNotice how OLS is asymptotically consistent with just weak exogeneity, without requiring full exogeneity. This implies that if we only meet weak exogeneity, our OLS estimates will be biased according to theorem 5.2 (since unbiasedness requires strict exogeneity). However, under only weak exogeneity, our OLS estimates will still be asymptotically consistent despite being biased in finite samples.\nThis means that if we have some reason to believe that we do not meet strict exogeneity, if we meet weak exogeneity, as long as our sample size is sufficiently large, our estimates can still be asymptotically consistent and accurate.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#statistical-inference",
    "href": "ols.html#statistical-inference",
    "title": "4  Least Squares Theory",
    "section": "4.9 Statistical Inference",
    "text": "4.9 Statistical Inference\nStandard errors are by definition, the square root of the variance of the estimator, which we derived for both OLS under spherical errors, and OLS under non-spherical errors.\nThere is an issue though: \\(\\sigma^2\\) is the population variance of error term \\(\\eps_i\\), and appears in the OLS variance under spherical errors. But we don’t know this population value. Thus, we will need an estimator \\(s^2\\) that will estimate \\(\\sigma^2\\):\n\\[\ns^2 = \\frac{\\b{\\hat\\eps^\\top \\hat\\eps}}{n - p-1} = \\frac{\\sum_{t=1}^n \\hat\\eps_t^2}{n-p-1}\n\\]\nWhere \\(\\hat{\\b\\eps}\\) are equal to \\(\\b y - \\hat{\\b y}\\), and can be calculated with residual maker \\(\\b M\\) as shown in eq. 4.1. \\(n\\) is the size of our sample, and \\(p\\) is the number of explanatory variables we have. We will not prove it here, but this is an unbiased estimator of \\(\\sigma^2\\)\nFor OLS variance in conditional heteroscedasticity (robust), we have the unknown population term \\(\\sigma^2_i\\), which we estimate with \\(s^2_i\\):\n\\[\n\\sigma^2_i \\approx s^2_i = \\hat\\eps_i^2\n\\]\nHowever, our estimate \\(s^2\\) and \\(s_i^2\\) has an implication - every estimator has variance and uncertainty.\nUnder the central limit theorem (theorem 2.2), our standardised sampling distribution of \\(\\hat\\beta_j\\) should be normally distributed. However, because we are estimating \\(\\sigma^2\\) with \\(s^2\\), this uncertainty in estimates \\(s^2\\) means we cannot use the normal distribution as given by the central limit theorem. Instead, we use a t-distribution to account for the uncertainty.\nOnce we have our correct standard errors, we can conduct hypothesis testing. There are two main hypothesis tests: the t-test for single parameters, and the f-test for multiple parameters or comparing models:\n\nT-TestF-Test\n\n\nThe t-test is a way to test single parameters in our linear model. Typically, our hypothesis are the following:\n\n\\(H_0: \\beta_j = 0\\), which states that there is no relationship between \\(X_j\\) and \\(Y\\).\n\\(H_1: \\beta_j ≠ 0\\), which states that there is a relationship between \\(X_j\\) and \\(Y\\).\n\n\nDefinition 4.17 (T-Test) In our t-test, our test statistic is the t-statistic.\n\\[\nt = \\frac{\\hat\\beta_j - \\text{null value}}{se(\\hat\\beta_j)}\n\\]\nAnd we will conduct a hypothesis test using the \\(t\\)-distribution with degrees of freedom of \\(n-p-1\\).\n\n\nWe then calculate a p-value. The procedure of conducting a hypothesis test was outlined in the last chapter. If our p-value is statistically significant, we can conclude there is a significant relationship between \\(X_j\\) and \\(Y\\).\n\n\nWe can also do a statistical inference test with multiple coefficients at a time with the F-test. The F-test compares two different models, a null model with less parameters, and a alternate model with all the parameters in the null and some more parameters:\n\n\\(M_0 : Y_t = \\beta_0 + \\sum\\limits_{j=1}^g \\beta_j X_{tj} + \\eps_t\\) (the smaller null model with \\(g\\) parameters).\n\\(M_a : Y_t = \\beta_0 + \\sum\\limits_{j=1}^g \\beta_j X_{tj} + \\sum\\limits_{j=g+1}^p \\beta_j X_{tj} \\eps_t\\) (the bigger model with the original \\(g\\) parameters in the null + additional parameters up to \\(p\\)).\n\nRecall \\(R^2\\) (definition 4.11), which is a measure of fit for our models. F-tests compare the \\(R^2\\) of the alternate model to the null model.\n\nDefinition 4.18 (F-test) F-tests are used to test a smaller model \\(M_0\\) and larger model \\(M_a\\) model. The F-test statistic is given by\n\\[\nF = \\frac{(SSR_0 - SSR_a) / (p_a - p_0)}{SSR_a /(n-p_A - 1)}\n\\]\nWhere \\(SSR\\) represents the sum of squared residuals, \\(p\\) represents the number of parameters in each model, and \\(n\\) is the sample size (should be the same between both \\(M_0\\) and \\(M_a\\)).\nWe then consult a F-distribution with \\(p_1 - p_0\\) and \\(n - p_a - 1\\) degrees of freedom.\n\n\nWe then calculate a p-value. The procedure of conducting a hypothesis test was outlined in the last chapter.\nIf our result is statistically significant, then the alternative model \\(M_a\\) is a statistically significantly better model than \\(M_0\\), and the extra parameters in \\(M_a\\) are jointly significant.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "gls.html",
    "href": "gls.html",
    "title": "3  Generalised Least Squares",
    "section": "",
    "text": "3.1 Conditional Heteroscedasticity\nFor the classical linear model, one of the assumptions was spherical errors (definition 5.6). This was an assumption made on the variance-covariance matrix of error term \\(\\eps_t\\). However, this assumption is often violated - in fact, in many fields like econometrics, we assume it is violated by default. What occurs when the other 4 assumptions of the classical model are met, but spherical errors is violated? This chapter explains how we can deal with this scenario.\nSpherical Errors was broken into two parts. No autocorrelation implied that the covariance elements were all equal to 0, and homoscedasticity implied that all the variances were equal to some constant \\(\\sigma^2\\).\nFor the first part of this chapter, we will keep the no autocorrelation assumption, but weaken homoscedasticity. Instead of assuming all observations \\(t\\) have the same error variable \\(\\sigma^2\\), we will now assume that each different observation has different error variances \\(\\sigma^2_t\\).\nThe above is a residual plot of OLS residuals \\(\\hat\\eps_i\\) against some explanatory variable \\(X\\). Notice how for homoscedasticity, the variance of the error terms (how spread out they are up-down wise) is constant for any value of \\(X\\).\nFor heteroscedasticity, we can clearly see that the residual variance is smaller for some \\(X\\) values, and larger for other \\(X\\) values. If you see a pattern in your residual plot, it is likely heteroscedasticity.\nWhen heteroscedasticity is present, that means our spherical errors assumption of the classical linear model is violated. What implications does this have?\nUnder heteroscedasticity, OLS is still unbiased, since by theorem 5.2, we see that the unbiasedness of OLS does not require spherical errors.\nHowever, OLS now has inaccurate standard error estimates since OLS variance (theorem 5.3) used the condition of spherical errors. OLS is also no longer the best linear unbiased estimator (the linear unbiased estimator with the least variance), since the Gauss-Markov Theorem (theorem 5.5) requires spherical errors.\nIn this chapter, we will introduce two ways to deal with heteroscedasticity. First, we will discuss ways to “correct” the incorrect OLS standard errors, while sticking with the OLS estimator. Then, we will introduce a new estimator that is BLUE under Gauss-Markov when heteroscedasticity is present.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generalised Least Squares</span>"
    ]
  },
  {
    "objectID": "gls.html#conditional-heteroscedasticity",
    "href": "gls.html#conditional-heteroscedasticity",
    "title": "3  Generalised Least Squares",
    "section": "",
    "text": "Definition 3.1 (Conditional Heteroscedasticity) Conditional heteroscedasticity says that the covariance matrix of errors \\(\\eps_t\\) takes the form:\n\\[\n\\V(\\b\\eps|\\b X) = \\b\\Omega= \\begin{pmatrix}\n\\sigma^2_1 & 0 & 0 & \\dots \\\\\n0 & \\sigma^2_2 & 0 & \\dots  \\\\\n0 & 0& \\sigma^2_3 & \\vdots \\\\\n\\vdots & \\vdots & \\dots & \\ddots\n\\end{pmatrix}\n\\]\nThis implies that the error variance \\(\\sigma^2_i\\) depends on observation \\(i\\), and specifically, the values of the explanatory variables \\(X_{i1}, \\dots, X_{ip}\\) for that observation \\(i\\).",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generalised Least Squares</span>"
    ]
  },
  {
    "objectID": "gls.html#robust-standard-errors",
    "href": "gls.html#robust-standard-errors",
    "title": "3  Generalised Least Squares",
    "section": "3.2 Robust Standard Errors",
    "text": "3.2 Robust Standard Errors\nIf conditional heteroscedasticity (definition 3.1) is present in our population model, then the standard OLS variance (theorem 5.3) is inaccurate, since that variance calculation used homoscedasticity.\nThus, we need to find the “true” standard errors under homoscedasticity.\n\nTheorem 3.1 (Heteroscedasticity Variance of OLS) The variance of the OLS estimator under heteroscedasticity is given by\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = (\\b{X^\\top X})^{-1}\\b X^\\top \\begin{pmatrix}\n\\sigma^2_1 & 0 & 0 & \\dots \\\\\n0 & \\sigma^2_2 & 0 & \\dots  \\\\\n0 & 0& \\sigma^2_3 & \\vdots \\\\\n\\vdots & \\vdots & \\dots & \\ddots\n\\end{pmatrix} \\b X(\\b{X^\\top X})^{-1}\n\\]\n\n\nProof: Let us start where we left off from eq. 5.3 during the proof of unbiasedness. This tells us the variance of the OLS estimator is:\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = \\V(\\b\\beta + (\\b{X^\\top X})^{-1} \\b{X^\\top \\eps})\n\\]\nWe know that \\(\\b\\beta\\) is a vector of fixed true population values. \\((\\b{X^\\top X})^{-1} \\b X^\\top\\) can also be considered a fixed constant matrix because we are conditioning our variance on \\(\\b X\\). Thus, we can use theorem 1.2 to rewrite the above as\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = (\\b{X^\\top X})^{-1}\\b X^\\top \\V(\\b\\eps| \\b X)[(\\b{X^\\top X})^{-1}\\b X^\\top]^{-1}\n\\]\nWith the properties of matrix inverses and transposes, we can determine that \\([(\\b{X^\\top X})^{-1}\\b X^\\top]^{-1}\\) is equivalent to \\(\\b X(\\b{X^\\top X})^{-1}\\). Thus, plugging this in, we get\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = (\\b{X^\\top X})^{-1}\\b X^\\top \\V(\\b\\eps| \\b X) \\b X(\\b{X^\\top X})^{-1}\n\\]\nNow, by conditional heteroscedasticity (definition 3.1), we can replace \\(\\V(\\b\\eps|\\b X)\\) with the heteroscedasticity variance matrix of errors:\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = (\\b{X^\\top X})^{-1}\\b X^\\top \\begin{pmatrix}\n\\sigma^2_1 & 0 & 0 & \\dots \\\\\n0 & \\sigma^2_2 & 0 & \\dots  \\\\\n0 & 0& \\sigma^2_3 & \\vdots \\\\\n\\vdots & \\vdots & \\dots & \\ddots\n\\end{pmatrix} \\b X(\\b{X^\\top X})^{-1}\n\\]\nAnd thus, we have proven the OLS variance under heteroscedasticity. When we actually write the formula, we will typically replace the matrix with \\(\\b\\Omega\\).\nHowever, we of course do not know what the population values of \\(\\sigma^2_1, \\dots, \\sigma^2_n\\) are. We can estimate them with OLS residuals as follows:\n\\[\n\\sigma_t^2 \\approx s_t^2 = \\eps^2_t\n\\]\nAnd our robust standard errors for any parameter \\(\\hat\\beta_j\\) are simply\n\\[\nse(\\hat\\beta_j) = \\sqrt{[(\\b{X^\\top X})^{-1}\\b X^\\top \\hat{\\b\\Omega} \\b X(\\b{X^\\top X})^{-1}]}_{jj}\n\\]\nWe can conduct statistical inference in the same way as we did for OLS, but replacing the standard errors with our new robust standard errors.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generalised Least Squares</span>"
    ]
  },
  {
    "objectID": "gls.html#heteroscedasticity-transformation",
    "href": "gls.html#heteroscedasticity-transformation",
    "title": "3  Generalised Least Squares",
    "section": "3.3 Heteroscedasticity Transformation",
    "text": "3.3 Heteroscedasticity Transformation\nWhen heteroscedasticity (definition 3.1) is present, we could just use the OLS estimator with robust standard errors. However, OLS with robust standard errors is no longer the best linear unbiased estimator, as the Gauss-Markov theorem (theorem 5.5) depends on spherical errors.\nThe weighted least squares (WLS) estimator is an alternative of OLS (and is considered a special case of the generalised least squares estimator we will see later in the chapter).\nSuppose that our heteroscedasticity is of the form:\n\\[\n\\V(\\eps_t|\\b X) = \\sigma^2 \\  \\Omega(X_t)\n\\tag{3.1}\\]\nWhere \\(\\sigma^2\\) is some constant (can be 1) and \\(\\Omega(X_t)\\) is some function of \\(X_t\\) that explained the difference in error variance between individuals.\nNow, consider the variance of this (modified) error term that is the original error \\(\\eps_t\\) divided by the square root \\(\\Omega(X_t)\\), which is a function of \\(X_t\\):\n\\[\n\\V \\left( \\frac{1}{\\sqrt{\\Omega(X_t)}}\\eps_t \\biggr| X_t \\right)\n\\]\nUsing theorem 1.2, we know \\(\\V(cu) = c^2 \\V(u)\\) if \\(c\\) is a constant and \\(u\\) is a random variable. This function also applies to a function \\(a(x)\\) where \\(\\V(a(x) u) = a(x)^2 \\V(u)\\). Using this, we can determine the variance of the modified error term is equal to\n\\[\n\\V \\left( \\frac{1}{\\sqrt{\\Omega(X_t)}}\\eps_t \\biggr| X_t \\right) = \\left(\\frac{1}{\\sqrt{\\Omega(X_t)}}\\right)^2 \\V(\\eps_t | X_t)\n\\]\nAnd from eq. 3.1, we can plug in \\(\\V(\\eps_t|X_t) = \\sigma^2 \\ \\Omega(X_t)\\) to get\n\\[\n\\V \\left( \\frac{1}{\\sqrt{\\Omega(X_t)}}\\eps_t \\biggr| X_t \\right) = \\frac{1}{\\Omega(X_t)}\\sigma^2 \\ \\Omega(X_t) = \\sigma^2\n\\]\nWhat does this tell us? Well it tells us the modified error term \\(\\frac{1}{\\sqrt{\\Omega(X_i)}} \\eps_t\\) has a variance of constant \\(\\sigma^2\\) for all units \\(i\\), which does not dependent on \\(X_i\\). What does this mean? Well, our modified error term is now meeting homoscedasticity (definition 5.6)!\nHowever, we obviously cannot just divide the error term by \\(1/\\sqrt{\\Omega(X_t)}\\) - that changes our linear model. What we can though do is divide every term of our linear model by \\(1/\\sqrt{\\Omega(X_t)}\\):\n\\[\n\\frac{Y_t}{\\sqrt{\\Omega(X_t)}} = \\beta_0\\left(\\frac{1}{\\sqrt{\\Omega(X_t)}}\\right) + \\beta_1 \\left(\\frac{X_{t1}}{\\sqrt{\\Omega(X_t)}}\\right) + \\dots +  \\frac{\\eps_t}{\\sqrt{\\Omega(X_t)}}\n\\tag{3.2}\\]\nAnd since we divide both side by \\(1/\\sqrt{\\Omega(X_t)}\\), and our model is conditional on individual \\(t\\) (see all the subscripts), that means this model is still “equivalent” to our original linear model.\nThus, the idea of weighted least squares is to “transform” our heteroscedastic linear model into one that meets homoscedasitcity. We can then just use OLS on our new homoscedastic regression, and since homoscedsaticity is met, Gauss-Markov (theorem 5.5) is met, and our estimator is once again the best linear unbiased estimator.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generalised Least Squares</span>"
    ]
  },
  {
    "objectID": "gls.html#weighted-least-squares",
    "href": "gls.html#weighted-least-squares",
    "title": "3  Generalised Least Squares",
    "section": "3.4 Weighted Least Squares",
    "text": "3.4 Weighted Least Squares\nIn the last section and eq. 3.2, we showed that dividing every term in the linear model by \\(1/\\sqrt{\\Omega(X_t)}\\), where \\(\\Omega(X_t)\\) is some function of \\(X_t\\), can get rid of heteroscedasticity, allowing us to use OLS to estimate the regression.\nLet us formalise this idea with our linear algebra representation of the linear model. First, let us define a matrix \\(\\b\\Omega^{-1/2}\\), which will be the inverse of the square root of the heteroscedastic covariance matrix given in definition 3.1:\n\\[\n\\b\\Omega^{-1/2} = \\begin{pmatrix}\n1/\\sigma_1 & 0 & \\dots & 0 \\\\\n0 & 1/\\sigma_2 & \\vdots & 0 \\\\\n\\vdots & \\dots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 1/\\sigma_n\n\\end{pmatrix}\n\\]\nNote that by this definition, \\(\\b\\Omega^{-1/2} \\b\\Omega^{-1/2} = \\b\\Omega^{-1}\\), since \\(\\b\\Omega^{-1/2}\\) is a square root.\nWe can rewrite our transformed linear model from eq. 3.2 in linear algebra form as:\n\\[\n\\underbrace{\\b\\Omega^{-1/2}}_{\\b y^*} \\b y = \\underbrace{\\b\\Omega^{-1/2} \\b X}_{\\b X^*} \\b \\beta + \\underbrace{\\b\\Omega^{-1/2} \\b \\eps}_{\\b \\eps^*}\n\\tag{3.3}\\]\nWe can see that this model is a transformed linear model \\(\\b y^* = \\b X^* \\b\\beta + \\b\\eps^*\\). Using our OLS estimator from definition 5.8, we know the OLS solution for \\(\\hat{\\b\\beta}\\) for this model is\n\\[\n\\hat{\\b\\beta} = (\\b X^{*\\top} \\b X^*)^{-1} \\b X^{*\\top} \\b y^*\n\\]\nAnd if we plug in our definitions of \\(\\b y^*\\), and \\(\\b X^*\\) from eq. 3.3, we can get\n\\[\n\\hat{\\b\\beta} = \\left[(\\b\\Omega^{-1/2} \\b X)^\\top (\\b\\Omega^{-1/2} \\b X) \\right]^{-1} (\\b\\Omega^{-1/2} \\b X) (\\b\\Omega^{-1/2} \\b y)\n\\]\nAnd using the properties of matrix transposes, and that \\(\\b\\Omega^{-1/2} \\b\\Omega^{-1/2} = \\b\\Omega^{-1}\\), we can get\n\\[\n\\begin{align}\n\\hat{\\b\\beta} & = [\\b X^\\top \\b\\Omega^{-1/2} \\b\\Omega^{-1/2} \\b X]^{-1} \\b X^\\top \\b\\Omega^{-1/2} \\b\\Omega^{-1/2} \\b y \\\\\n& = (\\b X^\\top \\b\\Omega^{-1} \\b X)^{-1} \\b X^\\top \\b\\Omega^{-1} \\b y\n\\end{align}\n\\]\n\nDefinition 3.2 (Weighted Least Squares Estimator) The \\(\\hat{\\b\\beta}\\) estimates produced by the weighted least squares estimator is\n\\[\n\\hat{\\b\\beta} = (\\b X^\\top \\b\\Omega^{-1} \\b X)^{-1} \\b X^\\top \\b\\Omega^{-1} \\b y\n\\]\n\n\n\n\n\n\n\nAlternative Way of Thinking about WLS\n\n\n\n\n\nWe focused on weighted least squares as first transforming a linear model to meet spherical errors, then performing OLS.\nHowever, we can also think about weighted least squares in terms of minimising a weighted sum of squared residuals. The normal OLS estimator minimises the standard sum of squared residuals:\n\\[\nSSR = \\sum (Y_t - \\hat Y_t)^2\n\\]\nThe weighted least squares minimises a weighted version of the sum of squared residuals:\n\\[\nWSSR = \\sum\\frac{1}{w_i}(Y_t - \\hat Y_t)^2\n\\]\nWhen we set our weights equal to \\(\\b\\Omega^{-1/2}\\), both the weighted sum of squared residuals solution and the transformed-model solution get the same result.\n\n\n\nSince the weighted least squares is essentially an OLS estimator on a transformed linear model that has homoscedasticity, it is also covered by the Gauss-Markov theorem (theorem 5.5). Thus, the Weighted Least Squares estimator is the linear unbiased estimator with the least variance, if heteroscedasticity is present in our original linear model.\nFor statistical inference with this estimator, see the generalised least squares section.\nOf course, the weighted least squares estimator requires us to know the structure of \\(\\b\\Omega\\), the covariance-variance matrix of the error term. This is a big limitation as we typically do not know the structure of this. We will explore weights to estimate \\(\\hat{\\b\\Omega}\\) in the feasible generalised least squares estimator later in the chapter.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generalised Least Squares</span>"
    ]
  },
  {
    "objectID": "gls.html#autocorrelation",
    "href": "gls.html#autocorrelation",
    "title": "3  Generalised Least Squares",
    "section": "3.5 Autocorrelation",
    "text": "3.5 Autocorrelation\nSo far, we have weakened the spherical errors assumption (definition 5.6) only through weakening homoscedasticity to heteroscedasticity.\nHowever, the spherical errors assumption also states another assumption - no autocorrelation - i.e. the covariance between two error terms \\(Cov(\\eps_i, \\eps_j) = 0\\). This assumption is typically implied by the i.i.d assumption.\nHowever, there are settings where we cannot assume i.i.d. and no autocorrelation. For example, in time series settings of one variable \\(Y\\) over a set of time periods \\(t\\), we would expect a previous value in time \\(t-1\\) to have some effect on \\(t\\), which implies that values from \\(t-1\\) and \\(t\\) are not independent random variables, and might have some covariance. We will discuss time series in far more detail later in the course.\nSimilarly, in datasets with spatial characteristics, we might expect some neighbouring areas to have correlated errors. For example, if we are modelling \\(Y\\) as unemployment rate, and we have a bunch of \\(X_1, \\dots, X_p\\), we might be missing out on some regional trends, like a regional natural disaster that is causing some neighbouring counties to have higher than expected unemployment rates \\(Y\\).\nThe presence of autocorrelation means that our error-covariance matrix will not be a diagonal matrix, as some \\(Cov(\\eps_i, \\eps_j) ≠ 0\\):\n\\[\n\\V(\\b\\eps | \\b X) = \\b\\Omega = \\begin{pmatrix}\n\\V \\eps_1 & Cov(\\eps_1, \\eps_2) & Cov(\\eps_1, \\eps_3) & \\dots \\\\\nCov(\\eps_2, \\eps_1) & \\V \\eps_2 & Cov(\\eps_2, \\eps_3) & \\dots \\\\\nCov(\\eps_3, \\eps_1) & Cov(\\eps_3, \\eps_2) & \\V\\eps_3 & \\vdots \\\\\n\\vdots & \\vdots & \\dots & \\ddots\n\\end{pmatrix}\n\\]\nJust like with conditional heteroscedasticity, the presence of autocorrelation alone does not cause bias in OLS, since OLS unbiasedness (theorem 5.2) does not depend on spherical errors. However, OLS will no longer be the best-linear unbiased estimator, and (as we will cover more in detail in the time-series chapters later), autocorrelation is often associated with violations of exogeneity (definition 5.4).\nIf we assume that only autocorrelation is present (and no violation of exogeneity is occurring), then we have two approaches, just like in heteroscedasticity.\nWe could stick with the OLS estimator, but use Autocorrelation and Heteroscedasticity Robust (HAC) standard errors. If we are sampling with clustered sampling, we can use clustered standard errors (see below). I will not derive them here, since these standard errors aren’t too commonly used, and are very difficult to derive.\n\n\n\n\n\n\nClustered Standard Errors\n\n\n\n\n\nClustered standard errors are when you have done clustered sampling for your observations. For example, you randomly sample 100 people from 100 different villages.\nThe errors of observations belonging to the same cluster (say village) might exhibit correlation, while errors of observtations from distinct clusters are assumed to be uncorrelated.\nOur covariance matrix might take the form:\n\\[\n\\b\\Omega = \\begin{pmatrix}\n\\b\\Sigma_1 & 0 & \\dots & 0 \\\\\n0 & \\b\\Sigma_2 & & 0 \\\\\n\\vdots & 0 & \\ddots & 0 \\\\\n0 & 0& \\dots & \\b\\Sigma_G\n\\end{pmatrix}\n\\]\nWhere \\(\\b\\Sigma_1, \\dots \\b\\Sigma_G\\) are intracluster covariance-variance error matrices, that exhibit autocorrelation. We just insert this matrix where we would insert it for robust standard errors.\n\n\n\nOr, we could use another estimator, such as the Cochrane-Orcutt Estimator or the Generalised Least Squares estimator, which we will introduce later in the chapter.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generalised Least Squares</span>"
    ]
  },
  {
    "objectID": "gls.html#cochrane-orcutt-estimator",
    "href": "gls.html#cochrane-orcutt-estimator",
    "title": "3  Generalised Least Squares",
    "section": "3.6 Cochrane-Orcutt Estimator",
    "text": "3.6 Cochrane-Orcutt Estimator\nLet us start off with a linear model, but in time-series form, so instead of observation \\(i\\), each variable/observation will be from a point in time \\(t\\). Our linear model will be\n\\[\nY_t = \\beta_0 + \\beta_1 X_{t}  + \\eps_t\n\\]\nLet us say some autocorrelation is present - which means the error term \\(\\eps_t\\) is related to some other error term of another observation. More specifically, let us assume an autoregressive order 1 autocorrelation, which means that \\(\\eps_t\\) is correlated with the error term of the time period before, \\(\\eps_{t-1}\\). We can model this as:\n\\[\n\\eps_t = \\rho \\eps_{t-1} + u_t\n\\]\nWhere \\(\\rho\\) is the coefficient describing the correlation between \\(\\eps_t\\) and \\(\\eps_{t-1}\\), and \\(u_t\\) is the error term of this smaller model that is the part of \\(\\eps_t\\) that is not explained by \\(\\eps_{t-1}\\).\nThus, our true linear model is:\n\\[\nY_t = \\beta_0 + \\beta_1 X_{t} + \\rho \\eps_{t-1} + u_t\n\\]\nIf we could get the \\(\\rho\\eps_{t-1}\\) term out of this equation, we will no longer have any autocorrelation, since \\(u_t\\) is not correlated/explained by past error terms. How do we do this?\nConsider the linear model for \\(Y_{t-1}\\):\n\\[\nY_{t-1} = \\beta_0 + \\beta_1 X_{t-1}+ \\eps_{t-1}\n\\]\nNow, let us multiply both sides (every term) with parameter \\(\\rho\\):\n\\[\n\\rho Y_{t-1} = \\rho\\beta_0 + \\rho\\beta_1 X_{t-1} + \\rho\\eps_{t-1}\n\\]\nNow, let us subtract our model for \\(\\rho Y_{t-1}\\) from our original \\(Y_t\\):\n\\[\n\\begin{array}{ccccccc}\nY_t & = & \\beta_0 & + & \\beta_1X_t & + & \\rho\\eps_{t-1} + u_t \\\\\n\\rho Y_{t-1} & = & \\rho\\beta_0 & + & \\rho\\beta_1X_{t-1} & + & \\rho\\eps_{t-1} \\\\\n\\hline\nY_t - \\rho Y_{t-1} & = & \\beta_0(1-\\rho) & + & \\beta_1(X_t - \\rho X_{t-1}) & + & u_t\n\\end{array}\n\\]\nNow we can see we have a new transformed model with only error term \\(u_t\\) which is not autocorrelated with \\(t-1\\).\n\\[\n\\underbrace{Y_t - \\rho Y_{t-1}}_{Y_t^*} = \\underbrace{\\beta_0(1-\\rho)}_{\\beta_0^*} + \\beta_1 \\underbrace{(X_t - \\rho X_{t-1})}_{X_t^*} + \\underbrace{u_t}_{\\eps_t^*}\n\\]\nWhich we can rewrite more simply as:\n\\[\nY_t^* = \\beta_0^* + \\beta_1 X_t^* + \\eps_t^*\n\\]\nSince this model no longer has autocorrelation and now meets spherical errors, we can use the OLS estimator on this transformed model, and this will be the best linear unbiased estimator.\nDo not that since this model depends on subtracting time \\(t-1\\) from time \\(t\\), the first time period in a time-series must be thrown out to complete this transformation. In small sample sizes, this can lead to efficiency losses.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generalised Least Squares</span>"
    ]
  },
  {
    "objectID": "gls.html#generalised-least-squares-estimator",
    "href": "gls.html#generalised-least-squares-estimator",
    "title": "3  Generalised Least Squares",
    "section": "3.7 Generalised Least Squares Estimator",
    "text": "3.7 Generalised Least Squares Estimator\nBoth the Cochrane-Ocrutt Estimation and the Weighted Least Squares estimator can be thought of as specific versions of the generalised least squares estimator.\nIn spherical errors, we assumed that the population variance-covariance matrix of errors had the following form:\n\\[\n\\V(\\b\\eps | \\b X) = \\E(\\b{\\eps\\eps^\\top}) =  \\sigma^2 \\b I_n\n\\]\nAnd the variance is equivalent to \\(\\E(\\b{\\eps\\eps^\\top})\\) because we assume by strict exogeneity (definition 5.4) that \\(\\E(\\b\\eps) = 0\\). So the formula for variance (definition 1.3) with strict exogeneity equals the variance of the error term.\nIn the generalised least squares estimator, we assume that the variance-covariance matrix of errors has the form:\n\\[\n\\V(\\b\\eps | \\b X) = \\E(\\b{\\eps\\eps^\\top}) = \\sigma^2 \\b\\Omega\n\\]\nWhere \\(\\sigma^2\\) is an unknown scalar constant, but \\(\\b\\Omega\\) is a known matrix that is equivalent to the population variance-covariance matrix of errors (up to a scalar factor).\nLet us define a matrix \\(\\b\\Omega^{-1/2}\\), which will be the inverse of the square root of \\(\\b\\Omega\\). This means that the following should be true:\n\\[\n\\b\\Omega^{-1/2} \\ \\b\\Omega \\ {\\b\\Omega^{-1/2}}^\\top = \\b I\n\\tag{3.4}\\]\nWe can use \\(\\b\\Omega^{-1/2}\\) to transform our original model \\(\\b y = \\b{X\\beta} + \\b\\eps\\) to get:\n\\[\n\\underbrace{\\b\\Omega^{-1/2}}_{\\b y^*} \\b y = \\underbrace{\\b\\Omega^{-1/2} \\b X}_{\\b X^*} \\b \\beta + \\underbrace{\\b\\Omega^{-1/2} \\b \\eps}_{\\b \\eps^*}\n\\tag{3.5}\\]\nThis transformed model should have spherical errors. Let us calculate the variance of \\(\\b\\eps^*\\) by plugging in the definition of \\(\\b\\eps^*\\):\n\\[\n\\begin{align}\n\\V (\\b\\eps^* | \\b X) & = \\E(\\b\\eps^* \\b\\eps^{*\\top}) \\\\\n& = \\E(\\b\\Omega^{-1/2} \\b \\eps \\b\\eps^\\top {\\b\\Omega^{-1/2}}^\\top) \\\\\n& = \\b\\Omega^{-1/2} \\E(\\b{\\eps \\eps^\\top}) \\b\\Omega^{-1/2} \\\\\n& = \\b\\Omega^{-1/2} \\sigma^2 \\b\\Omega \\b\\Omega^{-1/2}\n\\end{align}\n\\]\nAnd by moving scalar \\(\\sigma^2\\) to the front, and using the property from eq. 5.5, we get:\n\\[\n\\V (\\b\\eps^* | \\b X) = \\sigma^2 \\underbrace{\\b\\Omega^{-1/2} \\b\\Omega \\b\\Omega^{-1/2}}_{\\b I} = \\sigma^2 \\b I\n\\]\nThus proving this transformed model meets the spherical errors assumption (definition 5.6). Thus, we can use OLS on this transformed model, and it will be the best linear unbiased estimator. Our OLS estimator (definition 5.8) of the transformed model will be:\n\\[\n\\hat{\\b\\beta} = (\\b X^{*\\top} \\b X^*)^{-1} \\b X^{*\\top} \\b y^*\n\\]\nAnd if we plug in our definitions of \\(\\b y^*\\), and \\(\\b X^*\\) from eq. 5.6, we can get\n\\[\n\\hat{\\b\\beta} = \\left[(\\b\\Omega^{-1/2} \\b X)^\\top (\\b\\Omega^{-1/2} \\b X) \\right]^{-1} (\\b\\Omega^{-1/2} \\b X) (\\b\\Omega^{-1/2} \\b y)\n\\]\nAnd using the properties of matrix transposes, and that \\(\\b\\Omega^{-1/2} \\b\\Omega^{-1/2} = \\b\\Omega^{-1}\\), we can get\n\\[\n\\begin{align}\n\\hat{\\b\\beta} & = [\\b X^\\top \\b\\Omega^{-1/2} \\b\\Omega^{-1/2} \\b X]^{-1} \\b X^\\top \\b\\Omega^{-1/2} \\b\\Omega^{-1/2} \\b y \\\\\n& = (\\b X^\\top \\b\\Omega^{-1} \\b X)^{-1} \\b X^\\top \\b\\Omega^{-1} \\b y\n\\end{align}\n\\]\n\nDefinition 3.3 (Generalised Least Squares Estimator) The GLS estimator is\n\\[\n\\hat{\\b\\beta} = (\\b X^\\top \\b\\Omega^{-1} \\b X)^{-1} \\b X^\\top \\b\\Omega^{-1} \\b y\n\\]\nWhere \\(\\b\\Omega\\) is the population variance-covariance matrix of errors. The variance is\n\\[\n\\V\\hat{\\b\\beta} = (\\b X^\\top \\b\\Omega^{-1} \\b X)^{-1}\n\\]\n\n\nWe can see that OLS is a GLS estimator, when \\(\\b\\Omega = \\b I\\).",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generalised Least Squares</span>"
    ]
  },
  {
    "objectID": "gls.html#feasible-generalised-least-squares",
    "href": "gls.html#feasible-generalised-least-squares",
    "title": "3  Generalised Least Squares",
    "section": "3.8 Feasible Generalised Least Squares",
    "text": "3.8 Feasible Generalised Least Squares\nWe have derived the generalised least squares (GLS) estimator’s parameter estimates and variance. However, we do not know what \\(\\b\\Omega\\) is, as it is made up of the population variance and covariances of error terms \\(\\eps_i\\). Thus, we need some estimator of \\(\\hat{\\b\\Omega}\\) to actually implement generalised least squares. When combining this estimator of \\(\\hat{\\b\\Omega}\\) with GLS, we call this new estimator the feasible generalised least squares (FGLS) estimator.\nWe have two ways to get a consistent estimator of \\(\\b\\Omega\\). First, if we have some strong idea of the form of heteroscedasticity or autocorrelation in our population (perhaps due to past research or some strong internal reasoning), we could estimate \\(\\b\\Omega\\).\nFor example, if we believe that there is autocorrelation defined by the Autoregerssive first order process from the Cochrane-Orcutt Estimator, then we know the structure of \\(\\b\\Omega\\) is:\n\\[\n\\b\\Omega = \\begin{pmatrix}\n1 & \\rho & \\rho^2 & \\dots & \\rho^{n-1} \\\\\n\\rho & 1 & \\rho & \\dots & \\rho^{n-2} \\\\\n\\rho^2 & \\rho & 1 & \\dots & \\rho^{n-3} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho^{n-1} & \\rho^{n-2} & \\rho^{n-3} & \\dots & 1\n\\end{pmatrix}\n\\]\nOther examples of knowing \\(\\b\\Omega\\) include financial data heteroscedasticity being proportional to some known factor like the market capitalisation of a unit, or spatial data where autocorrelation can be modeled as a [decaying] function of distance.\nThe other option (which is far more common) is to estimate \\(\\b\\Omega\\). We typically produce a estimate \\(\\hat{\\b\\Omega}\\) by first running an OLS regression, in which we will obtain the residuals \\(\\hat\\eps_i\\). These can be used to estimate the structure of \\(\\b\\Omega\\), producing \\(\\hat{\\b\\Omega}\\). Then, using this estimate \\(\\hat{\\b\\Omega}\\), we can run feasible GLS, and obtain an estimator with less error.\n\nDefinition 3.4 (Feasible Generalised Least Squares Estimator) The feasible generalised least squares estimator produces estimates\n\\[\n\\hat{\\b\\beta} = (\\b X^\\top \\hat{\\b\\Omega}^{-1} \\b X)^{-1} \\b X^\\top \\hat{\\b\\Omega}^{-1} \\b y\n\\]\nWhere \\(\\hat{\\b\\Omega}\\) is the estimated variance-covariance matrix of the error term \\(\\eps_i\\), frequently through an OLS estimation before conducting feasible generalised least squares. The variance of the estimator is\n\\[\n\\V\\hat{\\b\\beta} = (\\b X^\\top \\hat{\\b\\Omega}^{-1} \\b X)^{-1}\n\\]\n\n\nThe square root of the variance of the FGLS estimator are the standard errors, and we can conduct t-tests just as we did for OLS.\nHowever, when we estimate \\(\\hat{\\b\\Omega}\\) with OLS (or any other method), we of course have some imprecision in our estimates. Econometricians have shown that the feasible GLS estimator often is far worse than the hypothetical perfect GLS. Very often, feasible GLS will actually result in larger variances of estimates.\nThere is also some risk with feasible GLS. Often, heteroscedasticity and autocorrelation occur in our estimated OLS models not because the population actually has heteroscedasticity or autocorrelation, but rather, our original linear model is missing some explanatory variables which causes other violations in our classical linear model, such as exogeneity violations. This mispecified nature will not only make FGLS even more imprecise, but also has the potential to bias FGLS estimates.\nThus, FGLS is not super popular in most applied statistician’s toolkit, and the default tends to be sticking to OLS with either robust standard errors or Heteroscedasticity-and-autocorrelation (HAC) robust standard errors.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generalised Least Squares</span>"
    ]
  },
  {
    "objectID": "mle.html",
    "href": "mle.html",
    "title": "3  Maximum Likelihood",
    "section": "",
    "text": "3.1 Likelihood Function\nThe maximum likelihood estimator allows us to estimate parameters \\(\\theta\\) as long as we know the form/probability density function of the data generating process.\nIn statistics, to estimate the population parameters \\(\\b\\theta\\), we gather some sample of observations \\((y_1, y_2, \\dots, y_n)\\). Our random sample \\((y_1, y_2, \\dots, y_n)\\) are each realisations of a random variable (our random selection from the population) \\(Y_1, Y_2, \\dots, Y_n\\).\nWhat is the probability that we draw \\(y_1\\) from our random variable \\(Y_1\\)? Well, we know that the probability of realising a specific \\(y\\) from a random variable \\(Y\\) is given by the probability density function (definition 1.1). Thus, the probability of drawing \\(y_1\\) is:\n\\[\n\\P(Y_1 = y_1) = f_{Y_1}(y_1)\n\\]\nHowever, our probability density function \\(f_{Y_1}\\) is determined by a set of parameters. For example, if \\(Y_1\\) is normally distributed \\(Y_1 \\sim \\mathcal N(\\mu_{Y_1}, \\sigma^2_{Y_1})\\), then the probability density function \\(f_{Y_1}\\) is determined by parameters \\(\\b \\theta = (\\mu_{Y_1}, \\sigma^2_{Y_1})\\). To represent this fact, in MLE, we will put the unknown parameters as a second input in our probability density function:\n\\[\n\\P(Y_1 = y_1) = f_{Y_1}(y_1; \\b \\theta)\n\\]\nSimilarly, the probability of drawing \\(y_2\\) from \\(Y_2\\) is \\(f_{Y_2}(y_2; \\b \\theta)\\), and the probability of drawing \\(y_t\\) from \\(Y_t\\) is \\(f_{Y_t}(y_t; \\b \\theta)\\).\nOkay, we know the probability of drawing any observation \\(y_t\\) from \\(Y_t\\) But, what is the probability of drawing the exact sample \\((y_1, y_2, \\dots, y_n)\\) that we got? Or more mathematically, what is the probability of\n\\[\n\\P(Y_1 = y_1, \\ Y_2 = y_2, \\ \\dots, \\ Y_n = y_n)\n\\]\nWell, we know that if random variables are independent (definition 1.6) of each other (i.e. drawing \\(y_1\\) doesn’t affect the probability of drawing \\(y_2\\)), the joint probability density function is just all the product of all the PDFs. Thus, if \\(Y_1, \\dots, Y_n\\) are independent random variables, we know the joint probability density function is\n\\[\n\\begin{align}\nf_{Y_1, \\dots , Y_n}(y_1, \\dots, y_n; \\ \\b\\theta ) & = f_{Y_1}(y_1; \\b\\theta) f_{Y_2}(y_2; \\b\\theta) \\dots f_{Y_n}(y_n; \\b\\theta) \\\\\n& = \\prod\\limits_{t=1}^nf_{Y_t}(y_t; \\b\\theta)\n\\end{align}\n\\]\nAnd this joint probability density function gives us the exact probability of getting our exact sample \\((y_1, \\dots, y_n)\\) through random sampling. We define this probability as the likelihood \\(L\\) of getting our sample \\((y_1, \\dots, y_n)\\), given our population parameters \\(\\b\\theta\\).",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "mle.html#likelihood-function",
    "href": "mle.html#likelihood-function",
    "title": "3  Maximum Likelihood",
    "section": "",
    "text": "Definition 3.1 (Likelihood Function) The likelihood function \\(L(\\b\\theta; \\b y)\\) describes the probability that we get some sample \\(\\b y = (y_1, \\dots, y_n)\\), given population parameters \\(\\b\\theta\\).\n\\[\nL(\\b\\theta; \\ y_1, y_2, \\dots , y_n) = \\prod\\limits_{t=1}^nf_{Y_t}(y_t; \\b\\theta)\n\\]\nWe can also write this in terms of vector \\(\\b y\\) containing all observations of the sample:\n\\[\nL(\\b\\theta; \\ \\b y) = f_Y(\\b y; \\b \\theta)\n\\]",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "mle.html#maximum-likelihood",
    "href": "mle.html#maximum-likelihood",
    "title": "3  Maximum Likelihood",
    "section": "3.2 Maximum Likelihood",
    "text": "3.2 Maximum Likelihood\nIn the above section, we defined the likelihood function (definition 3.1) as the likelihood of getting the exact sample we got, given the population parameters \\(\\b\\theta\\).\nThe maximum likelihood estimator is the idea that we should estimate our parameters \\(\\hat{\\b\\theta}\\) as the values of the parameters that maximise the likelihood we observe our specific sample \\(y_1, y_2, \\dots y_n\\).\nThe reason is because if there was a set of potential parameter values \\(\\b\\theta^*\\) that had a low chance of producing our sample \\(y_1, \\dots, y_n\\), we probably would not get our exact sample \\(y_1, \\dots, y_n\\). Since we would probably not get the exact sample we got, we either got really unlucky, or that set of potential parameter values \\(\\b\\theta^*\\) is wrong.\nBut if there was a set of potential parameter values \\(\\b\\theta^*\\) that was likely to produce our exact sample \\(y_1, \\dots, y_n\\), we have a good chance we actually getting our exact sample \\(y_1, \\dots, y_n\\). Thus, we want to find the set of parameters \\(\\hat{\\b\\theta}\\) that maximise our chances of observing our specific sample \\(y_1 \\dots, y_n\\).\nTo find the set of parameters \\(\\hat{\\b\\theta}\\) that maximise the likelihood we observe our specific sample, we need to find values of \\(\\b\\theta\\) that maximise our likelihood function \\(L(\\b \\theta, \\b y)\\). Thus our goal is\n\\[\n\\hat{\\b\\theta} = \\max\\limits_{\\b\\theta} L(\\b\\theta; \\b y)\n\\]\nHowever, this maximisation problem is quite difficult, as the likelihood function (definition 3.1) is a product. Finding the derivative of a product is not straight forward.\nLuckily, we have an alternative, the log-likelihood function \\(\\ell(\\b\\theta ; \\b y)\\), which is far easier to maximise, and the same \\(\\b\\theta\\) values that maximise \\(L\\) will also maximise \\(\\ell\\). The log-likelihood function can be derived by taking the log of the likelihood function\n\\[\n\\begin{align}\n\\ell(\\b \\theta; \\b y) & = \\log L(\\b \\theta; \\b y) \\\\\n& = \\log\\left( \\prod\\limits_{t=1}^n f_{Y_t}(y_t; \\b\\theta)\\right) \\\\\n& = \\log[ f_{Y_1}(y_1, \\b\\theta) f_{Y_2}(y_2; \\b\\theta) \\dots f_{Y_n}(y_n; \\b\\theta) ]\n\\end{align}\n\\]\nAnd using the property of logs that \\(\\log(ab) = \\log a + \\log b\\), we can determine\n\\[\n\\begin{align}\n\\ell(\\b \\theta; \\b y) & = \\log(f_{Y_1}(y_1; \\b\\theta)) + \\log(f_{Y_2}(y_2; \\b\\theta)) + \\dots + f_{Y_n}(y_n; \\b\\theta) \\\\\n& = \\sum\\limits_{t=1}^n\\log (f_{Y_t}(y_t; \\b\\theta))\n\\end{align}\n\\]\n\nDefinition 3.2 (Log-Likelihood Function) The log-likelihood function \\(\\ell\\) is the log of the likelihood function \\(L\\) (definition 3.1). The log-likelihood function takes the form\n\\[\n\\ell(\\b \\theta; \\b y) = \\sum\\limits_{t=1}^n\\log (f_{Y_t}(y_t; \\b\\theta))\n\\]\nWe can also write this in terms of vector \\(\\b y\\) containing all observations of the sample:\n\\[\n\\ell(\\b \\theta; \\b y)  = \\log(f_Y(\\b y; \\ \\b\\theta))\n\\]",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "mle.html#score-function",
    "href": "mle.html#score-function",
    "title": "3  Maximum Likelihood",
    "section": "3.3 Score Function",
    "text": "3.3 Score Function\nThe gradient of the log-likelihood \\(\\ell(\\b\\theta; \\b y)\\) in respect to vector \\(\\b\\theta\\) is the score function.\n\nDefinition 3.3 (Score Function) The score function \\(s\\) is given by:\n\\[\ns(\\b\\theta; \\b y) = \\frac{\\partial}{\\partial \\b\\theta} \\ell(\\b\\theta; \\b y) = \\frac{\\partial}{\\partial \\b\\theta}  \\sum\\limits_{t=1}^n \\log(f_{Y_t}(y_t; \\b\\theta))\n\\]\nWe can also write this in terms of vector \\(\\b y\\) containing all observations of the sample:\n\\[\ns(\\b\\theta; \\b y) = \\frac{\\partial}{\\partial\\b\\theta} \\log(f_Y(\\b y; \\b\\theta))\n\\]\n\n\nAs we know through calculus, to maximise a function, we set the gradient equal to zero. Thus, the estimates \\(\\hat{\\b\\theta}\\) of maximum likelihood estimation are the set of \\(\\b\\theta\\) that make \\(s(\\b\\theta; \\b y) = 0\\).\nLet us define the true parameter value in the population as \\(\\b\\theta_0\\). That means, the true score function of \\(\\b\\theta_0\\) is \\(s(\\b\\theta_0; \\b y)\\).\nVector \\(\\b y\\), our sample, is the reasliation of a set of random variables as we described earlier. Thus, the true population parameter \\(\\b\\theta_0\\)’s score function \\(s(\\b\\theta_0, \\b y)\\) is actually also a random variable in respect to random \\(\\b y\\). This means the true population parameter score function \\(s(\\b\\theta_0, \\b y)\\) has a expectation and variance.\n\nTheorem 3.1 The expectation of the true population parameter \\(\\b\\theta\\)’s score function is 0.\n\\[\n\\E(s(\\b\\theta_0; \\b y)) = 0\n\\]\n\n\nProof: Let us first start with the definition of expectation of a continuous variable (definition 1.2). That means we can deduce\n\\[\n\\E(s(\\b\\theta_0; \\b y)) = \\int \\underbrace{s(\\b\\theta_0; \\b y)}_{\\mathrm{s \\ given} \\ \\b y} \\overbrace{f_Y(\\b y ; \\b\\theta)}^{\\P(Y=\\b y)} dy\n\\]\nNow, let us plug in the score function (definition 3.3):\n\\[\n\\E(s(\\b\\theta_0; \\b y)) = \\int\\left[\\frac{\\partial}{\\partial\\b\\theta}\\log f_Y(\\b y; \\b\\theta)\\right] f_Y(\\b y; \\b\\theta)\n\\]\nUsing the derivative rule \\(\\frac{d}{dx}\\log u(x) = \\frac{u'(x)}{u(x)}\\), we get\n\\[\n\\begin{align}\n\\E(s(\\b\\theta_0; \\b y)) & = \\int\\frac{\\frac{\\partial}{\\partial\\b\\theta} f_Y(\\b y; \\b \\theta)}{f_Y(\\b y; \\b \\theta)}f_Y(\\b y; \\b \\theta) \\\\\n& = \\int \\frac{\\partial}{\\partial\\b\\theta} f_Y(\\b y; \\b \\theta)\n\\end{align}\n\\]\nWe can flip the derivative and anti-derivative to get\n\\[\n\\begin{align}\n\\E(s(\\b\\theta_0; \\b y)) & =  \\frac{\\partial} {\\partial\\b\\theta} \\int f_Y(\\b y; \\b \\theta) \\\\\n& = \\frac{\\partial} {\\partial\\b\\theta} 1 \\ = \\ 0\n\\end{align}\n\\]\nAnd the last step is because the integral (are under the curve) of a PDF is always 1 (the entire probability space). Thus, we see that the expectation of the score function at true population parameter \\(\\b\\theta_0\\) in respect to random vector \\(\\b y\\) is 0.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "mle.html#fisher-information",
    "href": "mle.html#fisher-information",
    "title": "3  Maximum Likelihood",
    "section": "3.4 Fisher Information",
    "text": "3.4 Fisher Information\nWe have established the expected value of the score function of the true population parameter \\(\\b\\theta_0\\). As a random variable, we can also consider its variance. From the definition of variance (definition 1.3), we know:\n\\[\n\\V[s(\\b\\theta_0; \\b y)] = \\E[s(\\b\\theta_0; \\b y) - \\E(s(\\b\\theta_0; \\b y))]\n\\]\nWe know the \\(E(s(\\b\\theta_0; \\b y)) = 0\\) from the proof above (theorem 3.1), so we can plug that in:\n\\[\n\\V[s(\\b\\theta_0; \\b y)] = \\E[s(\\b\\theta_0; \\b y) - 0)^2]\n\\]\nNow, plugging in the definition of the score function (definition 3.3), we see\n\\[\n\\begin{align}\n\\V[s(\\b\\theta_0; \\b y)] & = \\E \\left[ \\left(\\frac{\\partial \\ell(\\b\\theta_0; \\b y)}{\\partial\\b\\theta}\\right)^2 \\right] \\ \\equiv \\ \\b{\\mathcal I}(\\b\\theta_0)\n\\end{align}\n\\]\nWhere \\(\\mathcal I(\\b\\theta_0)\\) is called the fisher information matrix of \\(\\b\\theta_0\\). We can generalise this to any values of \\(\\b\\theta\\):\n\nDefinition 3.4 (Fisher Information Matrix) The fisher information matrix is given by the variance of the score function for any \\(\\b\\theta\\) value:\n\\[\n\\b{\\mathcal I}(\\b\\theta) = \\E \\left[ \\left( \\frac{\\partial}{\\partial \\b\\theta} \\ell(\\b\\theta ; \\b y) \\right)^2 \\biggr | \\b\\theta\\right]\n\\]\nWe can also define the fisher information matrix as the negative expectation of the hessian matrix of second derivatives of the log-likelihood function.\n\\[\n\\b{\\mathcal I}(\\b\\theta) = - \\E\\left[\\frac{\\partial^2}{\\partial \\b\\theta \\partial \\b\\theta^\\top} \\ell(\\b\\theta; \\b y) \\right]\n\\]\n\n\nThe fischer information is always positive: \\(\\mathcal I(\\theta) ≥ 0\\). Higher fisher information implies that the absolute value of the score is higher. Also importantly, the fisher information does not depend on the random realisation of sample \\(\\b y\\), since the expectation averages it our.\nHowever, the fischer information matrix can be sometimes difficult to calculate. Thus, we sometimes use what is called the observed information matrix \\(\\b I(\\b\\theta, \\b y)\\), without the expectation.\n\nDefinition 3.5 (Observed Information Matrix) The observed information matrix is defined as\n\\[\n\\b I(\\b\\theta; \\b y) = -\\frac{\\partial^2}{\\partial\\b\\theta \\partial \\b\\theta^\\top} \\ell(\\b\\theta; \\b y)\n\\]\nAnd it is the negative of the hessian matrix of second order derivatives of the log-likelihood function.\n\n\nUnlike the fisher information matrix, which is only dependent on \\(\\b\\theta\\) and not the realisation of sample \\(\\b y\\) (due to expectation), the observed information matrix is dependent on the random realisation of sample \\(\\b y\\).",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "mle.html#variance-and-asymptotics",
    "href": "mle.html#variance-and-asymptotics",
    "title": "3  Maximum Likelihood",
    "section": "3.4 Variance and Asymptotics",
    "text": "3.4 Variance and Asymptotics\nIt can be shown through complex math, that the inverse of the fischer information matrix \\(1/\\b{\\mathcal I}(\\b\\theta)\\) is the variance of the maximum likelihood estimator.\n\nTheorem 3.2 (Variance of MLE) The variance of the maximum likelihood estimator (in sufficiently large sample sizes) is:\n\\[\n\\V \\hat{\\b\\theta} = \\frac{1}{\\b{\\mathcal I}(\\b\\theta)}\n\\]\n\n\nYou probably have noticed that I stated the variance condition only holds for large sample sizes. This is because the fischer information matrix is under the assumption of an unbiased estimator, which as we will see in the next section, MLE is only unbiased asymptotically.\nTo estimate the variance for smaller sample sizes, we typically use the observed information matrix instead:\n\\[\n\\V \\hat{\\b\\theta} = \\frac{1}{\\b I (\\hat{\\b\\theta}; \\b y)}\n\\]\nThe Maximum Likelihood Estimator has some desirable asymptotic properties as sample size \\(n \\rightarrow ∞\\), that make is a very popular estimator in statistics.\nThrough central limit theorem (theorem 2.2), we know asymptotically that \\(\\hat\\theta\\) from MLE will be normally distributed. Through quite complex mathematics, we can also show that the MLE is asymptotically consistent (definition 2.6). Finally, in the last section, we determined the asymptotic variance of the MLE (theorem 3.2). Thus, we know the asymptotic properties of the MLE.\n\nTheorem 3.3 (Asymptotic Properties of MLE) The maximum likelihood estimator has the following asymptotic distribution:\n\\[\n\\hat{\\b\\theta} \\sim \\mathcal N(\\b\\theta_0, \\b{\\mathcal I}(\\b\\theta_0)^{-1} )\n\\]\nThis means that MLE is asymptotically consistent, asymptotically normal, and has an asymptotic variance of \\(1/\\b{\\mathcal I}(\\b\\theta_0)\\).\n\n\nNotably, the maximum likelihood estimator is the asymptotically unbiased (consistent) estimator with the lowest variance, which we will prove in the next section on Cramér Rao.\nIt is important to note that while MLE is asymptotically unbiased (consistent), it can be biased in lower sample sizes (although not always - this depends on the actual data generating process in question). This can have some implications on model selection (specifically fixed effects), that we will cover more in the applied chapters.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "mle.html#cramér-rao-bound",
    "href": "mle.html#cramér-rao-bound",
    "title": "3  Maximum Likelihood",
    "section": "3.5 Cramér-Rao Bound",
    "text": "3.5 Cramér-Rao Bound\nLet us focus on the asymptotic variance of the maximum likelihood estimator. We see that it is \\(1/\\b{\\mathcal I}(\\b\\theta_0)\\). Why is this important? Let us introduce a new theorem: Cramér-Rao.\n\nTheorem 3.4 (Cramér-Rao) The Cramér-Rao Bound states that for any unbiased estimator, the variance cannot be any lower than the inverse of the fischer information matrix:\n\\[\n\\V \\hat{\\b\\theta} ≥ \\frac{1}{\\b{\\mathcal I}(\\b\\theta)}\n\\]\nSince the MLE has that exact variance asymptotically, it is the asymptotically unbiased (consistent) estimator with the least variance.\n\n\nProof: An unbiased estimator \\(\\hat\\theta\\) of a parameter \\(\\theta\\), its estimates are a function of its sample \\(\\b y\\), so we will call the unbiased estimator \\(\\hat\\theta(\\b y)\\). We know an unbiased estimator \\(\\hat\\theta(\\b y)\\) should have a expectation equal to true population parameter, which also implies\n\\[\n\\E \\hat\\theta(\\b y) = \\theta \\quad \\implies \\quad \\E[\\hat\\theta(\\b y) - \\theta \\ | \\ \\theta \\ ] = 0\n\\]\nRegardless of the value of \\(\\theta\\). We can rewrite expectation with the definition of expectation (definition 1.2) to get:\n\\[\n\\int\\left( \\hat\\theta(y) - \\theta \\right) f_Y(y; \\theta)dy = 0\n\\]\nSince this expectation is independent of \\(\\theta\\) (true for all \\(\\theta\\)), the derivative in respect to \\(\\theta\\) should be 0 (since \\(\\theta\\) does not cause change in the expectation), and we can compute the derivative with product rule:\n\\[\n\\begin{align}\n0 & = \\frac{\\partial}{\\partial \\theta} \\int \\left(\\hat\\theta(y) - \\theta \\right)f_Y(y ; \\theta)dy \\\\\n0 & = \\int \\left(\\hat\\theta(y) - \\theta \\right)\\frac{\\partial f_Y}{\\partial \\theta}dy - \\int f_Y dy\n\\end{align}\n\\]\nWhere I have shortened \\(f_Y(y; \\theta)\\) to \\(f_Y\\) for simplicity. Since \\(f_Y\\) is a PDF, \\(\\int f_Y dy = 1\\), thus\n\\[\n\\begin{align}\n0 & = \\int \\left(\\hat\\theta(y) - \\theta \\right)\\frac{\\partial f_Y}{\\partial \\theta}dy - 1\\\\\n1 & = \\int \\left(\\hat\\theta(y) - \\theta \\right)\\frac{\\partial f_Y}{\\partial \\theta}dy\n\\end{align}\n\\]\nWe can substitute \\(\\frac{\\partial f_Y}{\\partial \\theta} = f_y \\frac{\\partial \\log f_Y}{\\partial \\theta}\\). You can prove this is true by using chain rule. Thus,\n\\[\n1 = \\int \\left(\\hat\\theta(y) - \\theta \\right) f_Y \\frac{\\partial \\log f_Y}{\\partial \\theta}dy\n\\]\nWe can factor \\(f_Y\\) in half to get:\n\\[\n1 = \\int \\left( (\\hat\\theta(y) - \\theta) \\sqrt{f_Y} \\right) \\left(\\sqrt{f_Y} \\frac{\\partial \\log f_Y}{\\partial \\theta} \\right)dy\n\\tag{3.1}\\]\nWe know by the Cauchy-Schwarz inequalty, that this must be true:\n\\[\n\\begin{align}\n\\int \\left( (\\hat\\theta(y) - \\theta) \\sqrt{f_Y} \\right)&  \\left(\\sqrt{f_Y} \\frac{\\partial \\log f_Y}{\\partial \\theta} \\right)dy \\\\  ≤ \\ & \\left(\\int (\\hat\\theta(y) - \\theta)^2 f_Y dy \\right) \\left( \\int f_Y \\left(\\frac{\\partial \\log f_Y}{\\partial \\theta}\\right)^2 dy \\right)\n\\end{align}\n\\]\nAnd we know the left side from eq. 3.1 is equal to 1, so we get the inequality:\n\\[\n1 ≤ \\left(\\int (\\hat\\theta(y) - \\theta)^2 f_Y dy \\right) \\left( \\int f_Y \\left(\\frac{\\partial \\log f_Y}{\\partial \\theta}\\right)^2 dy \\right)\n\\]\nWe can see both parts of the right side are in the form of expectations (definition 1.2), so let us write them as expectations.\n\\[\n1 ≤ \\color{blue}{\\E[(\\hat\\theta - \\theta)^2]} \\cdot  \\color{purple}{\\E\\left[ \\left(\\frac{\\partial \\log f_Y}{\\partial \\theta}\\right)^2 \\right]}\n\\]\nWe can see the blue is the variance (definition 1.3) of an unbiased estimator (since \\(\\E \\hat\\theta = \\theta\\)), and the purple part is the fisher information matrix (definition 3.4). Thus, isolating the variance on one side, we get\n\\[\n\\V \\hat\\theta ≤ \\frac{1}{\\mathcal I(\\theta)}\n\\]\nThis is the lowest bound any unbiased estimator’s variance can be, meaning any unbiased estimator who attains this variance is the unbiased estimator with the least variance.\nWe know that the maximum likelihood estimator is unbiased asymptotically (consistent), and its asymptotic variance is \\(1/\\mathcal I(\\theta)\\). Thus, the maximum likelihood estimator is the unbiased estimator with the least variance, and is the best asymptotic unbiased estimator.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "mle.html#newton-raphson-algorithm",
    "href": "mle.html#newton-raphson-algorithm",
    "title": "3  Maximum Likelihood",
    "section": "3.6 Newton-Raphson Algorithm",
    "text": "3.6 Newton-Raphson Algorithm\nWe know that the MLE estimates \\(\\hat\\theta\\) are obtained by minimising the score function \\(s(\\b\\theta, \\b y)\\). However, in many cases, there exists no closed form-analytical solution. Thus, we have to use iterated algorithms to minimise the score function.\nThe most common method is the Newton-Raphson algorithm. Suppose \\(\\theta\\) is a scalar of potential parameters. For values of \\(\\theta\\) that are close to the true population parameter \\(\\theta_0\\), the first order taylor series expansion of \\(s(\\theta, \\b y)\\) about \\(\\theta_0\\) states:\n\\[\ns(\\theta; \\b y) \\approx s(\\theta_0, \\b y) + s'(\\theta_0; \\b y)(\\theta - \\theta_0)\n\\]\nWhere \\(s'(\\theta_0; \\b y)\\) is the first derivative of the score function \\(s(\\theta_0, \\b y)\\) at the true population value \\(\\theta_0\\). We know from definition 3.5 that the first derivative of the score function is defined by the negative observed information \\(-\\b I(\\theta_0)\\). Thus, we get:\n\\[\ns(\\theta; \\b y) \\approx s(\\theta_0, \\b y)  - \\b I(\\theta_0)(\\theta - \\theta_0)\n\\]\nNow, we can get our \\(\\theta\\) that makes the score function equal 0 by setting \\(s(\\theta; \\b y) = 0\\), and then solving for \\(\\theta\\):\n\\[\n\\begin{align}\n0 &  \\approx s(\\theta_0, \\b y)  - \\b I(\\theta_0)(\\theta - \\theta_0) \\\\\n\\b I(\\theta_0)(\\theta - \\theta_0) & \\approx s(\\theta_0, \\b y) \\\\\n\\theta - \\theta_0 & \\approx \\b I(\\theta_0)^{-1}  s(\\theta_0, \\b y) \\\\\n\\theta & \\approx \\theta_0 + \\b I(\\theta_0)^{-1} s(\\theta_0, \\b y)\n\\end{align}\n\\tag{3.2}\\]\nSo know we have identified the \\(\\theta\\) that makes our score function equal to 0, which also maximises the log-likelihood \\(\\ell\\) and likelihood \\(L\\). However, there is an issue - our solution for \\(\\theta\\) includes \\(\\theta_0\\), the unknown true population parameter. Since \\(\\theta_0\\) is unknown, we cannot use this formula directly.\nInstead, we use an iterative procedure. We start with some initial value \\(\\theta^{(0)}\\) that is randomly chosen. Then, we use that \\(\\theta^{(0)}\\) to “update” to get a new \\(\\theta^{(1)}\\), based on eq. 3.2:\n\\[\n\\theta^{(1)} = \\theta^{(0)} + \\b I(\\theta^{(0)})^{-1}s(\\theta^{(0)}; \\b y)\n\\]\nThen, with our new \\(\\theta^{(1)}\\), we update to get \\(\\theta^{(2)}\\):\n\\[\n\\theta^{(2)} = \\theta^{(1)} + \\b I(\\theta^{(1)})^{-1}s(\\theta^{(1)}; \\b y)\n\\]\nAnd we keep doing this using \\(\\theta^{(m)}\\) to update to \\(\\theta^{(m+1)}\\):\n\\[\n\\theta^{(m+1)} = \\theta^{(m)} + \\b I(\\theta^{(m)})^{-1}s(\\theta^{(m)}; \\b y)\n\\tag{3.3}\\]\nAnd we keep doing this until the difference between \\(\\theta^{(m+1)}\\) and \\(\\theta^{(m)}\\) becomes very small (based on some pre-specified threshold). This is because our formula in eq. 3.2 is:\n\\[\n\\theta \\approx \\theta_0 + \\b I(\\theta_0)^{-1} s(\\theta_0, \\b y)\n\\]\nSo, if \\(\\theta^{(m+1)}\\) is very close to \\(\\theta^{(m)}\\) in eq. 3.3, we know that \\(\\theta^{(m+1)}\\) is approaching the true value \\(\\theta_0\\).\nAn alternative to the Newton-Raphson algorithm is the Fisher-scoring algorithm, which does the same thing, but using the fisher information matrix \\(\\mathcal I(\\theta)\\) (definition 3.4) instead of the observed information matrix \\(I(\\theta)\\), when \\(\\mathcal I (\\theta)\\) is not too difficult to compute. Fisher-scoring is the method most common for generalised linear models (that we will cover in later chapters).",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "mle.html#ols-as-a-maximum-likelihood",
    "href": "mle.html#ols-as-a-maximum-likelihood",
    "title": "3  Maximum Likelihood",
    "section": "3.8 OLS as a Maximum Likelihood",
    "text": "3.8 OLS as a Maximum Likelihood\nPreviously, we discussed the ordinary least squares estimator used on the classical linear model. We can actually show that the OLS estimator is a maximum likelihood estimator when we assume a linear data generating process\n\\[\nY_t \\sim \\mathcal N(\\beta_0 + \\beta_1 X_{t1} + \\dots + \\beta_p X_{tp}, \\ \\sigma^2)\n\\]\nWe will simplify this notation to\n\\[\nY_t \\sim \\mathcal N(\\b x_t^\\top \\b\\beta, \\ \\sigma^2)\n\\]\nWhere \\(\\b x_t\\) is a vector \\(\\b x_t = (1, X_{t1}, \\dots, X_{tp}\\) and \\(\\b\\beta\\) is a vector \\(\\b\\beta = (\\beta_0, \\dots, \\beta_p)\\). Using the probability density function of a normal distribution (definition 1.4) , we can express the PDF of \\(Y_i\\) as:\n\\[\nf_{Y_t}(y_t) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2}(y_t - \\b x_t^\\top \\b\\beta)^2}\n\\]\nNow, that means our log-likelihood \\(\\ell\\) (definition 3.2) for our sample \\((y_1, \\dots, y_n)\\) we have is:\n\\[\n\\ell(\\b\\beta, \\sigma^2; \\ \\b y) = \\sum\\limits_{t=1}^n \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2}(y_t - \\b x_t^\\top \\b\\beta)^2} \\right)\n\\]\nNow, using the property of logarithms, we can get\n\\[\n\\begin{align}\n\\ell(\\b\\beta, \\sigma^2; \\ \\b y) & = \\sum\\limits_{t=1}^n \\left[\\log (1) - \\log(\\sqrt{2 \\pi \\sigma^2}) + \\log \\left( e^{-\\frac{1}{2\\sigma^2}(y_t - \\b x_t^\\top \\b\\beta)^2} \\right) \\right] \\\\\n& = \\sum\\limits_{t=1}^n \\left[ 0 - \\frac{1}{2} \\log (2\\pi\\sigma^2) + \\left( -\\frac{1}{2\\sigma^2}(y_t  - \\b x_t^\\top \\b\\beta)^2 \\right) \\right] \\\\\n& = \\sum\\limits_{t=1}^n \\left[- \\frac{1}{2} \\log (2\\pi\\sigma^2)   -\\frac{1}{2\\sigma^2}(y_t  - \\b x_t^\\top \\b\\beta)^2 \\right]\n\\end{align}\n\\]\nNow, using the property of summations, we get\n\\[\n\\ell(\\b\\beta, \\sigma^2; \\ \\b y) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\color{blue}{\\sum\\limits_{t=1}^n (y_t - \\b x_t^\\top \\b\\beta)^2}\n\\]\nIf we look at the blue, we can see this is the sum of squared residuals (definition 4.7). Thus,\n\\[\n\\begin{align}\n\\ell(\\b\\beta, \\sigma^2; \\ \\b y) & = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\b y - \\hat{\\b y})^\\top (\\b y - \\hat{\\b y})\\\\\n& = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\b y  - \\b{X\\beta})^\\top (\\b y - \\b{X\\beta})\n\\end{align}\n\\]\nAnd if we take the gradient of the log-likelihood \\(\\ell\\) in resepct to vector \\(\\b\\beta\\), we get the same first order condition as OLS:\n\\[\n\\frac{\\partial \\ell}{\\partial \\b\\beta} = -2 \\b{X^\\top y} + 2 \\b{X^\\top X\\beta} = 0\n\\]\nWith the same first order conditions, we will get the same \\(\\hat{\\b\\beta}\\) estimates as OLS. Thus, we can see under the conditions of the classical linear model, the OLS estimator is a maximum likelihood estimator.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "mle.html#statistical-inference",
    "href": "mle.html#statistical-inference",
    "title": "3  Maximum Likelihood",
    "section": "3.7 Statistical Inference",
    "text": "3.7 Statistical Inference\nUnder the asymptotic properties of MLE (theorem 3.3), we know the estimator will be normally distributed. Using this fact, and the variance of MLE (theorem 3.2), we can do hypothesis testing with one parameter with the wald test:\n\nDefinition 3.6 (Wald Test) The wald test can determine if a parameter estimated by MLE is statistically significant. The wald test statistic is given by\n\\[\nW = \\left( \\frac{\\hat\\theta - H_0}{se(\\hat\\theta)} \\right)^2 = \\frac{(\\hat\\theta - H_0)^2}{\\V \\hat\\theta}\n\\]\nWhere \\(H_0\\) is the value of \\(\\theta\\) established by our null hypothesis. We then use a \\(\\chi^2\\) distribution with 1 degrees of freedom to obtain the p-value.\n\n\nIf our p-value is less than 0.05, we can conclude that our null hypothesis is incorrect. We could also use a z-test, which would produce the exact same result as the wald test:\n\\[\nz = \\frac{\\hat\\theta - H_0}{se(\\hat\\theta)}\n\\]\nAnd we consult a standard normal distribution \\(\\mathcal N(0, 1)\\) to get a p-value, and the interpretation is the same as the wald test.\n\nFor testing multiple coefficients at once, we can use the Likelihood Ratio test. The likelihood ratio test compares two models:\n\n\\(M_0 : Y_t = \\beta_0 + \\sum\\limits_{j=1}^g \\beta_j X_{tj} + \\eps_i\\) (the smaller null model with \\(g\\) parameters).\n\\(M_a : Y_t = \\beta_0 + \\sum\\limits_{j=1}^g \\beta_j X_{tj} + \\sum\\limits_{j=g+1}^p \\beta_j X_{tj} \\eps_i\\) (the bigger model with the original \\(g\\) parameters in the null + additional parameters up to \\(p\\)).\n\nTo compare the two models, we will use the likelihood function \\(L\\). After all, the likelihood \\(L\\) (definition 3.1) gives us the probability of observing a sample given parameters \\(\\b\\theta\\). If the probability is higher, that likely means our model is better. Thus, the likelihood ratio test compares the difference between the likelihoods of two models.\n\nDefinition 3.7 (Likelihood Ratio Test) The likelihood ratio test compares a smaller model \\(M_0\\) and larger model \\(M_a\\). The test statistic \\(L^2\\) is given by\n\\[\nL^2 = 2 \\log \\left(\\frac{L_a}{L_0}\\right) = 2 \\log (L_a) - 2 \\log (L_0)\n\\]\nWhere \\(L\\) is the likelihood of the model given by the likelihood function. Then, we consult a \\(\\chi^2\\) distribution with degrees of freedom equal to the number of extra parameters in model \\(M_a\\) compared to \\(M_0\\).\n\n\nIf the p-value is less than 0.05, that means our \\(M_a\\) model (and all of its extra coefficients) are statistically significant.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "mle.html#information-criterion-statistics",
    "href": "mle.html#information-criterion-statistics",
    "title": "3  Maximum Likelihood",
    "section": "3.8 Information Criterion Statistics",
    "text": "3.8 Information Criterion Statistics\nThe previously discussed likelihood ratio test (definition 3.7) allows us to compare bigger models to smaller models, as long as the bigger model has all of the smaller model’s parameters. However, what if we want to choose between models with different parameters?\nRecall that likelihood function \\(L\\) (definition 3.1) is the probability of observing a particular sample given parameters \\(\\b\\theta\\). If the probability is higher, that likely means our model is better. Thus, likelihood \\(L\\) allows us to compare different models.\nHowever, we often prefer simple models over more complex models. If we have two models with the same likelihood \\(L\\), but one has 40 parameters and the other has 5, you would prefer the one with 5, as it is far more efficient and achieves the same performance.\nThe information criterion statistics use an adjusted likelihood \\(L\\) accounting for the number of parameters to avoid good models that don’t have too many parameters. The most commonly used is the Akaike’s Information Criterion.\n\nDefinition 3.8 (AIC) Akaike’s Information Criterion can be used to measure the fit of models fitted with MLE.\n\\[\nAIC = -2 \\log L + 2p\n\\]\nWhere \\(L\\) is the likelihood of the model, and \\(p\\) is the number of parameters in the model. The lower the AIC is, the better the model is considered.\n\n\nAIC can be used to compare different models, including models that are not nested like in the likelihood ratio test. However, unlike \\(R^2\\), it does not have a real “substantive/real-world” meaning. The value itself means very little. An alternative to AIC is the Bayesian Information Criterion.\n\nDefinition 3.9 (BIC) Bayesian Information Criterion can be used to measure the fit of models fitted with MLE.\n\\[\nBIC = p \\log n - 2 \\log L\n\\]\nWhere \\(p\\) is the number of parameters in the model, \\(n\\) is the number of observations, and \\(L\\) is the likelihood of the given model. The lower the BIC is, the better the model is considered.\n\n\nThe BIC tends to penalise extra parameters more strongly than AIC. Generally, when comparing two models, AIC and BIC will agree.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "mom.html",
    "href": "mom.html",
    "title": "6  Method of Moments",
    "section": "",
    "text": "6.1 Moments and Estimation\nIn our original ordinary least squares estimator, we required 5 conditions: linearity, i.i.d., no perfect multicollinearity, exogeneity, and spherical errors. In the generalised least squares estimator, we dropped the spherical errors condition. In the maximum likelihood estimator, we dropped the spherical errors and linearity condition.\nHowever, all of these estimators require exogeneity. We previously defined exogeneity (definition 5.4, definition 5.5) as explanatory variables being uncorelated with the error term. However, exogeneity can also be described in another way - proper model specification. After all, exogeneity is only met if we properly specify our data generating process to seperate the mean \\(\\mu_Y\\) and variance of error \\(\\sigma^2\\).\nWhat if we cannot meet this proper model specification of the data generating process? An alternative estimator is possible - the method of moments estimator. The method of moments estimator requires very few assumptions about the form of our model. Importantly, we do not even need to know the probability density function of our data generating process.\nThe method of moments focuses on the moments of a distribution. Let us review what moments are. The \\(k\\)th raw moment is defined as\n\\[\n\\mu_k = \\E(X^k)\n\\]\nWhere the first moment \\(\\mu_X^1\\) is the expectation itself. There are also central moments, which are defined as\n\\[\n\\mu_k^1 = \\E((X - \\E X)^k)\n\\]\nThe method of moments equates each \\(k\\) moment with the sample equivalent:\n\\[\n\\mu_k = \\E(X^k) \\ \\approx \\ \\frac{1}{n}\\sum\\limits_{t=1}^nx_t^k\n\\]\nSo we equate the first moment \\(\\mu_X^1\\) with the first sample moment, the second moment \\(\\mu_X^2\\) with the second sample moment, and so on.\nWe should have as many moments-sample moment combinations as we have unknown parameters. Once we have our moments, we can solve for our parameters. In the next section, we will go over the simple example of a sample-mean and variance estimator.\nBecause of the law of large numbers (theorem 2.1), method of moments estimators as asymptotically consistent estimators. These estimators are also very simple - we do not require any information about the data generating process besides its moments.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Method of Moments</span>"
    ]
  },
  {
    "objectID": "mom.html#mean-and-variance-estimator",
    "href": "mom.html#mean-and-variance-estimator",
    "title": "6  Method of Moments",
    "section": "6.2 Mean and Variance Estimator",
    "text": "6.2 Mean and Variance Estimator\nLet us show how we can use method of moments to estimate the mean and variance of a population from a sample. Let \\((x_1, \\dots, x_n)\\) be a sample from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\). We want to use our method of moments estimator to find the true value of these two parameters.\nSince we have two parameters, we will need two moments. Our first moment \\(\\mu_1\\) is\n\\[\n\\mu_1 = \\E(X^1) = \\mu \\ \\approx \\ \\frac{1}{n}\\sum\\limits_{t=1}^n x_t\n\\]\nAnd our second population moment is\n\\[\n\\mu_2 = \\E(X^2) \\ \\approx \\ \\frac{1}{n}\\sum\\limits_{t=1}^nx_t^2\n\\]\nWe can rewrite our second population moment from \\(\\E(X^2)\\) as the following:\n\\[\n\\E(X^2) = \\E(X-\\mu)^2 + 2\\mu \\E(X) - \\E(\\mu)^2\n\\]\nYou can prove this by multiplying out and cancelling the right hand side. We can simplify this expression using the definition of variance (definition 1.3) and property of expectations to get\n\\[\n\\begin{align}\n\\E(X^2) & = \\E(X-\\mu)^2 + 2\\mu \\E(X) - \\E(\\mu)^2 \\\\\n& = \\sigma^2 + 2\\mu \\mu - \\mu^2 \\\\\n& = \\sigma^2 + \\mu^2\n\\end{align}\n\\]\nThus, we see the second population moment is equivalent to \\(\\sigma^2 + \\mu^2\\). Thus, our two moment conditions, and their sample equivalents are:\n\\[\n\\begin{align}\n& \\E(X^1): \\mu \\approx \\frac{1}{n} \\sum\\limits_{t=1}^n x_t \\\\\n& \\E(X^2) : \\mu^2 + \\sigma^2 \\approx \\frac{1}{n} \\sum\\limits_{t=1}^nx_t^2\n\\end{align}\n\\]\nThus, our sample estimate for sample mean \\(\\hat\\mu\\) will be the first moment condition\n\\[\n\\hat\\mu = \\frac{1}{n} \\sum\\limits_{t=1}^n x_t\n\\]\nAnd our sample estimate \\(\\sigma^2\\) will be a rearranged version (isolating \\(\\sigma^2\\)) of the second moment condition\n\\[\n\\begin{align}\n\\hat\\sigma^2 & = \\frac{1}{n}\\sum\\limits_{t=1}^n x_t^2 - \\hat\\mu^2 \\\\\n& =  \\frac{1}{n}\\sum\\limits_{t=1}^n (x_t^2 - \\hat\\mu)^2\n\\end{align}\n\\]\nAnd as we can see, according the the law of large numbers (theorem 2.1), and the definition of expectation (definition 1.2) and variance (definition 1.3), that both these estimators are asymptotically consistent.\nThis is a simple example of the implementation of the method of moments. From now on, we will focus on more complex cases of the method of moments, and how they can be used to address some of the issues with past estimators.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Method of Moments</span>"
    ]
  },
  {
    "objectID": "mom.html#moments-of-least-squares",
    "href": "mom.html#moments-of-least-squares",
    "title": "6  Method of Moments",
    "section": "6.3 Moments of Least Squares",
    "text": "6.3 Moments of Least Squares\nThe ordinary least squares estimator can be written as a method of moments estimator. Recall that the classical linear model\n\\[\nY \\sim \\mathcal N(\\b{X\\beta}, \\sigma^2)\n\\]\nWe can split this model into two sections - the expected \\(Y\\) and the variance:\n\\[\n\\b y = \\underbrace{\\b{X\\beta}}_{\\mu_Y} \\ + \\ \\underbrace{\\b\\eps \\sim \\mathcal N(0, \\sigma^2)}_{\\text{error term}}\n\\tag{6.1}\\]\nWhere \\(\\mu_Y = \\E(Y|X)\\), and the error term explains the variation/randomness where not all \\(Y\\) are exactly at the expectation \\(\\mu_Y\\). For the OLS estimator, we are interested in the central moment of \\(Y\\):\n\\[\n\\mu_1' = \\E(Y - \\E(Y|X)) = \\E( \\b y - \\b{X\\beta} \\ | \\ \\b X) = \\E (\\b\\eps | \\b X)\n\\]\nAnd since we know from eq. 6.1 that \\(\\b\\eps\\) by assumption has a expectation of 0, we get:\n\\[\n\\mu_1' = \\E(\\b\\eps | \\b X) = 0\n\\]\nHowever, there is an issue with this condition - this will only produce one equation to solve, when we have \\(p\\) parameters that need to be estimated for OLS. The solution to this is to multiply the central moment condition by \\(\\b X\\):\n\\[\n\\b X^\\top \\underbrace{\\E(\\b\\eps | \\b X)}_{= \\ 0} = 0\n\\]\nAnd this statement is still true since anything multiplied to 0 equals 0. Now, using the properties of exepctations, we can include \\(\\b X\\) within the expectation since it is a constant considering our expectation conditions of \\(\\b X\\):\n\\[\n\\E( \\b X^\\top \\b \\eps | \\b X) = 0\n\\]\nAnd using the law of iterated expectations (theorem 1.3), we know\n\\[\n\\E(\\b{X^\\top \\eps}) = \\E[\\E( \\b X^\\top \\b \\eps | \\b X)] = \\E[0] = 0\n\\]\nThus, we have our moment condition of OLS.\n\nDefinition 6.1 (OLS Moment Condition) The moment conditions of the ordinary least squares estimator is:\n\\[\n\\E(\\b{X^\\top \\eps}) = 0\n\\]\n\n\nNotice anything about this moment condition? It is exactly in the same form as our definition of exogeneity (definition 5.4). This is another reason why exogeneity is an important condition in OLS - when violated, our moment condition is invalid.\nTo actually estimate OLS coefficients \\(\\hat{\\b\\beta}\\) with method of moments, we use our sample equivalents:\n\\[\n\\E(\\b X^\\top \\b\\eps) \\approx \\b X^\\top \\hat{\\b\\eps} = 0\n\\]\nWhere we can rewrite with the fact that \\(\\hat{\\b\\eps} = \\b y - \\hat{\\b y}\\), and that \\(\\hat{\\b y} = \\b{X\\hat\\beta}\\). Thus, we get\n\\[\n\\begin{align}\n\\b X^\\top (\\b y - \\hat{\\b y}) & = 0 \\\\\n\\b X^\\top(\\b y - \\b X \\hat{\\b\\beta}) & = 0 \\\\\n\\b{X^\\top y} - \\b{X^\\top X \\hat\\beta} & = 0\n\\end{align}\n\\]\nNow solving for \\(\\b{\\hat\\beta}\\) with matrix inversion, we get:\n\\[\n\\begin{align}\n- \\b{X^\\top X \\hat\\beta} & = - \\b{X^\\top y} \\\\\n\\b{X^\\top X \\hat\\beta} & = \\b{X^\\top y} \\\\\n\\hat{\\b\\beta} & = (\\b{X^\\top X})^{-1}\\b{X^\\top y}\n\\end{align}\n\\]\nAnd we can see, we get the same solution as OLS (definition 5.8). Thus, OLS is a method of moments estimator.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Method of Moments</span>"
    ]
  },
  {
    "objectID": "mom.html#instrumental-variables",
    "href": "mom.html#instrumental-variables",
    "title": "6  Method of Moments",
    "section": "6.4 Instrumental Variables",
    "text": "6.4 Instrumental Variables\nRecall the OLS method of moments condition (definition 6.1):\n\\[\n\\E(\\b{X^\\top \\eps}) = 0\n\\]\nAs mentioned above, this is equivalent to the exogeneity condition (definition 5.4) - that the regressors \\(\\b X\\) are uncorrelated with the error term \\(\\b \\eps\\). However, what if this is violated? Our OLS estimates become biased, because exogeneity is a key condition in establishing the unbiasedness of OLS (theorem 5.2).\nThe instrumental variables estimator is a solution to this issue. The idea is to find a third variable (or more) \\(Z\\), that does meet this condition of exogeneity. Then, we will have the new condition\n\\[\n\\E(\\b{Z^\\top \\eps}) = 0\n\\]\nand we will have no exogeneity if \\(Z\\) is not correlated with the error term. We then use these instruments \\(Z\\) to predict \\(X\\), which will get us the parts of \\(X\\) that are explained by \\(Z\\) (and thus, uncorrelated with the error term). Then, we can use that exogenous part of \\(X\\) to estimate the relationship with \\(Y\\). However, this hinges on \\(Z\\) meeting that moments condition.\n\nDefinition 6.2 (Assumptions of Instruments) For instrument(s) \\(Z\\) to meet the moment condition \\(\\E(\\b{Z^\\top \\eps}) = 0\\), the following facts must be true:\n\n\\(Z\\) must be exogenous/ignorable, i.e. \\(Cov(Z, \\eps) = 0\\).\n\\(Z\\) must be relevant, i.e. \\(Cov(Z, X) ≠ 0\\).\n\\(Z\\) must meet the exclusions restriction (which is implied by exogenous). This means that \\(Z\\) cannot have an independent effect on \\(Y\\), outside of its impact on \\(Y\\) through \\(X\\).\n\n\n\n\n\n\n\nThe classic instrument setup is as above. We want to find the relationship between \\(D\\) and \\(Y\\). However, there is some unobserved variable \\(U\\) that is not in our regression, so it is in our error term \\(\\eps\\). But because \\(U\\) is correlated with \\(D\\) and \\(Y\\) and in the error term \\(\\eps\\), it implies \\(D\\) is correlated with the error term \\(\\eps\\), violating exogeneity.\nTo solve this, we introduce an instrument \\(Z\\). By using \\(Z\\) to predict \\(D\\), we are only using the part of \\(D\\) that is explained by \\(Z\\). That part of \\(D\\) is not explained by \\(U\\), so that part of \\(D\\) is now exogenous, and we can estimate the relationship between \\(D\\) and \\(Y\\).\n\n\n\n\n\nThis figure shows potential violations of the assumptions of instrument \\(Z\\), when trying to measure the relationship between \\(D\\) and \\(Y\\). Finding a valid instrument is very difficult, but possible. However, it typically takes a lot of domain knowledge to get a good instrument.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Method of Moments</span>"
    ]
  },
  {
    "objectID": "mom.html#instrumental-variables-estimator",
    "href": "mom.html#instrumental-variables-estimator",
    "title": "6  Method of Moments",
    "section": "6.5 Instrumental Variables Estimator",
    "text": "6.5 Instrumental Variables Estimator\nSo we know what an instrument \\(Z\\) is, and the requirements for \\(Z\\) to be a valid instrument. But how does \\(Z\\) help us estimate the effect between \\(X\\) and \\(Y\\)?\nOur moment condition and sample equivalent for instrumental variables is:\n\\[\n\\E(\\b{Z^\\top \\eps}) \\approx \\b Z^\\top \\b\\eps = 0\n\\]\nNow, we know that \\(\\b\\eps = \\b y - \\hat{\\b y}\\) and \\(\\hat{\\b y} = \\b{X\\hat\\beta}\\), so let us plug that in:\n\\[\n\\begin{align}\n\\b Z^\\top (\\b y - \\hat{\\b y}) & = 0 \\\\\n\\b Z^\\top (\\b y - \\b{X \\hat\\beta}) & = 0 \\\\\n\\b{Z^\\top y} - \\b{Z^\\top X \\hat\\beta} & = 0\n\\end{align}\n\\]\nAnd now, solving for \\(\\hat{\\b\\beta}\\) with matrix inversion, we get:\n\\[\n\\begin{align}\n- \\b{Z^\\top X \\hat\\beta} & = - \\b{Z^\\top y} \\\\\n\\b{Z^\\top X \\hat\\beta} & = \\b{Z^\\top y} \\\\\n\\hat{\\b\\beta} & = (\\b{Z^\\top X})^{-1} \\b{Z^\\top y}\n\\end{align}\n\\]\n\nDefinition 6.3 (Instrumental Variables Estimator) The IV estimator produces the estimates:\n\\[\n\\hat{\\b\\beta}^* = (\\b{Z^\\top X})^{-1} \\b{Z^\\top y}\n\\]\n\n\nThe instrumental variables estimator is a biased estimator in finite samples. This means we should be careful about using the instrumental variables in small sample sizes. However, it is asymptotically consistent (as we will demonstrate in the next section). This means that IV can give us good estimates of \\(\\b\\beta\\) in large sample sizes even if \\(X\\) is not exogenous.\n\nTheorem 6.1 (IV Bias) Instrumental variables estimator is a biased estimator.\n\n\nProof: Let us plug in the true model \\(\\b y = \\b{X\\beta} + \\b \\eps\\) into the IV estimator (definition 6.3) to get\n\\[\n\\begin{align}\n\\hat{\\b\\beta}^* & = (\\b{Z^\\top X})^{-1} \\b{Z^\\top y} \\\\\n& = (\\b{Z^\\top X})^{-1} \\b{Z^\\top}(\\b{X\\beta} + \\b \\eps) \\\\\n& = \\underbrace{(\\b{Z^\\top X})^{-1} \\b{Z^\\top X}}_{\\text{inverses cancel}}\\b\\beta + (\\b{Z^\\top X})^{-1} \\b{Z^\\top\\eps} \\\\\n& = \\b\\beta + (\\b{Z^\\top X})^{-1} \\b{Z^\\top\\eps}\n\\end{align}\n\\tag{6.2}\\]\nNow, let us consider the conditional expectation of \\(\\hat{\\b\\beta}^*\\) in respect to both \\(\\b X\\) and \\(\\b Z\\):\n\\[\n\\E(\\hat{\\b\\beta}^* | \\b X, \\b Z) = \\E( \\b\\beta + (\\b{Z^\\top X})^{-1} \\b{Z^\\top\\eps} \\ | \\b X, \\b Z)\n\\]\nNow, let us take out the constants from the expectation to get:\n\\[\n\\E(\\hat{\\b\\beta}^* | \\b X, \\b Z) = \\b\\beta + (\\b{Z^\\top X})^{-1} \\b{Z^\\top} \\E(\\b\\eps | \\b X, \\b Z)\n\\]\nBecause \\(\\b X\\) is endogenous (and violates exogeneity), we know \\(\\E(\\b\\eps | \\b X, \\b Z) ≠ 0\\). Thus, we cannot simplify this expression like we did for OLS. Thus, the instrumental variables estimator is biased.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Method of Moments</span>"
    ]
  },
  {
    "objectID": "mom.html#asymptotics-of-iv",
    "href": "mom.html#asymptotics-of-iv",
    "title": "6  Method of Moments",
    "section": "6.6 Asymptotics of IV",
    "text": "6.6 Asymptotics of IV\nDespite being biased, the instrumental variables estimator is an asymptotically consistent estimator of \\(\\b\\beta\\) even with exogeneity between \\(X\\) and \\(\\eps\\) violated (as long as \\(Z\\) is exogenous).\n\nTheorem 6.2 (IV Consistency) The IV estimator is asymptotically consistent, meaning\n\\[\n\\mathrm{plim}\\hat{\\b\\beta}^* = \\b\\beta\n\\]\n\n\nProof: Let us start from where we left off in eq. 6.2. We can rewrite our matrices in the form of vectors, scalars, and summation, just as we did in the proof of OLS consistency (theorem 5.4). We thus get\n\\[\n\\begin{align}\n\\hat{\\b\\beta}^* & = \\b\\beta + (\\b{Z^\\top X})^{-1} \\b{Z^\\top\\eps} \\\\\n& = \\b\\beta + \\left(\\sum\\limits_{t=1}^n \\b z_t \\b x_t^\\top \\right)^{-1} \\left(\\sum\\limits_{t=1}^n \\b z_t \\eps_t \\right)\n\\end{align}\n\\]\nNow, let us do a little algebra trick as follows:\n\\[\n\\hat{\\b\\beta}^* = \\b\\beta + \\left(\\frac{1}{n}\\sum\\limits_{t=1}^n \\b z_t \\b x_t^\\top \\right)^{-1} \\left(\\frac{1}{n}\\sum\\limits_{t=1}^n \\b z_t \\eps_t \\right)\n\\]\nThe reason we can do this is because the first \\(\\frac{1}{n}\\) is inversed as \\(\\frac{1}{n}^{-1}\\), so this cancels out the second one, maintaining the equality of our equation.\nNow, we want to prove \\(\\mathrm{plim}\\hat{\\b\\beta}^* = \\b\\beta\\), so let us take the probability limit of both sides:\n\\[\n\\mathrm{plim}\\hat{\\b\\beta}^* = \\mathrm{plim}\\b\\beta + \\left(\\mathrm{plim}\\frac{1}{n}\\sum\\limits_{t=1}^n \\b z_t \\b x_t^\\top \\right)^{-1} \\left(\\mathrm{plim}\\frac{1}{n}\\sum\\limits_{t=1}^n \\b z_t \\eps_t \\right)\n\\]\nWe know the probability limit of a constant is itself. Look at the other two terms on the right: they take the form of sample averages \\(\\frac{1}{n}\\sum\\). Using the law of large numbers (theorem 2.1), we can simplify to:\n\\[\n\\mathrm{plim}\\hat{\\b\\beta}^*  = \\b\\beta + (\\E(\\b z_t \\b x_t^\\top))^{-1} \\underbrace{\\E(\\b z_t \\eps_t)}_{= \\ 0} = \\b\\beta\n\\]\nAnd we know \\(\\E(\\b z_t \\eps_t) = 0\\) because this is the same condition as the moment condition \\(\\E(\\b{Z^\\top \\eps}) = 0\\), just written in terms of individual observations. Thus, the Instrumental variables estimator is asymptotically consistent.\nIt can also be shown that the asymptotic variance of the instrumental variables estimator, under the assumption of spherical errors is\n\\[\n\\V \\hat{\\b\\beta}^* = \\sigma^2(\\b{Z^\\top X})^{-1} \\b{Z^\\top Z} (\\b{X^\\top Z})^{-1}\n\\]\nAlthough the proof of this is beyond the scope of this chapter. There is also a more complex derivation for robust standard errors with instrumental variables estimation.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Method of Moments</span>"
    ]
  },
  {
    "objectID": "mom.html#two-stage-least-squares",
    "href": "mom.html#two-stage-least-squares",
    "title": "6  Method of Moments",
    "section": "6.7 Two Stage Least Squares",
    "text": "6.7 Two Stage Least Squares\nWhile sometimes we do compute the instrumental variables estimator as shown in definition 6.3, often, we use another estimator, the two-stage-least-squares (2SLS) estimator.\nThe 2SLS estimator is based on the intuitive interpretation of instrumental variables: that we use only the part of \\(X\\) explained by \\(Z\\) (which should be endogenous), and estimate that part of \\(X\\)’s relationship with \\(Y\\).\nThe two stage least squares estimator follows this exact procedure. In the first stage, we find the part of \\(X\\) that is explained by \\(Z\\), which we call \\(\\hat X_t\\):\n\\[\n\\hat X_t = \\hat\\beta_0 + \\hat\\beta_1 Z_t\n\\]\nWe do this by running a linear OLS model with \\(X_t\\) as the outcome variable, and \\(Z_t\\) as the explanatory variable. Our predicted \\(\\hat X_t\\) will be the part of \\(X\\) explained by instrument \\(Z\\).\nThen, for the second stange, we take this \\(\\hat X_t\\), and use it in a model with \\(Y\\) as follows:\n\\[\nY_t = \\delta_0 + \\delta_1 \\hat X_t + \\eps_t\n\\]\nHow is 2SLS in these two stages equal to instrumental variables estimator we derived in definition 6.3? Our estimates (in matrix form) for the second stage would be:\n\\[\n\\hat{\\b\\beta}_{2SLS} = (\\hat{\\b X}^\\top \\hat{\\b X})^{-1} \\hat{\\b X}^\\top \\b y\n\\tag{6.3}\\]\nWhere \\(\\hat{\\b X}\\) is given as the fitted values of the first stage:\n\\[\n\\hat{\\b X} = \\b Z \\hat{\\b \\delta} = \\b Z (\\b{Z^\\top Z})^{-1}\\b{Z^\\top X}\n\\]\nWe can plug \\(\\hat{\\b X}\\) into eq. 6.3 to get:\n\\[\n\\hat{\\b\\beta}_{2SLS} = [(\\b Z (\\b{Z^\\top Z})^{-1}\\b{Z^\\top X})^\\top (\\b Z (\\b{Z^\\top Z})^{-1}\\b{Z^\\top X})]^{-1}(\\b Z (\\b{Z^\\top Z})^{-1}\\b{Z^\\top X})^\\top \\b y\n\\]\nUsing the properties of matrix transposes, we get:\n\\[\n\\begin{align}\n\\hat{\\b\\beta}_{2SLS} & = [(\\b X^\\top \\b Z (\\b{Z^\\top Z})^{-1} \\b Z^\\top) (\\b Z (\\b{Z^\\top Z})^{-1}\\b{Z^\\top X})]^{-1}(\\b X^\\top \\b Z (\\b{Z^\\top Z})^{-1} \\b Z^\\top)\\b y \\\\\n& = [\\b X^\\top \\b Z \\underbrace{(\\b{Z^\\top Z})^{-1} \\b Z^\\top \\b Z}_{\\text{inverses cancel}} (\\b{Z^\\top Z})^{-1}\\b{Z^\\top X}]^{-1}\\b X^\\top \\b Z (\\b{Z^\\top Z})^{-1} \\b Z^\\top\\b y \\\\\n& = (\\underbrace{\\b X^\\top \\b Z (\\b{Z^\\top Z})^{-1}}_{\\text{cancel}}\\b{Z^\\top X})^{-1}\\underbrace{\\b X^\\top \\b Z (\\b{Z^\\top Z})^{-1}}_{\\text{cancel}} \\b Z^\\top\\b y \\\\\n& = \\underbrace{(\\b{Z^\\top X})^{-1} \\b{Z^\\top y}}_{\\text{equal to IV}}\n\\end{align}\n\\]\nAnd the second to last step is possible because one of the highlighted “cancel” parts is within an inverse and the other isn’t, so they cancel. Thus, we have shown 2SLS is equivalent to the instrumental variables estimator.\nThe main advantage of 2SLS is that we can include control variables in both stages. For example, recall this figure we saw previously:\n\n\n\n\n\nWe want to uncover the effect of \\(D\\) on \\(Y\\). Imagine that we only have one of the violations above, the one notated ignorability violation (i). In this case, \\(Z\\) is not exogenous to \\(Y\\), because \\(Z\\) and \\(Y\\) are correlated with the confounder \\(M\\) which is in the error term, meaning \\(Z\\) is correlated with the error term.\nHowever, if we included \\(M\\) in our model for both stages, then \\(Z\\) would be exogenous, since \\(M\\) would be a part of our model and not the error term \\(\\eps\\) anymore.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Method of Moments</span>"
    ]
  },
  {
    "objectID": "causal.html",
    "href": "causal.html",
    "title": "5  Causal Inference",
    "section": "",
    "text": "5.1 Potential Outcomes Framework\nA lot of social science and science is about understanding the causes of things. This involves understanding how some treatment variable \\(D\\) causes some outcome variable \\(Y\\). We call our treatment variable \\(D\\). For each unit \\(t\\), they have a treatment status of \\(D_t\\):\n\\[\nD_t = \\begin{cases}\n1 & \\text{if unit t received the treatment} \\\\\n0 & \\text{if unit t did not receive the treatment}\n\\end{cases}\n\\]\nNow, imagine that there are two parallel worlds. In one of these parallel worlds, unit \\(t\\) receives the treatment \\(D_t = 1\\). In the other parallel world, unit \\(t\\) does not receive the treatment \\(D_t = 0\\). Everything about these two parallel worlds besides \\(D_i\\) is identical. The outcome \\(Y\\) value in these two worlds is called the potential outcomes.\nIf we know the two hypothetical parallel worlds are identical to each other except for treatment \\(D_t\\), then we know any difference in \\(Y\\) outcomes between the two worlds must be a result of treatment \\(D\\). This introduces us to the causal estimands - the true causal effects in the population:\nHowever, there is an issue: in the real world, we obviously do not have two hypothetical parallel worlds. We only have one world - either unit \\(t\\) gets treated \\(D_t = 1\\) or unit \\(t\\) does not get treated \\(D_t = 0\\). The world we actually live in is called the observed outcome, and the parallel world we do not see is called the counterfactual. The fact we cannot observe the counterfactual is called the fundamental problem of causal inference.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal.html#potential-outcomes-framework",
    "href": "causal.html#potential-outcomes-framework",
    "title": "5  Causal Inference",
    "section": "",
    "text": "Definition 5.1 (Potential Outcomes) The potential outcomes \\(Y_t(d)\\) for unit \\(t\\) are the \\(Y\\) values in the two identical parallel worlds besides the \\(D_t\\) value.\n\\[\nY_t^{(d)} = \\begin{cases}\n\\pt & D_t = 1\\text{ parallel world outcome Y value for unit t} \\\\\n\\pc & D_t = 0\\text{ parallel world outcome Y value for unit t} \\\\\n\\end{cases}\n\\]\nTo make clear when talking about potential outcomes, I will always highlight them in purple.\n\n\n\n\n\n\n\nStable Unit Treatment Value Assumption\n\n\n\n\n\nThe above mentioned potential outcomes framework depends on the stablue unit treatment value assumption (SUTVA).\nIt basically says that unit \\(t\\)’s potential outcomes \\(Y_t(1)\\) and \\(Y_{t}(0)\\) are not affected in any way by another unit \\(j\\)’s treatment status \\(D_j\\). Basically, changing other individual’s treatment status has no effect on an individual’s own outcomes.\nIf SUTVA is violated, then our nice two parallel worlds example no longer is accurate. This is because if SUTVA is violated, unit \\(t\\) now has the potential outcomes of themselves receiving \\(D_t = 1, 0\\), but also other people \\(D_j = 0, 1\\). This will make the number of outcomes grow massively (especially if multiply other units affect an individual).\nCommon causes of SUTVA violations include:\n\nSpill-over effects: If we are testing a new curriculum, one student \\(j\\) getting the new curriculum may teach their friend \\(t\\) the new curriculum, which means if student \\(j\\) got or did not get the new curriculum would affect student \\(t\\)’s outcomes.\nDilution: For example, in vaccines, there is herd immunity. That means other people getting the vaccines also improves my health outcomes.\n\n\n\n\n\n\nITEATEATTATUCATE\n\n\n\nDefinition 5.2 (Individual Treatment Effect) The individual treatment effect of treatment \\(D\\) on \\(Y\\), for a specific unit \\(i\\), is given by the difference between the potential outcomes.\n\\[\n\\tau_t = \\pt - \\pc\n\\]\n\n\n\n\nDefinition 5.3 (Average Treatment Effect) The ATE is the average individual treatment effects \\(\\tau_i\\) in the population.\n\\[\n\\tau_{ATE} = \\E(\\tau_t) \\  = \\  \\E(\\pt - \\pc)  \\ = \\  \\E \\pt - \\E \\pc\n\\]\n\n\n\n\nDefinition 5.4 (Average Treatment Effect on the Treated) The ATT is the average individual treatment effect \\(\\tau_i\\) for only units that were assigned to the treatment group \\(D_i = 1\\):\n\\[\n\\tau_{ATT} = \\E(\\tau_t | D_t = 1) \\ = \\ \\E(\\pt - \\pc | D_t = 1)\n\\]\n\n\n\n\nDefinition 5.5 (Average Treatment Effect on the Untreated) The ATU is the average individual treatment effect \\(\\tau_i\\) for only units that were assigned to the control group \\(D_i = 0\\):\n\\[\n\\tau_{ATT} = \\E(\\tau_t | D_t = 0) \\ = \\ \\E(\\pt - \\pc | D_t = 0)\n\\]\n\n\n\n\nDefinition 5.6 (Conditional Average Treatment Effect) The CATE is the average treatment effect, conditional on some other characteristic/covariate \\(X\\) value:\n\\[\n\\tau_{CATE}(x) = \\E(\\tau_t| X = x) \\ = \\ \\E(\\pt - \\pc|X=x)\n\\]\nThis estimand is also sometimes called the local average treatment effect (LATE).\n\n\n\n\n\n\nDefinition 5.7 (Observed Outcomes and Counterfactuals) The observed outcome \\(Y_t\\) that we actually see for a unit \\(t\\) can be given by a function of potential outcomes:\n\\[\nY_t = D_t \\cdot \\pt + (1-D_i) \\cdot \\pc\n\\]\nIf we plug in the treatment status \\(D_t = 0, 1\\) of unit \\(t\\) into the above equation, we get observed outcomes\n\\[\nY_t = \\begin{cases}\n\\pt & \\text{if} D_i = 1 \\\\\n\\pc & \\text{if} D_i = 0\n\\end{cases}\n\\]\nThe potential outcome that is not observed is the counterfactual outcome.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal.html#causal-estimands",
    "href": "causal.html#causal-estimands",
    "title": "5  Causal Inference",
    "section": "5.2 Causal Estimands",
    "text": "5.2 Causal Estimands\nEarlier, we discussed the individual treatment effect \\(\\tau_t\\) from definition 5.2. However, because of the fundamental problem of causal inference, we do not know the counterfactuals.\nAt an individual level, these counterfactuals are almost impossible to estimate, so we typically do not use the individual treatment effect. Instead, we will typically look at the averages of treatment effects. The most common causal estimand (true value in the population) we will use is the average treatment effect.\n\nDefinition 5.4 (Average Treatment Effect) The ATE is the average individual treatment effects \\(\\tau_i\\) in the population.\n\\[\n\\tau_{ATE} = \\E(\\tau_t) \\  = \\  \\E(\\pt - \\pc)  \\ = \\  \\E \\pt - \\E \\pc\n\\]\n\n\nObviously, we do not observe the counterfactual outcomes. However, at a group level, their averages are more easily estimated, which we will explore later in the chapter.\nThe ATE is not the only causal estimand. We will often consider a few other causal estimands.\n\nDefinition 5.5 (Average Treatment Effect on the Treated) The ATT is the average individual treatment effect \\(\\tau_i\\) for only units that were assigned to the treatment group \\(D_i = 1\\):\n\\[\n\\tau_{ATT} = \\E(\\tau_t | D_t = 1) \\ = \\ \\E(\\pt - \\pc | D_t = 1)\n\\]\n\n\nThe ATT is usually not equal to the ATE, but in specific circumstances, they can be equal.\nThe opposite estimand also exists - the average treatment effect on the untreated (ATU). This is typically not something we calculate, but it is used in some proofs.\nSometimes, we also might believe there are heterogenous treatment effects - i.e. some third variable strengthens or weakens the strength of an estimand. For example, perhaps a policy has more positive effects on females than males. Thus, we can also find the ATE conditional on some third variable.\n\nDefinition 5.6 (Conditional Average Treatment Effect) The CATE is the average treatment effect, conditional on some other characteristic/covariate \\(X\\) value:\n\\[\n\\tau_{CATE}(x) = \\E(\\tau_t| X = x) \\ = \\ \\E(\\pt - \\pc|X=x)\n\\]\nThis estimand is also sometimes called the local average treatment effect (LATE).\n\n\nThese causal estimands are all estimands - true population parameters. Our goal will be to fill in the missing counterfactuals with some identification strategy, in order to estimate these true population estimands.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal.html#structural-causal-models",
    "href": "causal.html#structural-causal-models",
    "title": "5  Causal Inference",
    "section": "5.2 Structural Causal Models",
    "text": "5.2 Structural Causal Models\nAn alternative causal framework, pioneered by Pearl, is called the structural causal models. This framework uses graphical models (called directed acyclic graphs) to illustrate causality.\n\n\n\n\n\nEvery causal graph illustrates how different variables are connected to each other. Each graph contains:\n\nNodes: These are letters that represent different variables.\nDirected Edges: these are arrows that encode causal theories between the variables. For example, if you believe \\(Z\\) causes \\(D\\), you would draw an arrow \\(Z \\rightarrow D\\). These connections are either observable (solid lines) or unobservable (dashed lines).\n\nPaths are any route between any two variables, that do not have to follow the direction of the arrows. For example, in the figure above, between \\(D\\) and \\(Y\\), there are 3 paths:\n\nThe direct path \\(D \\rightarrow Y\\).\nThe inderect/backdoor path \\(D \\leftarrow Q \\rightarrow Y\\).\nThe indirect/backdoor path \\(D \\leftarrow Z \\leftarrow W \\rightarrow Y\\).\n\nThe goal in causal inference is to get rid of all the indirect/backdoor paths, allowing us to isolate the direct relationship between \\(D\\) and \\(Y\\). There are two ways to isolate the direct relationship: external intervention and blocking paths.\n\nExternal InterventionBlocking Paths\n\n\nOne way to isolate a causal path is through an external intervention. For example, in the figure below, variable \\(D\\) is directly caused by \\(Q\\) and \\(Z\\). This allows indirect paths between \\(D\\) and \\(Y\\) to flow between \\(Q\\) and \\(Z\\).\n\n\n\n\n\nHowever, by externally determining \\(D\\) (represented with the operator \\(d_0\\)), we can break the paths \\(Q \\rightarrow D\\) and \\(Z \\rightarrow D\\). This is because if we are deciding who gets \\(D\\) (such as by randomisation), that means we are deciding \\(D\\), not \\(Q\\) or \\(Z\\). This will allow us to block backdoor paths, and our new diagram will be:\n\n\n\n\n\nAnd thus, we have isolated the causal relationship.\n\n\nOn the other hand, we can block indirect/backdoor paths through conditioning on a set of variables/nodes \\(\\set X\\). A set of nodes \\(\\cal X\\) blocks a path if one of two is true:\n\nA path is blocked if our set of conditioning nodes \\(\\set X\\) includes at least one arrow-emitting node within that path.\nA path is blocked if the path contains a collision node (where multiple arrows point into it), and that collision node is not included in our set of conditioning nodes \\(\\set X\\).\n\n\n\n\n\n\nFor example, in the figure above, we can see:\n\nThe path \\(D \\rightarrow P \\rightarrow Y\\) is blocked by a set \\(\\set X = \\{P\\}\\), since \\(P\\) is one arrow emitting node within this path.\nThe path \\(D \\leftarrow M \\rightarrow Y\\) is blocked by a set \\(\\set X = \\{M\\}\\), because \\(M\\) is one arrow emitting node within this path.\nThe path \\(D \\leftarrow Z \\rightarrow M \\rightarrow Y\\) is blocked by either \\(\\set X = \\{M\\}, \\{Z\\}\\) or \\(\\{M, Z\\}\\).\nThe path \\(D \\leftarrow Z \\rightarrow M \\leftarrow Q \\rightarrow Y\\) is blocked by an empty set \\(\\set X = \\varnothing\\), because \\(M\\) is a collider node in this path and we do not need to include it.\n\nWe will discuss the idea of blocking paths more in the next chapter.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal.html#selection-bias",
    "href": "causal.html#selection-bias",
    "title": "5  Causal Inference",
    "section": "5.3 Selection Bias",
    "text": "5.3 Selection Bias\nYou always here the saying - correlation is not causation. But what does that actually mean? A simple model of correlation \\(\\rho\\) is finding how observed \\(Y\\) changes when \\(D\\) changes. More specifically, we want to find the differences in conditional expectations (given by definition 1.8) of observed \\(Y_t\\) values for when \\(D_t = 1\\) and \\(D_t = 0\\):\n\\[\n\\rho_{D, Y} = \\E(Y_t|D_t = 1) - \\E (Y_t | D_t = 0)\n\\]Now, let us do some algebra. First, we know that \\(Y_t|D_t = 1\\) is the observed potential outcome \\(\\pt\\). We also know that \\(Y_t|D_t = 0\\) is the observed potential outcome \\(\\pc\\). Thus, we can rewrite the above to\n\\[\n\\rho_{D, Y} = \\E(\\pt|D_i = 1) - \\E(\\pc|D_i = 0)\n\\]\nNow, let us do an algebra trick. We know that adding a zero doesn’t change the equality of the equation. Let us add the term \\(\\E(\\pc|D_t = 1)\\), but then also subtract that term, so we are essentially adding a 0. Then we get\n\\[\n\\begin{align}\n\\rho_{D, Y} = \\E(\\pt &|D_t = 1) - \\E(\\pc|D_t = 0) \\\\\n& + \\E(\\pc|D_t = 1) - \\E(\\pc|D_t = 1)\n\\end{align}\n\\]\nLet us rearrange the order of the terms to get\n\\[\n\\begin{align}\n\\rho_{D, Y} & = \\overbrace{\\E(\\pt|D_t = 1) - \\E(\\pc|D_t = 1)}^{\\tau_{ATT}} \\\\\n& \\qquad \\qquad \\qquad \\underbrace{+ \\E(\\pc|D_t = 1) - \\E(\\pc|D_t = 0)}_{\\text{selection bias}}\n\\end{align}\n\\tag{5.1}\\]\nWe can see that according to definition 5.4, the first part of this correlation between \\(D\\) and \\(Y\\) is the ATT, one of the causal estimands. However, the correlation \\(\\rho_{D,Y}\\) does not only equal the \\(\\tau_{ATT}\\), as there is an extra bit, the selection bias. If this selection bias is not 0, then our correlation is clearly not equal to our ATT.\nLet us look at the selection bias term more carefully:\n\\[\n\\underbrace{+ \\E(\\pc|D_t = 1) - \\E(\\pc|D_t = 0)}_{\\text{selection bias}}\n\\]\nThe first part is the average potential outcome under parallel world \\(D_t = 0\\) for the units that were assigned to treatment \\(D_t = 1\\). The second part is the average potential outcome under parallel world \\(D_t = 0\\) for units that were assigned to control \\(D_t = 0\\).\nIf this term is non-zero, that means the control group and treatment group have different average \\(\\pc\\) values. That means, before our experiment had even started, the control and treatment groups had different baseline potential outcome \\(\\pc\\). If for example, the treatment group \\(D_t = 1\\) initially had a very low \\(\\pc\\), even after treatment with true effect \\(+\\tau\\), their outcome \\(Y_t\\) may still be lower than people who didn’t get the treatment. So, our correlation \\(\\rho_{D,Y}\\) would pick up a negative treatment effect, when the actual treatment effect is positive \\(+\\tau\\).\nA good intuitive example is the question: does going to the hospital improve your life expectancy? If we were to just collect correlation data, we would see that actually, people who went to the hospital have lower life expectancy.\nBut, that is because people who go to the hospital in the first place already have low life expectancy \\(\\pc\\) compared to people who didn’t go. The hospital will cause these people with low life expectancy to have longer lives (treatment effect \\(\\tau\\)), but even with \\(\\pc + \\tau\\), their life expectancy may still be lower than the \\(\\pc\\) of the group who never went to the hospital.\nThus, our correlation measure shows a negative effect of going to the hospital on life expectancy, when in reality, going to the hospital does boost life expectancy, its just people who choose to go to the hospital start off with lower life expectancy than those who do not go.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal.html#confounders",
    "href": "causal.html#confounders",
    "title": "5  Causal Inference",
    "section": "5.4 Confounders",
    "text": "5.4 Confounders\nThe reason for this difference in pre-experiment \\(\\pc\\) is a third variable \\(X\\) is causing people to go to select treatment \\(D\\) more frequently, and also has some effect on \\(Y\\). \\(X\\) is thus causing selection bias. An \\(X\\) that causes selection bias is called a confounder.\n\nDefinition 5.8 (Confounder) A confounder is a third variable \\(X\\) that causes selection into treatment \\(D\\) and causes changes in outcome \\(Y\\). If there is such a variable, this creates selection bias, and makes correlation not equal causation.\n\n\nFor example, there is a well known correlation in the real world that ice cream sales are strongly correlated with shark attacks. So does ice cream sales actually cause more shark attacks?\nThe answer is (likely) no. The reason we see this correlation is because of a third variable - the weather. When the weather is warm, more people buy ice cream, and more people go to the ocean, hence increasing the amount of shark attacks.\nWeather is a third variable that we consider a confounder, as it affects both ice cream sales and shark attacks. There is actually no relationship between ice cream sales and shark attacks - the perceived correlation is caused by the confounder weather.\nOur goal in causal inference is to eliminate the effects of the confounders and isolate the effects between treatment and outcome. We can also visualise confounders in a structural causal model:\n\n\n\n\n\nFor example, the figure above, \\(X\\) is confounding the relationship between \\(D \\rightarrow Y\\). This is because both \\(X\\) is correlated with \\(D\\) and \\(Y\\). When we naively estimated the correlation \\(\\rho_{D,Y}\\), we are estimating the relationship between \\(D \\rightarrow Y\\) and the relationship \\(D \\leftarrow X \\rightarrow Y\\) . But, the true causal effect is \\(D \\rightarrow Y\\), without the other backdoor path. For accurate causal estimation, we need some way to “block” the path through \\(X\\).\nWhy isn’t \\(V\\) a confounder? Well, \\(V\\) does not cause selection into \\(D\\), as the arrow direction shows that \\(D\\) causes \\(V\\). Thus, \\(V\\) is not causing selection bias. We do not care about variables caused by \\(D\\).",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal.html#randomisation",
    "href": "causal.html#randomisation",
    "title": "5  Causal Inference",
    "section": "5.5 Randomisation",
    "text": "5.5 Randomisation\nOne way we can eliminate selection bias is through randomly assigning our subjects to either treatment \\(D_t = 1\\) or control \\(D_t = 0\\). Randomisation implies that the assignment probabilities do not depend on the potential outcomes - the values of \\(\\pc\\) and \\(\\pt\\) do not affect the probability of a unit \\(t\\) getting put into treatment \\(D_t = 1\\) or control \\(D_t = 0\\).\n\\[\n\\P(D_t = 1 | \\pc) = \\P(D_t = 1 | \\pt)  =  \\P(D_t = 1)\n\\]\nThis fact implies the critical assumption of randomisation, independence.\n\nDefinition 5.9 (Independence) Randomisation, if done properly, implies independence. Independence, also called unconfoundedness or ignorability, is the statement that potential outcomes are independent of treatment:\n\\[\n\\pc, \\pt \\ind D_t\n\\]\nThis assumption, and the defintition of independence from definition 1.6, implies that \\(\\E Y_{t}(0)\\) and \\(\\E Y_{t}(1)\\) are equal between treatment and control:\n\\[\n\\begin{align}\n& \\E(\\pc|D_t = 1) \\ = \\ \\E(\\pc|D_t = 0) \\ = \\ \\E \\pc \\\\\n& \\E(\\pt|D_t = 1) \\ = \\ \\E(\\pt|D_t = 0) \\ = \\ \\E \\pt\n\\end{align}\n\\]\n\n\n\n\n\n\n\nChecking if Independence is Met\n\n\n\n\n\nWhen we are writing a paper, we might want to convince our readers that randomisation has indeed successfully been implemented, and that the assumption of independence has been met. The most common way to check this is with a balance check. We essentially consider a few likely confounders \\(X\\). We then run a regression:\n\\[\nX_t = \\beta_0 + \\beta_1 D_t + \\eps_t\n\\]\n\\(\\beta_1\\) will show the difference between \\(\\E(X_t | D_t = 1)\\) and \\(\\E(X_t|D_t = 0)\\). If \\(\\beta_1\\) is not statistically significantly different, then we can conclude the treatment and control groups are similar, and that randomisation has suceeded.\nDo note that the important thing in randomisation (and the assumption of independence) is not that \\(X_t\\) is random between treatment and control, but rather potential outcomes are random. Although, if \\(X_t\\) is random between treatment and control, that generally implies potential outcomes are also random.\n\n\n\nWe can prove that randomisation and the independence assumption allow us to eliminate the selection bias shown in eq. 5.1. Let us start with our correlation from eq. 5.1:\n\\[\n\\begin{align}\n\\rho_{D, Y} & = \\overbrace{\\E(\\pt|D_t = 1)- \\E(\\pc|D_t = 1)}^{\\tau_{ATT}} \\\\\n& \\qquad \\qquad \\qquad + \\underbrace{\\E(\\pc|D_t = 1) - \\E(\\pc|D_t = 0)}_{\\text{selection bias}}\n\\end{align}\n\\]\nUsing the properties of independence from definition 5.9, we can get\n\\[\n\\begin{align}\n\\rho_{D, Y} & = \\underbrace{\\E(\\pt|D_t = 1)- \\E(\\pc|D_t = 1)}_{\\tau_{ATT}} + \\underbrace{\\E \\pc - \\E \\pc}_{\\text{selection bias}} \\\\\n& = \\underbrace{\\E(\\pt|D_t = 1)- \\E(\\pc|D_i = 1)}_{\\tau_{ATT}} + 0\n\\end{align}\n\\]\nThus, we can see the assumption of independence has removed our selection bias, and allowed us to accurately calculate our \\(\\tau_{ATT}\\) simply by looking at the correlation \\(\\rho_{D,Y}\\).\nWe can also identify the ATE from our correlation \\(\\rho_{D,Y}\\), by simplifying once again using the properties implied by the assumption of independence from definition 5.9:\n\\[\n\\begin{align}\n\\rho_{D, Y} & = \\underbrace{\\E(\\pt|D_i = 1)- \\E(\\pc|D_i = 1)}_{\\tau_{ATT}} \\\\\n& = \\underbrace{\\E \\pt - \\E \\pc}_{\\tau_{ATE}}\n\\end{align}\n\\]\nWhich according to definition 5.3, is the ATE. Thus, under randomisation, our correlation measure \\(\\rho_{D, Y} = \\tau_{ATT} = \\tau_{ATE}\\). Recall our correlation measure again \\(\\rho_{D, Y}\\) was simply a comparison of observed outcomes between treatment and control groups:\n\\[\n\\rho_{D, Y} = \\E(Y_t|D_t = 1) - \\E (Y_t | D_t = 0) = \\tau_{ATE} = \\tau_{ATT}\n\\]\nSince \\(\\rho_{D, Y}\\) only requires observed outcomes (not potential), we can calculate \\(\\rho_{D, Y}\\), which is also equal to the ATE and ATT under randomisation. Thus, we have identified a way to find the ATE and ATT with just observed outcomes. This means that under randomisation, simple correlation models also give us causal effects.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal.html#randomised-experiments",
    "href": "causal.html#randomised-experiments",
    "title": "5  Causal Inference",
    "section": "5.6 Randomised Experiments",
    "text": "5.6 Randomised Experiments\nSo we have established that randomly assigning treatment and control can allow us to establish that correlation equals causation (under succesfull randomisation only).\nBut how can we actually implement this design? There are several different ways to do randomisation:\n\nClassicBernoulliClusterStratified\n\n\nThe classic randomisation is the most common form of randomsiation.\nWe start out with \\(N\\) total number of individuals/observations in our experiment. We first decide the number of individuals we want to assign to treatment, \\(N_1\\). Frequently, \\(N_1\\) is 50% of \\(N\\).\nThen, a random set of \\(N_1\\) number individuals is assigned to treatment, and the remainder is sent to control. Now we have randomly assigned treatment and control individuals.\nThere is an issue: technically, not all units \\(t\\) have an equal chance of being assigned to treatment. This is because once we assign \\(N_1\\) of individuals to treatment, the remaining observations will have 0% chance of being assigned to treatment. Bernoulli Randomisation deals with this.\n\n\nBernoulli randomisation is another form of randomisation. Instead of specifying before-hand how many treated units \\(N_1\\) you want, we let \\(N_1\\) be un-fixed.\nFor every unit \\(t\\), they have some probability \\(p\\) of being assigned to treatment. We do this for every unit \\(t\\).\nThe result is that every unit has an equal chance of being assigned to treatment and control, unlike classic randomisation. The downside is that if we run the procedure multiple times, the number of units treated \\(N_1\\) will differ between each trial.\n\n\nCluster randomisation is when we randomly assign units (or have individuals naturally) in groups. Every unit within a group (called a cluster) will get the same treatment. We randomly sample the groups to get the treatment or control.\nFor example, we could randomise development treatment at the village level, or randomise treatment of a cirriculum at the school level.\nThe main reason for this is to prevent SUTVA violations. For example, imagine you are testing the effects of a new curriculum. If you randomise by each student, students will talk to their friends, and treated individuals may teach control individuals about the new curriculum. But by randomising by school (either an entire school gets or does not get the new curriculum), this concern is not a huge issue.\n\n\nStratified (also called blocked or conditional) randomisation are when randomisation occurs separately within levels of some covariates(s) \\(X\\). Generally, you separate your sample of \\(N\\) units into \\(J\\) subgroups. For example, you could split people up into male or female, then randomly sample within each group, rather than everyone together.\nThe reason you might want to do this is to ensure that both your treatment and control are balanced. For example, let us say you have 4 subjects, with pre-treatment potential outcomes \\(\\pc = \\{2, 2, 8, 8\\}\\).\nIf you are randomly assigning, you have a 33% chance you end up with an assignment where \\(\\{8, 8\\}\\) are placed in one group, and \\(\\{2, 2\\}\\) are placed in another group. In this case, our treatment/control groups would be very different, and this would violate our independence assumption.\nBy stratifying our sample before into two subgroups, with group 1 being \\(\\{2, 2\\}\\) and group 2 being \\(\\{8,8\\}\\), and randomly sampling one from each group into treatment, we are guaranteed to have more balance.\n\n\n\nWhen we are writing a paper, we might want to convince our readers that randomisation has indeed successfully been implemented, and that the assumption of independence has been met. The most common way to check this is with a balance check. We essentially consider a few likely confounders \\(X\\). We then run a regression:\n\\[\nX_t = \\underbrace{\\beta_0 + \\beta_1 D_t}_{\\E(X_t|D_t)} + \\eps_t\n\\]\n\\(\\beta_1\\) will show the difference between \\(\\E(X_t | D_t = 1)\\) and \\(\\E(X_t|D_t = 0)\\). If \\(\\beta_1\\) is not statistically significantly different, then we can conclude the treatment and control groups are similar, and that randomisation has succeeded.\nWe can estimate the ATE in a randomisation setting where independence is met simply with a correlation measure, such as a linear model:\n\\[\nY_t = \\underbrace{\\alpha + \\tau D_t}_{\\E(Y_t | D_t)} + \\eps_t\n\\]\nWhere \\(\\tau\\) can be estimated with OLS, GLS, or MLE (although OLS with robust standard errors is generally the preferred estimator). A difference-in-means estimator is also possible.\nSignificance testing will be done with a t-test (definition 4.17) like you do in OLS. Robust standard errors (to account for heteroscedasticity) should be used as a default, unless you can prove homoscedasticity is met. If you are using cluster randomisation, you should use cluster-robust standard errors. Bootstrap inference is also a common choice if your sample size is quite small.\nWhile the linear model is the best way to estimate the ATE specifically, if you are less concerned with the ATE, you can also use other models, such as the logistic model, poisson model, and many more. While the results may not be exactly the ATE, they can be interpreted causally if you are randomising.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal.html#randomisation-inference",
    "href": "causal.html#randomisation-inference",
    "title": "5  Causal Inference",
    "section": "5.7 Randomisation Inference",
    "text": "5.7 Randomisation Inference\nWhile you can use a traditional t-test like in OLS, we can also do a different form of inference: randomisation inference. This assumes a certain form of null hypothesis, called the sharp null hypothesis:\n\\[\nH_0^s: \\pt = \\pc, \\quad H_a^s: \\pt ≠ \\pc\n\\]\nThis null hypothesis basically asserts that there is no treatment effect \\(\\tau_t\\) for any individual \\(t\\) in the study.\nAssuming \\(H_0^s\\) is true, we can actually fully construct the potential outcomes \\(\\pc, \\pt\\), since we know every unit has 0 individual treatment effect. Thus, for any unit \\(t\\), their observed \\(Y_t = \\pc = \\pt\\) if there is no treatment effect.\nSince we can fully construct the potential outcomes, we can recreate the sampling distribution without asymptotic properties of our estimator, as we do not need the central limit theorem (theorem 2.2).\nFirst, we want to calculate the total number of randomisations possible. If we have \\(N\\) total units, and \\(N_1\\) in the treatment group and \\(N_0\\) in the control group, we have\n\\[\n\\begin{pmatrix} N \\\\ N_1 \\end{pmatrix} = \\frac{N!}{N_1!N_0!}\n\\]\nnumber of possible randomisation assignments. Then, for each randomisation assignment, we calculate the \\(\\hat\\tau\\) of that randomisation.\n\n\n\n\n\nAnd now with the \\(\\hat\\tau\\) of every possible randomisation, we can plot it in a distribution to create the sampling distribution under the null hypothesis:\n\n\n\n\n\nNow, we essentially conduct a hypothesis test on this sampling distribution, and calculate the p-value as normal. If our result is significant, we have evidence to reject our sharp null hypothesis \\(H_0^s\\).\nThe benefit of randomisation inference is that it is assumption free- we do not rely on asymptotic properties of estimators, which frequently require large sample sizes.\nHowever, the downside of randomisation inference is that it only tests the sharp null hypothesis \\(H_0^s\\). But this might not be the hypothesis we are actually interested in.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "identify.html",
    "href": "identify.html",
    "title": "6  Causal Identification",
    "section": "",
    "text": "6.1 Selection on Observables\nRandomisation is not always feasible. Selection on Observables is an alternative way to get rid of the selection bias problem by controlling for a set of confounders \\(\\mathcal X\\) to block backdoor paths and isolate the relationship between \\(D \\rightarrow Y\\). We will have controlled for the correct set of confounders \\(\\mathcal X\\), when we have eliminated all variables causing selection bias, and we meet the following assumptions:\nUsing these two assumptions, we can first identify the CATE, which will allow us to identify the ATE and ATT. Let us first start with the definition of CATE (definition 5.6):\n\\[\n\\begin{align}\n\\tau_{CATE}(x) & = \\E(\\pt - \\pc \\ | \\ \\set X_t = x) \\\\\n& = \\E(\\pt | \\set X_t = x) - \\E(\\pc|\\set X_t = x) \\\\\n\\end{align}\n\\]\nNow, from the properties implied by conditional ignorability given in definition 6.1, we get\n\\[\n\\begin{align}\n\\tau_{CATE}(x) & = \\E(\\pt|D_i = 1, \\set X_t = x) - \\E(\\pc|D_i = 0, \\set X_t = x) \\\\\n& = \\E(Y_{t}|D_t = 1, \\mathcal X_t = x) - \\E(Y_{t}|D_t = 0, \\set X_t = x) \\\\\n\\end{align}\n\\tag{6.1}\\]\nAnd the second step above is because \\(\\pt|D_i = 1\\) and \\(\\pc|D_i = 0\\) are observable outcomes (definition 5.7). Thus, with independence, we can identify the CATE with just observed outcomes. With the CATE identified, we can now identify both the ATE and ATT.\nAssuming we meet the assumptions given in definition 6.1, there are multiple estimators, including regression, matching, and weighting, which we will discuss in more detail in the chapter on causal estimation.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Causal Identification</span>"
    ]
  },
  {
    "objectID": "identify.html#selection-on-observables",
    "href": "identify.html#selection-on-observables",
    "title": "6  Causal Identification",
    "section": "",
    "text": "Definition 6.1 (Selection on Observables) 2 assumptions are needed for identification:\nConditional Ignorability (also called conditional independence) means that among units \\(i\\) with identical confounder values \\(\\mathcal X_i\\), treatment \\(D_i\\) is as-if randomly assigned. Potential outcomes are independent from treatment within each specific confounder value \\(\\mathcal X_i = x\\).\n\\[\n(\\pc, \\pt) \\ind D_t \\ | \\ \\set X_t = x, \\ \\forall \\ x \\in \\set X\n\\]\nThis assumption implies that given any value of confounders \\(\\set X_t = x\\), potential outcomes are equivalent between treatment and control groups:\n\\[\n\\begin{align}\n& \\E(\\pc|D_t = 1, \\set X_t = x) \\ = \\ \\E(\\pc|D_t = 0, \\set X_t = x) \\ = \\ \\E(\\pc|\\set X_t = x) \\\\\n& \\E(\\pt|D_t = 1, \\set X_t = x) \\ = \\ \\E(\\pt|D_t = 0, \\set X_t = x) \\ = \\ \\E(\\pt|\\set X_t = x)\n\\end{align}\n\\]\nCommon Support is the second assumption, and it states for any unit \\(t\\) with any value of \\(\\mathcal X_t\\), they have a non-zero probability they can be assigned to both control and treatment.\n\n\n\n\n\n\n\nGood and Bad Controls\n\n\n\n\n\nGood controls are confounders \\(X\\) who cause \\(D\\) (i.e. \\(X \\rightarrow D\\)), and cause/caused by \\(Y\\). They must be linked to both \\(D\\) and \\(Y\\).\nBad controls are any variable \\(W\\) that is caused by \\(D\\), i.e. \\(D \\rightarrow W\\). We do not need to control for this because \\(W\\) isn’t causing selection into \\(D\\), it is actually itself caused by \\(D\\).\nAnother bad control is a variable \\(Z\\) that only causes \\(D\\), and not \\(Y\\). This reduces the variation in \\(D\\) (since we are now only using the part of \\(D\\) caused by \\(Y\\)), which may amplify any other confounders unaccounted for.\nNeutral controls are variables that only cause \\(Y\\) and are not associated with \\(D\\). They do not affect the causal identification, but can reduce our standard errors of our estimates.\n\n\n\n\n\n\n\n\n\nATE IdentificationATT Identification\n\n\nLet us consider the definition of the ATE, which is \\(\\E(\\pt - \\pc)\\). From the definition of a continuous random variable’s expectation given in definition 1.2, we can rewrite the ATE as\n\\[\n\\tau_{ATE} = \\int\\tau_{CATE}(x) \\P(\\mathcal x) dy\n\\]\nWhich is a weighted average. We established in eq. 6.1 that we can identify the CATE. Thus, we can plug eq. 6.1 into the ATE to get our identified ATE:\n\\[\n\\tau_{ATE} = \\int [\\E(Y_{t}|D_t = 1, \\set X_t = x) - \\E(Y_{t}|D_t = 0, \\set X_t = x)] \\P(x) dy\n\\]\nAnd all the values in this equation are observed outcomes \\(Y_t\\), meaning if we control for set of confounders \\(\\mathcal X\\), our correlation becomes a causal effect.\n\n\nLet us consider the definition of the ATT, which is \\(\\E(\\pc = \\pt | D_i = 1)\\). From the definition of a continuous random variable’s expectation given in definition 1.2, we can rewrite the ATT as\n\\[\n\\tau_{ATT} = \\int \\tau_{CATE}(x) \\P(x|D_t = 1) dy\n\\]\nWhich is a weighted average. We established in eq. 6.1 that we can identify the CATE. Thus, we can plug eq. 6.1 into the ATE to get our identified ATT:\n\\[\n\\tau_{ATT} = \\int [\\E(Y_{t}|D_t = 1, \\set X_t = x) - \\E(Y_{t}|D_t = 0, \\mathcal X_t = x)] \\P(x|D_t = 1) dy\n\\]\nAnd all the values in this equation are observed outcomes \\(Y_t\\), meaning if we control for set of confounders \\(\\mathcal X\\), our correlation becomes a causal effect.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Causal Identification</span>"
    ]
  },
  {
    "objectID": "identify.html#instruments-and-non-compliance",
    "href": "identify.html#instruments-and-non-compliance",
    "title": "6  Causal Identification",
    "section": "6.2 Instruments and Non-Compliance",
    "text": "6.2 Instruments and Non-Compliance\nWhen we assign individuals to treatment/control in randomised experiments, we often cannot guarantee that individuals will actually follow through with treatment. Sometimes, someone who was supposed to get treatment refuses treatment, and sometimes, someone who shouldn’t get the treatment decides to get treatment.\nWe can formalise this idea. Let us assume an encouragement \\(Z_i \\in \\{0, 1\\}\\), which is our treatment assignment. Then, we have the treatment variable \\(D_i \\in \\{0,1\\}\\), which is someone who actually took the treatment or not. Given this framework, we can divide all units \\(i\\) into 4 categories:\n\nCompliers: People who comply with encouragement \\(Z_i\\). Their \\(Z_i = D_i\\).\nAlways-takers: People who no matter what encouragement \\(Z_i\\) is, always take treatment.\nNever-takers: People who no matter their encouragement \\(Z_i\\) is, never take treatment.\nDefiers: People who do the opposite of encouragement \\(Z_i\\), so always \\(D_i ≠ Z_i\\).\n\nWe can show what will happen with all 4 types of people in a table, called the principal strata:\n\n\n\n\n\\(Z_i = 1\\)\n\\(Z_i = 0\\)\n\n\n\\(D_i = 1\\)\nComplier/Always-Taker\nDefier/Always-Taker\n\n\n\\(D_i = 0\\)\nDefier/Never-Taker\nComplier/Never-Taker\n\n\n\nThe idea of the non-compliance designs is to use our encouragement/treatment assignment \\(Z\\) as an instrument for \\(D\\) - actually taking the treatment. This can solve any concerns we might have regarding selection into non-compliance.\nOne causal estimand that we can estimate is the Intent To Treat (ITT), which is essentially the ATE of \\(Z\\) on \\(Y\\), ignoring if people actually took the treatment or not. This is essentially the affect of encouragement. If there is any non-compliance, then the ITT will not equal the ATE.\n\\[\n\\tau_{ITT} = \\E(Y_i|Z_i = 1) - \\E(Y_i | Z_i = 0)\n\\]\nThis is quite easily estimatable in non-compliance randomisation settings, since \\(Z\\) is randomly assigned. Thus, we can calculate the ITT with a linear regression:\n\\[\nY_i = \\alpha + \\tau_{ITT} Z_i + \\eps_i\n\\]\nHowever, we might not care about the ITT. We might want the actual effect of \\(D\\) on \\(Y\\). An alternative is to find the Average Treatment Effect on only compliers, called the Local Average Treatment Effect (LATE) (a version of the CATE from definition 5.6). We need a few assumptions:\n\nDefinition 6.2 (Instrumental Variables Assumptions) There are 4 assumptions for us to identify the LATE. The first 3 are the normal assumptions from the IV estimator.\n\nRelevance: \\(Z\\) must be correlated to \\(D\\). Or in other words, compilers must exist, or else, encouragement would not affect treatment.\nIgnorability/Exogneity: There is no backdoor path between \\(Z\\) and \\(D\\), and no backdoor path between \\(Z\\) and \\(Y\\) (we can do controls/selection on observables to account for this).\nExclusions Restriction: \\(Z\\) must only have an effect on \\(Y\\) through \\(D\\). \\(Z\\) must not have any independent effect on \\(Y\\).\nMonotonicity: There are no defiers.\n\n\n\n\n\n\n\n\nVisual Examples of Violations\n\n\n\n\n\nThe figure below contains some visual examples of violations of the above to faciliate understanding of these assumptions:\n\n\n\n\n\n\n\n\nLet us show how we can prove the LATE is identifiable under these assumptions. First, the ITT itself in ?eq-itt is identifiable under exogeneity/ignorability alone. Now, let us define \\(c\\) as compliers, \\(a\\) as always-takers, \\(n\\) as never-takers, and \\(d\\) as defiers. We can break down the ITT into a weighted average:\n\\[\n\\tau_{ITT} = \\tau_{ITT}^c \\P(c) + \\tau_{ITT}^a \\P(a) + \\tau_{ITT}^n \\P(n) + \\tau_{ITT}^d \\P(d)\n\\]\nWe know that under our assumption of monotonicity, we assume no defiers, so \\(\\P(d) = 0\\):\n\\[\n\\tau_{ITT} = \\tau_{ITT}^c \\P(c) + \\tau_{ITT}^a \\P(a) + \\tau_{ITT}^n \\P(n)\n\\]\nOur exclusions restriction says that \\(Z\\) has no affect on \\(Y\\). Remember that the ITT from ?eq-itt is the relationship between \\(Z\\) and \\(Y\\). But since always-takers and never-takers ignore \\(Z\\) when deciding treatment, \\(Z\\) has no effect of them on \\(Y\\). Thus, we can further simplify:\n\\[\n\\tau_{ITT} = \\tau_{ITT}^c \\P(c)\n\\]\nRemember that the \\(\\tau_{ITT}\\) for compliers, \\(\\tau_{ITT}^c\\), is our LATE that we want to identify. So, let us isolate it to get:\n\\[\n\\tau_{LATE} = \\frac{\\tau_{ITT}}{\\P(c)} \\ = \\ \\frac{\\E(Y_i | Z_i = 1) - \\E(Y_i | Z_i = 0)}{\\E(D_i | Z_i = 1) - \\E(D_i | Z_i = 0)}\n\\]\nThe LATE is interpreted as the ATE of the compliers. This can be tricky to interpret sometimes, as you cannot actually identify who the compliers are, and different encouragements \\(Z\\) can result in different compliers.\nThe LATE is also generally not equal to the ATE or ATT without further assumptions. When there is only one-sided non-compliance when people who are not encouraged cannot get the treatment, the LATE does equal the ATT.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Causal Identification</span>"
    ]
  },
  {
    "objectID": "glm.html",
    "href": "glm.html",
    "title": "9  Generalised Linear Model",
    "section": "",
    "text": "9.1 Overview\nThe generalised linear model (GLM) are a series of models that help explain the relationship between a series of explanatory variables \\(X_1, X_2, \\dots, X_p\\) and an outcome variable \\(Y\\), for individual observations \\(t = 1, 2, \\dots, n\\).\nThe generalised linear model (GLM) is a grouping of many different models that take the following form:\n\\[\ng(\\mu) = \\beta_0 + \\beta_1 X_{t1} + \\beta_2 X_{t2} + \\dots + \\beta_p X_{tp}\n\\]\nWhere \\(\\mu\\) is the parameter of interest of variable \\(Y\\) (depends on the model) and \\(g(\\cdot)\\) is some link-function that allows the GLM to be applied to a variety of different types of \\(Y\\). \\(\\beta_0, \\dots, \\beta_p\\) are the parameters of the model that need to be estimated, that explain the relationship between \\(X_1, \\dots, X_p\\) and \\(Y\\).\nAll GLMs can accomodate any type of explanatory variable \\(X_1, \\dots, X_p\\). The choice of model depends on the type of variable \\(Y\\) is:\nBelow, I will introduce each of these models, their specification, their estimation process (with R-code), their interpretations, and how to predict with them. The last section of this chapter relates to common model specification issues that apply to all models, such as different types of explanatory variables and functional form/transformations.",
    "crumbs": [
      "Part II: Applied Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Generalised Linear Model</span>"
    ]
  },
  {
    "objectID": "glm.html#overview",
    "href": "glm.html#overview",
    "title": "9  Generalised Linear Model",
    "section": "",
    "text": "Note that relationships cannot be interpreted causally, unless the treatment in question was randomly assigned and meets the condition of independence (definition 5.9). If you are interested in causal estimation, see the next chapter.\n\n\n\n\n\n\nContinuousBinaryOrdinalCategoricalCount\n\n\nContinuous outcome variables \\(Y\\) can only be used with the linear regression model.\n\n\nBinary outcome variables \\(Y\\) can be used with:\n\nBinomial Logistic Regression: Good for prediction, but can be harder to interpret relationships between \\(X_j\\) and \\(Y\\).\nLinear Probability Model: For interpretation of relationship between \\(X_j\\) and \\(Y\\) only. Do not use for prediction.\n\n\n\nOrdinal outcome variables \\(Y\\) can be used with:\n\nLinear Regression Model: Good for interpreting relationships between \\(X_j\\) and \\(Y\\). Do not use for prediction.\nOrdinal Logistic Regression: Okay for interpreteting relationships between \\(X_j\\) and \\(Y\\). Can impose more strict assumptions for prediction.\nMultinomial Logistic Regression: Not useful for interpretation, but the best for prediction.\n\n\n\nCategorical outcome variables \\(Y\\) can only be used with multinomial logistic regression.\n\n\nCount outcome variables \\(Y\\) can be used with:\n\nLinear Regression Model: Good for interpreting relationships between \\(X_j\\) and \\(Y\\). Okay for prediction but not the best.\nNegative Binomial Model: Great for prediction. Harder to interpret relationships between \\(X_j\\) and \\(Y\\).\n\n\n\n\n\n\n\n\n\n\nReview of the Types of Variables\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\nExample\n\n\n\n\nContinuous\nCan take any value within an interval \\([a, b]\\).\nGDP of a country, temperature.\n\n\nBinary\nCan only take value 0 or 1.\nYes/no, true/false, did/didn’t.\n\n\nOrdinal\nCan only take a finite set of discrete outcomes \\(\\{a, b, c\\}\\), but these outcomes can be ordered.\nstrongly disagree - disagree - neutral - agree - strongly agree\n\n\nCategorical\nCan only take a finite set of discrete outcomes \\(\\{a, b, c\\}\\), and these outcomes have no natural order.\nCountry of birth.\n\n\nCount\nCan only take an integer value \\(\\{0, 1, 2, \\dots\\}\\).\nNumber of cases of a disease, number of phone calls received at a call centre in an hour.",
    "crumbs": [
      "Part II: Applied Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Generalised Linear Model</span>"
    ]
  },
  {
    "objectID": "glm.html#linear-regression-model",
    "href": "glm.html#linear-regression-model",
    "title": "9  Generalised Linear Model",
    "section": "9.2 Linear Regression Model",
    "text": "9.2 Linear Regression Model\nThe linear regression model is used for continuous outcome variables \\(Y\\), and a set of explanatory variables \\(\\set X = \\{X_1, \\dots, X_p\\}\\) for observations \\(t = 1, \\dots, n\\):\n\\[\nY_t = \\underbrace{\\beta_0 + \\beta_1X_{t1} + \\beta_2 X_{t2} + \\dots + \\beta_p X_{tp}}_{\\E(Y_t | \\set X_t)} + \\eps_t\n\\]\nWhere \\(\\beta_0\\) is the intercept, \\(\\beta_1, \\dots, \\beta_p\\) are the parameters that explain the relationship between \\(X_j\\) and \\(Y\\), and \\(\\eps_t\\) is the error term that accounts for random variation/noise in \\(Y\\). For more details on the linear model, see chapter 4.\nThe parameters \\(\\beta_0, \\dots, \\beta_p\\) need to be estimated with sample data to obtain estimates \\(\\hat\\beta_0, \\dots, \\hat\\beta_p\\). The linear model can be estimated with a variety of estimators:\n\nOLSOLS with Robust SEGLS/WLSFGLSIV/2SLS\n\n\nWe should use the Ordinary Least Squares estimator (definition 4.8) when our model meets all 5 of the classical linear model assumptions. If we are not sure spherical errors (definition 4.6) is met, we should instead use OLS with robust standard errors.\nFor estimation with OLS (and normal standard errors), the R-code is:\n\nmodel &lt;- lm(Y ~ X1 + X2 + X3, data = mydata)\nsummary(model)\n\nFor mechanical details on OLS, see here.\n\n\nWe should use the Ordinary Least Squares estimator (definition 4.8) with robust standard errors (theorem 4.6) if we believe our model meets all of the classical linear model assumptions except for spherical errors (definition 4.6), and instead we have conditional heteroscedasticity (?def-heteroscedasticity).\nFor estimation with OLS and robust standard errors in R, we should use the fixest package:\n\nlibrary(fixest)\nmodel &lt;- feols(Y ~ X1 + X2 + X3, data = mydata, se = \"hetero\")\nsummary(model)\n\nFor mechanical details on robust standard errors, see here.\n\n\nWe can use the generalised least squares estimator (definition 4.14) if we believe our model meets all of the classical linear model conditions except for spherical errors (definition 4.6), and we know the structure of our heteroscedasticity/autocorrelation.\nIf we do not know the structure of our heteroscedasticity/autocorrelation, we should use the feasible generalised least squares (FGLS) or just OLS with robust standard errors. I do not recommend using GLS/WLS over OLS with robust standard errors, as the chances of something going wrong are much higher.\nIf we have a weights matrix already, we can implement GLS in R:\n\nmodel &lt;- lm(Y ~ X1 + X2 + X3,\n            data = mydata,\n            weights = weightsmatrix)\nsummary(model)\n\nFor mechanical details on GLS, see here.\n\n\nWe should use the feasile generalised least squares estimator if we believe our model meets all of the classical linear model conditions except for spherical errors (definition 4.6), and we do not know the structure of our heteroscedasticity/autocorrelation.\nIf we do know the structure of our heteroscedasticity/autocorrelation, we can use the normal generalised least squares (GLS) estimator. I do not recommend using FGLS over OLS with robust standard errors, as the chances of something going wrong are much higher.\nFor FGLS in R, we first estimate a normal linear regression with OLS to obtain the estimated residuals, before using them as weights in a second model:\n\nols &lt;- lm(Y ~ X1 + X2 + X3, data = mydata)\nmodel &lt;- lm(Y ~ X1 + X2 + X3,\n            data = mydata,\n            weights = 1/ols$fitted.values^2)\nsummary(model)\n\nFor mechanical details on FGLS, see here.\n\n\nWe should use the instrumental variables estimator (definition 4.16)or 2-stage least squares (2SLS) when our model violates exogeneity (definition 4.5), and we care a lot about accurately estimating the correlation between \\(X_j\\) and \\(Y\\), and we have a suitable instrument \\(Z\\).\nYou should not use IV/2SLS unless you really only care about the accurate estimation of one \\(\\beta_j\\), and do not care about prediction purposes.\nFor estimation of IV/2SLS, we should use the fixest package:\n\nlibrary(fixest)\nmodel &lt;- feols(Y ~ 1 | D ~ Z, data = mydata, se = \"hetero\")\nsummary(model)\n\nFor mechanical details on IV/2SLS, see here.\n\n\n\nInterpretation of coefficient estimates are as follows (proof from theorem 4.1). Remember that these interpretations are not causal unless our \\(X_j\\) has been randomly assigned and meets the condition of independence (definition 1.6):\n\n\n\n\n\n\n\n\n\nContinuous \\(X_{j}\\)\nBinary \\(X_{j}\\)\n\n\n\\(\\hat\\beta_j\\)\nFor every one unit increase in \\(X_{j}\\), there is an expected \\(\\hat\\beta_j\\) unit change in \\(Y\\), holding all other explanatory variables constant.\nThere is a \\(\\hat\\beta_j\\) unit difference in \\(Y_i\\) between category \\(X_{j} = 1\\) and category \\(X_{j} = 0\\), holding all other explanatory variables constant.\n\n\n\\(\\hat\\beta_0\\)\nWhen all explanatory variables equal 0, the expected value of \\(Y\\) is \\(\\hat\\beta_0\\).\nFor category \\(X_{j} = 0\\), the expected value of \\(Y\\) is \\(\\hat\\beta_0\\) (when all other explanatory variables equal 0).\n\n\n\n\nPredictionT-TestsR-SquaredF-Tests\n\n\nTo predict new values of \\(Y\\) for new observations \\(t\\), we use our fitted values equation:\n\\[\n\\hat Y_t = \\hat\\beta_0 + \\hat\\beta_1 X_{t1} + \\dots + \\hat\\beta_p X_{tp}\n\\]\nIn R, we can use the predict command:\n\nmy_predictions &lt;- predict(model, newdata = my_new_data)\n\nWhere my_new_data is a data frame with \\(X_1, \\dots, X_p\\) values for all the new observations we want to predict \\(\\hat Y\\) for.\n\n\nHypothesis testing of each coefficient with a t-test (and the p-values) is provided in most regression outputs. The mechanics of t-test were provided in chapter 4.\n\nIf our coefficient \\(\\hat\\beta_j\\) is statistically significant, then we can reject that there is no relationship between \\(X_j\\) and \\(Y\\), and conclude there is a relationship between \\(X_j\\) and \\(Y\\).\nIf our coefficient \\(\\hat\\beta_j\\) is not statistically significant, then we cannot reject there is no relationship between \\(X_j\\) and \\(Y\\).\n\n\n\nR-Squared is provided in the regression output, and was discussed in definition 4.11.\nR-Squared is the percentage of variation in \\(Y\\) our model explains. It is always between 0 and 1. R-squared never decreases as we add more explanatory variables - so it is important not to overly focus on it.\nAdjusted R-Squared is similar but penalises models for having too many explanatory variables.\n\n\nF-tests can test the significance of multiple parameters at once. The mechanics of F-test were provided in chapter 4.\nTo conduct a f-test in R, we do the following (where model 0 is the null model, and model 1 is the alternative model):\n\nanova(model0, model1)\n\nIf the test is statistically significant, then all the additional parameters in model 1 are jointly significant. If the test is not statistically significant, then all the additional parameters in model 1 are not jointly significant.\n\n\n\nFor more information on categorical \\(X\\), interactions, polynomial and logarithmic transformations, see the final section on model specification.",
    "crumbs": [
      "Part II: Applied Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Generalised Linear Model</span>"
    ]
  },
  {
    "objectID": "ls.html",
    "href": "ls.html",
    "title": "6  Least Squares Theory",
    "section": "",
    "text": "6.1 Classical Linear Model\nThe Maximum Likelihood Estimator is great, however, it can be computationally intensive. The OLS estimator is another estimator for population estimands \\(\\theta\\). However, the OLS estimator can only be applied in one specific model: the classical linear model.\nLet us say there are individuals \\(t = 1, 2, \\dots, n\\) in the population. Each individual’s \\(Y\\) value is determined by a set of random variables \\(Y_1, \\dots, Y_n\\). Any individual random variable \\(Y_t\\) is data generating process defined as \\(Y_t \\sim \\set D(\\mu_Y, \\sigma^2_Y)\\), where \\(\\set D\\) represents any distributional form, \\(\\mu_Y\\) is the mean of the random variable, and \\(\\sigma^2_Y\\) is the variance of the random variable.\nThe classical linear model is a specification that the mean \\(\\mu_Y\\) of \\(Y_t\\) is linearly determined by a set of explanatory variables \\(\\set X = \\{X_1, \\dots, X_p\\}\\):\n\\[\n\\E(Y_t|\\set X_t) = \\beta_0 + \\beta_1 X_{1t} + \\beta_2 X_{2t} + \\dots + \\beta_p X_{pt}\n\\]\nWhere \\(\\beta_0, \\dots, \\beta_p\\) are a set of population parameters that determine how \\(\\mu_Y\\) changes in respect to explanatory variables \\(\\set X\\).\nYou will frequently see the linear model represented in another form:\n\\[\nY_t = \\underbrace{\\beta_0 + \\beta_1 X_{1t} + \\dots + \\beta_p X_{pt}}_{\\E(Y_t | \\set X_t)} + \\eps_t, \\quad \\eps_t \\sim \\set D(\\mu_{\\eps_t} = 0, \\sigma^2_{\\eps_t})\n\\]\nWhere the error term \\(\\eps_t\\) represents the variance/randomness in our data generating process, and the rest of the model represents \\(\\mu_y\\).\nWe know that this data generating process applies for random variables \\(Y_1, Y_2, \\dots, Y_n\\). To represent all of these random variables together, we often use the matrix representation of the linear model:\n\\[\n\\b y = \\b{X\\beta} + \\b\\eps \\iff \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\\end{pmatrix} =\n\\begin{pmatrix} 1 & x_{11} & \\dots & x_{1p} \\\\\n1 & x_{21} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\dots & \\vdots \\\\\n1 & x_{n1} & \\dots & x_{np}\\end{pmatrix}\n\\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p\\end{pmatrix} +\n\\begin{pmatrix} \\eps_1 \\\\ \\eps_2 \\\\ \\vdots \\\\ \\eps_n \\end{pmatrix}\n\\]\nHowever, there are a set of five assumptions that the above model has to meet to be considered a classical linear model.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ls.html#classical-linear-model",
    "href": "ls.html#classical-linear-model",
    "title": "6  Least Squares Theory",
    "section": "",
    "text": "Linearityi.i.d.Linear IndependenceExogeneitySpherical Errors\n\n\n\nDefinition 6.1 (Linearity in Parameters) A model is linear in parameters if it can be written in the form \\(\\b y = \\b{X\\beta} + \\b \\eps\\). Or in other words, all parameters \\(\\beta_0, \\dots, \\beta_p\\) must be either added or subtracted from each other.\n\nIt is important to note that linearity in parameters is not equal to linearity in the classical sense. For example, the model\n\\[\nY_t = \\beta_0 + \\beta_1X_t + \\beta_2 X_t^2 + \\beta_3X_t^3 + \\eps_t\n\\]\nwould be considered linear in parameters because all parameters \\(\\beta_0, \\dots, \\beta_p\\) are added together, not multiplied. It does not matter if the \\(X\\) are linear or not.\n\n\n\nDefinition 6.2 (Independent and Identically Distributed) A series of random variables \\(Y_1, Y_2, \\dots, Y_n\\) is considered to be independent and identically distributed if each random variable is independent (definition 1.6), and each random variable has the exact same distribution.\n\nThis assumption is often not met in its purest form - since when we randomly sample \\(Y_t\\) without replacement, technically the next observation \\(Y_{t+1}\\) will have slightly different distribution, since we took one observation out already.\nThis assumption is also frequently violated in time-series data, where \\(Y_t\\) at time \\(t\\) is likely to have an effect on \\(Y_{t+1}\\) at time \\(t+1\\).\n\n\n\nDefinition 6.3 (Linear Independence) Also called non-perfect multicollinearity, this assumption means that no explanatory variables \\(X_1, \\dots, X_p\\) can be perfectly correlated with any other explanatory variable, or perfectly correlated with any linear combination of other explanatory variables.\n\nThis assumption is required for estimation to be possible with the ordinary least squares estimator, as it requires matrix \\(\\b X\\) to be full rank which allows \\(\\b X^\\top \\b X\\) to be invertable.\n\n\n\nDefinition 6.4 (Strict Exogeneity) Strict Exogeneity is \\(\\E(\\b\\eps | \\b X) = 0\\). This implies that \\(\\E(\\b X^\\top \\b\\eps) = 0\\), which means all regressors \\(X_1, \\dots, X_p\\) should be uncorrelated with the error terms \\(\\eps\\), and any linear combination of \\(X_1, \\dots, X_p\\) should be uncorrelated with the error term.\n\nThere is also a weaker form of exogeneity, called weak exogeneity:\n\nDefinition 6.5 (Weak Exogeneity) Weak exogeneity is defined as \\(\\E(\\b x_t \\eps_t) = 0\\). Weak exogeneity only requires that regressors \\(X_1, \\dots, X_p\\) individually are uncorrelated with the error term. Weak exogeneity allows for combinations of \\(X_1, \\dots, X_p\\) to to be correlated with \\(\\eps\\), which is not allowed under strict exogeneity.\n\n\n\n\nDefinition 6.6 (Spherical Errors) The spherical errors assumption states that the covariance matrix of errors \\(\\eps_t\\) takes the form:\n\\[\n\\V(\\b\\eps|\\b X) = \\sigma^2 \\b I_n = \\begin{pmatrix}\n\\sigma^2 & 0 & 0 & \\dots \\\\\n0 & \\sigma^2 & 0 & \\dots  \\\\\n0 & 0& \\sigma^2 & \\vdots \\\\\n\\vdots & \\vdots & \\dots & \\ddots\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\nVariance-Covariance Matrix of Errors\n\n\n\n\n\nThe variance-covariance matrix of errors \\(\\eps_t\\) takes the form:\n\\[\n\\V(\\b\\eps | \\b X) = \\b\\Omega = \\begin{pmatrix}\n\\V \\eps_1 & Cov(\\eps_1, \\eps_2) & Cov(\\eps_1, \\eps_3) & \\dots \\\\\nCov(\\eps_2, \\eps_1) & \\V \\eps_2 & Cov(\\eps_2, \\eps_3) & \\dots \\\\\nCov(\\eps_3, \\eps_1) & Cov(\\eps_3, \\eps_2) & \\V\\eps_3 & \\vdots \\\\\n\\vdots & \\vdots & \\dots & \\ddots\n\\end{pmatrix}\n\\]\n\n\n\nSpherical errors implies two things: No Autocorrelation means that the covariance of any two error terms \\(Cov(\\eps_i, \\eps_j) = 0\\). This is reflected in spherical errors with all the 0’s in the non-diagonal positions. This assumption is generally met if we meet the i.i.d. assumption.\nHomoscedasticity means that every \\(\\eps_t\\) for any observation \\(t\\) has the same variance \\(\\sigma^2\\). The variance of the error \\(\\eps_t\\) does not depend on the values of \\(X_1, \\dots, X_p\\). If this condition is violated, as in each observation \\(t\\) has their own \\(\\sigma^2_t\\), we call this homoscedasticity.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ls.html#linearity",
    "href": "ls.html#linearity",
    "title": "6  Least Squares Theory",
    "section": "6.2 Linearity",
    "text": "6.2 Linearity",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ls.html#i.i.d.",
    "href": "ls.html#i.i.d.",
    "title": "6  Least Squares Theory",
    "section": "6.3 i.i.d.",
    "text": "6.3 i.i.d.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#classical-linear-model",
    "href": "ols.html#classical-linear-model",
    "title": "4  Least Squares Theory",
    "section": "",
    "text": "Linearityi.i.d.Linear IndependenceExogeneitySpherical Errors\n\n\n\nDefinition 4.1 (Linearity in Parameters) A model is linear in parameters if it can be written in the form \\(\\b y = \\b{X\\beta} + \\b \\eps\\). Or in other words, all parameters \\(\\beta_0, \\dots, \\beta_p\\) must be either added or subtracted from each other.\n\nIt is important to note that linearity in parameters is not equal to linearity in the classical sense. For example, the model\n\\[\nY_t = \\beta_0 + \\beta_1X_t + \\beta_2 X_t^2 + \\beta_3X_t^3 + \\eps_t\n\\]\nwould be considered linear in parameters because all parameters \\(\\beta_0, \\dots, \\beta_p\\) are added together, not multiplied. It does not matter if the \\(X\\) are linear or not.\n\n\n\nDefinition 4.2 (Independent and Identically Distributed) A series of random variables \\(Y_1, Y_2, \\dots, Y_n\\) is considered to be independent and identically distributed if each random variable is independent (definition 1.6), and each random variable has the exact same distribution.\n\nThis assumption is often not met in its purest form - since when we randomly sample \\(Y_t\\) without replacement, technically the next observation \\(Y_{t+1}\\) will have slightly different distribution, since we took one observation out already.\nThis assumption is also frequently violated in time-series data, where \\(Y_t\\) at time \\(t\\) is likely to have an effect on \\(Y_{t+1}\\) at time \\(t+1\\).\n\n\n\nDefinition 4.3 (Linear Independence) Also called non-perfect multicollinearity, this assumption means that no explanatory variables \\(X_1, \\dots, X_p\\) can be perfectly correlated with any other explanatory variable, or perfectly correlated with any linear combination of other explanatory variables.\n\nThis assumption is required for estimation to be possible with the ordinary least squares estimator, as it requires matrix \\(\\b X\\) to be full rank which allows \\(\\b X^\\top \\b X\\) to be invertable.\n\n\n\nDefinition 4.4 (Strict Exogeneity) Strict Exogeneity is \\(\\E(\\b\\eps | \\b X) = 0\\). This implies that \\(\\E(\\b X^\\top \\b\\eps) = 0\\), which means all regressors \\(X_1, \\dots, X_p\\) should be uncorrelated with the error terms \\(\\eps\\), and any linear combination of \\(X_1, \\dots, X_p\\) should be uncorrelated with the error term.\n\nThere is also a weaker form of exogeneity, called weak exogeneity:\n\nDefinition 4.5 (Weak Exogeneity) Weak exogeneity is defined as \\(\\E(\\b x_t \\eps_t) = 0\\). Weak exogeneity only requires that regressors \\(X_1, \\dots, X_p\\) individually are uncorrelated with the error term. Weak exogeneity allows for combinations of \\(X_1, \\dots, X_p\\) to to be correlated with \\(\\eps\\), which is not allowed under strict exogeneity.\n\n\n\n\nDefinition 4.6 (Spherical Errors) The spherical errors assumption states that the covariance matrix of errors \\(\\eps_t\\) takes the form:\n\\[\n\\V(\\b\\eps|\\b X) = \\sigma^2 \\b I_n = \\begin{pmatrix}\n\\sigma^2 & 0 & 0 & \\dots \\\\\n0 & \\sigma^2 & 0 & \\dots  \\\\\n0 & 0& \\sigma^2 & \\vdots \\\\\n\\vdots & \\vdots & \\dots & \\ddots\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\nVariance-Covariance Matrix of Errors\n\n\n\n\n\nThe variance-covariance matrix of errors \\(\\eps_t\\) takes the form:\n\\[\n\\V(\\b\\eps | \\b X) = \\b\\Omega = \\begin{pmatrix}\n\\V \\eps_1 & Cov(\\eps_1, \\eps_2) & Cov(\\eps_1, \\eps_3) & \\dots \\\\\nCov(\\eps_2, \\eps_1) & \\V \\eps_2 & Cov(\\eps_2, \\eps_3) & \\dots \\\\\nCov(\\eps_3, \\eps_1) & Cov(\\eps_3, \\eps_2) & \\V\\eps_3 & \\vdots \\\\\n\\vdots & \\vdots & \\dots & \\ddots\n\\end{pmatrix}\n\\]\n\n\n\nSpherical errors implies two things:\n\nNo Autocorrelation means that the covariance of any two error terms \\(Cov(\\eps_i, \\eps_j) = 0\\). This is reflected in spherical errors with all the 0’s in the non-diagonal positions.\nHomoscedasticity means that every \\(\\eps_t\\) for any observation \\(t\\) has the same variance \\(\\sigma^2\\). The variance of the error \\(\\eps_t\\) does not depend on the values of \\(X_1, \\dots, X_p\\). If this condition is violated, as in each observation \\(t\\) has their own \\(\\sigma^2_t\\), we call this homoscedasticity.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#regression-anatomy-1",
    "href": "ols.html#regression-anatomy-1",
    "title": "5  Least Squares Theory",
    "section": "5.3 Regression Anatomy",
    "text": "5.3 Regression Anatomy\nSo we now know how to derive OLS estimates \\(\\hat{\\b \\beta}\\), and we know the projection interpretation of OLS. But what does \\(\\b{\\hat\\beta}\\) actually mean? What does estimating these parameters tell us about the data generating process and the real-world? To understand the parameter estimates of OLS, we want to study the partitioned regression model.\n\nDefinition 5.12 (Partitioned Regression Model) A partitioned regression model is when we split up our matrices/vectors in our classical linear model \\(\\b y = \\b{X\\beta} + \\b\\eps\\).\nLet us say we only care about some of the explanatory variables in our data generating process. We can split matrix \\(\\b X\\) into two: \\(\\b X_1\\) contains the explanatory variables we care about, and \\(\\b X_2\\) contains the other explanatory variables. Similarly, we divide \\(\\b\\beta\\) into \\(\\b\\beta_1\\) and \\(\\b\\beta_2\\) in the same way. We can write our new regression model as:\n\\[\n\\b y = \\b X_1 \\b\\beta_1 + \\b X_2 \\b\\beta_2 + \\b\\eps\n\\]\n\n\nRecall the residual maker matrix \\(\\b M\\) (definition 5.10), and recall how \\(\\b M\\) is orthogonal to \\(\\b X\\), meaning \\(\\b{MX} = 0\\).\nNow, let us consider only the residual maker matrix of the second part of our partitioned model \\(\\b M_2\\), which means \\(\\b M_2 \\b X_2 = 0\\). Let us take our partitioned regression model, and pre-multiply \\(\\b M_2\\) to both sides:\n\\[\n\\b M_2 \\b y = \\b M_2(\\b X_1 \\b\\beta_1 + \\b X_2 \\b\\beta_2 + \\b\\eps)\n\\]\nNow, let us distribute out to get\n\\[\n\\b M_2 \\b y = \\b M_2 \\b X_1 \\b\\beta_1 + \\b M_2 \\b X_2 \\b\\beta_2 + \\b M_2 \\b\\eps\n\\]\nNow recall that \\(\\b M_2 \\b X_2 = 0\\). That means we can simplify the above to\n\\[\n\\b M_2 \\b y = \\b M_2 \\b X_1 \\b\\beta_1  + \\b M_2 \\b\\eps\n\\]\nNow, let us define \\(\\tilde{\\b y} := \\b M_2 \\b y\\), \\(\\tilde{\\b X_1} := \\b M_2 \\b X_1\\), and error \\(\\tilde{\\b\\eps} = \\b M_2 \\b\\eps\\). We can rewrite as\n\\[\n\\tilde{\\b y} = \\tilde{\\b X_1}\\b\\beta_1 + \\tilde{\\b \\eps}\n\\]\nRemember, since we multiplied \\(\\b M_2\\) to both sides, this above model is equivalent to that of our partitioned model and our original regression model. Using definition 5.8, we know the OLS estimate of \\(\\hat{\\b \\beta_1}\\) is:\n\\[\n\\hat{\\b\\beta}_1 = (\\tilde{\\b X_1^\\top} \\tilde{\\b X_1}) \\tilde{\\b X_1^\\top} \\tilde{\\b y}\n\\tag{5.2}\\]\nThis OLS estimate of \\(\\hat{\\b\\beta}_1\\) is the same as if we had calculated our OLS estimates normally without partitioning the model.\nWhat does this tell us? Notice how we have \\(\\tilde{\\b X_1} := \\b M_2 \\b X_1\\) in our formula. Well, we know \\(\\b M_2 \\b X_2 = 0\\). That means that any part of \\(\\b X_1\\) that was correlated to \\(\\b X_2\\) became 0, after it was multiplied by \\(\\b M_2\\). Thus, \\(\\tilde{\\b X_1}\\) is the part of \\(\\b X_1\\) that is uncorrelated with \\(\\b X_2\\). We see that our OLS estimates are calculated in respect to \\(\\tilde{\\b X_1}\\).\n\nTheorem 5.1 (Regression Anatomy Theorem) Our individual parameter estimates \\(\\hat\\beta_j \\in \\{\\hat\\beta_1, \\dots, \\hat\\beta_p \\}\\) are the relationship between \\(Y\\) and the part of \\(X_j \\in \\{X_1, \\dots, X_p\\}\\) uncorrelated with the other explanatory variables.\n\n\nEssentially we are partialling out the effect of other variables in our coefficient estimates. This is why we you will hear people “control” for other variables in linear models.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#properties-of-the-ols-estimator",
    "href": "ols.html#properties-of-the-ols-estimator",
    "title": "5  Least Squares Theory",
    "section": "5.3 Properties of the OLS Estimator",
    "text": "5.3 Properties of the OLS Estimator\nWe know that an estimator has two finite sample properties: unbiasedness (definition 2.4) and variance (definition 2.5). Let us focus on unbiasedness for now.\n\nTheorem 5.2 (OLS is an Unbiased Estimator) The Ordinary Least Squares estimator is an unbiased estimator under the assumptions of linearity, i.i.d., no perfect multicollinearity, and strict exogeneity.\n\\[\n\\E \\hat{\\b\\beta} = \\b\\beta\n\\]\nNote that the assumption of spherical errors is not needed for the unbiasedness of OLS.\n\n\nProof: Let us start with our OLS solution (definition 5.8), and plug in our original model \\(\\b y = \\b{X\\beta} + \\b\\eps\\) into where \\(\\b y\\) is in the OLS solution:\n\\[\n\\begin{align}\n\\hat{\\b\\beta} & = (\\b{X^\\top X})^{-1} \\b{X^\\top y} \\\\\n& = (\\b{X^\\top X})^{-1} \\b X^\\top (\\b{X\\beta} + \\b\\eps) \\\\\n& = \\underbrace{(\\b{X^\\top X})^{-1}\\b{X^\\top X}}_{\\text{inverses cancel}}\\b\\beta + (\\b{X^\\top X})^{-1} \\b{X^\\top \\eps} \\\\\n& = \\b\\beta + (\\b{X^\\top X})^{-1}\\b{X^\\top \\eps}\n\\end{align}\n\\tag{5.3}\\]\nNow, let us take the expectation of \\(\\hat{\\b\\beta}\\) conditional on \\(\\b X\\) (remember that \\(\\b X\\) and \\(\\b \\beta\\) are fixed constants, so they are not affected by the expectation). We can use the strict exogeneity assumption (definition 5.4) to simplify:\n\\[\n\\E(\\hat{\\b\\beta}|\\b X) = \\b\\beta + (\\b{X^\\top X})^{-1} \\underbrace{\\E (\\b \\eps | \\b X)}_{= \\ 0} = \\b\\beta\n\\]\nNow, we know \\(\\E(\\hat{\\b\\beta} | \\b X)\\). We can deduce \\(\\E(\\hat{\\b\\beta})\\) using the law of iterated expectations (theorem 1.3), and plugging in \\(\\E(\\hat{\\b\\beta} | \\b X) = \\b\\beta\\):\n\\[\n\\E(\\hat{\\b\\beta}) = \\E[\\E(\\hat{\\b\\beta}|\\b X)] = \\E[\\b\\beta] = \\b\\beta\n\\]\nThe final step is because the expectation of a constant \\(\\b\\beta\\) (the fixed true population value) is the constant itself. Thus, we have proven OLS is an unbiased.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#ols-estimator-properties",
    "href": "ols.html#ols-estimator-properties",
    "title": "4  Least Squares Theory",
    "section": "4.3 OLS Estimator Properties",
    "text": "4.3 OLS Estimator Properties\nWe know that an estimator has two finite sample properties: unbiasedness (definition 2.4) and variance (definition 2.5), and has the asymptotic property of consistency (definition 2.6). Before we explore these properties, let us first transform our OLS estimates (definition 4.8) into a form more useful for showing these properties of OLS.\nLet us start with our OLS solution (definition 4.8), and plug in our original model \\(\\b y = \\b{X\\beta} + \\b\\eps\\) into where \\(\\b y\\) is in the OLS solution:\n\\[\n\\begin{align}\n\\hat{\\b\\beta} & = (\\b{X^\\top X})^{-1} \\b{X^\\top y} \\\\\n& = (\\b{X^\\top X})^{-1} \\b X^\\top (\\b{X\\beta} + \\b\\eps) \\\\\n& = \\underbrace{(\\b{X^\\top X})^{-1}\\b{X^\\top X}}_{\\text{inverses cancel}}\\b\\beta + (\\b{X^\\top X})^{-1} \\b{X^\\top \\eps} \\\\\n& = \\b\\beta + (\\b{X^\\top X})^{-1}\\b{X^\\top \\eps}\n\\end{align}\n\\tag{4.2}\\]\nNow, we are ready to explore the properties of the ordinary least squares estimator: unbiasedness, variance, and consistency.\n\nUnbiasednessVarianceConsistency\n\n\n\nTheorem 4.2 (OLS is an Unbiased Estimator) The Ordinary Least Squares estimator is an unbiased estimator under the assumptions of linearity, i.i.d., linear independence, and strict exogeneity.\n\\[\n\\E \\hat{\\b\\beta} = \\b\\beta\n\\]\nNote that the assumption of spherical errors is not needed for the unbiasedness of OLS.\n\n\nProof: Let us take the expectation of \\(\\hat{\\b\\beta}\\) (from eq. 4.2) conditional on \\(\\b X\\) (remember that \\(\\b X\\) and \\(\\b \\beta\\) are fixed constants, so they are not affected by the expectation). We can use the strict exogeneity assumption (definition 4.4) to simplify:\n\\[\n\\E(\\hat{\\b\\beta}|\\b X) = \\b\\beta + (\\b{X^\\top X})^{-1} \\underbrace{\\E (\\b \\eps | \\b X)}_{= \\ 0} = \\b\\beta\n\\]\nNow, we know \\(\\E(\\hat{\\b\\beta} | \\b X)\\). We can deduce \\(\\E(\\hat{\\b\\beta})\\) using the law of iterated expectations (theorem 1.3), and plugging in \\(\\E(\\hat{\\b\\beta} | \\b X) = \\b\\beta\\):\n\\[\n\\E(\\hat{\\b\\beta}) = \\E[\\E(\\hat{\\b\\beta}|\\b X)] = \\E[\\b\\beta] = \\b\\beta\n\\]\nThe final step is because the expectation of a constant \\(\\b\\beta\\) (the fixed true population value) is the constant itself. Thus, we have proven OLS is an unbiased.\n\n\n\nTheorem 4.3 (Variance of the OLS Estimator) The variance of the OLS estimator under the assumptions of linearity, i.i.d., linear independence, strict exogeneity, and spherical errors, is given by\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = \\sigma^2 (\\b{X^\\top X})^{-1}\n\\]\n\n\nProof: Let us start where we left off from eq. 4.2. This tells us the variance of the OLS estimator is:\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = \\V(\\b\\beta + (\\b{X^\\top X})^{-1} \\b{X^\\top \\eps})\n\\]\nWe know that \\(\\b\\beta\\) is a vector of fixed true population values. \\((\\b{X^\\top X})^{-1} \\b X^\\top\\) can also be considered a fixed constant matrix because we are conditioning our variance on \\(\\b X\\). Thus, we can use theorem 1.2 to rewrite the above as\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = (\\b{X^\\top X})^{-1}\\b X^\\top \\V(\\b\\eps| \\b X)[(\\b{X^\\top X})^{-1}\\b X^\\top]^{-1}\n\\]\nWith the properties of matrix inverses and transposes, we can determine that \\([(\\b{X^\\top X})^{-1}\\b X^\\top]^{-1}\\) is equivalent to \\(\\b X(\\b{X^\\top X})^{-1}\\). Thus, plugging this in, we get\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = (\\b{X^\\top X})^{-1}\\b X^\\top \\V(\\b\\eps| \\b X) \\b X(\\b{X^\\top X})^{-1}\n\\]\nNow, according to the assumption of spherical errors (definition 4.6), we know that \\(\\V(\\b \\eps| \\b X) = \\sigma^2 \\b I_n\\). Thus, let us plug that into our equation to get\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = (\\b{X^\\top X})^{-1}\\b X^\\top \\sigma^2 \\b I_n \\b X(\\b{X^\\top X})^{-1}\n\\]\nSince \\(\\b I_n\\) is the identity matrix, it cancels out. For notation simplicity, we can move the scalar \\(\\sigma^2\\) to the front, and simplify:\n\\[\n\\begin{align}\n\\V(\\hat{\\b\\beta}|\\b X) & = \\sigma^2 \\underbrace{(\\b{X^\\top X})^{-1}\\b X^\\top \\b X}_{\\text{inverses cancel}}(\\b{X^\\top X})^{-1} \\\\\n& = \\sigma^2 (\\b{X^\\top X})^{-1}\n\\end{align}\n\\]\nThus, we have proved that the variance of the OLS estimator is as the theorem above.\n\n\n\nTheorem 4.4 (Consistency of OLS) The Ordinary Least Squares estimator is asymptotically consistent (definition 2.6), under the assumptions of linearity, i.i.d., linear independence, weak exogeneity, and spherical errors. Mathematically:\n\\[\n\\mathrm{plim}\\hat{\\b\\beta} = \\b\\beta\n\\]\n\n\nProof: Let us start off from eq. 4.2. We can rewrite our matrix notation in the form of vectors, scalars, and summation:\n\\[\n\\begin{align}\n\\hat{\\b\\beta} & = \\b\\beta + (\\b{X^\\top X})^{-1}\\b{X^\\top\\eps} \\\\\n& = \\b\\beta + \\left(\\sum\\limits_{t=1}^n \\b x_t \\b x_t^\\top\\right)^{-1} \\left(\\sum\\limits_{t=1}^n\\b x_t \\eps_t \\right)\n\\end{align}\n\\]\nNow, let us do a little algebra trick as follows:\n\\[\n\\hat{\\b\\beta} = \\b\\beta + \\left(\\frac{1}{n}\\sum\\limits_{t=1}^n \\b x_t \\b x_t^\\top\\right)^{-1} \\left(\\frac{1}{n}\\sum\\limits_{t=1}^n\\b x_t \\eps_t \\right)\n\\]\nThe reason we can do this is because the first \\(\\frac{1}{n}\\) is inversed as \\(\\frac{1}{n}^{-1}\\), so this cancels out the second one, maintaining the equality of our equation.\nNow, we want to prove \\(\\mathrm{plim}\\hat{\\b\\beta} =\\b\\beta\\), so let us take the probability limit of both sides.\n\\[\n\\mathrm{plim}\\hat{\\b\\beta} = \\mathrm{plim} \\b\\beta + \\left( \\mathrm{plim} \\frac{1}{n}\\sum\\limits_{t=1}^n \\b x_t \\b x_t^\\top \\right)^{-1} \\left( \\mathrm{plim}\\frac{1}{n}\\sum\\limits_{t=1}^n\\b x_t \\eps_t \\right)\n\\]\nWe know that the probability limit of a constant is itself, so \\(\\mathrm{plim} \\b\\beta = \\b\\beta\\), since \\(\\b\\beta\\) is a constant of true population parameters. Look at the other two terms on the right. They take the form of sample averages \\(\\frac{1}{n}\\sum\\). Using the law of of large numbers (theorem 2.1), we can simplify to:\n\\[\n\\mathrm{plim}\\hat{\\b\\beta} = \\b\\beta + (\\E(\\b x_t \\b x_t^\\top))^{-1} \\underbrace{\\E(\\b x_t \\eps_t)}_{=0} = \\b\\beta\n\\]\nAnd we know \\(\\E(\\b x_t \\eps_t) = 0\\) because of the condition of weak exogeneity (definition 4.5). Thus, we have proved that OLS is asymptotically consistent.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#true-variance-with-non-spherical-errors",
    "href": "ols.html#true-variance-with-non-spherical-errors",
    "title": "5  Least Squares Theory",
    "section": "5.5 True Variance with Non-Spherical Errors",
    "text": "5.5 True Variance with Non-Spherical Errors",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#ols-and-non-spherical-errors",
    "href": "ols.html#ols-and-non-spherical-errors",
    "title": "4  Least Squares Theory",
    "section": "4.5 OLS and Non-Spherical Errors",
    "text": "4.5 OLS and Non-Spherical Errors\nFor the classical linear model, one of the assumptions was spherical errors (definition 4.6). This was an assumption made on the variance-covariance matrix of error term \\(\\eps_t\\).\nThe spherical errors assumption can thus be violated in two ways. First, is conditional heteroscedasticity, where only homoscedasticity is violated, but no autocorrleation still holds. The variance-covariance matrix of errors will take the form:\n\\[\n\\V(\\b\\eps|\\b X) = \\b\\Omega= \\begin{pmatrix}\n\\sigma^2_1 & 0 & 0 & \\dots \\\\\n0 & \\sigma^2_2 & 0 & \\dots  \\\\\n0 & 0& \\sigma^2_3 & \\vdots \\\\\n\\vdots & \\vdots & \\dots & \\ddots\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\nIdentifying Heteroscedasticity\n\n\n\n\n\n\n\n\n\n\nThe above is a residual plot of OLS residuals \\(\\hat\\eps_i\\) against some explanatory variable \\(X\\). Notice how for homoscedasticity, the variance of the error terms (how spread out they are up-down wise) is constant for any value of \\(X\\).\nFor heteroscedasticity, we can clearly see that the residual variance is smaller for some \\(X\\) values, and larger for other \\(X\\) values. If you see a pattern in your residual plot, it is likely heteroscedasticity.\n\n\n\nThe second way spherical errors can be violated is with autocorrelation, where both of the assumptions of spherical errors are violated.\nWhat is the impact of violating spherical errors?\n\nOLS estimates remain unbiased. This is because our OLS unbiasedness proof (theorem 4.2) does not depend on the spherical errors assumption.\nOur derived OLS variance is incorrect. This is because our variance formula (theorem 4.3) depends on the spherical errors assumption.\nOLS is no longer the best linear unbiased estimator - the linear unbiased estimator with the lowest variance. This is because the Gauss-Markov Theorem (theorem 4.5) depends on the spherical errors assumption. Thus, there are other linear unbiased estimators with lower variance.\n\nSince OLS remains unbiased, we can still use OLS as an estimator. We just have to correct our OLS variance calculations to account for the fact that spherical errors is not met. There are three different variance formulas used for different forms of non-spherical errors.\n\nHeteroscedasticityClusteredHeteroscedasticity + Autocorrelation\n\n\n\nTheorem 4.6 (Heteroscedasticity Variance of OLS) The variance of the OLS estimator under heteroscedasticity is given by\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = (\\b{X^\\top X})^{-1}\\b X^\\top \\begin{pmatrix}\n\\sigma^2_1 & 0 & 0 & \\dots \\\\\n0 & \\sigma^2_2 & 0 & \\dots  \\\\\n0 & 0& \\sigma^2_3 & \\vdots \\\\\n\\vdots & \\vdots & \\dots & \\ddots\n\\end{pmatrix} \\b X(\\b{X^\\top X})^{-1}\n\\]\n\n\nProof: Let us start where we left off from eq. 4.2 during the proof of OLS properties. This tells us the variance of the OLS estimator is:\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = \\V(\\b\\beta + (\\b{X^\\top X})^{-1} \\b{X^\\top \\eps})\n\\]\nWe know that \\(\\b\\beta\\) is a vector of fixed true population values. \\((\\b{X^\\top X})^{-1} \\b X^\\top\\) can also be considered a fixed constant matrix because we are conditioning our variance on \\(\\b X\\). Thus, we can use theorem 1.2 to rewrite the above as\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = (\\b{X^\\top X})^{-1}\\b X^\\top \\V(\\b\\eps| \\b X)[(\\b{X^\\top X})^{-1}\\b X^\\top]^{-1}\n\\]\nWith the properties of matrix inverses and transposes, we can determine that \\([(\\b{X^\\top X})^{-1}\\b X^\\top]^{-1}\\) is equivalent to \\(\\b X(\\b{X^\\top X})^{-1}\\). Thus, plugging this in, we get\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = (\\b{X^\\top X})^{-1}\\b X^\\top \\V(\\b\\eps| \\b X) \\b X(\\b{X^\\top X})^{-1}\n\\]\nNow, we can replace \\(\\V(\\b\\eps|\\b X)\\) with the heteroscedasticity variance matrix of errors:\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = (\\b{X^\\top X})^{-1}\\b X^\\top \\begin{pmatrix}\n\\sigma^2_1 & 0 & 0 & \\dots \\\\\n0 & \\sigma^2_2 & 0 & \\dots  \\\\\n0 & 0& \\sigma^2_3 & \\vdots \\\\\n\\vdots & \\vdots & \\dots & \\ddots\n\\end{pmatrix} \\b X(\\b{X^\\top X})^{-1}\n\\]\nAnd thus, we have proven the OLS variance under heteroscedasticity. When we actually write the formula, we will typically replace the matrix with \\(\\b\\Omega\\).\n\n\nClustered standard errors are when you have done clustered sampling for your observations. For example, you randomly sample 100 people from 100 different villages.\nThe errors of observations belonging to the same cluster (say village) might exhibit correlation, while errors of observtations from distinct clusters are assumed to be uncorrelated.\n\nDefinition 4.12 (Clustered Sampling Variance of OLS) The variance of the OLS estimator under clustered sampling is given by\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = (\\b{X^\\top X})^{-1}\\b X^\\top \\begin{pmatrix}\n\\b\\Sigma_1 & 0 & \\dots & 0 \\\\\n0 & \\b\\Sigma_2 & & 0 \\\\\n\\vdots & 0 & \\ddots & 0 \\\\\n0 & 0& \\dots & \\b\\Sigma_G\n\\end{pmatrix} \\b X(\\b{X^\\top X})^{-1}\n\\]\nWhere \\(\\b\\Sigma_1, \\dots \\b\\Sigma_G\\) are intracluster covariance-variance error matrices, that exhibit autocorrelation within each cluster, but the different clusters are not autocorrelated.\n\n\nProof: Let us start off by taking the same steps as the heteroscedasticity proof, until we got to this point:\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = (\\b{X^\\top X})^{-1}\\b X^\\top \\V(\\b\\eps| \\b X) \\b X(\\b{X^\\top X})^{-1}\n\\]\nNow, we can replace \\(\\V(\\b\\eps|\\b X)\\) with the clustered sampling variance matrix of errors:\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = (\\b{X^\\top X})^{-1}\\b X^\\top \\begin{pmatrix}\n\\b\\Sigma_1 & 0 & \\dots & 0 \\\\\n0 & \\b\\Sigma_2 & & 0 \\\\\n\\vdots & 0 & \\ddots & 0 \\\\\n0 & 0& \\dots & \\b\\Sigma_G\n\\end{pmatrix} \\b X(\\b{X^\\top X})^{-1}\n\\]\nAnd thus, we have proven the OLS variance under heteroscedasticity. When we actually write the formula, we will typically replace the matrix with \\(\\b\\Omega\\).\n\n\nThis type of standard errors is quite complicated, so we usually trust the computer to compute it for us.\n\nDefinition 4.13 (HAC Variance of OLS) Heteroscedasticity and Autocorrelation (HAC) variance is given by:\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = (\\b{X^\\top X})^{-1}\\b X^\\top \\b\\Omega \\b X(\\b{X^\\top X})^{-1}\n\\]\nWhere \\(\\b\\Omega\\) is the variance-covariance matrix of the error term \\(\\eps_t\\).\n\nObviously, we probably do not know the form of \\(\\b\\Omega\\) ins our population. We can estimate \\(\\b X^\\top \\b\\Omega \\b X\\) with the Newey-West Estimator:\n\\[\n\\b X^\\top \\b\\Omega \\b X = \\frac{1}{n}\\sum\\limits_{t=1}^n \\eps_t^2 \\b x_t \\b x_t^\\top + \\frac{1}{n}\\sum\\limits_{\\ell = 1}^L \\sum\\limits_{t = \\ell + 1}^n w_\\ell\\eps_t\\eps_{t-\\ell}(\\b x_t \\b x_{t-\\ell}^\\top + \\b x_{t-\\ell} \\b x_t^\\top)\n\\]\nWhere \\(w_\\ell\\) is given by:\n\\[\nw_\\ell = 1 - \\frac{\\ell}{L + 1}\n\\]\nWhere \\(L\\) specifies the maximum lag considered for the control of autocorrelation (so how many previous residuals from previous time periods are correlated with \\(\\eps_t\\)). A common choice for \\(L\\) is \\(n^{1/4}\\).\nWe will not prove this, as it is a very complicated estimator.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#heteroscedasticity-autocorrelation",
    "href": "ols.html#heteroscedasticity-autocorrelation",
    "title": "5  Least Squares Theory",
    "section": "5.6 Heteroscedasticity + Autocorrelation",
    "text": "5.6 Heteroscedasticity + Autocorrelation\n:::",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#generalised-least-squares",
    "href": "ols.html#generalised-least-squares",
    "title": "4  Least Squares Theory",
    "section": "4.6 Generalised Least Squares",
    "text": "4.6 Generalised Least Squares\nWe mentioned that if spherical errors (definition 4.6) is violated, OLS is no longer the linear unbiased estimator with the least variance. Instead, another estimator, the Generalised Least Squares estimator, is the best linear unbiased estimator.\nIn the generalised least squares estimator, we assume that the variance-covariance is\n\\[\n\\V(\\b\\eps | \\b X) = \\E(\\b{\\eps\\eps^\\top}) = \\sigma^2 \\b\\Omega\n\\tag{4.4}\\]\nWhere \\(\\sigma^2\\) is an unknown scalar constant, but \\(\\b\\Omega\\) is a known matrix that is equivalent to the population variance-covariance matrix of errors. The variance is equivalent to \\(\\E(\\b{\\eps\\eps^\\top})\\) because we assume by strict exogeneity (definition 4.4) that \\(\\E(\\b\\eps) = 0\\).\nLet us define a matrix \\(\\b\\Omega^{-1/2}\\), which will be the inverse of the square root of \\(\\b\\Omega\\). This means that the following should be true:\n\\[\n\\b\\Omega^{-1/2} \\ \\b\\Omega \\ {\\b\\Omega^{-1/2}}^\\top = \\b I\n\\tag{4.5}\\]\nWe multipy \\(\\b\\Omega^{-1/2}\\) to all terms of model \\(\\b y = \\b{X\\beta} + \\b\\eps\\) to get a transformed model\n\\[\n\\underbrace{\\b\\Omega^{-1/2}}_{\\b y^*} \\b y = \\underbrace{\\b\\Omega^{-1/2} \\b X}_{\\b X^*} \\b \\beta + \\underbrace{\\b\\Omega^{-1/2} \\b \\eps}_{\\b \\eps^*}\n\\tag{4.6}\\]\nThis transformed model meets spherical errors, which we can prove by plugging in the definition of \\(\\b\\eps^*\\) from above, and the definition of \\(\\E(\\b{\\eps\\eps^\\top})\\) from eq. 4.4:\n\\[\n\\begin{align}\n\\V (\\b\\eps^* | \\b X) & = \\E(\\b\\eps^* \\b\\eps^{*\\top}) \\\\\n& = \\E(\\b\\Omega^{-1/2} \\b \\eps \\b\\eps^\\top {\\b\\Omega^{-1/2}}^\\top) \\\\\n& = \\b\\Omega^{-1/2} \\E(\\b{\\eps \\eps^\\top}) \\b\\Omega^{-1/2} \\\\\n& = \\b\\Omega^{-1/2} \\sigma^2 \\b\\Omega \\b\\Omega^{-1/2}\n\\end{align}\n\\]\nAnd by moving scalar \\(\\sigma^2\\) to the front, and using the property from eq. 4.5, we get:\n\\[\n\\V (\\b\\eps^* | \\b X) = \\sigma^2 \\underbrace{\\b\\Omega^{-1/2} \\b\\Omega \\b\\Omega^{-1/2}}_{\\b I} = \\sigma^2 \\b I\n\\]\nThus proving this transformed model meets the spherical errors assumption (definition 4.6). Thus, we can use OLS on this transformed model, and it will be the best linear unbiased estimator. Our OLS estimator (definition 4.8) of the transformed model will be:\n\\[\n\\hat{\\b\\beta} = (\\b X^{*\\top} \\b X^*)^{-1} \\b X^{*\\top} \\b y^*\n\\]\nAnd if we plug in our definitions of \\(\\b y^*\\), and \\(\\b X^*\\) from eq. 4.6, we can get\n\\[\n\\hat{\\b\\beta} = \\left[(\\b\\Omega^{-1/2} \\b X)^\\top (\\b\\Omega^{-1/2} \\b X) \\right]^{-1} (\\b\\Omega^{-1/2} \\b X) (\\b\\Omega^{-1/2} \\b y)\n\\]\nAnd using the properties of matrix transposes, and that \\(\\b\\Omega^{-1/2} \\b\\Omega^{-1/2} = \\b\\Omega^{-1}\\), we can get\n\\[\n\\begin{align}\n\\hat{\\b\\beta} & = [\\b X^\\top \\b\\Omega^{-1/2} \\b\\Omega^{-1/2} \\b X]^{-1} \\b X^\\top \\b\\Omega^{-1/2} \\b\\Omega^{-1/2} \\b y \\\\\n& = (\\b X^\\top \\b\\Omega^{-1} \\b X)^{-1} \\b X^\\top \\b\\Omega^{-1} \\b y\n\\end{align}\n\\]\n\nDefinition 4.14 (Generalised Least Squares Estimator) The GLS estimator is\n\\[\n\\hat{\\b\\beta} = (\\b X^\\top \\b\\Omega^{-1} \\b X)^{-1} \\b X^\\top \\b\\Omega^{-1} \\b y\n\\]\nWhere \\(\\b\\Omega\\) is the population variance-covariance matrix of errors. The variance is\n\\[\n\\V\\hat{\\b\\beta} = (\\b X^\\top \\b\\Omega^{-1} \\b X)^{-1}\n\\]\n\n\nThe obvious issue is that we do not generally know the form of \\(\\b\\Omega\\). This means the theoretical GLS estimator is often not feasible. Instead, we will use another estimator, called the Feasible Generalised Least Squares (FGLS) estimator.\nThe only times when GLS is feasible is when we are confident we have a specific form of autocorrelation (such as AR(1), MA(1), which we will cover in the stochastic processes chapter). This is because the covariance-variance matrix is known for these processes, so we can directly use them in GLS.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#feasible-generalised-least-squares",
    "href": "ols.html#feasible-generalised-least-squares",
    "title": "4  Least Squares Theory",
    "section": "4.7 Feasible Generalised Least Squares",
    "text": "4.7 Feasible Generalised Least Squares\nThe issue with GLS is that we do not know the form of \\(\\b\\Omega\\). Thus, the feasible generalised least squares estimator estimates \\(\\hat{\\b\\Omega}\\), before estimating the GLS estimator.\nThere are a few ways we can go about doings this, including the Cochrane-Orcutt Estimator, the Weighted Least Squares estimator, and the 2-stage GLS estimator:\n\nCochrane-OrcuttWeighted Least Squares2-Stage FGLS\n\n\nThe Cochrane-Orcutt Estimator is a way to estimate GLS assuming autocorrelation of a specific form (Autoregressive 1). Let us start off with a linear model:\n\\[\nY_t = \\beta_0 + \\beta_1 X_{t}  + \\eps_t\n\\]\nLet us say some autocorrelation is present - which means the error term \\(\\eps_t\\) is related to some other error term of another observation. More specifically, let us assume an autoregressive order 1 autocorrelation, which means that \\(\\eps_t\\) is correlated with the error term of the time period before, \\(\\eps_{t-1}\\). We can model this as:\n\\[\n\\eps_t = \\rho \\eps_{t-1} + u_t\n\\]\nWhere \\(\\rho\\) is the coefficient describing the correlation between \\(\\eps_t\\) and \\(\\eps_{t-1}\\), and \\(u_t\\) is the error term of this smaller model that is the part of \\(\\eps_t\\) that is not explained by \\(\\eps_{t-1}\\).\nThus, our true linear model is:\n\\[\nY_t = \\beta_0 + \\beta_1 X_{t} + \\rho \\eps_{t-1} + u_t\n\\]\nIf we could get the \\(\\rho\\eps_{t-1}\\) term out of this equation, we will no longer have any autocorrelation, since \\(u_t\\) is not correlated/explained by past error terms.\nConsider the linear model for \\(Y_{t-1}\\):\n\\[\nY_{t-1} = \\beta_0 + \\beta_1 X_{t-1}+ \\eps_{t-1}\n\\]\nNow, let us multiply both sides (every term) with parameter \\(\\rho\\):\n\\[\n\\rho Y_{t-1} = \\rho\\beta_0 + \\rho\\beta_1 X_{t-1} + \\rho\\eps_{t-1}\n\\]\nNow, let us subtract our model for \\(\\rho Y_{t-1}\\) from our original \\(Y_t\\):\n\\[\n\\begin{array}{ccccccc}\nY_t & = & \\beta_0 & + & \\beta_1X_t & + & \\rho\\eps_{t-1} + u_t \\\\\n\\rho Y_{t-1} & = & \\rho\\beta_0 & + & \\rho\\beta_1X_{t-1} & + & \\rho\\eps_{t-1} \\\\\n\\hline\nY_t - \\rho Y_{t-1} & = & \\beta_0(1-\\rho) & + & \\beta_1(X_t - \\rho X_{t-1}) & + & u_t\n\\end{array}\n\\]\nNow we can see we have a new transformed model with only error term \\(u_t\\) which is not autocorrelated with \\(t-1\\).\n\\[\n\\underbrace{Y_t - \\rho Y_{t-1}}_{Y_t^*} = \\underbrace{\\beta_0(1-\\rho)}_{\\beta_0^*} + \\beta_1 \\underbrace{(X_t - \\rho X_{t-1})}_{X_t^*} + \\underbrace{u_t}_{\\eps_t^*}\n\\]\nWhich we can rewrite more simply as:\n\\[\nY_t^* = \\beta_0^* + \\beta_1 X_t^* + \\eps_t^*\n\\]\nSince this model no longer has autocorrelation and now meets spherical errors, we can use the OLS estimator on this transformed model, and this will be the best linear unbiased estimator.\n\n\nThe weighted least squares (WLS) estimator is a FGLS when we have only conditional heteroscedasticity, and we believe that conditional heteroscedasticity to be in a specific form:\n\\[\n\\V(\\eps_t|\\b X) = \\sigma^2 \\  \\Omega(X_t)\n\\tag{4.7}\\]\nWhere \\(\\sigma^2\\) is some constant (can be 1) and \\(\\Omega(X_t)\\) is some function of \\(X_t\\) that explained the difference in error variance between individuals.\nNow, consider the variance of this (modified) error term that is the original error \\(\\eps_t\\) divided by the square root \\(\\Omega(X_t)\\), which is a function of \\(X_t\\):\n\\[\n\\V \\left( \\frac{1}{\\sqrt{\\Omega(X_t)}}\\eps_t \\biggr| X_t \\right)\n\\]\nUsing theorem 1.2, we know \\(\\V(cu) = c^2 \\V(u)\\) if \\(c\\) is a constant and \\(u\\) is a random variable. This function also applies to a function \\(a(x)\\) where \\(\\V(a(x) u) = a(x)^2 \\V(u)\\). Using this, we can determine the variance of the modified error term is equal to\n\\[\n\\V \\left( \\frac{1}{\\sqrt{\\Omega(X_t)}}\\eps_t \\biggr| X_t \\right) = \\left(\\frac{1}{\\sqrt{\\Omega(X_t)}}\\right)^2 \\V(\\eps_t | X_t)\n\\]\nAnd from eq. 4.7, we can plug in \\(\\V(\\eps_t|X_t) = \\sigma^2 \\ \\Omega(X_t)\\) to get\n\\[\n\\V \\left( \\frac{1}{\\sqrt{\\Omega(X_t)}}\\eps_t \\biggr| X_t \\right) = \\frac{1}{\\Omega(X_t)}\\sigma^2 \\ \\Omega(X_t) = \\sigma^2\n\\]\nWhat does this tell us? Well it tells us the modified error term \\(\\frac{1}{\\sqrt{\\Omega(X_i)}} \\eps_t\\) has a variance of constant \\(\\sigma^2\\) for all units \\(i\\), which does not dependent on \\(X_i\\). What does this mean? Well, our modified error term is now meeting homoscedasticity (definition 4.6)!\nHowever, we obviously cannot just divide the error term by \\(1/\\sqrt{\\Omega(X_t)}\\) - that changes our linear model. What we can though do is divide every term of our linear model:\n\\[\n\\frac{Y_t}{\\sqrt{\\Omega(X_t)}} = \\beta_0\\left(\\frac{1}{\\sqrt{\\Omega(X_t)}}\\right) + \\beta_1 \\left(\\frac{X_{t1}}{\\sqrt{\\Omega(X_t)}}\\right) + \\dots +  \\frac{\\eps_t}{\\sqrt{\\Omega(X_t)}}\n\\]\nAnd since we divide both side by \\(1/\\sqrt{\\Omega(X_t)}\\), and our model is conditional on individual \\(t\\) (see all the subscripts), that means this model is still “equivalent” to our original linear model.\nThus, the idea of weighted least squares is to “transform” our heteroscedastic linear model into one that meets homoscedasitcity. We can then just use OLS on our new homoscedastic regression, and since homoscedsaticity is met, Gauss-Markov (theorem 4.5) is met, and our estimator is once again the best linear unbiased estimator.\n\n\nThe previous two methods assumed we know some information about the variance-covariance errors matrix (is there only autocorrelation, or only heteroscedasticity, etc.).\nIf we do not konw anything, we can still do feasible GLS with a 2-stage process.\nWe typically produce a estimate \\(\\hat{\\b\\Omega}\\) by first running an OLS regression, in which we will obtain the residuals \\(\\hat\\eps_i\\). These can be used to estimate the structure of \\(\\b\\Omega\\), producing \\(\\hat{\\b\\Omega}\\). Then, using this estimate \\(\\hat{\\b\\Omega}\\), we can run feasible GLS, and obtain an estimator with less error.\nHowever, this specific form has many drawbacks. When we estimate \\(\\hat{\\b\\Omega}\\) with OLS (or any other method), we of course have some imprecision in our estimates. Econometricians have shown that the 2-stage feasible GLS estimator often is far worse than the hypothetical perfect GLS. Very often, feasible GLS will actually result in larger variances of estimates.\nThere is also some risk with 2-stage feasible GLS. Often, heteroscedasticity and autocorrelation occur in our estimated OLS models not because the population actually has heteroscedasticity or autocorrelation, but rather, our original linear model is missing some explanatory variables which causes other violations in our classical linear model, such as exogeneity violations. This mispecified nature will not only make FGLS even more imprecise, but also has the potential to bias FGLS estimates.\nThus, 2-stage FGLS is not super popular in most applied statistician’s toolkit, and the default tends to be sticking to OLS with either robust standard errors or Heteroscedasticity-and-autocorrelation (HAC) robust standard errors.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#instrumental-variables-estimator",
    "href": "ols.html#instrumental-variables-estimator",
    "title": "4  Least Squares Theory",
    "section": "4.8 Instrumental Variables Estimator",
    "text": "4.8 Instrumental Variables Estimator\nOne of the assumptions in the classical model is exogeneity (definition 4.4). This assumption is critical in the proofs of OLS unbiasedness and asymptotic consistency. This implies that when exogeneity is violated, our estimates of \\(\\hat\\beta\\) become unrealiable.\nThe instrumental variables estimator is a solution to this issue. The idea is to find a third variable (or more) \\(Z\\), that does meet this condition of exogeneity:\n\\[\n\\E(\\b{Z^\\top \\eps}) = 0\n\\tag{4.8}\\]\nand we will have no exogeneity if \\(Z\\) is not correlated with the error term. We then use these instruments \\(Z\\) to predict \\(X\\), which will get us the parts of \\(X\\) that are explained by \\(Z\\) (and thus, uncorrelated with the error term). Then, we can use that exogenous part of \\(X\\) to estimate the relationship with \\(Y\\). However, this hinges on \\(Z\\) meeting that moments condition.\n\nDefinition 4.15 (Assumptions of Instruments) For instrument(s) \\(Z\\) to meet the moment condition \\(\\E(\\b{Z^\\top \\eps}) = 0\\), the following facts must be true:\n\n\\(Z\\) must be exogenous/ignorable, i.e. \\(Cov(Z, \\eps) = 0\\).\n\\(Z\\) must be relevant, i.e. \\(Cov(Z, X) ≠ 0\\).\n\\(Z\\) must meet the exclusions restriction (which is implied by exogenous). This means that \\(Z\\) cannot have an independent effect on \\(Y\\), outside of its impact on \\(Y\\) through \\(X\\).\n\n\n\nLet us derive the IV estimator (and an alternative IV estimator called 2SLS), and explore the asymptotic properties of this estimator.\n\nIV Estimator2SLSAsymptotics of IV\n\n\nSo we know what an instrument \\(Z\\) is, and the requirements for \\(Z\\) to be a valid instrument. But how does \\(Z\\) help us estimate the effect between \\(X\\) and \\(Y\\)?\nOur moment condition and sample equivalent for instrumental variables is:\n\\[\n\\E(\\b{Z^\\top \\eps}) \\approx \\b Z^\\top \\b\\eps = 0\n\\]\nNow, we know that \\(\\b\\eps = \\b y - \\hat{\\b y}\\) and \\(\\hat{\\b y} = \\b{X\\hat\\beta}\\), so let us plug that in:\n\\[\n\\begin{align}\n\\b Z^\\top (\\b y - \\hat{\\b y}) & = 0 \\\\\n\\b Z^\\top (\\b y - \\b{X \\hat\\beta}) & = 0 \\\\\n\\b{Z^\\top y} - \\b{Z^\\top X \\hat\\beta} & = 0\n\\end{align}\n\\]\nAnd now, solving for \\(\\hat{\\b\\beta}\\) with matrix inversion, we get:\n\\[\n\\begin{align}\n- \\b{Z^\\top X \\hat\\beta} & = - \\b{Z^\\top y} \\\\\n\\b{Z^\\top X \\hat\\beta} & = \\b{Z^\\top y} \\\\\n\\hat{\\b\\beta} & = (\\b{Z^\\top X})^{-1} \\b{Z^\\top y}\n\\end{align}\n\\]\n\nDefinition 4.16 (Instrumental Variables Estimator) The IV estimator produces the estimates:\n\\[\n\\hat{\\b\\beta}^* = (\\b{Z^\\top X})^{-1} \\b{Z^\\top y}\n\\]\n\n\nNote that the IV estimator is biased in small sample sizes, and only asymptotically consistent (proof provided in the third tab). Thus, we should be careful when using IV in small sample sizes.\nIt can also be shown that the asymptotic variance of the instrumental variables estimator, under the assumption of spherical errors is\n\\[\n\\V \\hat{\\b\\beta}^* = \\sigma^2(\\b{Z^\\top X})^{-1} \\b{Z^\\top Z} (\\b{X^\\top Z})^{-1}\n\\]\nAlthough the proof of this is beyond the scope of this chapter. There is also a more complex derivation for robust standard errors with instrumental variables estimation.\n\n\nWhile sometimes we do compute the instrumental variables estimator as shown in definition 4.16, often, we use another estimator, the two-stage-least-squares (2SLS) estimator.\nThe 2SLS estimator is based on the intuitive interpretation of instrumental variables: that we use only the part of \\(X\\) explained by \\(Z\\) (which should be endogenous), and estimate that part of \\(X\\)’s relationship with \\(Y\\).\nThe two stage least squares estimator follows this exact procedure. In the first stage, we find the part of \\(X\\) that is explained by \\(Z\\), which we call \\(\\hat X_t\\):\n\\[\n\\hat X_t = \\hat\\beta_0 + \\hat\\beta_1 Z_t\n\\]\nWe do this by running a linear OLS model with \\(X_t\\) as the outcome variable, and \\(Z_t\\) as the explanatory variable. Our predicted \\(\\hat X_t\\) will be the part of \\(X\\) explained by instrument \\(Z\\).\nThen, for the second stange, we take this \\(\\hat X_t\\), and use it in a model with \\(Y\\) as follows:\n\\[\nY_t = \\delta_0 + \\delta_1 \\hat X_t + \\eps_t\n\\]\nHow is 2SLS in these two stages equal to instrumental variables estimator we derived in definition 4.16? Our estimates (in matrix form) for the second stage would be:\n\\[\n\\hat{\\b\\beta}_{2SLS} = (\\hat{\\b X}^\\top \\hat{\\b X})^{-1} \\hat{\\b X}^\\top \\b y\n\\tag{4.9}\\]\nWhere \\(\\hat{\\b X}\\) is given as the fitted values of the first stage:\n\\[\n\\hat{\\b X} = \\b Z \\hat{\\b \\delta} = \\b Z (\\b{Z^\\top Z})^{-1}\\b{Z^\\top X}\n\\]\nWe can plug \\(\\hat{\\b X}\\) into eq. 4.9 to get:\n\\[\n\\hat{\\b\\beta}_{2SLS} = [(\\b Z (\\b{Z^\\top Z})^{-1}\\b{Z^\\top X})^\\top (\\b Z (\\b{Z^\\top Z})^{-1}\\b{Z^\\top X})]^{-1}(\\b Z (\\b{Z^\\top Z})^{-1}\\b{Z^\\top X})^\\top \\b y\n\\]\nUsing the properties of matrix transposes, we get:\n\\[\n\\begin{align}\n\\hat{\\b\\beta}_{2SLS} & = [(\\b X^\\top \\b Z (\\b{Z^\\top Z})^{-1} \\b Z^\\top) (\\b Z (\\b{Z^\\top Z})^{-1}\\b{Z^\\top X})]^{-1}(\\b X^\\top \\b Z (\\b{Z^\\top Z})^{-1} \\b Z^\\top)\\b y \\\\\n& = [\\b X^\\top \\b Z \\underbrace{(\\b{Z^\\top Z})^{-1} \\b Z^\\top \\b Z}_{\\text{inverses cancel}} (\\b{Z^\\top Z})^{-1}\\b{Z^\\top X}]^{-1}\\b X^\\top \\b Z (\\b{Z^\\top Z})^{-1} \\b Z^\\top\\b y \\\\\n& = (\\underbrace{\\b X^\\top \\b Z (\\b{Z^\\top Z})^{-1}}_{\\text{cancel}}\\b{Z^\\top X})^{-1}\\underbrace{\\b X^\\top \\b Z (\\b{Z^\\top Z})^{-1}}_{\\text{cancel}} \\b Z^\\top\\b y \\\\\n& = \\underbrace{(\\b{Z^\\top X})^{-1} \\b{Z^\\top y}}_{\\text{equal to IV}}\n\\end{align}\n\\]\nAnd the second to last step is possible because one of the highlighted “cancel” parts is within an inverse and the other isn’t, so they cancel. Thus, we have shown 2SLS is equivalent to the instrumental variables estimator.\n\n\nDespite being biased, the instrumental variables estimator is an asymptotically consistent estimator of \\(\\b\\beta\\) even with exogeneity between \\(X\\) and \\(\\eps\\) violated (as long as \\(Z\\) is exogenous).\n\nTheorem 4.7 (IV Consistency) The IV estimator is asymptotically consistent, meaning\n\\[\n\\mathrm{plim}\\hat{\\b\\beta}^* = \\b\\beta\n\\]\n\n\nProof: Let us first re-arrange the IV estimator by plugging in the original model \\(\\b y = \\b{X\\beta} + \\b\\eps\\), and simplify:\n\\[\n\\begin{align}\n\\hat{\\b\\beta}^* & = (\\b{Z^\\top X})^{-1} \\b{Z^\\top y} \\\\\n& = (\\b{Z^\\top X})^{-1} \\b{Z^\\top}(\\b{X\\beta} + \\b \\eps) \\\\\n& = \\underbrace{(\\b{Z^\\top X})^{-1} \\b{Z^\\top X}}_{\\text{inverses cancel}}\\b\\beta + (\\b{Z^\\top X})^{-1} \\b{Z^\\top\\eps} \\\\\n& = \\b\\beta + (\\b{Z^\\top X})^{-1} \\b{Z^\\top\\eps}\n\\end{align}\n\\]\nWe can rewrite our matrices in the form of vectors, scalars, and summation:\n\\[\n\\begin{align}\n\\hat{\\b\\beta}^* & = \\b\\beta + (\\b{Z^\\top X})^{-1} \\b{Z^\\top\\eps} \\\\\n& = \\b\\beta + \\left(\\sum\\limits_{t=1}^n \\b z_t \\b x_t^\\top \\right)^{-1} \\left(\\sum\\limits_{t=1}^n \\b z_t \\eps_t \\right)\n\\end{align}\n\\]\nNow, let us do a little algebra trick as follows:\n\\[\n\\hat{\\b\\beta}^* = \\b\\beta + \\left(\\frac{1}{n}\\sum\\limits_{t=1}^n \\b z_t \\b x_t^\\top \\right)^{-1} \\left(\\frac{1}{n}\\sum\\limits_{t=1}^n \\b z_t \\eps_t \\right)\n\\]\nThe reason we can do this is because the first \\(\\frac{1}{n}\\) is inversed as \\(\\frac{1}{n}^{-1}\\), so this cancels out the second one, maintaining the equality of our equation.\nNow, we want to prove \\(\\mathrm{plim}\\hat{\\b\\beta}^* = \\b\\beta\\), so let us take the probability limit of both sides:\n\\[\n\\mathrm{plim}\\hat{\\b\\beta}^* = \\mathrm{plim}\\b\\beta + \\left(\\mathrm{plim}\\frac{1}{n}\\sum\\limits_{t=1}^n \\b z_t \\b x_t^\\top \\right)^{-1} \\left(\\mathrm{plim}\\frac{1}{n}\\sum\\limits_{t=1}^n \\b z_t \\eps_t \\right)\n\\]\nWe know the probability limit of a constant is itself. Look at the other two terms on the right: they take the form of sample averages \\(\\frac{1}{n}\\sum\\). Using the law of large numbers (theorem 2.1), we can simplify to:\n\\[\n\\mathrm{plim}\\hat{\\b\\beta}^*  = \\b\\beta + (\\E(\\b z_t \\b x_t^\\top))^{-1} \\underbrace{\\E(\\b z_t \\eps_t)}_{= \\ 0} = \\b\\beta\n\\]\nAnd we know \\(\\E(\\b z_t \\eps_t) = 0\\) because this is the same condition as the moment condition \\(\\E(\\b{Z^\\top \\eps}) = 0\\), just written in terms of individual observations. Thus, the Instrumental variables estimator is asymptotically consistent.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#clustered-sampling-variance-of-ols",
    "href": "ols.html#clustered-sampling-variance-of-ols",
    "title": "4  Least Squares Theory",
    "section": "4.6 Clustered Sampling Variance of OLS",
    "text": "4.6 Clustered Sampling Variance of OLS\nThe variance of the OLS estimator under clustered sampling is given by\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = (\\b{X^\\top X})^{-1}\\b X^\\top \\begin{pmatrix}\n\\b\\Sigma_1 & 0 & \\dots & 0 \\\\\n0 & \\b\\Sigma_2 & & 0 \\\\\n\\vdots & 0 & \\ddots & 0 \\\\\n0 & 0& \\dots & \\b\\Sigma_G\n\\end{pmatrix} \\b X(\\b{X^\\top X})^{-1}\n\\]\nWhere \\(\\b\\Sigma_1, \\dots \\b\\Sigma_G\\) are intracluster covariance-variance error matrices, that exhibit autocorrelation.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "ols.html#hac-variance-of-ols",
    "href": "ols.html#hac-variance-of-ols",
    "title": "4  Least Squares Theory",
    "section": "4.7 HAC Variance of OLS",
    "text": "4.7 HAC Variance of OLS\nHeteroscedasticity and Autocorrelation (HAC) variance is given by:\n\\[\n\\V(\\hat{\\b\\beta}|\\b X) = (\\b{X^\\top X})^{-1}\\b X^\\top \\b\\Omega \\b X(\\b{X^\\top X})^{-1}\n\\]\nWhere \\(\\b\\Omega\\) is the variance-covariance matrix of the error term \\(\\eps_t\\).",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Least Squares Theory</span>"
    ]
  },
  {
    "objectID": "mle.html#score-and-fisher-information",
    "href": "mle.html#score-and-fisher-information",
    "title": "3  Maximum Likelihood",
    "section": "3.3 Score and Fisher Information",
    "text": "3.3 Score and Fisher Information\nThe gradient of the log-likelihood \\(\\ell(\\b\\theta; \\b y)\\) in respect to vector \\(\\b\\theta\\) is the score function.\n\nDefinition 3.3 (Score Function) The score function \\(s\\) is given by:\n\\[\ns(\\b\\theta; \\b y) = \\frac{\\partial}{\\partial \\b\\theta} \\ell(\\b\\theta; \\b y) = \\frac{\\partial}{\\partial \\b\\theta}  \\sum\\limits_{t=1}^n \\log(f_{Y_t}(y_t; \\b\\theta))\n\\]\nWe can also write this in terms of vector \\(\\b y\\) containing all observations of the sample:\n\\[\ns(\\b\\theta; \\b y) = \\frac{\\partial}{\\partial\\b\\theta} \\log(f_Y(\\b y; \\b\\theta))\n\\]\n\n\nAs we know through calculus, to maximise a function, we set the gradient equal to zero. Thus, the estimates \\(\\hat{\\b\\theta}\\) of maximum likelihood estimation are the set of \\(\\b\\theta\\) that make \\(s(\\b\\theta; \\b y) = 0\\).\nLet us define the true parameter value in the population as \\(\\b\\theta_0\\). That means, the true score function of \\(\\b\\theta_0\\) is \\(s(\\b\\theta_0; \\b y)\\).\nVector \\(\\b y\\), our sample, is the reasliation of a set of random variables as we described earlier. Thus, the true population parameter \\(\\b\\theta_0\\)’s score function \\(s(\\b\\theta_0, \\b y)\\) is actually also a random variable in respect to random \\(\\b y\\). This means the true population parameter score function \\(s(\\b\\theta_0, \\b y)\\) has a expectation and variance.\n\nExpectationVariance and Fisher InformationObserved Information\n\n\n\nTheorem 3.1 The expectation of the true population parameter \\(\\b\\theta\\)’s score function is 0.\n\\[\n\\E(s(\\b\\theta_0; \\b y)) = 0\n\\]\n\n\nProof: Let us first start with the definition of expectation of a continuous variable (definition 1.2). That means we can deduce\n\\[\n\\E(s(\\b\\theta_0; \\b y)) = \\int \\underbrace{s(\\b\\theta_0; \\b y)}_{\\mathrm{s \\ given} \\ \\b y} \\overbrace{f_Y(\\b y ; \\b\\theta)}^{\\P(Y=\\b y)} dy\n\\]\nNow, let us plug in the score function (definition 3.3):\n\\[\n\\E(s(\\b\\theta_0; \\b y)) = \\int\\left[\\frac{\\partial}{\\partial\\b\\theta}\\log f_Y(\\b y; \\b\\theta)\\right] f_Y(\\b y; \\b\\theta)\n\\]\nUsing the derivative rule \\(\\frac{d}{dx}\\log u(x) = \\frac{u'(x)}{u(x)}\\), we get\n\\[\n\\begin{align}\n\\E(s(\\b\\theta_0; \\b y)) & = \\int\\frac{\\frac{\\partial}{\\partial\\b\\theta} f_Y(\\b y; \\b \\theta)}{f_Y(\\b y; \\b \\theta)}f_Y(\\b y; \\b \\theta) \\\\\n& = \\int \\frac{\\partial}{\\partial\\b\\theta} f_Y(\\b y; \\b \\theta)\n\\end{align}\n\\]\nWe can flip the derivative and anti-derivative to get\n\\[\n\\begin{align}\n\\E(s(\\b\\theta_0; \\b y)) & =  \\frac{\\partial} {\\partial\\b\\theta} \\int f_Y(\\b y; \\b \\theta) \\\\\n& = \\frac{\\partial} {\\partial\\b\\theta} 1 \\ = \\ 0\n\\end{align}\n\\]\nAnd the last step is because the integral (are under the curve) of a PDF is always 1 (the entire probability space). Thus, we see that the expectation of the score function at true population parameter \\(\\b\\theta_0\\) in respect to random vector \\(\\b y\\) is 0.\n\n\nWe have established the expected value of the score function of the true population parameter \\(\\b\\theta_0\\). As a random variable, we can also consider its variance. From the definition of variance (definition 1.3), we know:\n\\[\n\\V[s(\\b\\theta_0; \\b y)] = \\E[s(\\b\\theta_0; \\b y) - \\E(s(\\b\\theta_0; \\b y))]\n\\]\nWe know the \\(E(s(\\b\\theta_0; \\b y)) = 0\\) from the proof above (theorem 3.1), so we can plug that in:\n\\[\n\\V[s(\\b\\theta_0; \\b y)] = \\E[s(\\b\\theta_0; \\b y) - 0)^2]\n\\]\nNow, plugging in the definition of the score function (definition 3.3), we see\n\\[\n\\begin{align}\n\\V[s(\\b\\theta_0; \\b y)] & = \\E \\left[ \\left(\\frac{\\partial \\ell(\\b\\theta_0; \\b y)}{\\partial\\b\\theta}\\right)^2 \\right] \\ \\equiv \\ \\b{\\mathcal I}(\\b\\theta_0)\n\\end{align}\n\\]\nWhere \\(\\mathcal I(\\b\\theta_0)\\) is called the fisher information matrix of \\(\\b\\theta_0\\). We can generalise this to any values of \\(\\b\\theta\\):\n\nDefinition 3.4 (Fisher Information Matrix) The fisher information matrix is given by the variance of the score function for any \\(\\b\\theta\\) value:\n\\[\n\\b{\\mathcal I}(\\b\\theta) = \\E \\left[ \\left( \\frac{\\partial}{\\partial \\b\\theta} \\ell(\\b\\theta ; \\b y) \\right)^2 \\biggr | \\b\\theta\\right]\n\\]\nWe can also define the fisher information matrix as the negative expectation of the hessian matrix of second derivatives of the log-likelihood function.\n\\[\n\\b{\\mathcal I}(\\b\\theta) = - \\E\\left[\\frac{\\partial^2}{\\partial \\b\\theta \\partial \\b\\theta^\\top} \\ell(\\b\\theta; \\b y) \\right]\n\\]\n\n\nThe fischer information is always positive: \\(\\mathcal I(\\theta) ≥ 0\\). Higher fisher information implies that the absolute value of the score is higher. Also importantly, the fisher information does not depend on the random realisation of sample \\(\\b y\\), since the expectation averages it our.\n\n\nThe fischer information matrix can be sometimes difficult to calculate. Thus, we sometimes use what is called the observed information matrix \\(\\b I(\\b\\theta, \\b y)\\), without the expectation.\n\nDefinition 3.5 (Observed Information Matrix) The observed information matrix is defined as\n\\[\n\\b I(\\b\\theta; \\b y) = -\\frac{\\partial^2}{\\partial\\b\\theta \\partial \\b\\theta^\\top} \\ell(\\b\\theta; \\b y)\n\\]\nAnd it is the negative of the hessian matrix of second order derivatives of the log-likelihood function.\n\n\nUnlike the fisher information matrix, which is only dependent on \\(\\b\\theta\\) and not the realisation of sample \\(\\b y\\) (due to expectation), the observed information matrix is dependent on the random realisation of sample \\(\\b y\\).",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "causal.html#ate",
    "href": "causal.html#ate",
    "title": "5  Causal Inference",
    "section": "",
    "text": "Definition 5.3 (Average Treatment Effect) The ATE is the average individual treatment effects \\(\\tau_i\\) in the population.\n\\[\n\\tau_{ATE} = \\E(\\tau_t) \\  = \\  \\E(\\pt - \\pc)  \\ = \\  \\E \\pt - \\E \\pc\n\\]",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal.html#att",
    "href": "causal.html#att",
    "title": "5  Causal Inference",
    "section": "5.3 ATT",
    "text": "5.3 ATT\n\nDefinition 5.4 (Average Treatment Effect on the Treated) The ATT is the average individual treatment effect \\(\\tau_i\\) for only units that were assigned to the treatment group \\(D_i = 1\\):\n\\[\n\\tau_{ATT} = \\E(\\tau_t | D_t = 1) \\ = \\ \\E(\\pt - \\pc | D_t = 1)\n\\]",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal.html#atu",
    "href": "causal.html#atu",
    "title": "5  Causal Inference",
    "section": "5.4 ATU",
    "text": "5.4 ATU\n\nDefinition 5.5 (Average Treatment Effect on the Untreated) The ATU is the average individual treatment effect \\(\\tau_i\\) for only units that were assigned to the control group \\(D_i = 0\\):\n\\[\n\\tau_{ATT} = \\E(\\tau_t | D_t = 0) \\ = \\ \\E(\\pt - \\pc | D_t = 0)\n\\]",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal.html#cate",
    "href": "causal.html#cate",
    "title": "5  Causal Inference",
    "section": "5.5 CATE",
    "text": "5.5 CATE\n\nDefinition 5.6 (Conditional Average Treatment Effect) The CATE is the average treatment effect, conditional on some other characteristic/covariate \\(X\\) value:\n\\[\n\\tau_{CATE}(x) = \\E(\\tau_t| X = x) \\ = \\ \\E(\\pt - \\pc|X=x)\n\\]\nThis estimand is also sometimes called the local average treatment effect (LATE).",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "identify.html#non-compliance-instrumental-variables",
    "href": "identify.html#non-compliance-instrumental-variables",
    "title": "6  Causal Identification",
    "section": "6.2 Non-Compliance Instrumental Variables",
    "text": "6.2 Non-Compliance Instrumental Variables\nWhen we assign individuals to treatment/control in randomised experiments (definition 5.9) , we often cannot guarantee that individuals will actually follow through with treatment. Sometimes, someone who was supposed to get treatment refuses treatment, and sometimes, someone who shouldn’t get the treatment decides to get treatment.\nWe can formalise this idea. Let us assume an encouragement \\(Z_t \\in \\{0, 1\\}\\), which is our treatment assignment. Then, we have the treatment variable \\(D_t \\in \\{0,1\\}\\), which is someone who actually took the treatment or not. Given this framework, we can divide all units \\(i\\) into 4 categories:\n\nCompliers: People who comply with encouragement \\(Z_i\\). Their \\(Z_i = D_i\\).\nAlways-takers: People who no matter what encouragement \\(Z_i\\) is, always take treatment.\nNever-takers: People who no matter their encouragement \\(Z_i\\) is, never take treatment.\nDefiers: People who do the opposite of encouragement \\(Z_i\\), so always \\(D_i ≠ Z_i\\).\n\n\n\n\n\n\n\nPrinciple Strata\n\n\n\n\n\nWe can visually show what will happen with all 4 types of people in a table, called the principal strata:\n\n\n\n\n\\(Z_i = 1\\)\n\\(Z_i = 0\\)\n\n\n\\(D_i = 1\\)\nComplier/Always-Taker\nDefier/Always-Taker\n\n\n\\(D_i = 0\\)\nDefier/Never-Taker\nComplier/Never-Taker\n\n\n\n\n\n\nThe idea of the non-compliance designs is to use our encouragement/treatment assignment \\(Z\\) as an instrument for \\(D\\) - actually taking the treatment.\nThis is quite easily estimatable in non-compliance randomisation settings, since \\(Z\\) is randomly assigned. Thus, we can calculate the ITT with a linear regression:\n\\[\nY_i = \\alpha + \\tau_{ITT} Z_i + \\eps_i\n\\]\nHowever, we might not care about the ITT. We might want the actual effect of \\(D\\) on \\(Y\\). An alternative is to find the Average Treatment Effect on only compliers, called the Local Average Treatment Effect (LATE) (a version of the CATE from definition 5.6). We need a few assumptions:\n\nDefinition 6.2 (Non-Compliance IV) There are 4 assumptions for this design:\n\nRelevance: \\(Z\\) must be correlated to \\(D\\). Or in other words, compilers must exist, or else, encouragement would not affect treatment.\nIgnorability/Exogneity: There is no backdoor path between \\(Z\\) and \\(D\\), and no backdoor path between \\(Z\\) and \\(Y\\) (we can do controls/selection on observables to account for this).\nExclusions Restriction: \\(Z\\) must only have an effect on \\(Y\\) through \\(D\\). \\(Z\\) must not have any independent effect on \\(Y\\).\nMonotonicity: There are no defiers.\n\n\n\n\n\n\n\n\nVisual Examples of Violations\n\n\n\n\n\nThe figure below contains some visual examples of violations of the above to faciliate understanding of these assumptions:\n\n\n\n\n\n\n\n\nThere are two causal estimands of interest to us:\n\nIntent to Treat (ITT)Local Average Treatment Effect (LATE)\n\n\nOne causal estimand that we can estimate is the Intent To Treat (ITT), which is essentially the ATE of \\(Z\\) on \\(Y\\), ignoring if people actually took the treatment or not. This is essentially the affect of encouragement. If there is any non-compliance, then the ITT will not equal the ATE.\n\\[\n\\tau_{ITT} = \\E(Y_t|Z_t = 1) - \\E(Y_t | Z_t = 0)\n\\]\nAs long as \\(Z\\) is exogenous/ignorable, then the ITT is identifiable. However, the ITT does not tell us anything about the effect of \\(D\\) (the treatment), only \\(Z\\) (the encouragement).\n\n\nThe ITT only provides the effect for the encouragement \\(Z\\). The LATE instead provides the effect of the treatment \\(D\\) on \\(Y\\) for compliers.\n\\[\n\\tau_{LATE} = \\E(\\tau_t | \\mathrm{compliers})\n\\]\nThe LATE is generally not equivalent to the ATT or the ATE, unless everyone is a complier in the experiment. Thus, we must be careful when interpreting the LATE - it is only the treatment effect for compliers, and we might not know who the compliers are.\n\n\n\nLet us show how we can prove the LATE is identifiable under these assumptions. First, the ITT itself is identifiable under exogeneity/ignorability alone. Now, let us define \\(c\\) as compliers, \\(a\\) as always-takers, \\(n\\) as never-takers, and \\(d\\) as defiers. We can break down the ITT into a weighted average:\n\\[\n\\tau_{ITT} = \\tau_{ITT}^c \\P(c) + \\tau_{ITT}^a \\P(a) + \\tau_{ITT}^n \\P(n) + \\tau_{ITT}^d \\P(d)\n\\]\nWe know that under our assumption of monotonicity, we assume no defiers, so \\(\\P(d) = 0\\):\n\\[\n\\tau_{ITT} = \\tau_{ITT}^c \\P(c) + \\tau_{ITT}^a \\P(a) + \\tau_{ITT}^n \\P(n)\n\\]\nOur exclusions restriction says that \\(Z\\) has no affect on \\(Y\\). The ITT is the relationship between \\(Z\\) and \\(Y\\). But since always-takers and never-takers ignore \\(Z\\) when deciding treatment, \\(Z\\) has no effect of them on \\(Y\\). Thus, we can further simplify:\n\\[\n\\tau_{ITT} = \\tau_{ITT}^c \\P(c)\n\\]\nRemember that the \\(\\tau_{ITT}\\) for compliers, \\(\\tau_{ITT}^c\\), is our LATE that we want to identify. So, let us isolate it to get:\n\\[\n\\tau_{LATE} = \\frac{\\tau_{ITT}}{\\P(c)} \\ = \\ \\frac{\\E(Y_i | Z_i = 1) - \\E(Y_i | Z_i = 0)}{\\E(D_i | Z_i = 1) - \\E(D_i | Z_i = 0)}\n\\]\nThus, we have identified the LATE under the assumptions from definition 6.2.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Causal Identification</span>"
    ]
  },
  {
    "objectID": "identify.html#non-compliance-instruments",
    "href": "identify.html#non-compliance-instruments",
    "title": "6  Causal Identification",
    "section": "6.2 Non-Compliance Instruments",
    "text": "6.2 Non-Compliance Instruments\nWhen we assign individuals to treatment/control in randomised experiments, we often cannot guarantee that individuals will actually follow through with treatment. Let us assume an encouragement \\(Z_t \\in \\{0, 1\\}\\), which is our treatment assignment. Then, we have the treatment variable \\(D_t \\in \\{0,1\\}\\), which is someone who actually took the treatment or not. Given this framework, we can divide all units \\(i\\) into 4 categories:\n\nCompliers: People who comply with encouragement \\(Z_i\\). Their \\(Z_i = D_i\\).\nAlways-takers: People who no matter what encouragement \\(Z_i\\) is, always take treatment.\nNever-takers: People who no matter their encouragement \\(Z_i\\) is, never take treatment.\nDefiers: People who do the opposite of encouragement \\(Z_i\\), so always \\(D_i ≠ Z_i\\).\n\n\n\n\n\n\n\nPrinciple Strata\n\n\n\n\n\nWe can visually show what will happen with all 4 types of people in a table, called the principal strata:\n\n\n\n\n\\(Z_i = 1\\)\n\\(Z_i = 0\\)\n\n\n\\(D_i = 1\\)\nComplier/Always-Taker\nDefier/Always-Taker\n\n\n\\(D_i = 0\\)\nDefier/Never-Taker\nComplier/Never-Taker\n\n\n\n\n\n\nThe idea of the non-compliance designs is to use our encouragement/treatment assignment \\(Z\\) as an instrument for \\(D\\) - actually taking the treatment. Our causal estimands are:\n\nIntent to Treat (ITT)Local Average Treatment Effect (LATE)\n\n\nOne causal estimand that we can estimate is the Intent To Treat (ITT), which is essentially the ATE of \\(Z\\) on \\(Y\\), ignoring if people actually took the treatment or not. This is essentially the affect of encouragement. If there is any non-compliance, then the ITT will not equal the ATE.\n\\[\n\\tau_{ITT} = \\E(Y_t|Z_t = 1) - \\E(Y_t | Z_t = 0)\n\\]\nAs long as \\(Z\\) is randomly assigned (as should be the case in a non-compliance design), we can identify the ITT. However, the ITT does not tell us anything about the effect of \\(D\\) (the treatment), only \\(Z\\) (the encouragement).\n\n\nThe ITT only provides the effect for the encouragement \\(Z\\). The LATE instead provides the effect of the treatment \\(D\\) on \\(Y\\) for compliers (so the ITT or ATE for compliers):\n\\[\n\\tau_{LATE} = \\E(\\tau_t | \\mathrm{compliers})\n\\]\nThe LATE is generally not equivalent to the ATT or the ATE, unless everyone is a complier in the experiment. Thus, we must be careful when interpreting the LATE - it is only the treatment effect for compliers, and we might not know who the compliers are.\n\n\n\n\nDefinition 6.2 (Non-Compliance IV) There are 4 assumptions to identify the causal estimands:\n\nRelevance: \\(Z\\) must be correlated to \\(D\\). Or in other words, compilers must exist, or else, encouragement would not affect treatment.\nIgnorability/Exogneity: There is no backdoor path between \\(Z\\) and \\(D\\), and no backdoor path between \\(Z\\) and \\(Y\\) (we can do controls/selection on observables to account for this).\nExclusions Restriction: \\(Z\\) must only have an effect on \\(Y\\) through \\(D\\). \\(Z\\) must not have any independent effect on \\(Y\\).\nMonotonicity: There are no defiers.\n\n\n\n\n\n\n\n\nVisual Examples of Violations\n\n\n\n\n\nThe figure below contains some visual examples of violations of the above to faciliate understanding of these assumptions:\n\n\n\n\n\n\n\n\nLet us show how we can prove the LATE is identifiable under these assumptions. First, the ITT itself is identifiable under exogeneity/ignorability alone. Now, let us define \\(c\\) as compliers, \\(a\\) as always-takers, \\(n\\) as never-takers, and \\(d\\) as defiers. We can break down the ITT into a weighted average:\n\\[\n\\tau_{ITT} = \\tau_{ITT}^c \\P(c) + \\tau_{ITT}^a \\P(a) + \\tau_{ITT}^n \\P(n) + \\tau_{ITT}^d \\P(d)\n\\]\nWe know that under our assumption of monotonicity, we assume no defiers, so \\(\\P(d) = 0\\):\n\\[\n\\tau_{ITT} = \\tau_{ITT}^c \\P(c) + \\tau_{ITT}^a \\P(a) + \\tau_{ITT}^n \\P(n)\n\\]\nOur exclusions restriction says that \\(Z\\) has no affect on \\(Y\\). The ITT is the relationship between \\(Z\\) and \\(Y\\). But since always-takers and never-takers ignore \\(Z\\) when deciding treatment, \\(Z\\) has no effect of them on \\(Y\\). Thus, we can further simplify:\n\\[\n\\tau_{ITT} = \\tau_{ITT}^c \\P(c)\n\\]\nRemember that the \\(\\tau_{ITT}\\) for compliers, \\(\\tau_{ITT}^c = \\tau_{ATE}\\). So, let us isolate it to get:\n\\[\n\\tau_{LATE} = \\frac{\\tau_{ITT}}{\\P(c)} \\ = \\ \\frac{\\E(Y_i | Z_i = 1) - \\E(Y_i | Z_i = 0)}{\\E(D_i | Z_i = 1) - \\E(D_i | Z_i = 0)}\n\\]\nThus, we have identified the LATE under the assumptions from definition 6.2. We will generally estimate the LATE with the 2-stage-least-squares (2SLS) estimator.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Causal Identification</span>"
    ]
  },
  {
    "objectID": "identify.html#examiner-instruments",
    "href": "identify.html#examiner-instruments",
    "title": "6  Causal Identification",
    "section": "6.3 Examiner Instruments",
    "text": "6.3 Examiner Instruments",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Causal Identification</span>"
    ]
  },
  {
    "objectID": "identify.html#shift-share-instruments",
    "href": "identify.html#shift-share-instruments",
    "title": "6  Causal Identification",
    "section": "6.4 Shift-Share Instruments",
    "text": "6.4 Shift-Share Instruments",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Causal Identification</span>"
    ]
  },
  {
    "objectID": "identify.html#sharp-regression-discontinuity",
    "href": "identify.html#sharp-regression-discontinuity",
    "title": "6  Causal Identification",
    "section": "6.5 Sharp Regression Discontinuity",
    "text": "6.5 Sharp Regression Discontinuity",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Causal Identification</span>"
    ]
  },
  {
    "objectID": "identify.html#fuzzy-regression-discontinuity",
    "href": "identify.html#fuzzy-regression-discontinuity",
    "title": "6  Causal Identification",
    "section": "6.6 Fuzzy Regression Discontinuity",
    "text": "6.6 Fuzzy Regression Discontinuity",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Causal Identification</span>"
    ]
  },
  {
    "objectID": "identify.html#differences-in-differences",
    "href": "identify.html#differences-in-differences",
    "title": "6  Causal Identification",
    "section": "6.7 Differences-in-Differences",
    "text": "6.7 Differences-in-Differences",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Causal Identification</span>"
    ]
  },
  {
    "objectID": "glm.html#linear-probability-model",
    "href": "glm.html#linear-probability-model",
    "title": "9  Generalised Linear Model",
    "section": "9.3 Linear Probability Model",
    "text": "9.3 Linear Probability Model\nThe linear probability model is for binary \\(Y\\) (bernoulli distribution). It should generally only be used for interpreting relationships between \\(X_j\\) and binary \\(Y\\), and not for prediction (use binomial logistic instead). The outcome of interest is the \\(\\E(Y_t | \\set X_t) = \\P (Y_t = 1 | \\set X_t)\\), which we denote as \\(\\pr_t\\).\n\\[\n\\pr_t = \\beta_0 + \\beta_1 X_{1t} + \\beta_2 X_{2t} + \\dots + \\beta_p X_{tp}\n\\]\nWhere \\(\\beta_0\\) is the intercept, \\(\\beta_1, \\dots, \\beta_p\\) are the parameters that explain the relationship between \\(X_j\\) and \\(Y\\), that need to be estimated with sample data.\nThe model is always estimated with OLS + robust standard errors, as the bernoulli distribution’s variance is inherently heterscedastic. For estimation with OLS and robust standard errors in R, we should use the fixest package:\n\nlibrary(fixest)\nmodel &lt;- feols(Y ~ X1 + X2 + X3, data = mydata, se = \"hetero\")\nsummary(model)\n\nFor mechanical details on least squares and robust standard errors, see here.\nInterpretation of coefficient estimates are as follows (proof from theorem 4.1). Remember that these interpretations are not causal unless our \\(X_j\\) has been randomly assigned and meets the condition of independence (definition 1.6):\n\n\n\n\n\n\n\n\n\nContinuous \\(X_{j}\\)\nBinary \\(X_{j}\\)\n\n\n\\(\\hat\\beta_j\\)\nFor every one unit increase in \\(X_{j}\\), there is an expected \\(\\hat\\beta_j \\times 100\\) percentage point change in the probability of a unit being in category \\(Y_t=1\\), holding all other explanatory variables constant.\nThere is a \\(\\hat\\beta_j\\times 100\\) percentage point difference in the probability of a unit being in category \\(Y_t=1\\) between category \\(X_{j} = 1\\) and category \\(X_{j} = 0\\), holding all other explanatory variables constant.\n\n\n\\(\\widehat{\\beta_0}\\)\nWhen all explanatory variables equal 0, the expected probability of a unit being in category \\(Y_t=1\\) is \\(\\hat\\beta_0 \\times 100\\)\nFor category \\(X_{j} = 0\\), the expected probability of a unit being in category \\(Y_t=1\\) is \\(\\hat\\beta_j \\times 100\\) (when all other explanatory variables equal 0).\n\n\n\n\nPredictionT-TestsR-SquaredF-Tests\n\n\nYou should not use the linear probability model for prediction. This is because the linear probability model can produce probabilities \\(\\pr_t\\) that are outside of the range 0 and 1. This violates the axioms of probability, which state any event must have a probability between 0 and 1.\nIf you are interested in prediction, see the binomial logistic model, which ensures that probabilities \\(\\pr_t\\) are always between 0 and 1.\nThe linear probability model is only useful when we are considering the relationship between \\(X_j\\) and \\(Y\\), since the linear nature of the model makes interpreting parameters \\(\\beta_j\\) simple.\n\n\nHypothesis testing of each coefficient with a t-test (and the p-values) is provided in most regression outputs. The mechanics of t-test were provided in chapter 4.\n\nIf our coefficient \\(\\hat\\beta_j\\) is statistically significant, then we can reject that there is no relationship between \\(X_j\\) and \\(Y\\), and conclude there is a relationship between \\(X_j\\) and \\(Y\\).\nIf our coefficient \\(\\hat\\beta_j\\) is not statistically significant, then we cannot reject there is no relationship between \\(X_j\\) and \\(Y\\).\n\n\n\nR-Squared is provided in the regression output, and was discussed in definition 4.11.\nR-Squared is the percentage of variation in \\(Y\\) our model explains. It is always between 0 and 1. R-squared never decreases as we add more explanatory variables - so it is important not to overly focus on it.\nAdjusted R-Squared is similar but penalises models for having too many explanatory variables.\n\n\nF-tests can test the significance of multiple parameters at once. The mechanics of F-test were provided in chapter 4.\nTo conduct a f-test in R, we do the following (where model 0 is the null model, and model 1 is the alternative model):\n\nanova(model0, model1)\n\nIf the test is statistically significant, then all the additional parameters in model 1 are jointly significant. If the test is not statistically significant, then all the additional parameters in model 1 are not jointly significant.",
    "crumbs": [
      "Part II: Applied Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Generalised Linear Model</span>"
    ]
  },
  {
    "objectID": "glm.html#binomial-logistic-regression",
    "href": "glm.html#binomial-logistic-regression",
    "title": "9  Generalised Linear Model",
    "section": "9.4 Binomial Logistic Regression",
    "text": "9.4 Binomial Logistic Regression\nThe linear probability model is for binary \\(Y\\) (bernoulli distribution). The outcome of interest is the \\(\\E(Y_t | \\set X_t) = \\P (Y_t = 1 | \\set X_t)\\), which we denote as \\(\\pr_t\\). The logistic model applies a link function \\(g(\\cdot)\\) to \\(\\pr_t\\) to ensure that predicted probabilities are always between 0 and 1. The model is specified as:\n\\[\n\\log\\left(\\frac{\\pr_t}{1 - \\pr_t}\\right) = \\beta_0 + \\beta_1X_{t1} + \\beta_2X_{t2} + \\dots + \\beta_pX_{tp}\n\\tag{9.1}\\]\nUsing the property of logarithms, we can rewrite the model in respect to \\(\\pr_t\\):\n\\[\n\\pr_t = \\frac{e^{\\beta_0 + \\beta_1X_{t1} + \\beta_2X_{t2} + \\dots + \\beta_pX_{tp}}}{1+e^{\\beta_0 + \\beta_1X_{t1} + \\beta_2X_{t2} + \\dots + \\beta_pX_{tp}}}\n\\]\nWhere \\(\\beta_0\\) is the intercept, \\(\\beta_1, \\dots, \\beta_p\\) are the parameters that explain the relationship between \\(X_j\\) and \\(Y\\), that need to be estimated with sample data.\nThe model is always estimated with the maximum likelihood estimator. For estimation with OLS and robust standard errors in R, we should use the glm() function:\n\nmodel &lt;- glm(Y ~ X1 + X2 + X3,\n             data = mydata,\n             family = \"binomial\")\nsummary(model)\n\nInterpretation of coefficient estimates are very difficult - instead, we will interpret odds ratios, which are \\(e^{\\hat\\beta_j}\\). Remember that these interpretations are not causal unless our \\(X_j\\) has been randomly assigned and meets the condition of independence (definition 1.6):\n\n\n\n\n\n\nDetails of Odds Ratios\n\n\n\n\n\nOdds of an event \\(A\\) is the probability of \\(A\\) occuring divided by the probability of event \\(A\\) not occuring. We can apply the same logic to \\(\\P(Y_t = 1) = \\pr_t\\):\n\\[\n\\mathrm{odds}_A = \\frac{\\P(A)}{1 - \\P(A)} \\quad \\implies \\quad \\mathrm{odds}_{Y_t = 1} = \\frac{\\pr_t}{1-\\pr_t}\n\\]\nFrom eq. 9.1, if we exponent both sides, we can get the odds of \\(Y_t = 1\\) from the logistic regression:\n\\[\n\\frac{\\pr_t}{1-\\pr_t} = e^{\\beta_0 + \\beta_1X_{t1} + \\beta_2X_{t2} + \\dots + \\beta_pX_{tp}}\n\\]\nAn odds ratio is a ratio of two odds. For the odds of event \\(A\\) and \\(B\\), the odds ratio is:\n\\[\nOR = \\frac{\\mathrm{odds}_A}{\\mathrm{odds}_B} = \\frac{\\P A / ( 1 - \\P A)}{\\P B / (1 - \\P B)}\n\\]\nWe can apply the same to the logistic regression. We can find the odds of \\(\\pr_t | X_j = x+1\\) and \\(\\pr_t | X_j = x\\):\n\\[\nOR = \\frac{\\mathrm{odds}_{\\pr_t | X_j = x+1}}{\\mathrm{odds}_{\\pr_t | X_j = x}} = e^{\\beta_j}\n\\]\nThus, \\(e^{\\beta_j}\\) is the multiplicative change in the odds of event \\(Y_t = 1\\), for every one unit increase in \\(X_j\\).\n\n\n\n\n\n\n\n\n\n\n\n\nContinuous \\(X_{j}\\)\nBinary \\(X_{j}\\)\n\n\n\\(\\hat\\beta_j\\)\nFor every one unit increase in \\(X_{j}\\), there is an expected \\(\\times e^{\\hat\\beta_j}\\) multiplicative change in the odds of a unit being in category \\(Y_t=1\\), holding all other explanatory variables constant.\nThere is a \\(\\times e^{\\hat\\beta_j}\\) multiplicative difference in the odds of a unit being in category \\(Y_t=1\\) between category \\(X_{j} = 1\\) and category \\(X_{j} = 0\\), holding all other explanatory variables constant.\n\n\n\\(\\hat\\beta_0\\)\nThe odds of event \\(Y_t = 1\\) when all explanatory variables equal 0 is \\(e^{\\hat\\beta_0}\\).\nThe odds of event \\(Y_t = 1\\) for category \\(X_j = 0\\) is \\(e^{\\hat\\beta_0}\\), when all other explanatory variables equal 0.\n\n\n\n\nPredictionWald TestsLikelihood Ratio TestsAIC/BIC\n\n\nTo predict probabilities \\(\\pr_t = \\P(Y_t = 1)\\) for new observations \\(t\\), we use our fitted values equation:\n\\[\n\\hat\\pr_t = \\frac{e^{\\hat\\beta_0 + \\hat\\beta_1X_{t1} + \\hat\\beta_2X_{t2} + \\dots + \\hat\\beta_pX_{tp}}}{1+e^{\\hat\\beta_0 + \\hat\\beta_1X_{t1} + \\hat\\beta_2X_{t2} + \\dots + \\hat\\beta_pX_{tp}}}\n\\]\nIn R, we can use the predict command:\n\nmy_predictions &lt;- predict(model,\n                          newdata = my_new_data,\n                          type = \"response\")\n\nWhere my_new_data is a data frame with \\(X_1, \\dots, X_p\\) values for all the new observations we want to predict \\(\\hat Y\\) for.\nWe can also do classification, which is not about predicting the probability \\(\\pr_t\\), but instead predicting if \\(\\hat Y = 1\\) or \\(\\hat Y = 0\\). We generally assign units \\(\\pr_t &gt; 0.5\\) to \\(\\hat Y = 1\\), and others to \\(\\hat Y = 0\\).\n\n\nHypothesis testing of each coefficient with a wald test (and the p-values) is provided in most regression outputs. The mechanics of wald test were provided in chapter 3.\n\nIf our coefficient \\(\\hat\\beta_j\\) is statistically significant, then we can reject that there is no relationship between \\(X_j\\) and \\(Y\\), and conclude there is a relationship between \\(X_j\\) and \\(Y\\).\nIf our coefficient \\(\\hat\\beta_j\\) is not statistically significant, then we cannot reject there is no relationship between \\(X_j\\) and \\(Y\\).\n\n\n\nLikelihood Ratio tests can test the significance of multiple parameters at once. The mechanics of Likelihood Ratio test were provided in chapter 3.\nTo conduct a Likelihood Ratio test in R, we do the following (where model 0 is the null model, and model 1 is the alternative model):\n\nanova(model0, model1, test = \"LRT\")\n\nIf the test is statistically significant, then all the additional parameters in model 1 are jointly significant. If the test is not statistically significant, then all the additional parameters in model 1 are not jointly significant.\n\n\nAIC and BIC are model fit metrics (information criterion statistics) that are provided in the regression output, and was discussed in chapter 3.\nAIC and BIC can be used to compare how good different models are, which allows us to choose which model we want to use. Both metrics reward better explanatory power, and punish complexity (more parameters). Lower values of AIC and BIC indicate better model fits.\nThe BIC tends to penalise extra parameters more strongly than AIC. Generally, when comparing two models, AIC and BIC will agree.",
    "crumbs": [
      "Part II: Applied Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Generalised Linear Model</span>"
    ]
  },
  {
    "objectID": "glm.html#ordinal-logistic-regression",
    "href": "glm.html#ordinal-logistic-regression",
    "title": "9  Generalised Linear Model",
    "section": "9.5 Ordinal Logistic Regression",
    "text": "9.5 Ordinal Logistic Regression",
    "crumbs": [
      "Part II: Applied Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Generalised Linear Model</span>"
    ]
  },
  {
    "objectID": "glm.html#multinomial-logistic-regression",
    "href": "glm.html#multinomial-logistic-regression",
    "title": "9  Generalised Linear Model",
    "section": "9.6 Multinomial Logistic Regression",
    "text": "9.6 Multinomial Logistic Regression",
    "crumbs": [
      "Part II: Applied Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Generalised Linear Model</span>"
    ]
  },
  {
    "objectID": "glm.html#negative-binomial-regression",
    "href": "glm.html#negative-binomial-regression",
    "title": "9  Generalised Linear Model",
    "section": "9.7 Negative Binomial Regression",
    "text": "9.7 Negative Binomial Regression",
    "crumbs": [
      "Part II: Applied Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Generalised Linear Model</span>"
    ]
  },
  {
    "objectID": "glm.html#model-specification-issues",
    "href": "glm.html#model-specification-issues",
    "title": "9  Generalised Linear Model",
    "section": "9.8 Model Specification Issues",
    "text": "9.8 Model Specification Issues",
    "crumbs": [
      "Part II: Applied Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Generalised Linear Model</span>"
    ]
  },
  {
    "objectID": "multivariate.html",
    "href": "multivariate.html",
    "title": "8  Multivariate Statistics",
    "section": "",
    "text": "8.1 Principle Components\nPrinciple components analysis (PCA) is a way to combine multiple observed variables into fewer variables, which is a process called dimensional reduction. We start off with a set of observed variables \\(\\b x_t = (x_1, x_2, \\dots, x_p)_t\\) for each observation \\(t\\). Each observed variable \\(x_i\\) has a variance \\(\\V x_i\\), and their total variance is \\(\\V x_1 + \\dots + \\V x_p\\).\nPCA takes these \\(p\\) number of original variables \\(\\b x_t\\), and calculates a set of \\(p\\) new variables called principle components \\(y_1, \\dots, y_p\\). Each principle component \\(y_j\\) is made up a linear combination of the original variables:\n\\[\n\\begin{align}\ny_1 = & \\ a_{11}x_1 + a_{21}x_2 + \\dots + a_{p1}x_p \\\\\ny_2 = & \\ a_{12}x_1 + a_{22}x_2 + \\dots + a_{p2}x_p \\\\\n& \\qquad \\vdots \\\\\ny_p = & \\ a_{1p}x_1 + a_{2p}x_2 + \\dots + a_{pp}x_p \\\\\n\\end{align}\n\\]\nWith \\(a_{ij}\\) being the weights of the linear combinations. The larger a weight is for a specific \\(x_i\\) in a specific principle component \\(y_j\\), the more that principle component is measuring that \\(x_i\\). We can look at which weights are larger for which variables in a specific \\(y_j\\) to see and interpret what any \\(y_j\\) is measuring. The sum of all the weights for each principle component \\(y_j\\) should be 1. We can rewrite the above in terms of linear algebra:\n\\[\ny_j = \\b a^\\top_j \\b x \\quad \\iff \\quad y_j = \\begin{pmatrix}\na_{ij} & a_{2j} & \\dots & a_{pj} \\end{pmatrix}\n\\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_p\n\\end{pmatrix}\n\\]\nAnd all the principle components \\(\\b y = (y_1, \\dots, y_p)\\) can be expressed as\n\\[\n\\b y = \\b A^\\top \\b x , \\quad \\b A = \\begin{pmatrix}\n\\b a_1 & \\b a_2 & \\dots & \\b a_p \\end{pmatrix}\n\\tag{8.1}\\]\nAll of the principle components together have the same variance as the original variables: \\(\\sum \\V y_j = \\sum \\V x_i\\). Thus, the new principle components carry the same information/variation as the original variables, just with a different distribution between each variable. Each principle component is uncorrelated with the next principle component - thus each PC conveys distinct aspects of the data.\nThe weights \\(a_{ij}\\) of the PCs are calculated from eigenvalue decomposition of the covariance matrix \\(\\b\\Sigma\\) of observed variables \\(x_1, \\dots, x_p\\). We assume that \\(\\b\\Sigma\\) has \\(p\\) distinct positive eigenvalues, denoted \\(\\lambda_1 &gt; \\lambda_2 &gt; \\dots &gt; \\lambda_p &gt; 0\\). Each eigenvalue \\(\\lambda_j\\) corresponds to an eigenvector \\(\\b a_j\\), which is the weights vector of the \\(j\\)th principle component:\nBy applying eigenvalue decomposition to matrix \\(\\b\\Sigma\\), we get a matrix \\(\\b A\\) made up of eigenvectors of \\(\\b\\Sigma\\), and a diagonal matrix \\(\\b D\\) with eigenvalues \\(\\lambda\\) on its diagonal:\n\\[\n\\b\\Sigma = \\b{ADA}^{-1}, \\quad \\b D = \\begin{pmatrix}\n\\lambda_1 & & \\\\\n& \\lambda_2  & \\\\\n& & \\ddots\n\\end{pmatrix}, \\quad \\b A = \\begin{pmatrix}\n\\b a_1 & \\b a_2 & \\dots & \\b a_p \\end{pmatrix}\n\\]\nThe matrix \\(\\b A\\) is the same as from eq. 8.1, and each column are the weights of a principle components. The variance of each PC \\(y_j\\) is equivalent to \\(\\lambda_j\\), the \\(j\\)th eigenvalue.\nPrinciple components are labelled in order of the variance they contain. So, principle component \\(y_1\\) will have more variance than principle component \\(y_2\\), and so on. The proportion of total variance in all of the \\(x_1, \\dots, x_p\\) the first \\(q\\) principle components will explain is\n\\[\n\\frac{\\sum_{j=1}^q \\V y_j}{\\sum_{i=1}^p \\V x_i} = \\frac{\\lambda_1 + \\lambda_2 + \\dots + \\lambda_q}{\\lambda_1 + \\lambda_2 + \\dots + \\lambda_1 + \\dots + \\lambda_p}\n\\]\nFrequently, the first few principle components will explain around 70-80% of the total variation in all of \\(x_1, \\dots, x_p\\). Thus, we can reduce the number of variables from \\(p\\) to just 2-3 principle components. This can be very useful when we want to reduce the computation power needed to estimate models, or to reduce multicollinearity issues (since each principle component is uncorrelated with each other).\nWe will discuss the practical implementation of PCA in the multivariate methods chapter in the applied section.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multivariate Statistics</span>"
    ]
  },
  {
    "objectID": "multivariate.html#factor-analysis",
    "href": "multivariate.html#factor-analysis",
    "title": "8  Multivariate Statistics",
    "section": "8.2 Factor Analysis",
    "text": "8.2 Factor Analysis\nFactor analysis models are latent variable models - they assume that we have a set of observed variables (items) \\(X_1, X_2, \\dots, X_p\\), that are all the result of some unobserved (factor) variable \\(\\xi\\). For example:\n\n\n\n\n\nThe latent factor \\(\\xi\\) is assumed to be distributed \\(\\xi \\sim \\mathcal N(\\kappa = 0, \\ \\phi = 1)\\). We assume that each item \\(X_i\\) is normally distributed, and is related to the latent factor \\(\\xi\\) by a linear model:\n\\[\nX_i = \\tau_i + \\lambda_i\\xi + \\delta_i, \\quad \\delta_i \\sim \\mathcal N(0, \\theta_{ii})\n\\]\nWhere \\(\\lambda_i\\) is the slope (called the factor loadings), which determine the relationship between factor \\(\\xi\\) and a specific item \\(X_i\\). \\(\\delta_i\\) is the error term, and is called the unique factor.\n\n\n\n\n\n\nAssumptions of the Model\n\n\n\n\n\nWe make a few assumptions on this linear model above.\n\nError terms \\(\\delta_i\\) for each regression model between \\(\\xi\\) and \\(X_1, \\dots, X_p\\) is normally distributed with a mean of 0. \\(\\delta_i \\sim \\mathcal N(0, \\theta_{ii})\\).\nError terms \\(\\delta_1, \\dots, \\delta_p\\) of each model \\(i\\) are uncorrelated with each other. This implies that correlations between \\(X_1, \\dots, X_p\\) are entirely explained by the latent factor \\(\\xi\\).\nFactor \\(\\xi\\) is uncorrelated with the error term \\(\\delta_i\\) (exogeneity).\n\n\n\n\nGiven the linear models between \\(X_i\\) and \\(\\xi\\), we know that \\(\\E(X_i | \\xi) = \\tau_i + \\lambda_i \\xi\\). Since \\(X_i\\) is also assumed to be normally distributed, we can determine the distribution of each \\(X_i\\) as\n\\[\nX_i \\sim \\mathcal N(\\tau_i + \\lambda_i \\xi, \\ \\ \\lambda_i^2 \\phi + \\theta_{ii})\n\\]\nOur theoretical variance-covariance matrix between \\(X_1, \\dots, X_p\\), where the diagonals are the variances of \\(X_1, \\dots, X_p\\), and the non-diagonals are \\(Cov(x_n, x_m)\\) will be\n\\[\n\\begin{pmatrix}\n\\lambda_1^2 \\phi + \\theta_{11} &  \\lambda_1\\phi\\lambda_2 & \\dots &  \\lambda_1 \\phi \\lambda_p \\\\\n\\lambda_2 \\phi \\lambda_1 & \\lambda_2^2 \\phi + \\theta_{22} & \\dots & \\lambda_1 \\phi \\lambda_p \\\\\n\\vdots & \\dots & \\ddots & \\vdots \\\\\n\\lambda_p\\phi\\lambda_1 & \\lambda_p \\phi \\lambda_2 & \\dots & \\lambda_p^2 \\phi + \\theta_{pp}\n\\end{pmatrix}\n\\]\nThe estimation process is to find the values of \\(\\lambda_i\\) and \\(\\theta_{ii}\\) that make the above hypothetical covariance matrix (since \\(\\phi\\) is assumed to be 1), as close to our observed covariance matrix from our sample data. This is generally done with maximum likelihood estimation.\n\nInterpretationMultiple FactorsConfirmatory Analysis\n\n\nOur \\(\\hat\\lambda_i\\) will be the estimated covariances between any item \\(X_i\\) and the latent factor \\(\\xi\\). We can interpret \\(\\xi\\) based on the items \\(X_i\\) that have the largest factor loadings \\(\\hat\\lambda_i\\).\nRecall that when we assumed \\(\\xi\\) is standardly normally distributed, the variances of \\(X_i\\) (from the matrix above) become:\n\\[\n\\V X_i = \\lambda_i^2 + \\theta_{ii}\n\\]\n\n\\(\\lambda_i^2\\) is the part of the variance in \\(X_i\\) explained by the factor \\(\\xi\\). This is known as the communality of \\(X_i\\).\n\\(\\theta_{ii}\\) is the part of \\(X_i\\) not explained by the factor \\(\\xi\\), and is called the unique variance.\nThe proportion \\(\\rho_i =\\lambda_i^2 / (\\lambda_i^2 + \\theta_{ii})\\) is the proportion of variance in \\(X_i\\) explained by our factor \\(\\xi\\), called the reliability. This is the \\(R^2\\) of factor analysis.\n\nOnce we have estimated the factor analysis model, we can then use \\(X_1, \\dots, X_k\\) to create values for the latent variable \\(\\tilde\\xi_t\\) for each observation \\(t\\), called factor scores:\n\\[\n\\tilde\\xi_t = w_0 + w_1 X_{t1} + w_2 X_{t2} + \\dots + w_p X_{tp}\n\\]\nWhich are linear combinations of \\(X_1, \\dots, X_p\\), with weights \\(w_i\\) determined by the strength of the relationship between \\(X_i\\) and \\(\\xi\\) as estimated by \\(\\hat\\lambda_i\\) and the unique variance \\(\\theta_{ii}\\).\n\n\nWe can have more than one latent factor \\(\\b\\xi = (\\xi_1, \\dots, \\xi_q)\\). We assume all are standardly normally distributed as before. Each item \\(X_1, \\dots, X_p\\) is now related to each factor \\(\\xi_1, \\dots, \\xi_q\\) with a regression:\n\\[\nX_i = \\tau_i + \\lambda_{i1}\\xi_1 + \\lambda_{i2}\\xi_2 + \\dots + \\lambda_{i1} \\xi_q + \\delta_i, \\quad \\delta_i\\sim\\mathcal N(0, \\theta_{ii})\n\\]\nEach factor can be correlated with each other - which means \\(Cov(\\xi_j, \\xi_k) = \\phi_{jk}\\) must be estimated as well. Because of the additional parameters, identification becomes more tricky. The number of factors \\(q\\) must be small enough given the number of items \\(p\\) in order for our model to be identified:\n\\[\ndf = \\frac{(p-q)^2-(p+1)}{2}≥ 0\n\\]\nFinally, we have an issue of factor rotation - this is because our different factors can be rotated in infinitely many ways, and still produce the same model fit.\nThe default rotation is orthogonal (perpendicular), which means factors are uncorrelated. The result from this estimation is very similar to PCA, and is good for dimensional reduction.\nHowever, for interpretation ease, it is often useful to use oblique rotations, where factors can be correlated. This is because oblique rotations will have more factor loadings of 0, which will allow us to be more clear with what a factor is measuring.\nAlso note that in terms of interpretation, \\(\\hat\\lambda_{ij}\\) is only the covariance between \\(X_i\\) and \\(\\xi_j\\) if all factors are uncorrelated. If factors are correlated, we lose this nice interpretation.\n\n\nConfirmatory factor analysis is more about using factor analysis to test theories we already have. Instead of letting the estimation process estimate component loadings for all \\(X_i\\) and \\(\\xi_j\\), we might set some component loadings to 0 based on our theoretical beliefs.\nA loading of 0 implies that a factor \\(\\xi_j\\) is not being measured by an observed item \\(X_i\\). Ideally for CFA, we want a structure where each item \\(X_i\\) has only one non-0 loading - thus we know exactly what factor \\(X_i\\) is measuring. Below, we can see half of the items measure \\(\\xi_1\\), and the other half measure \\(\\xi_2\\):\n\n\n\n\n\nFor model identification to be possible with this structure, we need at least 2 items per factor, and for a 1-factor model, we need 3 items.\nWe can use a variety of significance tests (like the wald test with hypothesis \\(\\lambda_i = 0\\)) to test if a certain component should be set to 0 or not.\n\n\n\nWe will discuss the practical implementation of factor analysis in the multivariate methods chapter in the applied section.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multivariate Statistics</span>"
    ]
  },
  {
    "objectID": "multivariate.html#single-factor-analysis",
    "href": "multivariate.html#single-factor-analysis",
    "title": "8  Multivariate Statistics",
    "section": "8.2 Single Factor Analysis",
    "text": "8.2 Single Factor Analysis\nFactor analysis models are latent variable models - they assume that we have a set of observed variables (items) \\(X_1, X_2, \\dots, X_p\\), that are all the result of some unobserved (factor) variable \\(\\xi\\). For example:\n\n\n\n\n\nThe latent factor \\(\\xi\\) is assumed normally distributed: \\(\\xi \\sim \\mathcal N(\\kappa, \\phi)\\). We assume that each item \\(X_i\\) is also normally distributed, and is related to the latent factor \\(\\xi\\) by a linear model:\n\\[\nX_i = \\tau_i + \\lambda_i\\xi + \\delta_i\n\\]\nWhere \\(\\lambda_i\\) is the slope (called the factor loadings), which determine the relationship between factor \\(\\xi\\) and a specific item \\(X_i\\). \\(\\delta_i\\) is the error term, and is called the unique factor. Assumptions of this model include:\n\nError terms \\(\\delta_i\\) are normally distributed: \\(\\delta_i \\sim \\mathcal N(0, \\theta_{ii})\\).\nError terms \\(\\delta_1, \\dots, \\delta_p\\) for each item \\(X_1, \\dots, X_p\\) are uncorrelated with each other. We assume any correlation between items \\(X_1, \\dots, X_p\\) is explained by factor \\(\\xi\\).\nWe assume \\(\\xi\\) is uncorrelated with error \\(\\delta_i\\) (exogeneity).\n\nGiven the linear models between \\(X_i\\) and \\(\\xi\\), we know that \\(\\E(X_i | \\xi) = \\tau_i + \\lambda_i \\xi\\). Since \\(X_i\\) is also assumed to be normally distributed, we can determine the distribution of each \\(X_i\\) as\n\\[\nX_i \\sim \\mathcal N(\\tau_i + \\lambda_i \\xi, \\ \\ \\lambda_i^2 \\phi + \\theta_{ii})\n\\]\nOur theoretical variance-covariance matrix between \\(X_1, \\dots, X_p\\), where the diagonals are the variances of \\(X_1, \\dots, X_p\\), and the non-diagonals are \\(Cov(x_n, x_m)\\) will be\n\\[\n\\begin{pmatrix}\n\\lambda_1^2 \\phi + \\theta_{11} &  \\lambda_1\\phi\\lambda_2 & \\dots &  \\lambda_1 \\phi \\lambda_p \\\\\n\\lambda_2 \\phi \\lambda_1 & \\lambda_2^2 \\phi + \\theta_{22} & \\dots & \\lambda_1 \\phi \\lambda_p \\\\\n\\vdots & \\dots & \\ddots & \\vdots \\\\\n\\lambda_p\\phi\\lambda_1 & \\lambda_p \\phi \\lambda_2 & \\dots & \\lambda_p^2 \\phi + \\theta_{pp}\n\\end{pmatrix}\n\\]\nThe estimation process is to find the values of \\(\\lambda_i\\) and \\(\\theta_{ii}\\) that make the above hypothetical covariance matrix, as close to our observed covariance matrix from our sample data. This is generally done with maximum likelihood estimation.\nYou might notice that we have \\(\\phi\\) in our covariance matrix. This actually means that depending on the value of \\(\\phi\\), we can have multiple \\(\\lambda_i\\) and \\(\\theta_{ii}\\) values that achieve the same solutions. Thus, this is a problem of unique identification - we have many possible solutions to this estimation problem. To get a unique result, we generally fix \\(\\xi\\) to be normally distributed - \\(\\xi \\sim \\mathcal N(\\kappa = 0, \\ \\phi = 1)\\). By fixing \\(\\phi\\) this way, we will have unique solutions.\nOur estimated \\(\\hat\\lambda_i\\) will be the estimated covariances between any observed item \\(X_i\\) and the latent vactor \\(\\xi\\). We can interpret the factor \\(\\xi\\) based on the items \\(X_i\\) that have the largest factor loadings \\(\\hat\\lambda_i\\).\nRecall that when we assumed \\(\\xi\\) is standardly normally distributed, the variances of \\(X_i\\) (from the matrix above) become:\n\\[\n\\V X_i = \\lambda_i^2 + \\theta_{ii}\n\\]\n\\(\\lambda_i^2\\) is the part of the variance in \\(X_i\\) explained by the factor \\(\\xi\\). This is known as the communality of \\(X_i\\). \\(\\theta_{ii}\\) is the part of \\(X_i\\) not explained by the factor \\(\\xi\\), and is called the unique variance. The proportion \\(\\rho_i =\\lambda_i^2 / (\\lambda_i^2 + \\theta_{ii})\\) is the proportion of variance in \\(X_i\\) explained by our factor \\(\\xi\\), called the reliability. This is the \\(R^2\\) of factor analysis.\nOnce we have estimated the factor analysis model, we can then use \\(X_1, \\dots, X_k\\) to create values for the latent variable \\(\\tilde\\xi_t\\) for each observation \\(t\\), called factor scores:\n\\[\n\\tilde\\xi_t = w_0 + w_1 X_{t1} + w_2 X_{t2} + \\dots + w_p X_{tp}\n\\]\nWhich are linear combinations of \\(X_1, \\dots, X_p\\), with weights \\(w_i\\) determined by the strength of the relationship between \\(X_i\\) and \\(\\xi\\) as estimated by \\(\\hat\\lambda_i\\) and the unique variance \\(\\theta_{ii}\\).\nWe will discuss the practical implementation of factor analysis in the multivariate methods chapter in the applied section.",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multivariate Statistics</span>"
    ]
  },
  {
    "objectID": "multivariate.html#latent-trait-models",
    "href": "multivariate.html#latent-trait-models",
    "title": "8  Multivariate Statistics",
    "section": "8.3 Latent Trait Models",
    "text": "8.3 Latent Trait Models",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multivariate Statistics</span>"
    ]
  },
  {
    "objectID": "multivariate.html#latent-class-models",
    "href": "multivariate.html#latent-class-models",
    "title": "8  Multivariate Statistics",
    "section": "8.4 Latent Class Models",
    "text": "8.4 Latent Class Models",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multivariate Statistics</span>"
    ]
  },
  {
    "objectID": "multivariate.html#cluster-analysis",
    "href": "multivariate.html#cluster-analysis",
    "title": "8  Multivariate Statistics",
    "section": "8.5 Cluster Analysis",
    "text": "8.5 Cluster Analysis",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multivariate Statistics</span>"
    ]
  },
  {
    "objectID": "multivariate.html#structural-equation-modelling",
    "href": "multivariate.html#structural-equation-modelling",
    "title": "8  Multivariate Statistics",
    "section": "8.6 Structural Equation Modelling",
    "text": "8.6 Structural Equation Modelling",
    "crumbs": [
      "Part I: Theoretical Statistics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multivariate Statistics</span>"
    ]
  }
]