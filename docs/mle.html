<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Maximum Likelihood – Statistics for Political Science</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<link href="./ols.html" rel="next">
<link href="./inference.html" rel="prev">
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="mathjax-config.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./random.html">Part I: Theoretical Statistics</a></li><li class="breadcrumb-item"><a href="./mle.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Maximum Likelihood</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistics for Political Science</a> 
    </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Part I: Theoretical Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./random.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Random Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mle.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Maximum Likelihood</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Least Squares Theory</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./causal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Causal Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./identify.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Causal Identification</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Part II: Applied Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Generalised Linear Model</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul class="collapse">
  <li><a href="#likelihood-function" id="toc-likelihood-function" class="nav-link active" data-scroll-target="#likelihood-function"><span class="header-section-number">3.1</span> Likelihood Function</a></li>
  <li><a href="#maximum-likelihood" id="toc-maximum-likelihood" class="nav-link" data-scroll-target="#maximum-likelihood"><span class="header-section-number">3.2</span> Maximum Likelihood</a></li>
  <li><a href="#score-function" id="toc-score-function" class="nav-link" data-scroll-target="#score-function"><span class="header-section-number">3.3</span> Score Function</a></li>
  <li><a href="#fisher-information" id="toc-fisher-information" class="nav-link" data-scroll-target="#fisher-information"><span class="header-section-number">3.4</span> Fisher Information</a></li>
  <li><a href="#variance-and-asymptotics" id="toc-variance-and-asymptotics" class="nav-link" data-scroll-target="#variance-and-asymptotics"><span class="header-section-number">3.5</span> Variance and Asymptotics</a></li>
  <li><a href="#cramér-rao-bound" id="toc-cramér-rao-bound" class="nav-link" data-scroll-target="#cramér-rao-bound"><span class="header-section-number">3.6</span> Cramér-Rao Bound</a></li>
  <li><a href="#newton-raphson-algorithm" id="toc-newton-raphson-algorithm" class="nav-link" data-scroll-target="#newton-raphson-algorithm"><span class="header-section-number">3.7</span> Newton-Raphson Algorithm</a></li>
  <li><a href="#statistical-inference" id="toc-statistical-inference" class="nav-link" data-scroll-target="#statistical-inference"><span class="header-section-number">3.8</span> Statistical Inference</a></li>
  <li><a href="#information-criterion-statistics" id="toc-information-criterion-statistics" class="nav-link" data-scroll-target="#information-criterion-statistics"><span class="header-section-number">3.9</span> Information Criterion Statistics</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./random.html">Part I: Theoretical Statistics</a></li><li class="breadcrumb-item"><a href="./mle.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Maximum Likelihood</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Maximum Likelihood</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In the previous chapters, we have mostly talked about the theory of random variables and statistical inference.</p>
<p>In this chapter, we cover the Maximum Likelihood Estimator (MLE), the most commonly used estimator in statistics. We discuss the intuition and mathematics behind the MLE, and how statistical inference is conducted with the MLE.</p>
<p><br></p>
<section id="likelihood-function" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="likelihood-function"><span class="header-section-number">3.1</span> Likelihood Function</h2>
<p>The maximum likelihood estimator allows us to estimate parameters <span class="math inline">\(\theta\)</span> as long as we know the form/probability density function of the <a href="./inference.html#data-generating-process">data generating process</a>.</p>
<p>In statistics, to estimate the population parameters <span class="math inline">\(\b\theta\)</span>, we gather some sample of observations <span class="math inline">\((y_1, y_2, \dots, y_n)\)</span>. Our random sample <span class="math inline">\((y_1, y_2, \dots, y_n)\)</span> are each realisations of a random variable (our random selection from the population) <span class="math inline">\(Y_1, Y_2, \dots, Y_n\)</span>.</p>
<p>What is the probability that we draw <span class="math inline">\(y_1\)</span> from our random variable <span class="math inline">\(Y_1\)</span>? Well, we know that the probability of realising a specific <span class="math inline">\(y\)</span> from a random variable <span class="math inline">\(Y\)</span> is given by the probability density function (<a href="random.html#def-pdf" class="quarto-xref">definition&nbsp;<span>1.1</span></a>). Thus, the probability of drawing <span class="math inline">\(y_1\)</span> is:</p>
<p><span class="math display">\[
\P(Y_1 = y_1) = f_{Y_1}(y_1)
\]</span></p>
<p>However, our probability density function <span class="math inline">\(f_{Y_1}\)</span> is determined by a set of parameters. For example, if <span class="math inline">\(Y_1\)</span> is normally distributed <span class="math inline">\(Y_1 \sim \mathcal N(\mu_{Y_1}, \sigma^2_{Y_1})\)</span>, then the probability density function <span class="math inline">\(f_{Y_1}\)</span> is determined by parameters <span class="math inline">\(\b \theta = (\mu_{Y_1}, \sigma^2_{Y_1})\)</span>. To represent this fact, in MLE, we will put the unknown parameters as a second input in our probability density function:</p>
<p><span class="math display">\[
\P(Y_1 = y_1) = f_{Y_1}(y_1; \b \theta)
\]</span></p>
<p>Similarly, the probability of drawing <span class="math inline">\(y_2\)</span> from <span class="math inline">\(Y_2\)</span> is <span class="math inline">\(f_{Y_2}(y_2; \b \theta)\)</span>, and the probability of drawing <span class="math inline">\(y_t\)</span> from <span class="math inline">\(Y_t\)</span> is <span class="math inline">\(f_{Y_t}(y_t; \b \theta)\)</span>.</p>
<p>Okay, we know the probability of drawing any observation <span class="math inline">\(y_t\)</span> from <span class="math inline">\(Y_t\)</span> But, what is the probability of drawing the exact sample <span class="math inline">\((y_1, y_2, \dots, y_n)\)</span> that we got? Or more mathematically, what is the probability of</p>
<p><span class="math display">\[
\P(Y_1 = y_1, \ Y_2 = y_2, \ \dots, \ Y_n = y_n)
\]</span></p>
<p>Well, we know that if random variables are independent (<a href="random.html#def-independence" class="quarto-xref">definition&nbsp;<span>1.6</span></a>) of each other (i.e.&nbsp;drawing <span class="math inline">\(y_1\)</span> doesn’t affect the probability of drawing <span class="math inline">\(y_2\)</span>), the joint probability density function is just all the product of all the PDFs. Thus, if <span class="math inline">\(Y_1, \dots, Y_n\)</span> are independent random variables, we know the joint probability density function is</p>
<p><span class="math display">\[
\begin{align}
f_{Y_1, \dots , Y_n}(y_1, \dots, y_n; \ \b\theta ) &amp; = f_{Y_1}(y_1; \b\theta) f_{Y_2}(y_2; \b\theta) \dots f_{Y_n}(y_n; \b\theta) \\
&amp; = \prod\limits_{t=1}^nf_{Y_t}(y_t; \b\theta)
\end{align}
\]</span></p>
<p>And this joint probability density function gives us the exact probability of getting our exact sample <span class="math inline">\((y_1, \dots, y_n)\)</span> through random sampling. We define this probability as the <strong>likelihood</strong> <span class="math inline">\(L\)</span> of getting our sample <span class="math inline">\((y_1, \dots, y_n)\)</span>, given our population parameters <span class="math inline">\(\b\theta\)</span>.</p>
<div id="def-likelihood" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 3.1 (Likelihood Function)</strong></span> The likelihood function <span class="math inline">\(L(\b\theta; \b y)\)</span> describes the probability that we get some sample <span class="math inline">\(\b y = (y_1, \dots, y_n)\)</span>, given population parameters <span class="math inline">\(\b\theta\)</span>.</p>
<p><span class="math display">\[
L(\b\theta; \ y_1, y_2, \dots , y_n) = \prod\limits_{t=1}^nf_{Y_t}(y_t; \b\theta)
\]</span></p>
<p>We can also write this in terms of vector <span class="math inline">\(\b y\)</span> containing all observations of the sample:</p>
<p><span class="math display">\[
L(\b\theta; \ \b y) = f_Y(\b y; \b \theta)
\]</span></p>
</div>
<p><br></p>
</section>
<section id="maximum-likelihood" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="maximum-likelihood"><span class="header-section-number">3.2</span> Maximum Likelihood</h2>
<p>In the above section, we defined the likelihood function (<a href="#def-likelihood" class="quarto-xref">definition&nbsp;<span>3.1</span></a>) as the likelihood of getting the exact sample we got, given the population parameters <span class="math inline">\(\b\theta\)</span>.</p>
<p>The <strong>maximum likelihood estimator</strong> is the idea that we should estimate our parameters <span class="math inline">\(\hat{\b\theta}\)</span> as the values of the parameters that maximise the likelihood we observe our specific sample <span class="math inline">\(y_1, y_2, \dots y_n\)</span>.</p>
<p>The reason is because if there was a set of potential parameter values <span class="math inline">\(\b\theta^*\)</span> that had a low chance of producing our sample <span class="math inline">\(y_1, \dots, y_n\)</span>, we probably would not get our exact sample <span class="math inline">\(y_1, \dots, y_n\)</span>. Since we would probably not get the exact sample we got, we either got really unlucky, or that set of potential parameter values <span class="math inline">\(\b\theta^*\)</span> is wrong.</p>
<p>But if there was a set of potential parameter values <span class="math inline">\(\b\theta^*\)</span> that was likely to produce our exact sample <span class="math inline">\(y_1, \dots, y_n\)</span>, we have a good chance we actually getting our exact sample <span class="math inline">\(y_1, \dots, y_n\)</span>. Thus, we want to find the set of parameters <span class="math inline">\(\hat{\b\theta}\)</span> that maximise our chances of observing our specific sample <span class="math inline">\(y_1 \dots, y_n\)</span>.</p>
<p>To find the set of parameters <span class="math inline">\(\hat{\b\theta}\)</span> that maximise the likelihood we observe our specific sample, we need to find values of <span class="math inline">\(\b\theta\)</span> that maximise our likelihood function <span class="math inline">\(L(\b \theta, \b y)\)</span>. Thus our goal is</p>
<p><span class="math display">\[
\hat{\b\theta} = \max\limits_{\b\theta} L(\b\theta; \b y)
\]</span></p>
<p>However, this maximisation problem is quite difficult, as the likelihood function (<a href="#def-likelihood" class="quarto-xref">definition&nbsp;<span>3.1</span></a>) is a product. Finding the derivative of a product is not straight forward.</p>
<p>Luckily, we have an alternative, the log-likelihood function <span class="math inline">\(\ell(\b\theta ; \b y)\)</span>, since the same <span class="math inline">\(\b\theta\)</span> values that maximise <span class="math inline">\(L\)</span> will also maximise <span class="math inline">\(\ell\)</span>. The log-likelihood function can be derived by taking the log of the likelihood function</p>
<p><span class="math display">\[
\begin{align}
\ell(\b \theta; \b y) &amp; = \log L(\b \theta; \b y) \\
&amp; = \log\left( \prod\limits_{t=1}^n f_{Y_t}(y_t; \b\theta)\right) \\
&amp; = \log[ f_{Y_1}(y_1, \b\theta) f_{Y_2}(y_2; \b\theta) \dots f_{Y_n}(y_n; \b\theta) ]
\end{align}
\]</span></p>
<p>And using the property of logs that <span class="math inline">\(\log(ab) = \log a + \log b\)</span>, we can determine</p>
<p><span class="math display">\[
\begin{align}
\ell(\b \theta; \b y) &amp; = \log(f_{Y_1}(y_1; \b\theta)) + \log(f_{Y_2}(y_2; \b\theta)) + \dots + f_{Y_n}(y_n; \b\theta) \\
&amp; = \sum\limits_{t=1}^n\log (f_{Y_t}(y_t; \b\theta))
\end{align}
\]</span></p>
<div id="def-loglike" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 3.2 (Log-Likelihood Function)</strong></span> The log-likelihood function <span class="math inline">\(\ell\)</span> is the log of the likelihood function <span class="math inline">\(L\)</span> (<a href="#def-likelihood" class="quarto-xref">definition&nbsp;<span>3.1</span></a>). The log-likelihood function takes the form</p>
<p><span class="math display">\[
\ell(\b \theta; \b y) = \sum\limits_{t=1}^n\log (f_{Y_t}(y_t; \b\theta))
\]</span></p>
<p>We can also write this in terms of vector <span class="math inline">\(\b y\)</span> containing all observations of the sample:</p>
<p><span class="math display">\[
\ell(\b \theta; \b y)  = \log(f_Y(\b y; \ \b\theta))
\]</span></p>
</div>
<p><br></p>
<p>The summation notation in the log-likelihood makes it far easier to maximise by finding the derivative. Thus, in most applications, we will deal with the log-likelihood function for estimation purposes.</p>
<p><br></p>
</section>
<section id="score-function" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="score-function"><span class="header-section-number">3.3</span> Score Function</h2>
<p>The gradient of the log-likelihood <span class="math inline">\(\ell(\b\theta; \b y)\)</span> in respect to vector <span class="math inline">\(\b\theta\)</span> is the score function.</p>
<div id="def-scorefunc" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 3.3 (Score Function)</strong></span> The score function <span class="math inline">\(s\)</span> is given by:</p>
<p><span class="math display">\[
s(\b\theta; \b y) = \frac{\partial}{\partial \b\theta} \ell(\b\theta; \b y) = \frac{\partial}{\partial \b\theta}  \sum\limits_{t=1}^n \log(f_{Y_t}(y_t; \b\theta))
\]</span></p>
<p>We can also write this in terms of vector <span class="math inline">\(\b y\)</span> containing all observations of the sample:</p>
<p><span class="math display">\[
s(\b\theta; \b y) = \frac{\partial}{\partial\b\theta} \log(f_Y(\b y; \b\theta))
\]</span></p>
</div>
<p><br></p>
<p>As we know through calculus, to maximise a function, we set the gradient equal to zero. Thus, the estimates <span class="math inline">\(\hat{\b\theta}\)</span> of maximum likelihood estimation are the set of <span class="math inline">\(\b\theta\)</span> that make <span class="math inline">\(s(\b\theta; \b y) = 0\)</span>.</p>
<p>Let us define the true parameter value in the population as <span class="math inline">\(\b\theta_0\)</span>. That means, the true score function of <span class="math inline">\(\b\theta_0\)</span> is <span class="math inline">\(s(\b\theta_0; \b y)\)</span>.</p>
<p>Vector <span class="math inline">\(\b y\)</span>, our sample, is the reasliation of a set of random variables as we described earlier. Thus, the true population parameter <span class="math inline">\(\b\theta_0\)</span>’s score function <span class="math inline">\(s(\b\theta_0, \b y)\)</span> is actually also a random variable in respect to random <span class="math inline">\(\b y\)</span>. This means the true population parameter score function <span class="math inline">\(s(\b\theta_0, \b y)\)</span> has a expectation and variance.</p>
<div id="thm-scorezero" class="theorem" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Theorem 3.1</strong></span> The expectation of the true population parameter <span class="math inline">\(\b\theta\)</span>’s score function is 0.</p>
<p><span class="math display">\[
\E(s(\b\theta_0; \b y)) = 0
\]</span></p>
</div>
<p><br></p>
<p><strong>Proof</strong>: Let us first start with the definition of expectation of a continuous variable (<a href="random.html#def-exp" class="quarto-xref">definition&nbsp;<span>1.2</span></a>). That means we can deduce</p>
<p><span class="math display">\[
\E(s(\b\theta_0; \b y)) = \int \underbrace{s(\b\theta_0; \b y)}_{\mathrm{s \ given} \ \b y} \overbrace{f_Y(\b y ; \b\theta)}^{\P(Y=\b y)} dy
\]</span></p>
<p>Now, let us plug in the score function (<a href="#def-scorefunc" class="quarto-xref">definition&nbsp;<span>3.3</span></a>):</p>
<p><span class="math display">\[
\E(s(\b\theta_0; \b y)) = \int\left[\frac{\partial}{\partial\b\theta}\log f_Y(\b y; \b\theta)\right] f_Y(\b y; \b\theta)
\]</span></p>
<p>Using the derivative rule <span class="math inline">\(\frac{d}{dx}\log u(x) = \frac{u'(x)}{u(x)}\)</span>, we get</p>
<p><span class="math display">\[
\begin{align}
\E(s(\b\theta_0; \b y)) &amp; = \int\frac{\frac{\partial}{\partial\b\theta} f_Y(\b y; \b \theta)}{f_Y(\b y; \b \theta)}f_Y(\b y; \b \theta) \\
&amp; = \int \frac{\partial}{\partial\b\theta} f_Y(\b y; \b \theta)
\end{align}
\]</span></p>
<p>We can flip the derivative and anti-derivative to get</p>
<p><span class="math display">\[
\begin{align}
\E(s(\b\theta_0; \b y)) &amp; =  \frac{\partial} {\partial\b\theta} \int f_Y(\b y; \b \theta) \\
&amp; = \frac{\partial} {\partial\b\theta} 1 \ = \ 0
\end{align}
\]</span></p>
<p>And the last step is because the integral (are under the curve) of a PDF is always 1 (the entire probability space). Thus, we see that the expectation of the score function at true population parameter <span class="math inline">\(\b\theta_0\)</span> in respect to random vector <span class="math inline">\(\b y\)</span> is 0.</p>
<p><br></p>
</section>
<section id="fisher-information" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="fisher-information"><span class="header-section-number">3.4</span> Fisher Information</h2>
<p>We have established the expected value of the score function of the true population parameter <span class="math inline">\(\b\theta_0\)</span>. As a random variable, we can also consider its variance. From the definition of variance (<a href="random.html#def-var" class="quarto-xref">definition&nbsp;<span>1.3</span></a>), we know:</p>
<p><span class="math display">\[
\V[s(\b\theta_0; \b y)] = \E[s(\b\theta_0; \b y) - \E(s(\b\theta_0; \b y))]
\]</span></p>
<p>We know the <span class="math inline">\(E(s(\b\theta_0; \b y)) = 0\)</span> from the proof above (<a href="#thm-scorezero" class="quarto-xref">theorem&nbsp;<span>3.1</span></a>), so we can plug that in:</p>
<p><span class="math display">\[
\V[s(\b\theta_0; \b y)] = \E[s(\b\theta_0; \b y) - 0)^2]
\]</span></p>
<p>Now, plugging in the definition of the score function (<a href="#def-scorefunc" class="quarto-xref">definition&nbsp;<span>3.3</span></a>), we see</p>
<p><span class="math display">\[
\begin{align}
\V[s(\b\theta_0; \b y)] &amp; = \E \left[ \left(\frac{\partial \ell(\b\theta_0; \b y)}{\partial\b\theta}\right)^2 \right] \ \equiv \ \b{\mathcal I}(\b\theta_0)
\end{align}
\]</span></p>
<p>Where <span class="math inline">\(\mathcal I(\b\theta_0)\)</span> is called the fisher information matrix of <span class="math inline">\(\b\theta_0\)</span>. We can generalise this to any values of <span class="math inline">\(\b\theta\)</span>:</p>
<div id="def-fischerinfo" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 3.4 (Fisher Information Matrix)</strong></span> The fisher information matrix is given by the variance of the score function for any <span class="math inline">\(\b\theta\)</span> value:</p>
<p><span class="math display">\[
\b{\mathcal I}(\b\theta) = \E \left[ \left( \frac{\partial}{\partial \b\theta} \ell(\b\theta ; \b y) \right)^2 \biggr | \b\theta\right]
\]</span></p>
<p>We can also define the fisher information matrix as the negative expectation of the hessian matrix of second derivatives of the log-likelihood function.</p>
<p><span class="math display">\[
\b{\mathcal I}(\b\theta) = - \E\left[\frac{\partial^2}{\partial \b\theta \partial \b\theta^\top} \ell(\b\theta; \b y) \right]
\]</span></p>
</div>
<p><br></p>
<p>The fischer information is always positive: <span class="math inline">\(\mathcal I(\theta) ≥ 0\)</span>. Higher fisher information implies that the absolute value of the score is higher. Also importantly, the fisher information does not depend on the random realisation of sample <span class="math inline">\(\b y\)</span>, since the expectation averages it our.</p>
<p>However, the fischer information matrix can be sometimes difficult to calculate. Thus, we sometimes use what is called the observed information matrix <span class="math inline">\(\b I(\b\theta, \b y)\)</span>, without the expectation.</p>
<div id="def-obsinfo" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 3.5 (Observed Information Matrix)</strong></span> The observed information matrix is defined as</p>
<p><span class="math display">\[
\b I(\b\theta; \b y) = -\frac{\partial^2}{\partial\b\theta \partial \b\theta^\top} \ell(\b\theta; \b y)
\]</span></p>
<p>And it is the negative of the hessian matrix of second order derivatives of the log-likelihood function.</p>
</div>
<p><br></p>
<p>Unlike the fisher information matrix, which is only dependent on <span class="math inline">\(\b\theta\)</span> and not the realisation of sample <span class="math inline">\(\b y\)</span> (due to expectation), the observed information matrix is dependent on the random realisation of sample <span class="math inline">\(\b y\)</span>.</p>
<p><br></p>
</section>
<section id="variance-and-asymptotics" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="variance-and-asymptotics"><span class="header-section-number">3.5</span> Variance and Asymptotics</h2>
<p>It can be shown through complex math, that the inverse of the fischer information matrix <span class="math inline">\(1/\b{\mathcal I}(\b\theta)\)</span> is the variance of the maximum likelihood estimator.</p>
<div id="thm-mlevariance" class="theorem" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Theorem 3.2 (Variance of MLE)</strong></span> The variance of the maximum likelihood estimator (in sufficiently large sample sizes) is:</p>
<p><span class="math display">\[
\V \hat{\b\theta} = \frac{1}{\b{\mathcal I}(\b\theta)}
\]</span></p>
</div>
<p><br></p>
<p>You probably have noticed that I stated the variance condition only holds for large sample sizes. This is because the fischer information matrix is under the assumption of an unbiased estimator, which as we will see in the next section, MLE is only unbiased asymptotically.</p>
<p>To estimate the variance for smaller sample sizes, we typically use the observed information matrix instead:</p>
<p><span class="math display">\[
\V \hat{\b\theta} = \frac{1}{\b I (\hat{\b\theta}; \b y)}
\]</span></p>
<p>The Maximum Likelihood Estimator has some desirable asymptotic properties as sample size <span class="math inline">\(n \rightarrow ∞\)</span>, that make is a very popular estimator in statistics.</p>
<p>Through central limit theorem (<a href="inference.html#thm-clt" class="quarto-xref">theorem&nbsp;<span>2.2</span></a>), we know asymptotically that <span class="math inline">\(\hat\theta\)</span> from MLE will be normally distributed. Through quite complex mathematics, we can also show that the MLE is asymptotically consistent (<a href="inference.html#def-consistency" class="quarto-xref">definition&nbsp;<span>2.6</span></a>). Finally, in the last section, we determined the asymptotic variance of the MLE (<a href="#thm-mlevariance" class="quarto-xref">theorem&nbsp;<span>3.2</span></a>). Thus, we know the asymptotic properties of the MLE.</p>
<div id="thm-mleasymptotics" class="theorem" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Theorem 3.3 (Asymptotic Properties of MLE)</strong></span> The maximum likelihood estimator has the following asymptotic distribution:</p>
<p><span class="math display">\[
\hat{\b\theta} \sim \mathcal N(\b\theta_0, \b{\mathcal I}(\b\theta_0)^{-1} )
\]</span></p>
<p>This means that MLE is asymptotically consistent, asymptotically normal, and has an asymptotic variance of <span class="math inline">\(1/\b{\mathcal I}(\b\theta_0)\)</span>.</p>
</div>
<p><br></p>
<p>Notably, the maximum likelihood estimator is the asymptotically unbiased (consistent) estimator with the lowest variance, which we will prove in the next section on Cramér Rao.</p>
<p>It is important to note that while MLE is asymptotically unbiased (consistent), it can be biased in lower sample sizes (although not always - this depends on the actual data generating process in question). This can have some implications on model selection (specifically fixed effects), that we will cover more in the applied chapters.</p>
<p><br></p>
</section>
<section id="cramér-rao-bound" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="cramér-rao-bound"><span class="header-section-number">3.6</span> Cramér-Rao Bound</h2>
<p>Let us focus on the asymptotic variance of the maximum likelihood estimator. We see that it is <span class="math inline">\(1/\b{\mathcal I}(\b\theta_0)\)</span>. Why is this important? Let us introduce a new theorem: Cramér-Rao.</p>
<div id="thm-cramerrao" class="theorem" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Theorem 3.4 (Cramér-Rao)</strong></span> The Cramér-Rao Bound states that for any unbiased estimator, the variance cannot be any lower than the inverse of the fischer information matrix:</p>
<p><span class="math display">\[
\V \hat{\b\theta} ≥ \frac{1}{\b{\mathcal I}(\b\theta)}
\]</span></p>
<p>Since the MLE has that exact variance asymptotically, it is the asymptotically unbiased (consistent) estimator with the least variance.</p>
</div>
<p><br></p>
<p><strong>Proof:</strong> An unbiased estimator <span class="math inline">\(\hat\theta\)</span> of a parameter <span class="math inline">\(\theta\)</span>, its estimates are a function of its sample <span class="math inline">\(\b y\)</span>, so we will call the unbiased estimator <span class="math inline">\(\hat\theta(\b y)\)</span>. We know an unbiased estimator <span class="math inline">\(\hat\theta(\b y)\)</span> should have a expectation equal to true population parameter, which also implies</p>
<p><span class="math display">\[
\E \hat\theta(\b y) = \theta \quad \implies \quad \E[\hat\theta(\b y) - \theta \ | \ \theta \ ] = 0
\]</span></p>
<p>Regardless of the value of <span class="math inline">\(\theta\)</span>. We can rewrite expectation with the definition of expectation (<a href="random.html#def-exp" class="quarto-xref">definition&nbsp;<span>1.2</span></a>) to get:</p>
<p><span class="math display">\[
\int\left( \hat\theta(y) - \theta \right) f_Y(y; \theta)dy = 0
\]</span></p>
<p>Since this expectation is independent of <span class="math inline">\(\theta\)</span> (true for all <span class="math inline">\(\theta\)</span>), the derivative in respect to <span class="math inline">\(\theta\)</span> should be 0 (since <span class="math inline">\(\theta\)</span> does not cause change in the expectation), and we can compute the derivative with product rule:</p>
<p><span class="math display">\[
\begin{align}
0 &amp; = \frac{\partial}{\partial \theta} \int \left(\hat\theta(y) - \theta \right)f_Y(y ; \theta)dy \\
0 &amp; = \int \left(\hat\theta(y) - \theta \right)\frac{\partial f_Y}{\partial \theta}dy - \int f_Y dy
\end{align}
\]</span></p>
<p>Where I have shortened <span class="math inline">\(f_Y(y; \theta)\)</span> to <span class="math inline">\(f_Y\)</span> for simplicity. Since <span class="math inline">\(f_Y\)</span> is a PDF, <span class="math inline">\(\int f_Y dy = 1\)</span>, thus</p>
<p><span class="math display">\[
\begin{align}
0 &amp; = \int \left(\hat\theta(y) - \theta \right)\frac{\partial f_Y}{\partial \theta}dy - 1\\
1 &amp; = \int \left(\hat\theta(y) - \theta \right)\frac{\partial f_Y}{\partial \theta}dy
\end{align}
\]</span></p>
<p>We can substitute <span class="math inline">\(\frac{\partial f_Y}{\partial \theta} = f_y \frac{\partial \log f_Y}{\partial \theta}\)</span>. You can prove this is true by using chain rule. Thus,</p>
<p><span class="math display">\[
1 = \int \left(\hat\theta(y) - \theta \right) f_Y \frac{\partial \log f_Y}{\partial \theta}dy
\]</span></p>
<p>We can factor <span class="math inline">\(f_Y\)</span> in half to get:</p>
<p><span id="eq-simplifycramer"><span class="math display">\[
1 = \int \left( (\hat\theta(y) - \theta) \sqrt{f_Y} \right) \left(\sqrt{f_Y} \frac{\partial \log f_Y}{\partial \theta} \right)dy
\tag{3.1}\]</span></span></p>
<p>We know by the Cauchy-Schwarz inequalty, that this must be true:</p>
<p><span class="math display">\[
\begin{align}
\int \left( (\hat\theta(y) - \theta) \sqrt{f_Y} \right)&amp;  \left(\sqrt{f_Y} \frac{\partial \log f_Y}{\partial \theta} \right)dy \\  ≤ \ &amp; \left(\int (\hat\theta(y) - \theta)^2 f_Y dy \right) \left( \int f_Y \left(\frac{\partial \log f_Y}{\partial \theta}\right)^2 dy \right)
\end{align}
\]</span></p>
<p>And we know the left side from <a href="#eq-simplifycramer" class="quarto-xref">eq.&nbsp;<span>3.1</span></a> is equal to 1, so we get the inequality:</p>
<p><span class="math display">\[
1 ≤ \left(\int (\hat\theta(y) - \theta)^2 f_Y dy \right) \left( \int f_Y \left(\frac{\partial \log f_Y}{\partial \theta}\right)^2 dy \right)
\]</span></p>
<p>We can see both parts of the right side are in the form of expectations (<a href="random.html#def-exp" class="quarto-xref">definition&nbsp;<span>1.2</span></a>), so let us write them as expectations.</p>
<p><span class="math display">\[
1 ≤ \color{blue}{\E[(\hat\theta - \theta)^2]} \cdot  \color{purple}{\E\left[ \left(\frac{\partial \log f_Y}{\partial \theta}\right)^2 \right]}
\]</span></p>
<p>We can see the blue is the variance (<a href="random.html#def-var" class="quarto-xref">definition&nbsp;<span>1.3</span></a>) of an unbiased estimator (since <span class="math inline">\(\E \hat\theta = \theta\)</span>), and the purple part is the fisher information matrix (<a href="#def-fischerinfo" class="quarto-xref">definition&nbsp;<span>3.4</span></a>). Thus, isolating the variance on one side, we get</p>
<p><span class="math display">\[
\V \hat\theta ≤ \frac{1}{\mathcal I(\theta)}
\]</span></p>
<p>This is the lowest bound any unbiased estimator’s variance can be, meaning any unbiased estimator who attains this variance is the unbiased estimator with the least variance.</p>
<p>We know that the maximum likelihood estimator is unbiased asymptotically (consistent), and its asymptotic variance is <span class="math inline">\(1/\mathcal I(\theta)\)</span>. Thus, the maximum likelihood estimator is the unbiased estimator with the least variance, and is the <u>best asymptotic unbiased estimator</u>.</p>
<p><br></p>
</section>
<section id="newton-raphson-algorithm" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="newton-raphson-algorithm"><span class="header-section-number">3.7</span> Newton-Raphson Algorithm</h2>
<p>We know that the MLE estimates <span class="math inline">\(\hat\theta\)</span> are obtained by minimising the score function <span class="math inline">\(s(\b\theta, \b y)\)</span>. However, in many cases, there exists no closed form-analytical solution. Thus, we have to use iterated algorithms to minimise the score function.</p>
<p>The most common method is the Newton-Raphson algorithm. Suppose <span class="math inline">\(\theta\)</span> is a scalar of potential parameters. For values of <span class="math inline">\(\theta\)</span> that are close to the true population parameter <span class="math inline">\(\theta_0\)</span>, the first order taylor series expansion of <span class="math inline">\(s(\theta, \b y)\)</span> about <span class="math inline">\(\theta_0\)</span> states:</p>
<p><span class="math display">\[
s(\theta; \b y) \approx s(\theta_0, \b y) + s'(\theta_0; \b y)(\theta - \theta_0)
\]</span></p>
<p>Where <span class="math inline">\(s'(\theta_0; \b y)\)</span> is the first derivative of the score function <span class="math inline">\(s(\theta_0, \b y)\)</span> at the true population value <span class="math inline">\(\theta_0\)</span>. We know from <a href="#def-obsinfo" class="quarto-xref">definition&nbsp;<span>3.5</span></a> that the first derivative of the score function is defined by the negative observed information <span class="math inline">\(-\b I(\theta_0)\)</span>. Thus, we get:</p>
<p><span class="math display">\[
s(\theta; \b y) \approx s(\theta_0, \b y)  - \b I(\theta_0)(\theta - \theta_0)
\]</span></p>
<p>Now, we can get our <span class="math inline">\(\theta\)</span> that makes the score function equal 0 by setting <span class="math inline">\(s(\theta; \b y) = 0\)</span>, and then solving for <span class="math inline">\(\theta\)</span>:</p>
<p><span id="eq-nrimpractical"><span class="math display">\[
\begin{align}
0 &amp;  \approx s(\theta_0, \b y)  - \b I(\theta_0)(\theta - \theta_0) \\
\b I(\theta_0)(\theta - \theta_0) &amp; \approx s(\theta_0, \b y) \\
\theta - \theta_0 &amp; \approx \b I(\theta_0)^{-1}  s(\theta_0, \b y) \\
\theta &amp; \approx \theta_0 + \b I(\theta_0)^{-1} s(\theta_0, \b y)
\end{align}
\tag{3.2}\]</span></span></p>
<p>So know we have identified the <span class="math inline">\(\theta\)</span> that makes our score function equal to 0, which also maximises the log-likelihood <span class="math inline">\(\ell\)</span> and likelihood <span class="math inline">\(L\)</span>. However, there is an issue - our solution for <span class="math inline">\(\theta\)</span> includes <span class="math inline">\(\theta_0\)</span>, the unknown true population parameter. Since <span class="math inline">\(\theta_0\)</span> is unknown, we cannot use this formula directly.</p>
<p>Instead, we use an iterative procedure. We start with some initial value <span class="math inline">\(\theta^{(0)}\)</span> that is randomly chosen. Then, we use that <span class="math inline">\(\theta^{(0)}\)</span> to “update” to get a new <span class="math inline">\(\theta^{(1)}\)</span>, based on <a href="#eq-nrimpractical" class="quarto-xref">eq.&nbsp;<span>3.2</span></a>:</p>
<p><span class="math display">\[
\theta^{(1)} = \theta^{(0)} + \b I(\theta^{(0)})^{-1}s(\theta^{(0)}; \b y)
\]</span></p>
<p>Then, with our new <span class="math inline">\(\theta^{(1)}\)</span>, we update to get <span class="math inline">\(\theta^{(2)}\)</span>:</p>
<p><span class="math display">\[
\theta^{(2)} = \theta^{(1)} + \b I(\theta^{(1)})^{-1}s(\theta^{(1)}; \b y)
\]</span></p>
<p>And we keep doing this using <span class="math inline">\(\theta^{(m)}\)</span> to update to <span class="math inline">\(\theta^{(m+1)}\)</span>:</p>
<p><span id="eq-iterative"><span class="math display">\[
\theta^{(m+1)} = \theta^{(m)} + \b I(\theta^{(m)})^{-1}s(\theta^{(m)}; \b y)
\tag{3.3}\]</span></span></p>
<p>And we keep doing this until the difference between <span class="math inline">\(\theta^{(m+1)}\)</span> and <span class="math inline">\(\theta^{(m)}\)</span> becomes very small (based on some pre-specified threshold). This is because our formula in <a href="#eq-nrimpractical" class="quarto-xref">eq.&nbsp;<span>3.2</span></a> is:</p>
<p><span class="math display">\[
\theta \approx \theta_0 + \b I(\theta_0)^{-1} s(\theta_0, \b y)
\]</span></p>
<p>So, if <span class="math inline">\(\theta^{(m+1)}\)</span> is very close to <span class="math inline">\(\theta^{(m)}\)</span> in <a href="#eq-iterative" class="quarto-xref">eq.&nbsp;<span>3.3</span></a>, we know that <span class="math inline">\(\theta^{(m+1)}\)</span> is approaching the true value <span class="math inline">\(\theta_0\)</span>.</p>
<p>An alternative to the Newton-Raphson algorithm is the <strong>Fisher-scoring algorithm</strong>, which does the same thing, but using the fisher information matrix <span class="math inline">\(\mathcal I(\theta)\)</span> (<a href="#def-fischerinfo" class="quarto-xref">definition&nbsp;<span>3.4</span></a>) instead of the observed information matrix <span class="math inline">\(I(\theta)\)</span>, when <span class="math inline">\(\mathcal I (\theta)\)</span> is not too difficult to compute. Fisher-scoring is the method most common for generalised linear models (that we will cover in later chapters).</p>
<p><br></p>
</section>
<section id="statistical-inference" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="statistical-inference"><span class="header-section-number">3.8</span> Statistical Inference</h2>
<p>Under the asymptotic properties of MLE (<a href="#thm-mleasymptotics" class="quarto-xref">theorem&nbsp;<span>3.3</span></a>), we know the estimator will be normally distributed. Using this fact, and the variance of MLE (<a href="#thm-mlevariance" class="quarto-xref">theorem&nbsp;<span>3.2</span></a>), we can do hypothesis testing with one parameter with the wald test:</p>
<div id="def-waldtest" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 3.6 (Wald Test)</strong></span> The wald test can determine if a parameter estimated by MLE is statistically significant. The wald test statistic is given by</p>
<p><span class="math display">\[
W = \left( \frac{\hat\theta - H_0}{se(\hat\theta)} \right)^2 = \frac{(\hat\theta - H_0)^2}{\V \hat\theta}
\]</span></p>
<p>Where <span class="math inline">\(H_0\)</span> is the value of <span class="math inline">\(\theta\)</span> established by our null hypothesis. We then use a <span class="math inline">\(\chi^2\)</span> distribution with 1 degrees of freedom to obtain the p-value.</p>
</div>
<p><br></p>
<p>If our p-value is less than 0.05, we can conclude that our null hypothesis is incorrect. We could also use a z-test, which would produce the exact same result as the wald test:</p>
<p><span class="math display">\[
z = \frac{\hat\theta - H_0}{se(\hat\theta)}
\]</span></p>
<p>And we consult a standard normal distribution <span class="math inline">\(\mathcal N(0, 1)\)</span> to get a p-value, and the interpretation is the same as the wald test.</p>
<p><br></p>
<p>For testing multiple coefficients at once, we can use the Likelihood Ratio test. The likelihood ratio test compares two models:</p>
<ul>
<li><span class="math inline">\(M_0 : Y_t = \beta_0 + \sum\limits_{j=1}^g \beta_j X_{tj} + \eps_i\)</span> (the smaller null model with <span class="math inline">\(g\)</span> parameters).</li>
<li><span class="math inline">\(M_a : Y_t = \beta_0 + \sum\limits_{j=1}^g \beta_j X_{tj} + \sum\limits_{j=g+1}^p \beta_j X_{tj} \eps_i\)</span> (the bigger model with the original <span class="math inline">\(g\)</span> parameters in the null + additional parameters up to <span class="math inline">\(p\)</span>).</li>
</ul>
<p>To compare the two models, we will use the likelihood function <span class="math inline">\(L\)</span>. After all, the likelihood <span class="math inline">\(L\)</span> (<a href="#def-likelihood" class="quarto-xref">definition&nbsp;<span>3.1</span></a>) gives us the probability of observing a sample given parameters <span class="math inline">\(\b\theta\)</span>. If the probability is higher, that likely means our model is better. Thus, the likelihood ratio test compares the difference between the likelihoods of two models.</p>
<div id="def-likelihoodratio" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 3.7 (Likelihood Ratio Test)</strong></span> The likelihood ratio test compares a smaller model <span class="math inline">\(M_0\)</span> and larger model <span class="math inline">\(M_a\)</span>. The test statistic <span class="math inline">\(L^2\)</span> is given by</p>
<p><span class="math display">\[
L^2 = 2 \log \left(\frac{L_a}{L_0}\right) = 2 \log (L_a) - 2 \log (L_0)
\]</span></p>
<p>Where <span class="math inline">\(L\)</span> is the likelihood of the model given by the likelihood function. Then, we consult a <span class="math inline">\(\chi^2\)</span> distribution with degrees of freedom equal to the number of extra parameters in model <span class="math inline">\(M_a\)</span> compared to <span class="math inline">\(M_0\)</span>.</p>
</div>
<p><br></p>
<p>If the p-value is less than 0.05, that means our <span class="math inline">\(M_a\)</span> model (and all of its extra coefficients) are statistically significant.</p>
<p><br></p>
</section>
<section id="information-criterion-statistics" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="information-criterion-statistics"><span class="header-section-number">3.9</span> Information Criterion Statistics</h2>
<p>The previously discussed likelihood ratio test (<a href="#def-likelihoodratio" class="quarto-xref">definition&nbsp;<span>3.7</span></a>) allows us to compare bigger models to smaller models, as long as the bigger model has all of the smaller model’s parameters. However, what if we want to choose between models with different parameters?</p>
<p>Recall that likelihood function <span class="math inline">\(L\)</span> (<a href="#def-likelihood" class="quarto-xref">definition&nbsp;<span>3.1</span></a>) is the probability of observing a particular sample given parameters <span class="math inline">\(\b\theta\)</span>. If the probability is higher, that likely means our model is better. Thus, likelihood <span class="math inline">\(L\)</span> allows us to compare different models.</p>
<p>However, we often prefer simple models over more complex models. If we have two models with the same likelihood <span class="math inline">\(L\)</span>, but one has 40 parameters and the other has 5, you would prefer the one with 5, as it is far more efficient and achieves the same performance.</p>
<p>The information criterion statistics use an adjusted likelihood <span class="math inline">\(L\)</span> accounting for the number of parameters to avoid good models that don’t have too many parameters. The most commonly used is the Akaike’s Information Criterion.</p>
<div id="def-AIC" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 3.8 (AIC)</strong></span> Akaike’s Information Criterion can be used to measure the fit of models fitted with MLE.</p>
<p><span class="math display">\[
AIC = -2 \log L + 2p
\]</span></p>
<p>Where <span class="math inline">\(L\)</span> is the likelihood of the model, and <span class="math inline">\(p\)</span> is the number of parameters in the model. The lower the AIC is, the better the model is considered.</p>
</div>
<p><br></p>
<p>AIC can be used to compare different models, including models that are not nested like in the likelihood ratio test. However, unlike <span class="math inline">\(R^2\)</span>, it does not have a real “substantive/real-world” meaning. The value itself means very little. An alternative to AIC is the Bayesian Information Criterion.</p>
<div id="def-bic" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 3.9 (BIC)</strong></span> Bayesian Information Criterion can be used to measure the fit of models fitted with MLE.</p>
<p><span class="math display">\[
BIC = p \log n - 2 \log L
\]</span></p>
<p>Where <span class="math inline">\(p\)</span> is the number of parameters in the model, <span class="math inline">\(n\)</span> is the number of observations, and <span class="math inline">\(L\)</span> is the likelihood of the given model. The lower the BIC is, the better the model is considered.</p>
</div>
<p><br></p>
<p>The BIC tends to penalise extra parameters more strongly than AIC. Generally, when comparing two models, AIC and BIC will agree.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./inference.html" class="pagination-link" aria-label="Statistical Inference">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical Inference</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./ols.html" class="pagination-link" aria-label="Least Squares Theory">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Least Squares Theory</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>