<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Classical Least Squares Theory – Political Science &amp; Political Economy</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="mathjax-config.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">
      Kevin's PSPE Resources
      </li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Kevin’s PSPE Resources</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
 <span class="menu-text">Quantitative Methods (Causal Inference)</span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 The Classical Linear Model</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 More Least Squares Theory</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 Causal Frameworks</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 Randomised Controlled Trials</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 Selection on Observables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant6.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 Instrumental Variables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant7.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7 Regression Discontinuity</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quant8.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8 Differences-in-Differences</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./games.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guide to Game Theory</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./math.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Background Mathematics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Background Statistics</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
 <span class="menu-text">More Statistical Models</span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model1.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Logistic Regression Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model2.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Regression for Counts</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Forecasting and Prediction Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Unsupervised Learning Methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Factor Analysis Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model6.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Latent Trait Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model7.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Latent Class Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model8.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Structural Equation Models</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On This Page</h2>
   
  <ul>
  <li><a href="#ordinary-least-squares-estimator" id="toc-ordinary-least-squares-estimator" class="nav-link active" data-scroll-target="#ordinary-least-squares-estimator"><strong>Ordinary Least Squares Estimator</strong></a>
  <ul class="collapse">
  <li><a href="#deriving-the-estimator" id="toc-deriving-the-estimator" class="nav-link" data-scroll-target="#deriving-the-estimator">Deriving the Estimator</a></li>
  <li><a href="#projection-and-residual-maker" id="toc-projection-and-residual-maker" class="nav-link" data-scroll-target="#projection-and-residual-maker">Projection and Residual Maker</a></li>
  <li><a href="#orthogonal-projection-of-ols" id="toc-orthogonal-projection-of-ols" class="nav-link" data-scroll-target="#orthogonal-projection-of-ols">Orthogonal Projection of OLS</a></li>
  <li><a href="#error-covariance-matrix" id="toc-error-covariance-matrix" class="nav-link" data-scroll-target="#error-covariance-matrix">Error Covariance Matrix</a></li>
  <li><a href="#homoscedasticity-and-heteroscedasticity" id="toc-homoscedasticity-and-heteroscedasticity" class="nav-link" data-scroll-target="#homoscedasticity-and-heteroscedasticity">Homoscedasticity and Heteroscedasticity</a></li>
  </ul></li>
  <li><a href="#sample-properties-of-ols" id="toc-sample-properties-of-ols" class="nav-link" data-scroll-target="#sample-properties-of-ols"><strong>Sample Properties of OLS</strong></a>
  <ul class="collapse">
  <li><a href="#ols-as-an-unbiased-estimator" id="toc-ols-as-an-unbiased-estimator" class="nav-link" data-scroll-target="#ols-as-an-unbiased-estimator">OLS as an Unbiased Estimator</a></li>
  <li><a href="#deriving-variance" id="toc-deriving-variance" class="nav-link" data-scroll-target="#deriving-variance">Deriving Variance</a></li>
  <li><a href="#gauss-markov-theorem" id="toc-gauss-markov-theorem" class="nav-link" data-scroll-target="#gauss-markov-theorem">Gauss-Markov Theorem</a></li>
  <li><a href="#asymptotic-consistency-of-ols" id="toc-asymptotic-consistency-of-ols" class="nav-link" data-scroll-target="#asymptotic-consistency-of-ols">Asymptotic Consistency of OLS</a></li>
  </ul></li>
  <li><a href="#regression-anatomy-and-specification" id="toc-regression-anatomy-and-specification" class="nav-link" data-scroll-target="#regression-anatomy-and-specification"><strong>Regression Anatomy and Specification</strong></a>
  <ul class="collapse">
  <li><a href="#partitioned-regression-model" id="toc-partitioned-regression-model" class="nav-link" data-scroll-target="#partitioned-regression-model">Partitioned Regression Model</a></li>
  <li><a href="#omitted-variable-bias" id="toc-omitted-variable-bias" class="nav-link" data-scroll-target="#omitted-variable-bias">Omitted Variable Bias</a></li>
  </ul></li>
  <li><a href="#method-of-moments-estimator" id="toc-method-of-moments-estimator" class="nav-link" data-scroll-target="#method-of-moments-estimator"><strong>Method of Moments Estimator</strong></a>
  <ul class="collapse">
  <li><a href="#method-of-moments" id="toc-method-of-moments" class="nav-link" data-scroll-target="#method-of-moments">Method of Moments</a></li>
  <li><a href="#population-mean-estimator" id="toc-population-mean-estimator" class="nav-link" data-scroll-target="#population-mean-estimator">Population Mean Estimator</a></li>
  <li><a href="#ols-as-a-method-of-moments" id="toc-ols-as-a-method-of-moments" class="nav-link" data-scroll-target="#ols-as-a-method-of-moments">OLS as a Method of Moments</a></li>
  </ul></li>
  <li><a href="#maximum-likelihood-estimator" id="toc-maximum-likelihood-estimator" class="nav-link" data-scroll-target="#maximum-likelihood-estimator"><strong>Maximum Likelihood Estimator</strong></a>
  <ul class="collapse">
  <li><a href="#likelihood-functions" id="toc-likelihood-functions" class="nav-link" data-scroll-target="#likelihood-functions">Likelihood Functions</a></li>
  <li><a href="#properties-and-information-criterion" id="toc-properties-and-information-criterion" class="nav-link" data-scroll-target="#properties-and-information-criterion">Properties and Information Criterion</a></li>
  <li><a href="#ols-as-a-maximum-likelihood-estimator" id="toc-ols-as-a-maximum-likelihood-estimator" class="nav-link" data-scroll-target="#ols-as-a-maximum-likelihood-estimator">OLS as a Maximum Likelihood Estimator</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Classical Least Squares Theory</h1>
<p class="subtitle lead">Chapter 3, Quantitative Methods (Causal Inference)</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Last chapter, we discussed the multiple linear regression model, and how it can help us measure relationships between explanatory and outcome variables.</p>
<p>This chapter introduces some key theory regarding the ordinary least squares estimator behind linear regression. Topics covered includes properties of estimators, the OLS estimator, and the Method of Moments estimator.</p>
<p>Use the right sidebar for quick navigation. This chapter is heavy on linear algebra, so consulting the linear algebra reference is useful.</p>
<hr>
<section id="ordinary-least-squares-estimator" class="level1">
<h1><strong>Ordinary Least Squares Estimator</strong></h1>
<section id="deriving-the-estimator" class="level3">
<h3 class="anchored" data-anchor-id="deriving-the-estimator">Deriving the Estimator</h3>
<p>Our <a href="./quant2.html">linear regression</a> model, and the fitted values <span class="math inline">\(\hat y\)</span>, take the following form:</p>
<p><span class="math display">\[
y = X\beta + \eps, \quad \hat y = X\hat\beta
\]</span></p>
<p>OLS minimises the <a href="./quant2.html#estimation-process">sum of squared residuals</a> <span class="math inline">\(S(\hat{\boldsymbol\beta})\)</span> - the differences between the actual <span class="math inline">\(\mathbf y\)</span> and our predicted <span class="math inline">\(\hat{\mathbf y}\)</span>:</p>
<p><span class="math display">\[
\begin{align}
S(\hat\beta) &amp;  = (y - \hat y)^\top (y - \hat y)\\
&amp; = (y - \color{blue}{X \hat\beta}\color{black} )^\top (y - \color{blue}{X \hat\beta}\color{black})  &amp;&amp; (\because \color{blue}{\hat y = X \hat\beta}\color{black})\\
&amp; = y^\top y - \hat\beta^\top X^\top y - y^\top X \hat\beta +  \hat\beta^\top X^\top X \hat\beta &amp;&amp; (\text{distribute out}) \\
&amp; = y^\top y \ \color{blue}{-  2 \hat\beta^\top X^\top y} \color{black}  +  \underbrace{\hat\beta^\top X^\top X \hat\beta}_{\text{quadratic}} &amp;&amp; (\because \color{blue}{- \hat\beta^\top X^\top y - y^\top X \hat\beta = - 2 \hat\beta^\top X^\top y} \color{black})
\end{align}
\]</span></p>
<p>Now, let us take the gradient to find the first order condition:</p>
<p><span class="math display">\[
\frac{\partial S(\hat\beta)}{\partial \hat\beta} = -2 X^\top y + 2 X^\top X \hat\beta = 0
\]</span></p>
<p>When assuming <span class="math inline">\(X^\top X\)</span> is invertable (which is true if <span class="math inline">\(X\)</span> is full rank), we can isolate <span class="math inline">\(\hat\beta\)</span> to find the solution to OLS:</p>
<p><span id="eq-ols"><span class="math display">\[
\begin{align}
-2 X^\top y + 2 X^\top X \hat\beta &amp; = 0 \\
2 X^\top X \hat\beta &amp; = 2 X^\top y &amp;&amp; ( + 2X^\top y \text{ to both sides}) \\
\hat\beta &amp; = (2X^\top X)^{-1} -2 X^\top y &amp;&amp; (\times (2X^\top X)^{-1} \text{ to both sides}) \\
\hat\beta &amp; = (X^\top X)^{-1} X^\top y &amp;&amp; (2^{-1}, 2 \text{ cancel out})
\end{align}
\tag{1}\]</span></span></p>
<p>Those are our coefficient solutions to OLS.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Alternative Derivation for Simple Linear Regression
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Currently, we are deriving the first order conditions for multiple linear regression using linear algebra.</p>
<p>For simple linear regression (with one explanatory variable), we can use summation notation. Recall our sum of squared residuals in summation form:</p>
<p><span class="math display">\[
SSR = S(\hat\beta_0, \hat\beta_1)= \sum\limits_{i=1}^n(Y_i - \hat\beta_0 - \hat\beta_1X_i)^2
\]</span></p>
<p>We want to minimise the SSR in respect to both <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>. We can do this by finding our first order conditions:</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial S(\hat\beta_0, \hat\beta_1)}{\partial \hat\beta_0} &amp; = \sum\limits_{i=1}^n(Y_i - \hat\beta_0 - \hat\beta_1X_i) = 0 \\
\frac{\partial S(\hat\beta_0, \hat\beta_1)}{\partial \hat\beta_1} &amp; = \sum\limits_{i=1}^n X_i(Y_i - \hat\beta_0 - \hat\beta_1X_i) = 0 \\
\end{align}
\]</span></p>
<p>These conditions create a system of equations, which you can solve for the OLS solutions of <span class="math inline">\(\widehat{\beta_0}\)</span> and <span class="math inline">\(\widehat{\beta_1}\)</span>. I will not show it step by step, as it is tedious (and not that important). The OLS solutions are</p>
<p><span class="math display">\[
\begin{align}
\hat\beta_0 &amp; = \bar Y - \widehat{\beta_1} \bar X \\
\hat\beta_1 &amp; = \frac{\sum_{i=1}^n(X_i - \bar X)(Y_i - \bar Y)}{\sum_{i=1}^n(X_i - \bar X)^2} = \frac{Cov(X_i, Y_i)}{\V Y_i}
\end{align}
\]</span></p>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="projection-and-residual-maker" class="level3">
<h3 class="anchored" data-anchor-id="projection-and-residual-maker">Projection and Residual Maker</h3>
<p>We can use the OLS solution from <a href="#eq-ols" class="quarto-xref">Equation&nbsp;1</a> to get our fitted values <span class="math inline">\(\hat{y}\)</span>:</p>
<p><span id="eq-projection"><span class="math display">\[
\begin{align}
\hat y &amp; = X\hat\beta \\
&amp; = X \color{blue}{(X^\top X)^{-1}X^\top y} &amp;&amp; \color{black}(\because \color{blue}{\hat\beta = (X^\top X)^{-1}X^\top y} \color{black}) \\
&amp; = \color{red}{P}\color{black}y &amp;&amp; (\because \color{red}{P:= X(X^\top X)^{-1}X^\top})
\end{align}
\tag{2}\]</span></span></p>
<p>Matrix <span class="math inline">\(\color{red}{P}\)</span>, called the <strong>projection matrix</strong>, is a matrix operator that performs the <a href="./math.html#linear-mappings-and-combinations">linear mapping</a> <span class="math inline">\(y \rightarrow \hat{ y}\)</span>.</p>
<p>We can also use the OLS solution from <a href="#eq-ols" class="quarto-xref">Equation&nbsp;1</a> to get our residuals <span class="math inline">\(\hat{\eps}\)</span> (note <span class="math inline">\(I\)</span> is the identity matrix):</p>
<p><span class="math display">\[
\begin{align}
\hat\eps &amp; = y - \hat y  \\
&amp; = y - \color{blue}{Py} &amp;&amp; \color{black}( \because \color{blue}{\hat y = Py}\color{black}) \\
&amp; = (I-P)y &amp;&amp; (\text{factor out y}) \\
&amp; = \color{purple}{M}\color{black}y &amp;&amp; (\because \color{purple}{M:= I - P}\color{black})
\end{align}
\]</span></p>
<p>Matrix <span class="math inline">\(\color{purple}{M}\)</span>, called the <strong>residual maker</strong>, is a matrix operator that performs the <a href="./math.html#linear-mappings-and-combinations">linear mapping</a> <span class="math inline">\(y \rightarrow \hat{\eps}\)</span>.</p>
<p>Both <span class="math inline">\(\color{red}{P}\)</span> and <span class="math inline">\(\color{purple}{M}\)</span> are <a href="./math.html#types-of-matrices">symmetric matrices</a>: <span class="math inline">\(P^\top = P, \ M^\top = M\)</span>. They are also both <a href="./math.html#types-of-matrices">idempotent matrices</a>: <span class="math inline">\(PP = P, \ MM = M\)</span>. We can prove this second statement using the first (I will only do it for <span class="math inline">\(P\)</span>, but the same applies for <span class="math inline">\(M\)</span>:</p>
<p><span id="eq-pp"><span class="math display">\[
\begin{align}
PP &amp; = X(X^\top X)^{-1} \underbrace{X^\top X(X^\top X)^{-1}}_{= I} X^\top \\
&amp; = X(X^\top X)^{-1} X^\top = P
\end{align}
\tag{3}\]</span></span></p>
<p><span class="math inline">\(\color{red}{ P}\)</span> and <span class="math inline">\(\color{purple}{ M}\)</span> are also <a href="./math.html#types-of-matrices">orthogonal</a> to each other - i.e.&nbsp;<span class="math inline">\(P^\top M = 0\)</span>:</p>
<p><span class="math display">\[
\begin{align}
P^\top M &amp; = \color{blue}{P}\color{black}M &amp;&amp; (\because \color{blue}{P^\top = P}\color{black}) \\
&amp; = P(\color{blue}{I-P}\color{black}) &amp;&amp; (\because \color{blue}{M:= I - P}\color{black}) \\
&amp; = P - PP &amp;&amp; \text{(distribute out)} \\
&amp; = P - \color{blue}{P} &amp;&amp; \color{black}(\because \color{blue}{PP = P}\color{black}) \\
&amp; = 0
\end{align}
\]</span></p>
<p><br></p>
</section>
<section id="orthogonal-projection-of-ols" class="level3">
<h3 class="anchored" data-anchor-id="orthogonal-projection-of-ols">Orthogonal Projection of OLS</h3>
<p>We know that our fitted values <span class="math inline">\(\hat{y}\)</span> are created as a linear combination of our explanatory variables <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
\hat Y_i = \hat\beta_0 + \hat\beta_1X_{i1} + \dots + \hat\beta_pX_{ip}
\]</span></p>
<p>That means, by the definition of <a href="./math.html#vector-spaces">vector spaces</a>, that our explanatory variable vectors <span class="math inline">\(x_1, x_2, \dots, x_p\)</span> span a space that includes our fitted values vector <span class="math inline">\(\hat{y}\)</span>. So, what <span class="math inline">\(\color{red}{ P}\)</span> is doing is taking our original data vector <span class="math inline">\(y\)</span>, and projecting it into the space spanned by our explanatory variables <span class="math inline">\(X\)</span> (called the <strong>column space</strong>).</p>
<p>We can see in the figure below, our observed <span class="math inline">\(y\)</span> vector is being projected onto the blue plane spanned by <span class="math inline">\(X\)</span> to create our fitted values vector <span class="math inline">\(\hat{y}\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-880030792.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>Residual maker matrix <span class="math inline">\(\color{purple}{M}\)</span> projects <span class="math inline">\(y\)</span> onto the space orthogonal to the column space of <span class="math inline">\(X\)</span> to get our residuals <span class="math inline">\(\hat{\eps}\)</span>. We can see this in the figure above, where the residuals vector (notated <span class="math inline">\(\mathbf e\)</span> in the figure) is orthogonal/perpendicular to the space of <span class="math inline">\(\mathbf X\)</span>.</p>
<p><br></p>
</section>
<section id="error-covariance-matrix" class="level3">
<h3 class="anchored" data-anchor-id="error-covariance-matrix">Error Covariance Matrix</h3>
<p>Aside from the population parameters <span class="math inline">\(\beta\)</span>, there is another part of the linear model that needs to be estimated: the population <strong>covariance matrix</strong> of error terms <span class="math inline">\(\eps_1, \dots, \eps_n\)</span>:</p>
<p><span class="math display">\[
\underbrace{\V(\eps|X)}_{\mathrm{cov. \ matrix}} = \begin{pmatrix}
\V\eps_1 &amp; cov(\eps_1, \eps_2) &amp; cov(\eps_1, \eps_3) &amp; \dots \\
cov(\eps_2, \eps_1) &amp; \V\eps_2 &amp; cov(\eps_2, \eps_3) &amp; \dots \\
cov(\eps_3, \eps_1) &amp; cov(\eps_3, u_2) &amp; \V\eps_3 &amp; \vdots \\
\vdots &amp; \vdots &amp; \dots &amp; \ddots
\end{pmatrix}
\]</span></p>
<p>Under the assumption of <strong>independence of observations</strong> (a key assumption of the linear model), the covariance elements should all equal 0, i.e.&nbsp;<span class="math inline">\(cov(\eps_i, \eps_k) = 0, \ \forall \ i, k\)</span>. This assumption is also called <strong>no autocorrelation</strong>.</p>
<p>Thus, under this assumption, we have a diagonal matrix.</p>
<p><span class="math display">\[
\underbrace{\V(\eps| X)}_{\mathrm{cov. \ matrix}} = \begin{pmatrix}
\V\eps_1 &amp; 0 &amp; 0 &amp; \dots \\
0 &amp; \V\eps_2 &amp; 0 &amp; \dots \\
0 &amp; 0 &amp; \V \eps_3 &amp; \vdots \\
\vdots &amp; \vdots &amp; \dots &amp; \ddots
\end{pmatrix}
\]</span></p>
<p>We are not really going to discuss what happens when autocorrelation is present, as generally for many of our purposes, ruling out autocorrelation is okay. However, if you are interested in time series (common in economics), or spatial statistics, these are types of data that frequently have autocorrelation issues, and this creates further complications.</p>
<p><br></p>
</section>
<section id="homoscedasticity-and-heteroscedasticity" class="level3">
<h3 class="anchored" data-anchor-id="homoscedasticity-and-heteroscedasticity">Homoscedasticity and Heteroscedasticity</h3>
<p>Now, there are two possible forms of our covariance matrix of errors. <strong>Homoscedasticity</strong> assumes that every single unit <span class="math inline">\(i\)</span> has the same variance in error <span class="math inline">\(\V \eps_i = \sigma^2\)</span>. Or in other words, the error of <span class="math inline">\(\eps_i\)</span> does is independent of which unit <span class="math inline">\(i\)</span> (which also means <span class="math inline">\(\eps_i \perp\!\!\!\perp X\)</span>). This assumption (with no autocorrelation) is also called <strong>Spherical Errors</strong>.</p>
<p><span id="eq-homo"><span class="math display">\[
\V(\eps | X) = \sigma^2 I_n = \begin{pmatrix}
\sigma^2 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \sigma^2 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; \dots &amp; \sigma^2
\end{pmatrix}
\tag{4}\]</span></span></p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Visualisation of <span class="math inline">\(\sigma^2\)</span>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Below is a figure illustrating different residual standard deviations, with the same best-fit line.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-2649765153.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
</div>
</div>
</div>
<p><span class="math inline">\(\sigma^2\)</span> is a population parameter. We can estimate it with the following unbiased estimator <span class="math inline">\(s^2 = \frac{\hat{\eps}^\top \hat{\eps}}{n-k-1}\)</span>. This is the reason we use a t-distribution in hypotheses tests - to account for the uncertainty of this estimator. We will show this later when deriving variance.</p>
<p><strong>Heteroscedasticity</strong> is when we do not believe the assumption of a constant variance for all units. Instead, we assume each unit <span class="math inline">\(i = 1, \dots, n\)</span> has their own variance <span class="math inline">\(\sigma^2_1, \dots, \sigma^2_n\)</span>, which also implies that <span class="math inline">\(\eps_i\)</span> is dependent on <span class="math inline">\(X\)</span> values for unit <span class="math inline">\(i\)</span>.</p>
<p><span id="eq-hetero"><span class="math display">\[
\V(\eps| X) = \begin{pmatrix}
\sigma^2_1 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \sigma^2_2 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; \dots &amp; \sigma^2_n
\end{pmatrix}
\tag{5}\]</span></span></p>
<p>We can estimate each individual population parameter <span class="math inline">\(\sigma^2_i\)</span> with <span class="math inline">\(s^2_i = \hat\eps_i^2\)</span>.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Visualisation of Homoscedasticity
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>An easy way to identify homoscedasticity is to look at a residual plot (just the plot of all <span class="math inline">\(\hat\eps_i\)</span>):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-1713529842.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%"></p>
</figure>
</div>
<p>Notice how the homoscedasticity residuals seem to have the same up-down variance, no matter the value of <span class="math inline">\(X_i\)</span>.</p>
<p>The heteroscedasticity residuals have a clear pattern - the up-down variance is smaller when <span class="math inline">\(X_i\)</span> is smaller, and the up-down variance is larger when <span class="math inline">\(X_i\)</span> is larger.</p>
<p>Essentially, if you see a pattern in the residual plot, it is likely heteroscedasticity.</p>
</div>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="sample-properties-of-ols" class="level1">
<h1><strong>Sample Properties of OLS</strong></h1>
<section id="ols-as-an-unbiased-estimator" class="level3">
<h3 class="anchored" data-anchor-id="ols-as-an-unbiased-estimator">OLS as an Unbiased Estimator</h3>
<p>OLS is an <a href="./quant1.html#finite-sample-properties-of-estimators">unbiased estimator</a> of the relationship between any <span class="math inline">\(X_{ij}\)</span> and <span class="math inline">\(Y_i\)</span> under 4 conditions:</p>
<ol type="1">
<li><strong>Linearity</strong> in parameters: the population model can be modelled as <span class="math inline">\(y = X\beta + \eps\)</span>.</li>
<li><strong>Random Sampling</strong>: our sample is randomly sampled (implying independence of observations).</li>
<li><strong>No Perfect Multicolinearity</strong>: There is no exact linear relationships between the regressors. This ensures that <span class="math inline">\(X^\top X\)</span> is invertible, which is required for the derivation of OLS.</li>
<li><strong>Zero Conditional Mean</strong>: <span class="math inline">\(\E(\eps| X) = 0\)</span>. This implies that no <span class="math inline">\(X_{ij}\)</span> is correlated with <span class="math inline">\(\eps\)</span> (exogeneity), and no function of multiple regressors is correlated with <span class="math inline">\(\eps\)</span>.</li>
</ol>
<p>Let us prove OLS is unbiased - i.e.&nbsp;<span class="math inline">\(\E\hat\beta = \beta\)</span>. Let us manipulate our OLS solution:</p>
<p><span id="eq-simplify"><span class="math display">\[
\begin{align}
\hat\beta &amp; = (X^\top X)^{-1} X^\top y \\
&amp; = (X^\top X)^{-1} X^\top(\color{blue}{X\beta + \eps}\color{black}) &amp;&amp; (\because \color{blue}{y = X\beta + \eps}\color{black}) \\
&amp; = \underbrace{(X^\top X)^{-1} X^\top X}_{= \ I}\beta + (X^\top X)^{-1}X^\top \eps &amp;&amp; \text{(multiply out)} \\
&amp; = \beta + (X^\top X)^{-1}X^\top \eps
\end{align}
\tag{6}\]</span></span></p>
<p>Now, let us take the expectation of <span class="math inline">\(\hat\beta\)</span> conditional on <span class="math inline">\(X\)</span>. Remember condition 4, <span class="math inline">\(\E(\eps | X) = 0\)</span>:</p>
<p><span class="math display">\[
\E(\hat\beta | X) = \beta + (X^\top X)^{-1} \underbrace{\E(\eps | X)}_{= \ 0} \  = \ \beta
\]</span></p>
<p>Now, we can use the <a href="./quant1.html#expectation-and-variance">law of iterated expectations (LIE)</a> to conclude this proof:</p>
<p><span class="math display">\[
\begin{align}
\E \hat\beta &amp; = \E(\E(\hat\beta|X)) &amp;&amp; (\because \mathrm{LIE}) \\
&amp; = \E(\color{blue}{\beta}\color{black}) &amp;&amp; (\because \color{blue}{\E(\hat\beta | X = \beta)}\color{black}) \\
&amp; = \beta &amp;&amp; \text{(expectation of a constant)}
\end{align}
\]</span></p>
<p>Thus, OLS is unbiased under the 4 conditions above.</p>
<p><br></p>
</section>
<section id="deriving-variance" class="level3">
<h3 class="anchored" data-anchor-id="deriving-variance">Deriving Variance</h3>
<p>We want to find the variance of our estimator, <span class="math inline">\(\V(\hat\beta | X)\)</span>. First, let us start off where we left off in <a href="#eq-simplify" class="quarto-xref">Equation&nbsp;6</a> .</p>
<p><span class="math display">\[
\begin{align}
&amp; \hat\beta = \beta + (X^\top X)^{-1} X^\top \eps \\
&amp; \V(\hat\beta | X) = \V(\beta + (X^\top X)^{-1} X^\top \eps)
\end{align}
\]</span></p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lemma: Variance
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>If <span class="math inline">\(\eps\)</span> is an <span class="math inline">\(n\)</span> dimensional vector of random variables, <span class="math inline">\(c\)</span> is an <span class="math inline">\(m\)</span> dimensional vector, and <span class="math inline">\(B\)</span> is an <span class="math inline">\(n \times m\)</span> dimensional matrix with fixed constants, then the following is true:</p>
<p><span id="eq-lemma"><span class="math display">\[
\V(c + B\eps) =  B \V(\eps) B^\top
\tag{7}\]</span></span></p>
<p>I will not prove this lemma here, but it is provable.</p>
</div>
</div>
</div>
<p><span class="math inline">\(\beta\)</span> is a vector of fixed constants. <span class="math inline">\((X^\top X)^{-1} X^\top \eps\)</span> can be imagined as a matrix of fixed constants, since we are conditioning the variance on <span class="math inline">\(X\)</span> (so for each <span class="math inline">\(X\)</span>, it is fixed). With the Lemma above:</p>
<p><span class="math display">\[
\begin{align}
\V (\hat\beta | X) &amp; = (X^\top X)^{-1}X^\top \V(\eps|X) [(X^\top X)^{-1}X^\top]^{-1} &amp;&amp; \text{(lemma)} \\
&amp; = (X^\top X)^{-1}X^\top \V(\eps|X) \color{blue}{X(X^\top X)^{-1}} &amp;&amp; \color{black}(\because \color{blue}{[(X^\top X)^{-1}X^\top]^{-1} = X(X^\top X)^{-1}} \color{black})
\end{align}
\]</span></p>
<p>From here on, <a href="#homoscedasticity-and-heteroscedasticity">homoscedasticity and heteroscedasticity</a> matter. Let us first start by deriving variance with homoscedasticity, using the definition given by <a href="#eq-homo" class="quarto-xref">Equation&nbsp;4</a> :</p>
<p><span class="math display">\[
\begin{align}
\V (\hat\beta | X) &amp; = (X^\top X)^{-1}X^\top \color{blue}{\sigma^2I_n}\color{black}{X} (X^\top X)^{-1} &amp;&amp; (\because \color{blue}{\V(\eps|X) = \sigma^2 I_n}\color{black}) \\
&amp; = \color{blue}{\sigma^2}\color{black}{\underbrace{(X^\top X)^{-1}X^\top X}_{= \ I}(X^\top X)^{-1}} &amp;&amp; \text{(rearrange and simplify)} \\
&amp; = \sigma^2 (X^\top X)^{-1}
\end{align}
\]</span></p>
<p>Now, let us calculate the variance for when heteroscedasticity is present, as defined by <a href="#eq-hetero" class="quarto-xref">Equation&nbsp;5</a> :</p>
<p><span class="math display">\[
\V(\hat{\beta}| X)  = (X^\top X)^{-1} X^\top \color{blue}{\begin{pmatrix}
\sigma^2_1 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \sigma^2_2 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; \dots &amp; \sigma^2_n
\end{pmatrix}}\color{black} X ( X^\top X)^{-1}
\]</span></p>
<p>To calculate both sets of standard errors (normal and robust), we estimate <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\sigma_i^2\)</span> as discussed <a href="#error-variance-and-homoscedasticity">previously</a>. The standard errors are then the square root of our estimated variances.</p>
<p><br></p>
</section>
<section id="gauss-markov-theorem" class="level3">
<h3 class="anchored" data-anchor-id="gauss-markov-theorem">Gauss-Markov Theorem</h3>
<p>The Gauss-Markov Theorem states that the OLS estimator is the <strong>best linear unbiased estimator</strong> (BLUE) - the unbiased linear estimator with the lowest variance, under 5 conditions: linearity, random sampling, no perfect multicollinearity, zero-conditional mean, and <a href="#homoscedasticity-and-heteroscedasticity">homoscedasticity</a>.</p>
<p>Any linear estimator takes the form <span class="math inline">\(\tilde{\beta} = Cy\)</span>. For example, the OLS estimator is of the form <span class="math inline">\(\hat\beta = (X^\top X)^{-1} X^\top y\)</span>, which is the same as form as <span class="math inline">\(Cy\)</span> if you define <span class="math inline">\(C:= ( X^\top X)^{-1} X^\top\)</span>. For any linear estimator <span class="math inline">\(\tilde{\beta} = Cy\)</span> to be unbiased, we need to assume <span class="math inline">\(\color{red}{CX = I}\)</span>.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof <span class="math inline">\(CX = I\)</span> For a Unbiased Linear Estimator
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For any linear estimator <span class="math inline">\(\tilde{\beta} = Cy\)</span> to be unbiased, we need to assume <span class="math inline">\(\color{red}{CX = I}\)</span>. The proof of this is as follows:</p>
<p><span class="math display">\[
\begin{align}
\tilde\beta =  C &amp; (\color{blue}{C\beta + \eps}\color{black}) &amp;&amp; (\because \color{blue}{y = X\beta + \eps}\color{black}) \\
=  C &amp; X\beta + C\eps &amp;&amp; \text{(multiply out)} \\
\E(\tilde\beta | X) &amp; = \E(C X\beta + C\eps) \\
&amp; = CX\beta + C \underbrace{\E(\eps | X)}_{= \ 0} &amp;&amp; \text{(take constants out of exp.)} \\
&amp; = CX\beta \\
&amp; = \color{red}{I}\color{black}\beta = \beta &amp;&amp; (\because \color{red}{CX = I}\color{black}) \\
\E \tilde\beta &amp; = \E( \E(\tilde\beta|X)) &amp;&amp; \text{(law of iterated expect.)} \\
&amp; = \E(\color{blue}{\beta}\color{black}) &amp;&amp; (\because \color{blue}{\E(\tilde\beta|X) = \beta}\color{black}) \\
&amp; = \beta &amp;&amp; \text{(expect. of a constant)}
\end{align}
\]</span></p>
<p>Thus, we have shown <span class="math inline">\(\color{red}{CX = I}\)</span> is a necessary condition for any linear estimator <span class="math inline">\(\tilde{\beta} = Cy\)</span> to be unbiased.</p>
</div>
</div>
</div>
<p>Now, let us calculate the variance of <span class="math inline">\(\tilde{\beta}\)</span>, taking into consideration the lemma (<a href="#eq-lemma" class="quarto-xref">Equation&nbsp;7</a>) used in the OLS variance:</p>
<p><span class="math display">\[
\begin{align}
\V(\tilde\beta | X) &amp; = \V(Cy|X) \\
&amp; = \V(C(\color{blue}{X\beta + \eps}\color{black})|X) &amp;&amp; (\because \color{blue}{y = X\beta + \eps}\color{black}) \\
&amp; = \V(\underbrace{CX}_{= I}\beta + C\eps | X) &amp;&amp; \text{(multiply out)} \\
&amp; = \V(\beta + C\eps | X) \\
&amp; = C \V(\eps | X) C^\top &amp;&amp; \text{(using lemma)} \\
&amp; = C \color{blue}{\sigma^2 I_n} \color{black} C^\top &amp;&amp; (\mathrm{homoscedasticity} \ \color{blue}{\V(\eps|X) = \sigma^2 I_n}\color{black}) \\
&amp; = \sigma^2 CC^\top &amp;&amp; \text{(rearrange and simplify)}
\end{align}
\]</span></p>
<p>Now, we want to show that the variance of the OLS estimator <span class="math inline">\(\hat{\beta}\)</span> (under homoscedasticity) is smaller than any linear estimator <span class="math inline">\(\tilde{\beta}\)</span>. Let us find the difference between the variances of estimator <span class="math inline">\(\tilde{\beta}\)</span> and <span class="math inline">\(\hat{\beta}\)</span>. Note: since <span class="math inline">\(\color{red}{CX = I}\)</span>, the following is also true: <span class="math inline">\(\color{red}{ X^\top C^\top = (CX)^\top = I}\)</span>.</p>
<p><span class="math display">\[
\begin{align}
\V(\tilde\beta | X)  - \V(\hat\beta|X) &amp; = \sigma^2 CC^\top - \sigma^2 (X^\top X)^{-1} \\
&amp; = \sigma^2(CC^\top - (X^\top X)^{-1}) &amp;&amp; (\text{factor out }\sigma^2) \\
&amp; = \sigma^2(CC^\top - \color{red}{CX}\color{black}(X^\top X)^{-1} \color{red}{X^\top C^\top}\color{black}) &amp;&amp; (\because \color{red}{X^\top C^\top = CX = I}\color{black}) \\
&amp; = \sigma^2 C(I - X(X^\top X)^{-1} X^\top) C^\top &amp;&amp; (\text{factor out }C, C^\top) \\
&amp; = \sigma^2 C \color{blue}{M}\color{black}C^\top &amp;&amp; (\text{residual maker matrix } \color{blue}{M}\color{black})
\end{align}
\]</span></p>
<p>Since <span class="math inline">\(\sigma^2 CM C^\top\)</span> is positive semi-definite (I will not prove this, but it is provable with the properties of <span class="math inline">\(M\)</span> introduced earlier), we know that <span class="math inline">\(V(\tilde{\beta}| X) &gt; V(\hat{\beta}| X)\)</span>. Thus, OLS is BLUE under the Gauss-Markov Theorem.</p>
<p><br></p>
</section>
<section id="asymptotic-consistency-of-ols" class="level3">
<h3 class="anchored" data-anchor-id="asymptotic-consistency-of-ols">Asymptotic Consistency of OLS</h3>
<p>OLS is an <a href="./quant1.html#asymptotic-properties-of-estimators">asymptotically consistent estimator</a> of the relationship between any <span class="math inline">\(x_j\)</span> and <span class="math inline">\(y\)</span> under 4 conditions. These conditions are identical to the unbiasedness conditions EXCEPT condition 4, which is weakened from the original unbiasedness condition.</p>
<ol type="1">
<li><strong>Linearity</strong> (see unbiasedness)</li>
<li><strong>Random Sampling</strong> (…)</li>
<li><strong>No Perfect Multicolinearity</strong> (…)</li>
<li><strong>Zero Mean and Exogeneity</strong>: <span class="math inline">\(\E(\eps_i) = 0\)</span>, and <span class="math inline">\(Cov(x_i, \eps_i) = 0\)</span>, which implies <span class="math inline">\(E(X_i \eps_i) = 0\)</span>. This means that no regressor should be correlated with <span class="math inline">\(\mathbf u\)</span>. This is weaker than Zero-Conditional mean, since it means a function of regressors can be correlated with <span class="math inline">\(\eps_i\)</span>.</li>
</ol>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lemma: Vector Notation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The following statements are true (with <span class="math inline">\(x_i\)</span> being a vector and <span class="math inline">\(\eps_i\)</span> being a scalar):</p>
<p><span class="math display">\[
\begin{split}
&amp; X^\top X = \sum\limits_{i=1}^n x_i x_i^\top\\
&amp;  X^\top \mathbf \eps = \sum\limits_{i=1}^n x_i \eps_i
\end{split}
\]</span></p>
</div>
</div>
</div>
<p>Let us start of where we left of from <a href="#eq-simplify" class="quarto-xref">Equation&nbsp;6</a>. Using vector notation, <a href="./quant1.html#asymptotically-consistent-estimators">law of large numbers</a>, and zero-mean and exogeneity condition:</p>
<p><span class="math display">\[
\begin{align}
\hat\beta &amp; = \beta + (X^\top X)^{-1} X^\top \eps \\
&amp; = \beta \left( \sum\limits_{i=1}^n x_i x_i^\top \right)^{-1} \left( \sum\limits_{i=1}^n x_i \eps_i \right) &amp;&amp; \text{(vector notation)} \\
&amp; = \beta + \left( \frac{1}{n}\sum\limits_{i=1}^n x_i x_i^\top \right)^{-1} \left( \frac{1}{n} \sum\limits_{i=1}^n x_i \eps_i \right) &amp;&amp; (\left(\frac{1}{n}\right)^{-1}, \frac{1}{n} \text{ cancel out})
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
\mathrm{plim}\hat\beta &amp; = \beta + \left( \mathrm{plim} \frac{1}{n}\sum\limits_{i=1}^n x_i x_i^\top \right)^{-1} \left( \mathrm{plim}\frac{1}{n} \sum\limits_{i=1}^n x_i \eps_i \right) \\
&amp; = \beta + (\E(x_i x_i^\top))^{-1} \underbrace{\E(x_i \eps_i)}_{= 0} = \beta &amp;&amp; \text{(law of large numbers)}
\end{align}
\]</span></p>
<p>Thus, OLS is asymptotically consistent under the 4 conditions above.</p>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="regression-anatomy-and-specification" class="level1">
<h1><strong>Regression Anatomy and Specification</strong></h1>
<section id="partitioned-regression-model" class="level3">
<h3 class="anchored" data-anchor-id="partitioned-regression-model">Partitioned Regression Model</h3>
<p>We can split up matrix <span class="math inline">\(X\)</span> into two matrices - <span class="math inline">\(X_1\)</span> containing the regressors we care about, and <span class="math inline">\(X_2\)</span> containing regressors we do not care about. Vector <span class="math inline">\(\beta\)</span> will be split in the same way. Our partitioned model is:</p>
<p><span class="math display">\[
y = X_1 \beta_1 + X_2 \beta_2 + \eps
\]</span></p>
<p>Recall our “residual maker” matrix <span class="math inline">\(M\)</span>. First, note a unique property: <span class="math inline">\(\color{red}{MX = 0}\)</span>. Now, let us define the residual making matrix for the second part of the regression <span class="math inline">\(M_2\)</span>:</p>
<p><span class="math display">\[
M_2 = I - X_2 (X_2^\top X_2)^{-1}X_2^\top
\]</span></p>
<p>Now, let us multiply both sides of our above partitioned model by <span class="math inline">\(M_2\)</span>:</p>
<p><span class="math display">\[
\begin{align}
M_2 y &amp; = M2(X_1\beta_1 + X_2\beta_2 + \eps) \\
M_2 y &amp; = M_2X_1 \beta_1 + M_2 X_2 \beta_2 + M_2 \eps &amp;&amp; \text{(multiply out)} \\
M_2 y &amp; = M_2 X_1 \beta_1 + M_2 \eps &amp;&amp; (\because M_2X_2 = 0, \ \because \color{red}{MX = 0}\color{black})
\end{align}
\]</span></p>
<p>Now, let us denote <span class="math inline">\(\tilde{y} := M_2 y\)</span>, <span class="math inline">\(\tilde{X}_1: = M_2 X_1\)</span>, and error <span class="math inline">\(\tilde\eps := M_2 \eps\)</span>. Then we get the following regression equation and OLS coefficient estimates:</p>
<p><span class="math display">\[
\tilde y = \tilde X_1 \beta_1 + \tilde\eps
\]</span></p>
<p><span class="math display">\[
\hat\beta_1 = (\tilde X_1^\top \tilde X_1)^{-1}\tilde X_1 ^\top \tilde y
\]</span></p>
<p>Remember that vector <span class="math inline">\(\hat{\beta}_1\)</span> is our coefficient estimates for <span class="math inline">\(X_1\)</span>, the portion of <span class="math inline">\(X\)</span> we are interested in. This is equivalent to the coefficient estimates had we not partitioned the model.</p>
<p>Notice how in the formula, we have <span class="math inline">\(\tilde{X}_1\)</span>. What is <span class="math inline">\(\tilde{X}_1 := M_2 X_1\)</span>? Well, we know that $ M_2 X_2 = 0$. That tells us that any part of <span class="math inline">\(X_1\)</span> that was correlated to <span class="math inline">\(X_2\)</span> also became 0. <u>Thus, <span class="math inline">\(\tilde{X}_1\)</span> is the part of <span class="math inline">\(X_1\)</span> that is uncorrelated with <span class="math inline">\(X_2\)</span>.</u></p>
<p>Generalised, What this essentially means is that the coefficient estimates of OLS <span class="math inline">\(\hat\beta_j\)</span> actually measure the effect on <span class="math inline">\(Y_i\)</span> of the part of <span class="math inline">\(X_{ij}\)</span> uncorrelated with the other explanatory variables <span class="math inline">\(X_{i1}, \dots, x_{ip}\)</span>. Essentially, we are <strong>partialling out</strong> the effect of other variables. <u>This is why we can “control” for other variables when focusing on the coefficient of one (or a few) variables</u>.</p>
<p><br></p>
</section>
<section id="omitted-variable-bias" class="level3">
<h3 class="anchored" data-anchor-id="omitted-variable-bias">Omitted Variable Bias</h3>
<p>From the regression anatomy theorem, we know that <span class="math inline">\(\hat\beta_j\)</span> is the relationship of <span class="math inline">\(Y_i\)</span> and the part of <span class="math inline">\(X_{ij}\)</span> that is uncorrelated with all the other explanatory variables. That implies that if we omit a variable that is correlated with both <span class="math inline">\(X_{ij}\)</span> and <span class="math inline">\(Y_i\)</span>, that we will get a different (biased) coefficient estimate. This is called <strong>omitted variable bias</strong>.</p>
<p>Suppose there is some variable <span class="math inline">\(Z_i\)</span> that we have not included in a “short” regression. The actual, “true” regression of the population, would include this confounder <span class="math inline">\(Z_i\)</span></p>
<p><span class="math display">\[
\underbrace{y = X\beta + \eps}_{\text{short regression}}
\qquad \underbrace{y = X\beta + z\delta + \eps}_{\text{true regression with z} }
\]</span></p>
<p>The <a href="#deriving-the-estimator">OLS estimate</a> of the “short regression” excluding confounder <span class="math inline">\(Z_i\)</span> is:</p>
<p><span class="math display">\[
\begin{align}
\hat\beta &amp; = (X^\top X)^{-1}X^\top y \\
&amp; = (X^\top X)^{-1}X^\top(\color{blue}{X\beta + z\delta + \eps}\color{black}) &amp;&amp; (\because \color{blue}{y = X\beta + z\delta + \eps}\color{black}) \\
&amp; = \underbrace{(X^\top X)^{-1}X^\top X}_{= \ I}\beta + (X^\top X)^{-1}X^\top z\delta + (X^\top X)^{-1} X^\top \eps &amp;&amp; \text{(multiply out)} \\
&amp; = \beta + (X^\top X)^{-1}X^\top z\delta + (X^\top X)^{-1} X^\top \eps
\end{align}
\]</span></p>
<p>Now, let us find the expected value of <span class="math inline">\(\hat\beta\)</span>, which is conditional on <span class="math inline">\(X, z\)</span>, and simplify (using <a href="#ols-as-an-unbiased-estimator">zero conditional mean</a>):</p>
<p><span class="math display">\[
\begin{align}
\E(\hat\beta | X, z) &amp; = \beta + (X^\top X)^{-1}X^\top z \delta + (X^\top X)^{-1} X^\top \underbrace{\E(\eps | X, z)}_{= 0} \\
&amp; = \beta + (X^\top X)^{-1}X^\top z \delta
\end{align}
\]</span></p>
<p>Now, what if we had a regression of outcome variable being the confounder <span class="math inline">\(z\)</span>, on the explanatory variables <span class="math inline">\(X\)</span>, such that <span class="math inline">\(z = X\eta + u\)</span>. Our OLS estimate would have the solution:</p>
<p><span class="math display">\[
\hat\eta = (X^\top X)^{-1} X^\top z
\]</span></p>
<p>Now, we can plug <span class="math inline">\(\hat\eta\)</span> into our expected value of <span class="math inline">\(\hat\beta\)</span>. Assume our estimator <span class="math inline">\(\hat{\eta}\)</span> is unbiased:</p>
<p><span class="math display">\[
\begin{align}
\E(\hat\beta | X, z) &amp; = \beta + (X^\top X)^{-1}X^\top z \delta\\
&amp; = \beta + \color{blue}{\hat\eta}\color{black}\delta &amp;&amp; (\because \color{blue}{\hat\eta = (X^\top X)^{-1} X^\top z }\color{black}) \\
\E\hat\beta &amp; = \E(\E(\hat\beta|X, z)) &amp;&amp; \text{(law of iterated expect.)} \\
&amp; = E(\color{blue}{\beta + \hat\eta \delta}\color{black}) &amp;&amp; (\because \color{blue}{\E(\hat\beta|X, z) = \beta + \hat\eta\delta} \color{black}) \\
&amp; = \beta + \E\hat\eta \ \delta &amp;&amp; \text{(take out constants from exp.)} \\
&amp; = \beta + \eta\delta &amp;&amp; (\text{unbiased estimator } \E\hat\eta = \eta)
\end{align}
\]</span></p>
<p>Thus, we can see by not including confounder <span class="math inline">\(z\)</span> in our “short regression”, the estimator is now biased by <span class="math inline">\(\hat\eta \delta\)</span>. In the next chapter when we start discussing causality, we will see omitted confounders as a huge issue in our estimation.</p>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="method-of-moments-estimator" class="level1">
<h1><strong>Method of Moments Estimator</strong></h1>
<section id="method-of-moments" class="level3">
<h3 class="anchored" data-anchor-id="method-of-moments">Method of Moments</h3>
<p>The Method of Moments Estimator is another estimator of the true value of populations in the parameter. The estimator defines key population <strong>moments</strong> of interest - which are the population parameters written in terms of expected value functions set equal to 0.</p>
<p>Then, the Method of Moments uses the sample equivalents of the population moments to estimate the population parameter. For example, to estimate the population mean, the Method of Moments uses the sample mean.</p>
<p>In order to define a method of moments for a set of parameters <span class="math inline">\(\theta_1, \dots, \theta_p\)</span>, we need to specify at least one population moment per parameter. Or in other words, we must have more than <span class="math inline">\(p\)</span> population moments.</p>
<p>Our population moments can be defined as the expected value of some function <span class="math inline">\(m(\theta; Y_i)\)</span> that consists of both the variable <span class="math inline">\(Y_i\)</span> and our unknown parameter <span class="math inline">\(\theta\)</span>. The expectation of the function <span class="math inline">\(m(\theta; Y_i)\)</span> should equal 0.</p>
<p><span class="math display">\[
\E(m(\theta; Y_i)) = 0
\]</span></p>
<p>Our sample moments will be the sample analogues of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(Y_i\)</span>, which are <span class="math inline">\(\hat\theta\)</span> and <span class="math inline">\(Y_i\)</span>:</p>
<p><span class="math display">\[
\frac{1}{n}\sum\limits_{i=1}^n m(\hat\theta; Y_i) = 0
\]</span></p>
<p>Method of moments estimators are <a href="./quant1.html#asymptotic-properties-of-estimators">asymp</a><a href="quant1.html#asymptotic-properties-of-estimators">totically consistent</a>, because of the law of large numbers.</p>
<p><br></p>
</section>
<section id="population-mean-estimator" class="level3">
<h3 class="anchored" data-anchor-id="population-mean-estimator">Population Mean Estimator</h3>
<p>Let us say that we have some random variable <span class="math inline">\(y\)</span>, with a true population mean <span class="math inline">\(\mu\)</span>. We want to estimate <span class="math inline">\(\mu\)</span>, but we only have a sample of the population. How can we define <span class="math inline">\(\mu\)</span> in a moment of the form: <span class="math inline">\(\E(m(\mu, y)) = 0\)</span>? Well, we know <span class="math inline">\(\mu\)</span> is the expectation of <span class="math inline">\(Y_i\)</span>, so <span class="math inline">\(\mu = \E(Y_i)\)</span>. Since they are equal, <span class="math inline">\(\mu - E(y) = 0\)</span>. Thus, we can define the mean as a moment of the following condition:</p>
<p><span class="math display">\[
\E(Y_i - \mu) = 0
\]</span></p>
<p>The method of moments estimator uses the sample equivalent of the population moment. The sample equivalent of <span class="math inline">\(\mu\)</span>, is the sample mean <span class="math inline">\(\bar y\)</span>:</p>
<p><span class="math display">\[
\E(Y_i - \hat\mu) = \frac{1}{n}\sum\limits_{i=1}^n (Y_i - \hat\mu) = 0
\]</span></p>
<p>With this equation, we can then solve for <span class="math inline">\(\hat\mu\)</span>:</p>
<p><span class="math display">\[
\begin{align}
0 &amp; = \frac{1}{n}\sum\limits_{i=1}^n (Y_i - \hat\mu) \\
0 &amp; = \frac{1}{n}\sum\limits_{i=1}^nY_i - \frac{1}{n}\sum\limits_{i=1}^n \hat\mu  &amp;&amp; (\text{multiply out})\\
0 &amp; = \frac{1}{n}\sum\limits_{i=1}^nY_i - \frac{1}{n} n \hat\mu &amp;&amp;(\text{summation property of constant } \hat\mu)\\
0 &amp; = \bar Y - \hat \mu &amp;&amp; (\text{definition of mean }\frac{1}{n}\sum\limits_{i=1}^nY_i = \bar Y)\\
\hat\mu &amp; = \bar Y &amp;&amp; (+\hat\mu\text{ to both sides})
\end{align}
\]</span></p>
<p>So, we see the method of moments estimates our true population mean <span class="math inline">\(\mu\)</span>, with the sample mean <span class="math inline">\(\bar Y\)</span>. As a method of moments estimator, it is also asymptotically consistent.</p>
<p><br></p>
</section>
<section id="ols-as-a-method-of-moments" class="level3">
<h3 class="anchored" data-anchor-id="ols-as-a-method-of-moments">OLS as a Method of Moments</h3>
<p>OLS is a special case of the Method of Moments Estimator. Consider the bivariate regression model. The OLS estimator can be derived as a method of moments estimator, with 2 moments (expectation functions set equal to 0), one for each parameter (<span class="math inline">\(\beta_0, \beta_1\)</span>):</p>
<p><span class="math display">\[
\begin{split}
&amp; \E(Y_i-\beta_0 -\beta_1X_i) = E(\eps_i) = 0 \\
&amp; \E(X_i(Y_i - \beta_0 - \beta_1 X_i)) = E(x_i \eps_i) = 0
\end{split}
\]</span></p>
<p>The estimates of these moments would use the sample equivalents: <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>.</p>
<p><span class="math display">\[
\begin{split}
&amp; \E(Y_i-\hat\beta_0 -\hat\beta_1X_i) = 0 \\
&amp; \E(X_i(Y_i - \hat\beta_0 - \hat\beta_1 X_i)) = 0
\end{split}
\]</span></p>
<p>Now, recall our OLS minimisation conditions (simple linear regression, presented in the extra info box above). Since by definition, average/expectation is <span class="math inline">\(\E(x) = \frac{1}{n} \sum x_i\)</span>, we can rewrite as:</p>
<p><span class="math display">\[
\begin{split}
&amp; \sum\limits_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) = \ n \times \E(Y_i - \hat\beta_0 - \hat\beta_1X_i) = 0 \\
&amp; \sum\limits_{i=1}^n X_i (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) = \  n \times \E(X_i(Y_i - \hat\beta_0 - \hat\beta_1X_i)) = 0
\end{split}
\]</span></p>
<p>And since anything multiplied to a zero turns into zero, we can ignore the <span class="math inline">\(n\)</span> in the first order condition. Which as we can see, are the exact same minimisation conditions as the method of moments estimator.</p>
<p>Thus, the OLS estimator is a special case of the Method of Moments estimator, and they produce the same coefficients. This is an important property for the instrumental variables method that will be covered later.</p>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="maximum-likelihood-estimator" class="level1">
<h1><strong>Maximum Likelihood Estimator</strong></h1>
<section id="likelihood-functions" class="level3">
<h3 class="anchored" data-anchor-id="likelihood-functions">Likelihood Functions</h3>
<p>We have a set of population parameters <span class="math inline">\(\boldsymbol\theta\)</span> we want to estimate. This set of parameters determines our population distribution of <span class="math inline">\(y\)</span>, which we can describe with some <a href="./quant1.html#probability-density-functions">probability density function</a> <span class="math inline">\(\varphi(y, \boldsymbol\theta)\)</span>. For example, in linear regression, our population <span class="math inline">\(y\)</span> is determined by the population parameters <span class="math inline">\(\beta_0, \dots, \beta_k\)</span>.</p>
<p>When we are estimating parameters, we will have sample data with <span class="math inline">\(n\)</span> number of observations, with each observation <span class="math inline">\(i\)</span> having its own <span class="math inline">\(y_i\)</span> value. Thus, our sample looks something like <span class="math inline">\((y_1, \dots, y_n)\)</span>. Based on the probability density function of the population <span class="math inline">\(y\)</span>, the probability of selecting point <span class="math inline">\(y_1\)</span> from the population data is <span class="math inline">\(\varphi(y_1, \boldsymbol\theta)\)</span>, and the probability of selection point <span class="math inline">\(y_i\)</span> is <span class="math inline">\(\varphi(y_i, \boldsymbol\theta)\)</span>.</p>
<p>We know by the rules of probability, that the <a href="./math.html#basics-of-probability">probability of multiple independent events</a> is the product of their probabilities. Thus, the probability that we get a specific sample with <span class="math inline">\(y\)</span> values <span class="math inline">\((y_1, \dots ,y_n)\)</span>, based on the value of our population parameters <span class="math inline">\(\boldsymbol\theta\)</span> is given by the <strong>likelihood function</strong> <span class="math inline">\(L\)</span>:</p>
<p><span class="math display">\[
\begin{align}
L(\boldsymbol\theta, y_1, \dots y_n) &amp; = \varphi(y_1;\boldsymbol\theta) \times \varphi(y_2;\boldsymbol\theta) \times \dots \times \varphi(y_n;\boldsymbol\theta) \\
&amp; = \prod\limits_{i=1}^n \varphi(y_i, \boldsymbol\theta) \\
\end{align}
\]</span></p>
<p>We want to find some values of <span class="math inline">\(\boldsymbol\theta\)</span> that make it the highest probability we observe our sample <span class="math inline">\(y_1, \dots ,y_n\)</span>. This is done by maximising the likelihood function <span class="math inline">\(L\)</span>. However, maximising the actual likelihood function is very difficult, because of the product notation. Luckily, we can use the log of the likelihood function, which retains the same maximum/minimum points. Using the properties of logarithms, we can also rewrite our log-likelihood function in terms of summation notation, making maximisation far easier.</p>
<p><span id="eq-loglike"><span class="math display">\[
\begin{align}
\log L(\boldsymbol\theta, y_1, \dots y_n) &amp; = \log \left(\prod_{i=1}^n \varphi(y_i; \theta) \right) \\
&amp; = \log[\varphi(y_1, \boldsymbol\theta) \times \varphi(y_2, \boldsymbol\theta) \times \dots \times \varphi(y_n, \boldsymbol\theta)] &amp;&amp; \text{(expand product notation)} \\
&amp; = \log[\varphi(y_1, \boldsymbol\theta)] + \log [\varphi(y_2, \boldsymbol\theta)] + \dots + \log[\varphi(y_n, \boldsymbol\theta)] &amp;&amp; \text{(property of logs)} \\
&amp; = \sum\limits_{i=1}^n \log[\varphi(y_i, \boldsymbol\theta)] &amp;&amp; \text{(condense into sum)}
\end{align}
\tag{8}\]</span></span></p>
<p>We then maximise this log-likelihood function to obtain our estimates <span class="math inline">\(\hat{\boldsymbol\theta}\)</span>. Sometimes, maximisation is not possible with mathematics, so we need algorithms to do this.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Gradient Descent Algorithms
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In some more complex models, we cannot mathematically find the minimum of the log-likelihood function. So instead, we resort to a series of computer algorithms called gradient descent. The algorithm takes the following form:</p>
<ol type="1">
<li>We randomly choose values of <span class="math inline">\(\boldsymbol\theta\)</span> to start (let us notate the chosen as <span class="math inline">\(\boldsymbol\theta^*\)</span>), and calculate the likelihood <span class="math inline">\(L\)</span> with those chosen at <span class="math inline">\(\boldsymbol\theta^*\)</span>.</li>
<li>We then slightly shift the values of <span class="math inline">\(\boldsymbol\theta^*\)</span> upwards and downwards, calculating all the likelihoods. We see in which shift-direction does the likelihood <span class="math inline">\(L\)</span> increase the most.</li>
<li>Once we determine the direction that <span class="math inline">\(L\)</span> increases the most, we shift int hat direction to a new <span class="math inline">\(\boldsymbol\theta'\)</span> value. Once again, we slightly shift values of <span class="math inline">\(\boldsymbol\theta'\)</span> upwards and downwards, and see which shift-direction does the likelihood <span class="math inline">\(L\)</span> increase the most.</li>
<li>We keep repeating this process of moving in the direction that increases <span class="math inline">\(L\)</span> the most, shifting around in all directions at that point, and once again moving in the direction that increases <span class="math inline">\(L\)</span> the most.</li>
<li>We stop when we are at some point <span class="math inline">\(\boldsymbol\theta^!\)</span> where all directions of shits decrease <span class="math inline">\(L\)</span>. We are “at the top of the mountain”, and that becomes our estimate.</li>
</ol>
<p>This is a very simple gradient descent algorithm. You might point out that this algorithm only works if there is one global maximum, and no local maxima (since we would stop the algorithm at a local extrema if this were the case). This usually is not an issue since in common regression models (linear, logistic, poisson), their is only one global maximum. The figure below shows this:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-3075379377.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%"></p>
</figure>
</div>
<p>A solution for this problem of local maxima (which becomes more an issue with machine learning models) is to basically do the estimation algorithm multiple times, each time starting at some random <span class="math inline">\(\boldsymbol\theta^*\)</span>. Then, we find the time with the largest <span class="math inline">\(L\)</span>. This will in theory help us determine which are local and global maxima.</p>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="properties-and-information-criterion" class="level3">
<h3 class="anchored" data-anchor-id="properties-and-information-criterion">Properties and Information Criterion</h3>
<p>The Maximum Likelihood Estimator has a few very desirable properties for us.</p>
<ol type="1">
<li>The maximum likelihood estimator is <a href="./quant1.html#asymptotic-properties-of-estimators">asymp</a><a href="./quant1.html#asymptotically-consistent-estimators">totically consistent</a> <span class="math inline">\(plim(\hat\theta_{MLE}) = \theta\)</span>.</li>
<li>The maximum likelihood estimator is <a href="./quant1.html#asymptotic-properties-of-estimators">asymptotically normal</a>, which allows for statistical inference.</li>
<li>The maximum likelihood estimator also has the smallest asymptotic variance in the general class of estimators. Thus in large sample sizes, the MLE is asymptotically efficient.</li>
</ol>
<p>Another reason the maximum likelihood estimator is so useful is because it is relatively intuitive. The MLE is simply maximising the likelihood we observe our sample given the population parameters. It is not imposing some strange criteria of maximisation.</p>
<p>Finally, maximum likelihood estimation allows us to use <strong>Information Criterion</strong> statistics, which can tell us how good our model is. Recall that the likelihood function <span class="math inline">\(L\)</span> is essentially the probability of observing our sample data given our model and parameters. Thus, we should expect better specified models to have a higher likelihood <span class="math inline">\(L\)</span>.</p>
<p>You could compare the likelihood <span class="math inline">\(L\)</span> between models without any modification. In fact, the likelihood ratio test (similar to f-test but for MLE) does exactly this, and is commonly used in non-OLS estimated models (logistic, poisson, factor anlaysis, etc.).</p>
<p>However, statisticians don’t just want the best fitting model - they also want a model that balances complexity. This implies some punishment for adding a ton of extra variables. The most popular is <strong>Akaike’s Information Criterion (AIC)</strong>, which is calculated as follows:</p>
<p><span class="math display">\[
AIC = -2 \log L + 2k
\]</span></p>
<ul>
<li>Where <span class="math inline">\(L\)</span> is the likelihood of the model, and <span class="math inline">\(k\)</span> is the number of parameters.</li>
</ul>
<p>The lower the AIC, the better the model is considered. There are also alternatives, including <strong>Bayesian Information Criterion (BIC)</strong>, but these are less commonly used.</p>
<p><br></p>
</section>
<section id="ols-as-a-maximum-likelihood-estimator" class="level3">
<h3 class="anchored" data-anchor-id="ols-as-a-maximum-likelihood-estimator">OLS as a Maximum Likelihood Estimator</h3>
<p>One common question of students regarding OLS is - why least squares (SSR), and why not minimising the absolute values of errors (or the quartic of errors)? Well the answer is that <u>OLS is a maximum likelihood estimator</u> when assuming homoscedasticity and error term <span class="math inline">\(u \sim \mathcal N (0, \sigma^2)\)</span>.</p>
<p>We know in a simple linear regression, <span class="math inline">\(\mu = E(y|x) = \beta_0 - \beta_1 x_i\)</span>. Let us plug that into the <a href="./quant1.html#the-normal-distribution">probability density function of a normal distribution</a> to get the PDF of <span class="math inline">\(y\)</span> in a simple linear regression:</p>
<p><span class="math display">\[
\varphi(y) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{\left( -\frac{(y- \mu)^2}{2 \sigma^2}\right)}, \quad \varphi(y_i|\beta_0, \beta_1, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{\left( -\frac{1}{2 \sigma^2}(y_i - \beta_0 - \beta_1 x_i)^2\right)}
\]</span></p>
<p>By <a href="#eq-loglike" class="quarto-xref">Equation&nbsp;8</a>, the log-likelihood function of our sample for linear regression is:</p>
<p><span class="math display">\[
\begin{align}
\log L(y_i | \beta_0, \beta_1 \sigma^2) &amp; = \log \left(\prod\limits_{i=1}^n \varphi(y_i | \beta_0, \beta_1, \sigma^2) \right) \\
&amp; = \sum\limits_{i=1}^n \log \varphi(y_i, \beta_0, \beta_1, \sigma^2) &amp;&amp; (\text{from eq. (8)}) \\
&amp; = \sum\limits_{i=1}^n \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} e^{\left( -\frac{1}{2 \sigma^2}(y_i - \beta_0 - \beta_1 x_i)^2\right)} \right) \\
&amp; = \sum\limits_{i=1}^n \log \left( \frac{1}{\sqrt{2\pi\sigma^2}}\right) + \log\left( e^{\left( -\frac{1}{2 \sigma^2}(y_i - \beta_0 - \beta_1 x_i)^2\right)}\right) &amp;&amp; \text{(prop. of logs)} \\
&amp; = \sum\limits_{i=1}^n \log (1) - \log (\sqrt{2\pi\sigma^2})  + \log\left( e^{\left( -\frac{1}{2 \sigma^2}(y_i - \beta_0 - \beta_1 x_i)^2\right)}\right) &amp;&amp; \text{(prop. of logs)} \\
&amp; = \sum\limits_{i=1}^n \log (1) - \log (\sqrt{2\pi})  - \log(\sqrt{\sigma^2}) + \log\left( e^{\left( -\frac{1}{2 \sigma^2}(y_i - \beta_0 - \beta_1 x_i)^2\right)}\right) &amp;&amp; \text{(prop. of roots)} \\
&amp; = \sum\limits_{i=1}^n 0 - \frac{1}{2}\log ({2\pi})  - \frac{1}{2}\log(\sqrt{\sigma^2}) + \log\left( e^{\left( -\frac{1}{2 \sigma^2}(y_i - \beta_0 - \beta_1 x_i)^2\right)}\right) &amp;&amp; \text{(prop. of logs)} \\
&amp; = \sum\limits_{i=1}^n -\frac{1}{2} \log (2\pi) - \frac{1}{2} \log (\sigma^2) + \left( -\frac{1}{2 \sigma^2}(y_i - \beta_0 - \beta_1 x_i)^2\right) \\
&amp; = -\frac{n}{2} \log (2\pi) - \frac{n}{2} \log (\sigma^2)  -\frac{1}{2 \sigma^2}\sum\limits_{i=1}^n(y_i - \beta_0 - \beta_1 x_i)^2 &amp;&amp; \text{(prop. of sums)}
\end{align}
\]</span></p>
<p>Now, let us find the first-order conditions of this equation in respect to parameters <span class="math inline">\(\beta_0, \beta_1, \sigma^2\)</span> to maximise:</p>
<p><span class="math display">\[
\begin{align}
&amp; \frac{\partial \log L}{\partial \hat\beta_0} = \frac{1}{\sigma^2}\sum\limits_{i=1}^n (y_i - \hat\beta_0 - \hat\beta_1x_i) = 0\\
&amp; \frac{\partial \log L}{\partial \hat\beta_1} = \frac{1}{\sigma^2}\sum\limits_{i=1}^n x_i(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0\\
&amp; \frac{\partial \log L}{\partial \hat\sigma^2} = -\frac{n}{2 \hat\sigma^2} + \frac{1}{2\hat\sigma^4} \sum\limits_{i=1}^n (y_i - \hat\beta_0 - \hat\beta_1x_i) = 0\\
\end{align}
\]</span></p>
<p>We can ignore the third condition. If we look at the first two conditions, since anything times 0 equals 0, we can ignore the <span class="math inline">\(\frac{1}{\sigma^2}\)</span>. Ignoring those, we see we have the exact same <a href="#deriving-the-estimator">first order conditions as OLS</a>, which will yield the same estimates. This illustrates that OLS is a case of MLE, and the same intuitive reasoning of MLE can be applied to OLS.</p>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>