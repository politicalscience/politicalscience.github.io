<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Statistical Inference – Statistics for Political Science</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<link href="./ols.html" rel="next">
<link href="./random.html" rel="prev">
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="mathjax-config.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./random.html">Part I: Theoretical Statistics</a></li><li class="breadcrumb-item"><a href="./inference.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical Inference</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistics for Political Science</a> 
    </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Part I: Theoretical Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./random.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Random Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./inference.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Classical Least Squares</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gls.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Generalised Least Squares</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mle.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Maximum Likelihood</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Method of Moments</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./causal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Causal Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./identify.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Causal Identification</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Part II: Applied Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Generalised Linear Model</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul class="collapse">
  <li><a href="#data-generating-process" id="toc-data-generating-process" class="nav-link active" data-scroll-target="#data-generating-process"><span class="header-section-number">2.1</span> Data Generating Process</a></li>
  <li><a href="#sampling-and-estimators" id="toc-sampling-and-estimators" class="nav-link" data-scroll-target="#sampling-and-estimators"><span class="header-section-number">2.2</span> Sampling and Estimators</a></li>
  <li><a href="#sampling-distributions" id="toc-sampling-distributions" class="nav-link" data-scroll-target="#sampling-distributions"><span class="header-section-number">2.3</span> Sampling Distributions</a></li>
  <li><a href="#finite-sample-properties" id="toc-finite-sample-properties" class="nav-link" data-scroll-target="#finite-sample-properties"><span class="header-section-number">2.4</span> Finite Sample Properties</a></li>
  <li><a href="#asymptotic-properties" id="toc-asymptotic-properties" class="nav-link" data-scroll-target="#asymptotic-properties"><span class="header-section-number">2.5</span> Asymptotic Properties</a></li>
  <li><a href="#central-limit-theorem" id="toc-central-limit-theorem" class="nav-link" data-scroll-target="#central-limit-theorem"><span class="header-section-number">2.6</span> Central Limit Theorem</a></li>
  <li><a href="#hypothesis-testing" id="toc-hypothesis-testing" class="nav-link" data-scroll-target="#hypothesis-testing"><span class="header-section-number">2.7</span> Hypothesis Testing</a></li>
  <li><a href="#nonparametric-bootstrap" id="toc-nonparametric-bootstrap" class="nav-link" data-scroll-target="#nonparametric-bootstrap"><span class="header-section-number">2.8</span> Nonparametric Bootstrap</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./random.html">Part I: Theoretical Statistics</a></li><li class="breadcrumb-item"><a href="./inference.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical Inference</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical Inference</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In this chapter, we introduce the basics of statistical inference. We start by introducing the population data generating process (DGP), and how we can model real-world scenarios. Then, we discuss how we can estimate the parameters of these real world DGPs, including the properties of estimators and hypothesis testing.</p>
<p><br></p>
<section id="data-generating-process" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="data-generating-process"><span class="header-section-number">2.1</span> Data Generating Process</h2>
<p>Let us say we are interested in studying some variable <span class="math inline">\(Y\)</span> in the population. To study <span class="math inline">\(Y\)</span>, we can imagine how <span class="math inline">\(Y\)</span> was “generated” in the population through a model of the data generating process - how we believe the real values of <span class="math inline">\(Y\)</span> in the population came to be.</p>
<div id="def-dgp" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 2.1 (Data Generating Process)</strong></span> The data generating process is the process in the real world that “generates” the values of variable <span class="math inline">\(Y\)</span> for every individual <span class="math inline">\(t = 1, 2, \dots, n\)</span> in the population. We can model the data generating process of each individual’s <span class="math inline">\(Y_i\)</span> value as some function of a vector of parameters <span class="math inline">\(\b\theta\)</span>.</p>
<p><span class="math display">\[
Y_t = f(\b\theta)
\]</span></p>
</div>
<p><br></p>
<p>For example, let us say we are interested in the variable <em>height</em> <span class="math inline">\(Y\)</span> in the population (let us say in the UK). Let us (boldly, and for simplification) assume any individual <span class="math inline">\(t\)</span>’s <em>height</em> is completely random and selected from a normal distribution. Then we can model the data generating process for any individual’s <span class="math inline">\(Y_t\)</span> value as</p>
<p><span class="math display">\[
Y_t \sim \mathcal N(\mu_Y, \ \sigma^2_Y)
\]</span></p>
<p>The parameters of this model are <span class="math inline">\(\b\theta = (\mu_Y, \sigma^2_Y)\)</span>, that determine the value of <span class="math inline">\(Y_t\)</span>. If we can find the values of <span class="math inline">\(\b\theta\)</span>, then we can explain how variable <em>height</em> <span class="math inline">\(Y\)</span> works in the population.</p>
<p>Obviously, most variables <span class="math inline">\(Y\)</span> in the real world are not completely random. For example, if <span class="math inline">\(Y\)</span> is <em>income</em>, we might assume that another variable <em>education</em> <span class="math inline">\(X\)</span> is associated with income <span class="math inline">\(Y\)</span>. In this scenario, we can model the data generating process as</p>
<p><span class="math display">\[
Y_t \sim \mathcal N( X\beta, \sigma^2_Y)
\]</span></p>
<p>Where <span class="math inline">\(X\beta\)</span> is now the expectation of the normal distribution. In this model, our parameters <span class="math inline">\(\theta\)</span> are now <span class="math inline">\(\b\theta = (\beta, \sigma^2_Y)\)</span>. This model states that as education <span class="math inline">\(X\)</span> increases by 1, the expected <span class="math inline">\(Y\)</span> value changes by <span class="math inline">\(\beta\)</span>. This parameter <span class="math inline">\(\beta\)</span>, if we can estimate it, will tell us the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>This model still incorporates randomness through the random normal distribution with variance <span class="math inline">\(\sigma^2_Y\)</span> -this represents that not everyone with the same education will make the same exact income.</p>
<p>We can make our data generating process more complex. Perhaps, we believe that multiple variables <span class="math inline">\(X_1, X_2, \dots, X_p\)</span> are correlated with <span class="math inline">\(Y\)</span>. We can model this as</p>
<p><span class="math display">\[
Y_t \sim \mathcal N(\beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p, \ \sigma^2_Y)
\]</span></p>
<p>Where now the parameters are <span class="math inline">\(\b\theta = (\beta_1, \beta_2, \dots, \beta_p, \sigma^2_Y)\)</span>, and finding the values of these parameters will help us understand <span class="math inline">\(Y\)</span> and its relationship with <span class="math inline">\(X_1, \dots, X_p\)</span>.</p>
<p>A lot of natural phenomena are normally distributed. However, we are not limited to the normal distribution: any distribution is possible. We can also change the functional form of the relationships (maybe instead of <span class="math inline">\(\beta X\)</span>, the mean is described by <span class="math inline">\(\beta X^2\)</span>).</p>
<p>We choose the way we model our DGP based on what we know about <span class="math inline">\(Y\)</span> from our own intuition and prior research. Once we have a model of the DGP, we will know that the value of <span class="math inline">\(Y_i\)</span> depends on some parameters <span class="math inline">\(\b\theta\)</span>, for which we do not know the value of. The next step of statistics will be trying to figure out the values of <span class="math inline">\(\b\theta\)</span>.</p>
<p><br></p>
</section>
<section id="sampling-and-estimators" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="sampling-and-estimators"><span class="header-section-number">2.2</span> Sampling and Estimators</h2>
<p>Let us say we model the DGP in the population as <span class="math inline">\(Y_t = f(\b\theta)\)</span>. We know that if we can figure out the values of parameters <span class="math inline">\(\b\theta\)</span>, then we can understand how <span class="math inline">\(Y\)</span> works in our population. The true values of <span class="math inline">\(\b\theta\)</span> in the population is called the estimand.</p>
<div id="def-estimand" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 2.2 (Estimand)</strong></span> The estimand is <span class="math inline">\(\theta\)</span>, the true population value of some parameter. This could be the true mean <span class="math inline">\(\mu_Y\)</span>, the true variance <span class="math inline">\(\sigma^2_Y\)</span>, or some true relationship <span class="math inline">\(\beta\)</span> between two variables from the population.</p>
</div>
<p><br></p>
<p>However, a vector of estimands <span class="math inline">\(\b\theta\)</span> is often not intuitive or easy to calculate. For example, let us say we are interested in people’s <em>heights</em> <span class="math inline">\(Y\)</span> in the UK. For simplicity, assume a DGP of <span class="math inline">\(Y_i \sim \mathcal N(\mu_Y, \sigma^2_Y)\)</span>. We want to find <span class="math inline">\(\theta = \mu_Y\)</span>, the average height of all people in the UK. How can we do this?</p>
<p>Of course, we could ask all 70 million people in the UK, and find the average. But clearly that would take an enormous amount of effort and resources to ask 70 million people. In some future applications we will discuss, the population is hypothetical, so asking the whole population is not even an option with unlimited resources.</p>
<p>The answer to this issue is <strong>sampling</strong>. We randomly take a subset of the population (let us say 1,000 people out of the 70 million), and then calculate the sample average height <span class="math inline">\(\bar Y\)</span>.</p>
<p>If we believe that our sample is representative/similar to the population, then we might be able to say something about the population average height <span class="math inline">\(\mu_Y\)</span> from our sample average <span class="math inline">\(\bar Y\)</span>. This procedure is called the estimator, and produces an estimate <span class="math inline">\(\hat\theta_n\)</span> of our true population.</p>
<div id="def-estimator" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 2.3 (Estimator and Estimates)</strong></span> The estimator is a procedure/process to turn sample data, into an estimate <span class="math inline">\(\hat\theta_n\)</span> of our true population estimand <span class="math inline">\(\theta\)</span>. For example, taking a sample and calculating the mean <span class="math inline">\(\bar Y\)</span> is a estimator of the true mean <span class="math inline">\(\mu_Y\)</span>.</p>
</div>
<p><br></p>
<p>A lot of statistics is about trying to find a good estimator in order to accurately (to the best of our ability) estimate the true population estimand <span class="math inline">\(\theta\)</span>. In the next few sections, we will explore the properties of estimators, and determine how we determine what a good estimator is.</p>
<p><br></p>
</section>
<section id="sampling-distributions" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="sampling-distributions"><span class="header-section-number">2.3</span> Sampling Distributions</h2>
<p>We have taken a sample from the population, and using an estimator, produced a sample estimate <span class="math inline">\(\hat\theta_n\)</span>. However, there is an issue with our sample estimate <span class="math inline">\(\hat\theta_n\)</span> that we have gotten: if we took another random sample, we would get different individuals in our sample due to randomness, which would result in a slightly different sample estimate <span class="math inline">\(\hat\theta_m\)</span>.</p>
<p>Now, let us run a <u>hypothetical</u> thought experiment. We take 1 sample, and calculate our estimate <span class="math inline">\(\hat\theta_1\)</span>. We take a second sample, and calculate our estimate <span class="math inline">\(\hat\theta_2\)</span>, which is slightly different than the first. We then keep taking more and more samples (close to infinity number of samples), until we have estimates <span class="math inline">\(\hat\theta_1, \hat\theta_2, \hat\theta_3, \dots, \hat\theta_N\)</span>.</p>
<p>Now, we can plot our different sample estimates <span class="math inline">\(\hat\theta_1, \dots, \hat\theta_n\)</span> on a distribution. For example, if we return back to the average height in the UK example, each sample produces a different sample average height. We can plot them as follows:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-2444367374.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%"></p>
</figure>
</div>
<p>This is called a <strong>sampling distribution</strong>, and illustrates the possible estimates <span class="math inline">\(\hat\theta_n\)</span> we could get under repeated sampling. The actual form of the distribution (expected value, variance, shape) are determined by the estimator.</p>
<p>Now, let us stop thinking in this hypothetical thought experiment, and return to the real world. In the real world, we are only going to take one random sample. But which sample in real-life did we get when sampling? Which <span class="math inline">\(\hat\theta_n\)</span> is our specific sample estimate from this big distribution of potential sample estimates?</p>
<p>Well, we can actually think of our individual sample estimate <span class="math inline">\(\hat\theta_n\)</span> as a <strong>random variable</strong>, whose probabilities is based on this hypothetical sampling distribution. When we actually draw a sample and calculate <span class="math inline">\(\hat\theta_n\)</span>, we are essentially randomly selecting one of <span class="math inline">\(\hat\theta_1, \hat\theta_2, \dots, \hat\theta_N\)</span> from the sampling distribution.</p>
<p>This idea of our sample estimate <span class="math inline">\(\hat\theta_n\)</span> being a random variable with a distribution defined by the sampling distribution also implies that the other properties of random variable distributions mentioned in the last chapter apply to sampling distributions. Sampling distributions have a mean, variance, and other properties that we will explore.</p>
<p>Since sampling distributions are determined by our estimator (and its properties), we will often interchangeably switch between the terminology of estimators and sampling distributions.</p>
<p><br></p>
</section>
<section id="finite-sample-properties" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="finite-sample-properties"><span class="header-section-number">2.4</span> Finite Sample Properties</h2>
<p>As we discussed above, our sample estimates <span class="math inline">\(\hat\theta_n\)</span> are a random draw from a sampling distribution. We also mentioned that the shape, form, and parameters of the sampling distribution are a direct result of our estimator.</p>
<p>We know that our sampling distribution has some expectation, like any random variable. That means our estimates <span class="math inline">\(\hat\theta_n\)</span> have some expected value. One property relating to the expectation of our estimator/sampling distribution is unbiasedness.</p>
<div id="def-unbiased" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 2.4 (Unbiased Estimator)</strong></span> An estimator is considered unbiased, if its estimates <span class="math inline">\(\hat\theta_n\)</span> have an expectation that is equal to the value of the true population parameter.</p>
<p><span class="math display">\[
\E \hat\theta_n = \theta
\]</span></p>
</div>
<p><br></p>
<p>Or in other words, if we repeatedly sample and use an unbiased estimator to calculate our estimates <span class="math inline">\(\hat\theta_n\)</span>, on average, the estimates will be equal to the true population value <span class="math inline">\(\theta\)</span> of interest. In terms of the sampling distirbution, this means that the expectation (centre) of the sampling distribution is equal to the true population value <span class="math inline">\(\theta\)</span>.</p>
<p>We want an unbiased estimator, because if <span class="math inline">\(\E\hat\theta_n = \theta\)</span>, then we know that our “expected” value of our estimator, our best guess of the estimator’s value, is indeed, the correct true population parameter <span class="math inline">\(\theta\)</span>.</p>
<p>However, just like with random variables, we do not just care about the expected value of the estimator. We also care about its variance - how spread out/precise the individual estimates <span class="math inline">\(\hat\theta_n\)</span> of an estimator are.</p>
<p>For example, let us say we have some true population parameter <span class="math inline">\(\theta = 0\)</span>, and two estimators <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. <span class="math inline">\(A\)</span> produces sample estimates -1 and 1, while <span class="math inline">\(B\)</span> produces sample estimates -100 and 100.</p>
<p>Both estimators are unbiased, but, clearly, estimator <span class="math inline">\(A\)</span>’s individual sample estimates are on average, much closer to the true <span class="math inline">\(\theta = 0\)</span>. Thus, we care about the variance of the estimator.</p>
<div id="def-varest" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 2.5 (Variance of an Estimator)</strong></span> The variance of an estimator, which is also the variance of the sampling distribution, can be quantified as</p>
<p><span class="math display">\[
\V \hat\theta_n = \E[(\hat\theta_n - \E \hat\theta_n)^2]
\]</span></p>
</div>
<p><br></p>
<p>In an ideal world, we want an estimator that has the least variance of any unbiased estimator. This way, we know the estimator is on average correct, and that each individual estimate <span class="math inline">\(\hat\theta_n\)</span> the estimator produces is not very far from the true population value of <span class="math inline">\(\theta\)</span>.</p>
<p>An estimator that is both unbiased, and has the least variance of any unbiased estimator, is called the <strong>best</strong> or <strong>most efficient</strong> unbiased estimator.</p>
<p><br></p>
</section>
<section id="asymptotic-properties" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="asymptotic-properties"><span class="header-section-number">2.5</span> Asymptotic Properties</h2>
<p>In the previous section, we discussed properties that apply to estimators/sampling distributions no matter the sample size <span class="math inline">\(n\)</span> (the size of each individual sample). That is what we call finite sample properties.</p>
<p>However, we also are interested in <strong>asymptotic</strong> properties (also called large sample properties). Asymptotic properties are what happens to our estimators/sampling distributions when we increase sample size towards infinity <span class="math inline">\(n \rightarrow ∞\)</span>.</p>
<p>One of the most important asymptotic properties are if our estimator is consistent/unbiased in asymptotic sample sizes.</p>
<div id="def-consistency" class="theorem definition" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Definition 2.6 (Asymptotic Consistency)</strong></span> An estimator is asymptotically consistent (or asymptotically unbiased), if as we increase sample size towards infinity, the estimate will become more and more concentrated around the true population value <span class="math inline">\(\theta\)</span>. At <span class="math inline">\(n=∞\)</span>, we should expect our sample distribution to “collapse”, with only one potential outcome, the actual value of <span class="math inline">\(\theta\)</span> in the population.</p>
<p><span class="math display">\[
\P (|\hat\theta_n - \theta| &gt; \epsilon) \rightarrow 0, \quad \mathrm{as} \ n \rightarrow ∞
\]</span></p>
<p>What the above formula essentially states is that the difference between our estimate <span class="math inline">\(\hat\theta_n\)</span> and the actual population <span class="math inline">\(\theta\)</span> is greater than some arbitrarily small value <span class="math inline">\(\epsilon\)</span> with 0 probability asymptotically.</p>
</div>
<p><br></p>
<p>Note that an estimator can be biased in finite samples, but consistent/unbiased in asymptotic samples. This implies that as we increase our sample size, the bias of this type of estimator will decrease.</p>
<p>One of the principles establishing asymptotic consistency is the law of large numbers, which shows that the sample average estimator is asymptotically consistent.</p>
<div id="thm-lawoflargenumbers" class="theorem" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Theorem 2.1 (Law of Large Numbers)</strong></span> The law of large numbers says that the sample average of a random sample is a asymptotically consistent estimator of the population mean. In other words, if we have a sample of <span class="math inline">\(x_1, \dots, x_n\)</span>, the sample mean <span class="math inline">\(\bar x_n\)</span> has the property</p>
<p><span class="math display">\[
\mathrm{plim}(\bar x_n) = \mu_X
\]</span></p>
<p>Where <span class="math inline">\(\mathrm{plim}\)</span> stats that as <span class="math inline">\(n \rightarrow ∞\)</span>, the probability distribution of the sample estimator <span class="math inline">\(\bar x_n\)</span> collapses around the true mean <span class="math inline">\(\mu_X\)</span>.</p>
</div>
<p><br></p>
<p><strong>Proof</strong>: Let us restate the sample mean estimator by its formula (definition of average):</p>
<p><span class="math display">\[
\bar x_n = \frac{1}{n}\sum\limits_{t=1}^n x_t
\]</span></p>
<p>Now, let us find the variance of this sample estimator <span class="math inline">\(\bar x_n\)</span>.</p>
<p><span class="math display">\[
\V \bar x_n = \V \left( \frac{1}{n}\sum\limits_{t=1}^n x_t \right)
\]</span></p>
<p>Using the properties of variance in <a href="random.html#thm-variance" class="quarto-xref">theorem&nbsp;<span>1.2</span></a>, we establish that the above is</p>
<p><span class="math display">\[
\V \bar x_n= \left(\frac{1}{n}\right)^2 \V \left(\sum\limits_{t=1}^n x_t \right)
\]</span></p>
<p>Let us define the variance of our sample <span class="math inline">\(x_1 \dots, x_n\)</span> as <span class="math inline">\(\V x_t = \sigma^2\)</span>. We get</p>
<p><span id="eq-varsamplemean"><span class="math display">\[
\V \bar x_n = \frac{1}{n^2} n\sigma^2 = \frac{1}{n}\sigma^2
\tag{2.1}\]</span></span></p>
<p>And if sample size <span class="math inline">\(n \rightarrow ∞\)</span>, we have a limit:</p>
<p><span class="math display">\[
\lim\limits_{n \rightarrow ∞} \V \bar x_n = \lim\limits_{n \rightarrow ∞} \frac{1}{n}\sigma^2 = 0
\]</span></p>
<p>Thus, we can see the variance becomes 0, and the estimator’s sampling distribution collapses to a single value. This law will be used frequently in proving asymptotic properties, since many different parameters (such as variance and covariance for example) involve expectations/averages.</p>
<p><br></p>
</section>
<section id="central-limit-theorem" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="central-limit-theorem"><span class="header-section-number">2.6</span> Central Limit Theorem</h2>
<p>Aside from the expected value and variance of a sampling distribution, we also want to know what shape/form the sampling distribution is taking.</p>
<p>After all, if we can define a PDF for the sampling distribution, we can figure out the probabilities of getting certain <span class="math inline">\(\hat\theta_n\)</span> estimates, which is a key part of hypothesis testing (shown later in the chapter).</p>
<p>The central limit theory provides a way for us to know the form of the sampling distribution.</p>
<div id="thm-clt" class="theorem" style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;">
<p><span class="theorem-title"><strong>Theorem 2.2 (Central Limit Theorem)</strong></span> Let <span class="math inline">\(Z_1, \dots, Z_n\)</span> be independent and identically distributed random variables that when realised, produce a sample <span class="math inline">\(z_1, \dots, z_n\)</span>. Let us say the true mean <span class="math inline">\(\E Z_i = \mu_Z\)</span>, and the true variance <span class="math inline">\(\sigma^2_Z = \V Z_i\)</span>.</p>
<p>Consider the sample average <span class="math inline">\(\bar z_n\)</span>, and its variance <span class="math inline">\(\V \bar z_n = \sigma^2_Z/n\)</span> (from <a href="#eq-varsamplemean" class="quarto-xref">eq.&nbsp;<span>2.1</span></a>). Now, let us define a new variable <span class="math inline">\(W_n\)</span>, which is <span class="math inline">\(\bar z_n\)</span> standardised (from <a href="random.html#eq-standardisation" class="quarto-xref">eq.&nbsp;<span>1.8</span></a>). The central limit theorem states that <span class="math inline">\(w_n\)</span> is normally distributed as sample size <span class="math inline">\(n \rightarrow ∞\)</span>:</p>
<p><span class="math display">\[
W_n  = \frac{\bar z_n - \mu}{\sigma_Z / \sqrt{n}} \sim \mathcal N(0, 1) \quad \mathrm{as} \ n \rightarrow ∞
\]</span></p>
<p>This property holds no matter the original distribution of <span class="math inline">\(Z_i\)</span> or <span class="math inline">\(\bar z_n\)</span>.</p>
</div>
<p><br></p>
<p>The central limit theorem says that as our sample size becomes larger and larger, our sample means estimator <span class="math inline">\(\bar z_n\)</span>’s standardised version <span class="math inline">\(W_n\)</span> will be standardly normally distributed.</p>
<p>If we recall previously, the standard normal distribution has a probability density function of:</p>
<p><span class="math display">\[
\varphi(z) = f_Z(z) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}z^2}
\]</span></p>
<p>This means that if we have a sufficiently large sample size such that we can invoke the central limit theorem, we can calculate the exact probability of our sample estimate <span class="math inline">\(\hat\theta_n\)</span> being the outcome of the random variable of the sampling distribution, by using the probability density function defined in <a href="random.html#def-pdf" class="quarto-xref">definition&nbsp;<span>1.1</span></a>.</p>
<p>This ability to calculate the probability of our realised sample estimate <span class="math inline">\(\hat\theta_n\)</span> allows us to conduct statistical inference with hypothesis testing, which we will introduce below.</p>
<p><br></p>
</section>
<section id="hypothesis-testing" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="hypothesis-testing"><span class="header-section-number">2.7</span> Hypothesis Testing</h2>
<p>Science is about disproving a status-quo belief about a parameter <span class="math inline">\(\theta\)</span>, and proving a new theory. We call the status-quo belief the <strong>null hypothesis</strong> <span class="math inline">\(H_0\)</span>, and our new theory the <strong>alternate hypothesis</strong> <span class="math inline">\(H_1\)</span>.</p>
<p>For example, your status quo theory might be that there is no relationship between education and income <span class="math inline">\(\theta = 0\)</span>. Your alternate hypothesis is that there is a relatinoship between education and income <span class="math inline">\(\theta ≠ 0\)</span>.</p>
<p>How can we “disprove” the status quo theory and conclude our alternate hypothesis is correct? What we essentially do is first assume that the null hypothesis <span class="math inline">\(H_0\)</span> is true. We then gather a sample, and create a sample estimate <span class="math inline">\(\hat\theta_n\)</span>. We then calculate the probability of getting a sample estimate <span class="math inline">\(\hat\theta_n\)</span>, assuming that the null hypothesis is true.</p>
<p>If the probability of getting our specific sample estimate <span class="math inline">\(\hat\theta_n\)</span> is very low, we typically conclude that assuming the status quo theory <span class="math inline">\(H_0\)</span> is incorrect, and our alternative hypothesis <span class="math inline">\(H_1\)</span>, our <span class="math inline">\(\hat\theta_n\)</span> would actually be more plausible.</p>
<p>To run a hypothesis test of this sort, we take a series of steps. First, we define our null and alternate hypotheses:</p>
<ul>
<li>Null hypothesis <span class="math inline">\(H_0 : \theta = \theta_0\)</span>.</li>
<li>Alternate hypothesis <span class="math inline">\(H_1 : \theta ≠ \theta_0\)</span></li>
</ul>
<p>Second, we have to calculate our test statistic. The actual test statistic (t-statistic, z-statistic, wald statistic) used depends on our estimator/sampling distribution. We will specify which test statistic to use for every estimator when we introduce specific estimators. However, every test statistic is calculated in the same way:</p>
<p><span class="math display">\[
\mathrm{test \ statistic} = \frac{\hat\theta_n - \theta_0}{se(\hat\theta_n)}
\]</span></p>
<p>Where <span class="math inline">\(\hat\theta_n\)</span> is our sample estimate, <span class="math inline">\(\theta_0\)</span> is the null hypothesis belief of the true value of <span class="math inline">\(\theta\)</span>, and <span class="math inline">\(se(\hat\theta_n)\)</span> is the standard deviation of our sampling distribution (square root of variance). The test statistic is measuring the distance in standard errors between <span class="math inline">\(\hat\theta_n\)</span> and <span class="math inline">\(\theta_0\)</span>.</p>
<p>Then, once we have a test statistic, we go to a standard normal distribution (or t-distribution or <span class="math inline">\(\chi^2\)</span> distribution, depending on the test). We start from the expectation of this distribution, then go <em>test-statistic-number-of-units</em> to both sides of the expectation. We then highlight the areas further from the expectation on both sides. The figure below illustrates this with a test statistic of 2.228.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-1533818238.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%"></p>
</figure>
</div>
<p>Now, we calculate the probability of the highlighted areas using the probability density function of the distribution we are using. This step is typically done with a calculator/computer since integrals are complex to compute.</p>
<p>The probability we get is called the <strong>p-value</strong>. The p-value describes the probability of getting a test statistic as or more extreme than the one we got, assuming the null hypothesis <span class="math inline">\(H_0: \theta = \theta_0\)</span> is true.</p>
<p>If the p-value is below 0.05 (5%), we believe that it is unlikely that the null hypothesis is true, so we reject the null hypothesis <span class="math inline">\(H_0\)</span> and conclude our alternate hypothesis <span class="math inline">\(H_1\)</span> is true. If the p-value is above 0.05 (5%), we have no evidence to reject the null hypothesis <span class="math inline">\(H_0\)</span>, so we still remain with the status quo <span class="math inline">\(H_0\)</span>.</p>
<p>Note that you do not need to choose a 5% significance level - although that is the standard in many social science field and computer software.</p>
<p><br></p>
</section>
<section id="nonparametric-bootstrap" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="nonparametric-bootstrap"><span class="header-section-number">2.8</span> Nonparametric Bootstrap</h2>
<p>In <a href="#thm-clt" class="quarto-xref">theorem&nbsp;<span>2.2</span></a>, we discussed how the central limit theorem allows us to know that in large sample sizes, our standardised sampling distribution will be normally distributed. However, what if we have a small sample size? After all, it is not easy to collect a large sample.</p>
<p>Bootstrap methods are a method to “simulate” the sampling distribution. Essentially, we start with some sample of size <span class="math inline">\(n\)</span> with values <span class="math inline">\(z_1, \dots, z_n\)</span>. This original sample gets us some estimate <span class="math inline">\(\hat\theta_n\)</span>.</p>
<p>Now, to simulate a sampling distribution, what we do is to sample with replacement from our original sample <span class="math inline">\(z_1, \dots, z_n\)</span>. This will create a new bootstrap sample <span class="math inline">\(b_1\)</span> of size <span class="math inline">\(n\)</span>.</p>
<p>When we sample with replacement, we allow for observations to be sampled multiple times into our bootstrap sample. For example, our new bootstrap sample might take the form <span class="math inline">\(z_1, z_2, z_2, z_2, z_3, \dots, z_n\)</span>.</p>
<p>Using this bootstrap sample <span class="math inline">\(b_1\)</span>, we can calculate a sample estimate <span class="math inline">\(\hat\theta_1\)</span>. Now, let us repeat the process, and create multiple bootstrap samples <span class="math inline">\(b_1, b_2, \dots, b_B\)</span>. For each, we calculate <span class="math inline">\(\hat\theta_b\)</span>, such that by the end of this process, we have <span class="math inline">\(\hat\theta_1, \hat\theta_2, \dots, \hat\theta_B\)</span>.</p>
<p>Now, we have basically simulated a sampling distribution. The benefit of this method is that we don’t need to rely on some fancy statistical theories like the central limit theorem to conclude the shape of our sampling distribution.</p>
<p>The standard error from bootstraping is:</p>
<p><span class="math display">\[
se_B(\hat\theta) = \sqrt{\frac{1}{B-1} \sum\limits_{b=1}^B (\hat\theta_b - \E \hat\theta_b})
\]</span></p>
<p>The downside of bootstrap is that it is fully reliant on our original sample being reflective of the population. If our original sample has some issues that misrepresent the population, the bootstrapped sampling distribution may be flawed.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./random.html" class="pagination-link" aria-label="Random Variables">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Random Variables</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./ols.html" class="pagination-link" aria-label="Classical Least Squares">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Classical Least Squares</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>