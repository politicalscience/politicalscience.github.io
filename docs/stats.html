<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Background Statistics – Political Science &amp; Political Economy</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="mathjax-config.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./stats.html">Background Statistics</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Kevin’s PSPE Resources</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
 <span class="menu-text">Quantitative Methods</span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 The Classical Linear Model</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 The Generalised Linear Model</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./causal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 Causal Frameworks</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 Randomised Controlled Trials</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./soo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 Selection on Observables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./iv.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 Instrumental Variables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rd.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7 Regression Discontinuity</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./did.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8 Differences-in-Differences</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Further Statistical Methods</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./predict.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Forecasting and Prediction Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unsupervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Unsupervied Learning Methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./factor.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Factor Analysis Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./latent.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Latent Trait and Class Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sem.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Structural Equation Models</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./games.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guide to Game Theory</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./math.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Background Mathematics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stats.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Background Statistics</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On This Page</h2>
   
  <ul>
  <li><a href="#random-variables" id="toc-random-variables" class="nav-link active" data-scroll-target="#random-variables"><strong>Random Variables</strong></a>
  <ul class="collapse">
  <li><a href="#random-variables-and-distributions" id="toc-random-variables-and-distributions" class="nav-link" data-scroll-target="#random-variables-and-distributions">Random Variables and Distributions</a></li>
  <li><a href="#probability-density-functions" id="toc-probability-density-functions" class="nav-link" data-scroll-target="#probability-density-functions">Probability Density Functions</a></li>
  <li><a href="#expectation-and-variance" id="toc-expectation-and-variance" class="nav-link" data-scroll-target="#expectation-and-variance">Expectation and Variance</a></li>
  <li><a href="#conditional-distributions" id="toc-conditional-distributions" class="nav-link" data-scroll-target="#conditional-distributions">Conditional Distributions</a></li>
  </ul></li>
  <li><a href="#distributions" id="toc-distributions" class="nav-link" data-scroll-target="#distributions"><strong>Distributions</strong></a>
  <ul class="collapse">
  <li><a href="#the-normal-distribution" id="toc-the-normal-distribution" class="nav-link" data-scroll-target="#the-normal-distribution">The Normal Distribution</a></li>
  <li><a href="#properties-of-the-normal-distribution" id="toc-properties-of-the-normal-distribution" class="nav-link" data-scroll-target="#properties-of-the-normal-distribution">Properties of the Normal Distribution</a></li>
  <li><a href="#the-standard-normal-distribution" id="toc-the-standard-normal-distribution" class="nav-link" data-scroll-target="#the-standard-normal-distribution">The Standard Normal Distribution</a></li>
  <li><a href="#the-t-distribution" id="toc-the-t-distribution" class="nav-link" data-scroll-target="#the-t-distribution">The T-Distribution</a></li>
  <li><a href="#bernoulli-and-binomial-distribution" id="toc-bernoulli-and-binomial-distribution" class="nav-link" data-scroll-target="#bernoulli-and-binomial-distribution">Bernoulli and Binomial Distribution</a></li>
  </ul></li>
  <li><a href="#estimators-and-statistical-inference" id="toc-estimators-and-statistical-inference" class="nav-link" data-scroll-target="#estimators-and-statistical-inference"><strong>Estimators and Statistical Inference</strong></a>
  <ul class="collapse">
  <li><a href="#estimators-and-sampling-distributions" id="toc-estimators-and-sampling-distributions" class="nav-link" data-scroll-target="#estimators-and-sampling-distributions">Estimators and Sampling Distributions</a></li>
  <li><a href="#finite-sample-properties-of-estimators" id="toc-finite-sample-properties-of-estimators" class="nav-link" data-scroll-target="#finite-sample-properties-of-estimators">Finite Sample Properties of Estimators</a></li>
  <li><a href="#asymptotic-properties-of-estimators" id="toc-asymptotic-properties-of-estimators" class="nav-link" data-scroll-target="#asymptotic-properties-of-estimators">Asymptotic Properties of Estimators</a></li>
  <li><a href="#intuition-of-hypothesis-testing" id="toc-intuition-of-hypothesis-testing" class="nav-link" data-scroll-target="#intuition-of-hypothesis-testing">Intuition of Hypothesis Testing</a></li>
  <li><a href="#mechanics-of-hypothesis-testing" id="toc-mechanics-of-hypothesis-testing" class="nav-link" data-scroll-target="#mechanics-of-hypothesis-testing">Mechanics of Hypothesis Testing</a></li>
  <li><a href="#confidence-intervals" id="toc-confidence-intervals" class="nav-link" data-scroll-target="#confidence-intervals">Confidence Intervals</a></li>
  <li><a href="#nonparametric-bootstrap" id="toc-nonparametric-bootstrap" class="nav-link" data-scroll-target="#nonparametric-bootstrap">Nonparametric Bootstrap</a></li>
  </ul></li>
  <li><a href="#correlations-between-variables" id="toc-correlations-between-variables" class="nav-link" data-scroll-target="#correlations-between-variables"><strong>Correlations Between Variables</strong></a>
  <ul class="collapse">
  <li><a href="#correlated-variables" id="toc-correlated-variables" class="nav-link" data-scroll-target="#correlated-variables">Correlated Variables</a></li>
  <li><a href="#quantifying-correlations" id="toc-quantifying-correlations" class="nav-link" data-scroll-target="#quantifying-correlations">Quantifying Correlations</a></li>
  <li><a href="#best-fit-lines-and-magnitude" id="toc-best-fit-lines-and-magnitude" class="nav-link" data-scroll-target="#best-fit-lines-and-magnitude">Best-Fit Lines and Magnitude</a></li>
  <li><a href="#simple-linear-regression" id="toc-simple-linear-regression" class="nav-link" data-scroll-target="#simple-linear-regression">Simple Linear Regression</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Background Statistics</h1>
<p class="subtitle lead">Kevin’s PSPE Resources</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This chapter is about the very building blocks of statistics. We start off with a discussion over probability, random variables, and distributions. Then, we talk about the basics of statistical inference and hypothesis testing. We conclude by discussing correlations between variables and the basics of regression.</p>
<p>Use the right sidebar to navigate quickly.</p>
<hr>
<section id="random-variables" class="level1">
<h1><strong>Random Variables</strong></h1>
<section id="random-variables-and-distributions" class="level3">
<h3 class="anchored" data-anchor-id="random-variables-and-distributions">Random Variables and Distributions</h3>
<p>Random variables are outcomes that have some randomness/uncertainty. There is a set of possible outcomes <span class="math inline">\(\Omega\)</span>, called the sample space. Within the sample space there exists events <span class="math inline">\(\omega \in \Omega\)</span> (potential outcomes). We do not know which event will be observed, but we know each outcome <span class="math inline">\(\omega\)</span>’s probability of realisation <span class="math inline">\(\P(\omega \in \Omega)\)</span>.</p>
<p>We can represent <span class="math inline">\(\omega \in \Omega\)</span>, and the associated probabilities with each outcome <span class="math inline">\(\P(\omega)\)</span>, in a <strong>distribution</strong>. On the horizontal axis is all <span class="math inline">\(\omega \in \Omega\)</span>, and the vertical axis is <span class="math inline">\(\P(\omega)\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-713724518.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:45.0%"></p>
</figure>
</div>
<p><strong>Continuous random variables</strong> <span class="math inline">\(\omega \in [a, b]\)</span> can take any possible value within a range <span class="math inline">\(a\)</span> to <span class="math inline">\(b\)</span>, including any decimal. <strong>Discrete random variables</strong> can only take a certain set of values <span class="math inline">\(\omega \in \{a, b, \dots \}\)</span>, not any possible decimal. For example, a dice roll can only take the values of 1, 2, 3,…, and not 3.23478. A discrete random variable with only 2 potential outcomes <span class="math inline">\(\omega \in \{0, 1 \}\)</span> is a <strong>binary random variable</strong>.</p>
<p>We will denote random variables with a capital letter (ex. <span class="math inline">\(Z\)</span>, <span class="math inline">\(X\)</span>). In later chapters, we will denote them with a subscript <span class="math inline">\(Z_i, X_i\)</span> to distinguish them from matrices. The realisation of a random variable (the outcome) is denoted with lower case letters (ex. <span class="math inline">\(z, x\)</span>).</p>
<p><br></p>
</section>
<section id="probability-density-functions" class="level3">
<h3 class="anchored" data-anchor-id="probability-density-functions">Probability Density Functions</h3>
<p>We can describe these distributions mathematically. A <strong>probability density function</strong> is a function <span class="math inline">\(\varphi\)</span> that takes one potential outcome as an input, and spits out the probability of that outcome. For example, the probability density function of a dice <span class="math inline">\(z \in \{ 1, 2, 3, 4, 5, 6\}\)</span> is:</p>
<p><span class="math display">\[
\P(z) = \varphi(z) = 1/6
\]</span></p>
<p>We can see that if we want to see the probability of getting a 5 on our dice roll, we plug in <span class="math inline">\(z=5\)</span> to get <span class="math inline">\(\varphi(5) = 1/6\)</span>, which is the probability of rolling a 5.</p>
<p>For continuous random variables, it does not make sense to calculate the probability of one specific outcome <span class="math inline">\(z\)</span>. For example, let us say your random variable is the time it will take you to get to school tomorrow. We really do not care about the probability of taking 13.4357 minutes to get to school. What we do care about is a range - lets say the probability of getting to school between 13 and 14 minutes.</p>
<p>More mathematically, the probability density function of a continuous random variable gives you the probability of an outcome between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>:</p>
<p><span class="math display">\[
\P(a&lt;z&lt;b) =\int\limits_a^b \varphi(z)dz
\]</span></p>
<p>This integral is important - it tells us that the area under a distribution gives us the probability of an event.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-1038590923.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:45.0%"></p>
</figure>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Cumulative Density Functions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Cumulative density functions measure the cumulative probability below a certain point.</p>
<p>For example, to get the probability of anything below <span class="math inline">\(a\)</span> occurring as an outcome, our cumulative density function is:</p>
<p><span class="math display">\[
\P(z&lt;a) = \int\limits_{-∞}^a \varphi(z)dz
\]</span></p>
<p>This will be useful in some cases to calculate values for probability density functions.</p>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="expectation-and-variance" class="level3">
<h3 class="anchored" data-anchor-id="expectation-and-variance">Expectation and Variance</h3>
<p>The <strong>expectation</strong>, also called the <strong>mean</strong> or <strong>expected value</strong>, is the “average” outcome value that you would expect to get from the distribution. It is essentially our “best guess” of what the random variable’s outcome would be.</p>
<p>Expectation for a discrete random variable <span class="math inline">\(X\)</span> is calculated as a weighted sum of all potential outcomes <span class="math inline">\(x_i\)</span> and their respective probabilities given by the probability density function <span class="math inline">\(p(x_i)\)</span>:</p>
<p><span class="math display">\[
\E Z  = \sum\limits_{i=1}^n(z_i \times \varphi(z_i))
\]</span></p>
<p>For a continuous random variable <span class="math inline">\(X\)</span>, the idea is the same but with an integral:</p>
<p><span class="math display">\[
\E Z = \int\limits_{-∞}^∞ (z \times \varphi(z))dz
\]</span></p>
<p>Expectations have a few unique algebraic properties:</p>
<ol type="1">
<li>The expectation of a constant is itself: <span class="math inline">\(\E c = c\)</span>.</li>
<li>Expectations can be added: <span class="math inline">\(\E(X+Y) = \E X + \E Y\)</span>.</li>
<li>Expectations multiplied with constants: <span class="math inline">\(\E(cX) = c \cdot \E X\)</span>.</li>
<li>Law of iterated expectations: <span class="math inline">\(\E(X) = \E[\E(X|Y)]\)</span>.</li>
</ol>
<p><strong>Variance</strong> measures the spread of a distribution. It is essentially the average distance of each individual outcome from the expected value <span class="math inline">\(\mu\)</span> of the random variable:</p>
<p><span class="math display">\[
\V Z = \sigma^2= \E(z - \E Z)^2
\]</span></p>
<p>The <strong>standard deviation</strong> <span class="math inline">\(\sigma\)</span> is simply the square root of variance.</p>
<p><br></p>
</section>
<section id="conditional-distributions" class="level3">
<h3 class="anchored" data-anchor-id="conditional-distributions">Conditional Distributions</h3>
<p>Let us say we have two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, each with their own respective distributions.</p>
<p>Often in statistics, we are interested in how two variables interact with each other. For example, we are interested in how democracy affects economic growth. Or how education affects income.</p>
<p>Conditional distributions are a distribution of one random variable <span class="math inline">\(X\)</span>, given we hold another variable <span class="math inline">\(Y\)</span> fixed at some value.</p>
<p>For example, imagine <span class="math inline">\(X\)</span> is income and <span class="math inline">\(Y\)</span> is age. The conditional distribution of <span class="math inline">\(X|Y\)</span> is the distribution of income <span class="math inline">\(X\)</span> at every specific age <span class="math inline">\(Y\)</span>. For example, <span class="math inline">\(X|Y = 20\)</span> is the distribution of income <span class="math inline">\(X\)</span> for 20 year olds. <span class="math inline">\(X|Y=60\)</span> would be the distribution of income <span class="math inline">\(X\)</span> for 60 year olds.</p>
<p>Conditional distributions have all the same properties as normal distributions. The most important of these is the <strong>conditional expectation</strong>, which we denotate <span class="math inline">\(\E(X|Y)\)</span>. In the context of above, <span class="math inline">\(\E(X|Y=20)\)</span> would be the <strong>expected</strong> income for a 20 year old.</p>
<p>The reason conditional expectations are so important is because they illustrate how one variable affects another. If we see a pattern in going between <span class="math inline">\(\E(X|Y=20)\)</span>, <span class="math inline">\(\E(X|Y = 21)\)</span>, and <span class="math inline">\(\E(X|Y = 24)\)</span>, we might be tempted to say that increasing age <span class="math inline">\(Y\)</span> has some effect on income <span class="math inline">\(X\)</span>.</p>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="distributions" class="level1">
<h1><strong>Distributions</strong></h1>
<section id="the-normal-distribution" class="level3">
<h3 class="anchored" data-anchor-id="the-normal-distribution">The Normal Distribution</h3>
<p>The <strong>normal distribution</strong> is for continuous random variables, and takes a famous “bell shape” (hence why it is also called the bell curve). Every normal distribution and its <a href="#probability-density-functions">probability density function (PDF)</a> can be defined by two parameters: the mean and the variance:</p>
<p><span class="math display">\[
Z \sim \mathcal N(\mu, \sigma^2), \quad \P(a&lt;z&lt;b) = \int\limits_a^b \frac{1}{\sqrt{2\pi\sigma^2}}e^{\left( -\frac{(z - \E Z)^2}{2 \sigma^2}\right)}dx
\]</span></p>
<p>Where <span class="math inline">\(\mathcal N\)</span> represents the normal distribution, <span class="math inline">\(\mu\)</span> represents the expected value <span class="math inline">\(\E Z\)</span> of the distribution, and <span class="math inline">\(\sigma^2\)</span> represents the variance. <span class="math inline">\(Z\)</span> is the variable we have assigned the normal distribution to. The figure below shows how normal distributions change when you alter <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-2268260367.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<p><br></p>
</section>
<section id="properties-of-the-normal-distribution" class="level3">
<h3 class="anchored" data-anchor-id="properties-of-the-normal-distribution">Properties of the Normal Distribution</h3>
<p>Normal Distributions have a unique property - within any standard deviations <span class="math inline">\(\sigma\)</span> from the mean, every single normal distribution contains the same amount of area under the curve (which is also the probability):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-2691725554.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>These properties don’t just apply to 1, 2, or 3 standard deviations away from the mean. They apply to any amount of standard deviations (such as 1.23478 standard deviations), and there are online tables/calculators that can derive this. This means that we can find the probability between any two points of a normal distribution, just by knowing the mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<p>Another property of the normal distribution is that we can manipulate them as follows:</p>
<ol type="1">
<li>We can add a constant <span class="math inline">\(c\)</span> to every single outcome/value in a normal distribution. The resulting distribution will still be a normal distribution, just with its mean shifted by <span class="math inline">\(c\)</span>. Or in other words: <span class="math inline">\(Z \sim \mathcal N(\mu + c, \sigma^2)\)</span>.</li>
<li>We can multiply by constant <span class="math inline">\(c\)</span> to every single outcome/value in a normal distribution. The resulting distribution will still be a normal distribution, just with its mean multiplied by <span class="math inline">\(c\)</span>, and its standard deviation multipled by <span class="math inline">\(c^2\)</span>. Or in other words: <span class="math inline">\(Z \sim \mathcal N(c \mu, (c \mu)^2)\)</span>.</li>
</ol>
<p><br></p>
</section>
<section id="the-standard-normal-distribution" class="level3">
<h3 class="anchored" data-anchor-id="the-standard-normal-distribution">The Standard Normal Distribution</h3>
<p>The standard normal distribution (often called the <span class="math inline">\(Z\)</span>-distribution) is a special version of the normal distribution, with a mean of 0 and a variance of 1:</p>
<p><span class="math display">\[
Z \sim\mathcal N(0, 1), \quad \P(a&lt;z&lt;b) = \int\limits_a^b \frac{e^{-\frac{z^2}{2}}}{\sqrt{2\pi}}dz = \int\limits_a^b\phi(z)dz
\]</span></p>
<p>Note: we denote the PDF of the standard normal as <span class="math inline">\(\phi\)</span>, and the CDF of the standard normal as <span class="math inline">\(\Phi\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/img_standard_normal.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
<p>The best part about the standard normal distribution is that we can transform any other normal distribution into a standard normal. Recall the properties of adding and multiplying constants to a normal distribution from above. That means we can apply a transformation to every value/unit in a normal distribution <span class="math inline">\(X\)</span> to get a standard normal distribution <span class="math inline">\(Z\)</span>:</p>
<p><span class="math display">\[
\text{if } X \sim \mathcal N(\mu, \sigma^2), \quad Z = \frac{x - \E X}{\sigma} \sim \mathcal N(0,1)
\]</span></p>
<p>Or in other words, for every value of <span class="math inline">\(x \in X\)</span>, subtract the mean, then divide by the standard deviation. If you do this for every value <span class="math inline">\(x \in X\)</span>, you will get a standard normal.</p>
<p><br></p>
</section>
<section id="the-t-distribution" class="level3">
<h3 class="anchored" data-anchor-id="the-t-distribution">The T-Distribution</h3>
<p>The <strong>T-distribution</strong> looks very similar to that of the normal distribution, with the same bell curve shape. However, the t-distribution’s tails are slightly fatter than the normal distribution, and the peak is slightly shorter.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-1130923565.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:45.0%"></p>
</figure>
</div>
<p>The key difference between the t-distribution and the normal distribution is the parameters. Unlike the normal distribution, which has 2 parameters, the t-distribution has only one parameter: degrees of freedom.</p>
<p>The t-distribution is always centered on 0. Higher degrees of freedom means less thick tails. As degrees of freedom get larger (past 30), it almost perfectly fits a normal distribution.</p>
<p>The importance of the T-distribution is that it is often used for statistical inference when the normal distribution, for whatever reason, cannot be used. This is often because with the normal distribution, we need to know variance <span class="math inline">\(\sigma^2\)</span>, but we do not need to know this to use the T-distribution.</p>
<p><br></p>
</section>
<section id="bernoulli-and-binomial-distribution" class="level3">
<h3 class="anchored" data-anchor-id="bernoulli-and-binomial-distribution">Bernoulli and Binomial Distribution</h3>
<p>A <strong>bernoulli trial</strong> is an experiment that has two possible outcomes: a success <span class="math inline">\(z= 1\)</span> and a failure <span class="math inline">\(z = 0\)</span>. We denote the probability of a success as <span class="math inline">\(p\)</span>, and the probability of a failure as <span class="math inline">\(q = 1-p\)</span>. For example, a coin flip could be seen as a bernoulli trial, with <span class="math inline">\(p = 0.5\)</span>.</p>
<p>For example, below is a bernoulli distribution with <span class="math inline">\(p = 0.15\)</span>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-2068255268.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%"></p>
</figure>
</div>
<p>The probability density function of a bernoulli distribution is:</p>
<p><span class="math display">\[
\varphi(z) = \begin{cases}
p \quad \text{if } z = 1 \\
1 - p \quad \text{if } z= 0
\end{cases}
\]</span></p>
<p>The expected value of the bernoulli trial is equal to the probability of a a success <span class="math inline">\(p\)</span>.</p>
<p><span class="math display">\[
\E Z = \P (z = 1) = p
\]</span></p>
<p>The bernoulli distribution is a special case of the <strong>Binomial distribution</strong>, which is basically the question of how many success you will get with <span class="math inline">\(n\)</span> number of trials. When <span class="math inline">\(n=1\)</span> (just one coin flip), we get the bernoulli distribution.</p>
<p>A bernoulli distribution is often employed in randomisation of treatment during experiments, as we will see later.</p>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="estimators-and-statistical-inference" class="level1">
<h1><strong>Estimators and Statistical Inference</strong></h1>
<section id="estimators-and-sampling-distributions" class="level3">
<h3 class="anchored" data-anchor-id="estimators-and-sampling-distributions">Estimators and Sampling Distributions</h3>
<p>An <strong>estimand</strong> is the true value of some true parameter <span class="math inline">\(\theta\)</span> in the population we are trying to measure.</p>
<p>We often do not have data on the population. We typically have a sample from the population, and use an <strong>estimator</strong> (procedure) to produce a sample <strong>estimate</strong> <span class="math inline">\(\hat\theta\)</span>. Estimators and estimates are denoted with either a hat <span class="math inline">\(\hat\theta\)</span> or tilde <span class="math inline">\(\tilde\theta\)</span>.</p>
<ul>
<li>For example, if we wanted to find the average trust in institutions in the UK, we cannot possibly ask 70 million people. So, instead, we take a sample from the population, and produce a sample estimate.</li>
</ul>
<p>However, because of <u>sampling variability</u> (not all random samples will be identical), each sample <span class="math inline">\(n\)</span> will have a different estimate <span class="math inline">\(\hat\theta_n\)</span>.</p>
<p>Imagine if we keep taking <span class="math inline">\(N\)</span> number of samples, we will have <span class="math inline">\(N\)</span> number of estimates <span class="math inline">\(\hat\theta_1, \hat\theta_2, \dots, \hat\theta_N\)</span>. Thus, any specific estimate <span class="math inline">\(\theta_n\)</span> from sample <span class="math inline">\(n\)</span> can be thought of as a random draw from the <strong>sampling distribution</strong> <span class="math inline">\(\hat\theta_1, \hat\theta_2, \dots, \hat\theta_N\)</span>.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example of a Sampling Distribution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let us say we want to find the mean salary of all individuals in the UK. The true value of the mean salary for every individual is <span class="math inline">\(\theta\)</span>.</p>
<p>However, asking all 60 million people is nearly impossible. So, we take a randomly sample of 1000 individuals, and then find the sample mean. Our estimator is thus the sample mean estimator.</p>
<p>Our first sample of 1000 individuals yields an estimate <span class="math inline">\(\hat\theta_1\)</span>. If we take another sample, we will get slightly different people in this sample, and get another estimate <span class="math inline">\(\hat\theta_2\)</span>. We keep taking samples, and get more and more estimates <span class="math inline">\(\hat\theta_3, \hat\theta_4, \dots, \hat\theta_n\)</span>.</p>
<p>We plot all of these samples into a distribution as follows:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-2444367374.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:45.0%"></p>
</figure>
</div>
<p>This indicates the potential estimates we can get. If we were to conduct only one sample, we would essentially be selecting a random <span class="math inline">\(\hat\theta_i\)</span> value from this distribution.</p>
<p>The sampling distribution of an estimator is the key property of estimators. The two parameters of interest from this sampling distribution are its <strong>expectation</strong> and <strong>variance</strong>.</p>
</div>
</div>
</div>
<p>We can describe sampling distributions with their expectation and variance.</p>
<p><br></p>
</section>
<section id="finite-sample-properties-of-estimators" class="level3">
<h3 class="anchored" data-anchor-id="finite-sample-properties-of-estimators">Finite Sample Properties of Estimators</h3>
<p>An estimator of a parameter is <strong>unbiased</strong>, if its estimates <span class="math inline">\(\hat\theta_n\)</span> have an expectation equal to the true population value of the parameter: <span class="math inline">\(\E \hat\theta_n = \theta\)</span>. Or in other words, if we repeatedly sample and use the estimator, on average, the estimates will be equal to the true population value (the sampling distribution will have an expectation of the true population value).</p>
<p>We want an unbiased estimator, because if <span class="math inline">\(\E \hat\theta_n = \theta\)</span>, that means our “best guess” of the estimator value is the true parameter value <span class="math inline">\(\theta\)</span>. That means any one estimate <span class="math inline">\(\hat\theta_n\)</span> is on average, correct.</p>
<p>Unbiasedness is not the only desirable property of estimators - we also care about the variance. After all, if we have two unbiased estimators, the one with less variance will be on average, closer to the true population value, for any one estimate <span class="math inline">\(\hat\theta\)</span>.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example of the Importance of Variance
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For example, let us say the true population parameter is <span class="math inline">\(\theta = 0\)</span>. We will have two estimators: estimator <span class="math inline">\(A\)</span> and estimator <span class="math inline">\(B\)</span>:</p>
<ul>
<li>Estimator <span class="math inline">\(A\)</span>, after two samples (for simplicity), produces estimates -1 and 1.</li>
<li>Estimator <span class="math inline">\(B\)</span>, after two samples, produces estimates -100 and 100.</li>
</ul>
<p>Both estimators are unbiased <span class="math inline">\(\E \hat\theta_n = 0\)</span>. However, clearly, estimator <span class="math inline">\(A\)</span> is, on average, closer to <span class="math inline">\(\theta =0\)</span> than estimator <span class="math inline">\(B\)</span>. This is because while both estimators are unbiased, estimator <span class="math inline">\(A\)</span> has a smaller <strong>variance</strong> than estimator <span class="math inline">\(B\)</span> - that is on average, estimator <span class="math inline">\(A\)</span>’s estimators are more closely “packed around” the expectation of the estimator.</p>
</div>
</div>
</div>
<p>The <strong>variance</strong> of an estimator (and variance of the sampling distribution) can be quantified as:</p>
<p><span class="math display">\[
\V \hat\theta_n = \E[(\hat\theta_n - \E \hat\theta_n)^2]
\]</span></p>
<p>An <strong>efficient</strong> estimator is one that, on average, has the closest estimated value <span class="math inline">\(\hat\theta_n\)</span> to the true population parameter. If two estimators are both unbiased, the one with lower variance is more efficient. Efficiency can be quantified as the estimator with the lowest <strong>mean squared error</strong>:</p>
<p><span class="math display">\[
\mathrm{MSE}(\hat\theta_n) = \E[(\hat\theta_n - \theta)^2] =\V \hat\theta_n + \underbrace{(\E \theta_n - \theta)}_{\mathrm{bias}}
\]</span></p>
<p>We generally want an efficient estimator, since gives us the closest guess to the true population parameter <span class="math inline">\(\theta\)</span>.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Efficient but Biased
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Interestingly, it is possible for a biased estimator to be more efficient than an unbiased estimator.</p>
<p>This is particularly the case when the biased estimator has a slight bias but small variance, while the unbiased estimator has a giant variance. In this case, the biased estimator is producing estimates <span class="math inline">\(\hat\theta\)</span> that on average, are closer to the true population parameter <span class="math inline">\(\theta\)</span>.</p>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="asymptotic-properties-of-estimators" class="level3">
<h3 class="anchored" data-anchor-id="asymptotic-properties-of-estimators">Asymptotic Properties of Estimators</h3>
<p>Asymptotic properties are properties of estimators as the sample size <span class="math inline">\(n\)</span> approaches infinity.</p>
<p>An estimator is <strong>asymptotically consistent</strong>, if as we increase sample size towards infinity, the estimate will become more and more concentrated around the true population value <span class="math inline">\(\theta\)</span>. At <span class="math inline">\(n = ∞\)</span>, our sampling distribution collapses to just one value, the true population value <span class="math inline">\(\theta\)</span>. Mathematically:</p>
<p><span class="math display">\[
\P(|\hat\theta_n - \theta|&gt; \varepsilon) \rightarrow 0, \text { as } n \rightarrow ∞
\]</span></p>
<p>That means with a consistent estimator, increasing the sample size will get us more and more accurate results.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Biased but Consistent
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>An estimator can be both biased, but consistent. In smaller sample sizes, the estimator might not be on average correct, but over a large enough sample size, it will become “unbiased”.</p>
<p>For example, in the figure below, we can see that this estimator is biased at small values of <span class="math inline">\(n\)</span>, but as <span class="math inline">\(n\)</span> increases, it becomes more consistent, collapsing its distribution around the true <span class="math inline">\(\theta\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-1215503621.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:55.0%"></p>
</figure>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Law of Large Numbers and Consistency
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The law of large numbers states that the sample average of a random sample, is a consistent estimator of the population mean.</p>
<p>For example, let us say we have a random variable <span class="math inline">\(x\)</span>. We take a random sample of <span class="math inline">\(n\)</span> units, so our sample is <span class="math inline">\((x_1, \dots, x_n)\)</span>.</p>
<ul>
<li>Let us define <span class="math inline">\(\bar x_n\)</span> as our sample average.</li>
<li>Let us define <span class="math inline">\(\mu\)</span> as the true population mean of variable <span class="math inline">\(x\)</span>.</li>
</ul>
<p>The law of large numbers states that:</p>
<p><span class="math display">\[
plim( \bar x_n) = \mu
\]</span></p>
<ul>
<li>Where <span class="math inline">\(plim\)</span> states that as <span class="math inline">\(n\)</span> approaches infinity, the probability distribution of <span class="math inline">\(\bar x_n\)</span> collapses around <span class="math inline">\(\mu\)</span>.</li>
</ul>
<p>Why is this the case? This sample mean estimator is calculated simply through the formula for mean:</p>
<p><span class="math display">\[
\bar x_n = \frac{1}{n}\sum\limits_{i=1}^n x_i
\]</span></p>
<p>Let us define the variance of our sample of <span class="math inline">\(x_1, \dots, x_n\)</span> as <span class="math inline">\(Var(x_i) = \sigma^2\)</span>. We can now find the variance of our sampling distribution of estimator <span class="math inline">\(\bar x_n\)</span>:</p>
<p><span class="math display">\[
\begin{split}
Var(\bar x_n) &amp; = Var\left( \frac{1}{n}\sum\limits_{i=1}^n x_i \right) \\
&amp; = \frac{1}{n^2} Var \left(\sum\limits_{i=1}^n x_i\right) \\
&amp; = \frac{1}{n^2} \sum\limits_{i=1}^n Var(x_i) \\
&amp; = \frac{1}{n^2} \sigma^2 \\
&amp; = \frac{\sigma^2}{n}
\end{split}
\]</span></p>
<p>And as sample size <span class="math inline">\(n\)</span> increases to infinity, we get:</p>
<p><span class="math display">\[
\lim\limits_{n \rightarrow ∞} Var(\bar x_n) = \lim\limits_{n \rightarrow ∞} \frac{\sigma^2}{n} = 0
\]</span></p>
<p>Thus, the variance of our estimator <span class="math inline">\(\bar x_n\)</span> shrinks to zero, so as sample size increases to infinity <span class="math inline">\(n\)</span>, the sampling distribution of estimator <span class="math inline">\(\bar x_n\)</span> collapses around the true population mean.</p>
</div>
</div>
</div>
<p>Another asymptotic property of estimators, as sample size <span class="math inline">\(n\)</span> approaches infinity, is that the sampling distribution approaches a normal distribution. The <strong>central limit theorem</strong> establishes asymptotic normality of estimators. Let us say we have <span class="math inline">\(N\)</span> number of random variables <span class="math inline">\(\hat\theta_1, \dots, \hat\theta_N\)</span> (estimates are realisations of random variables). The central limit theorem states that:</p>
<p><span class="math display">\[
P(z_n &lt; w) \rightarrow \Phi(z) \quad \text{as } n \rightarrow ∞
\]</span></p>
<ul>
<li>Where <span class="math inline">\(z_n\)</span> is a transformed version of the random variable <span class="math inline">\(\hat\theta_n\)</span>, defined as <span class="math inline">\(z_n = \frac{\bar\theta_n - \E \hat\theta_n}{\sigma / \sqrt{n}}\)</span>.</li>
<li>Where <span class="math inline">\(\P(z_n &lt; z)\)</span> is the cumulative density function of the random variable <span class="math inline">\(w_n\)</span>.</li>
<li>Where <span class="math inline">\(\Phi(z)\)</span> is the cumulative density function (cdf) of the standard normal distribution <span class="math inline">\(\mathcal N(0, 1)\)</span>.</li>
</ul>
<p>The importance of CLM comes from the fact that as we increase sample size, our sampling distribution becomes more and more normally distributed. Recall that we know that the normal distribution has unique <a href="#properties-of-the-normal-distribution">properties of distribution</a> that allow us to calculate the probability between standard deviations (and any point). Since CLM says that our sampling distribution is normally distributed, we can apply the same properties to learn about the probabilities of getting a certain sample estimates <span class="math inline">\(\hat\theta_n\)</span>.</p>
<p><br></p>
</section>
<section id="intuition-of-hypothesis-testing" class="level3">
<h3 class="anchored" data-anchor-id="intuition-of-hypothesis-testing">Intuition of Hypothesis Testing</h3>
<p>We have status-quo theory, called the <strong>null hypothesis</strong> <span class="math inline">\(H_0\)</span>. A status quo theory is the generally accepted value of the true population parameter <span class="math inline">\(\theta\)</span> in our field. However, you might want to prove this status-quo theory wrong, and have an <strong>alternative hypothesis</strong> <span class="math inline">\(H_1\)</span>. You need some way to prove that the status-quo null hypothesis is wrong.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example of Hypotheses
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let us say we are interested in measuring the relationship between democracy and economic growth.</p>
<p>The status-quo theory is that there is no relationship - since we need to prove there is a relationship. Thus, our null hypothesis is <span class="math inline">\(H_0 : \theta = 0\)</span> (0 representing 0 relationship).</p>
<p>Our alternative hypothesis might be <span class="math inline">\(H_1: \theta ≠ 0\)</span> - i.e.&nbsp;there is a relationship between democracy and economic growth.</p>
<p>To prove our alternative hypothesis, we need to disprove the null hypothesis.</p>
</div>
</div>
</div>
<p>We cannot know if the null hypothesis is wrong. However, we can calculate the probability of getting a certain sample estimate <span class="math inline">\(\hat\theta_n\)</span> assuming the null hypothesis is true. If we assume an unbiased estimator, that means our hypothetical sampling distribution should have an expected value of our null hypothesis: <span class="math inline">\(\E \hat\theta_n = H_0\)</span>.</p>
<p>Then, we can gather a sample, and calculate some sample estimate <span class="math inline">\(\hat\theta\)</span>. Using some math (we will see this later with regression), we can also estimate the variance of the sampling distribution.</p>
<p>Thus, we now know the expectation of the sampling distribution <span class="math inline">\(\E \hat\theta_n = H_0\)</span> and the variance. We also know that the sampling distribution is normally distributed by the central limit theorem. Thus, using the properties of the normal distribution, we can calculate the probability of getting our sample estimate <span class="math inline">\(\hat\theta\)</span> or an estimate even further from the <span class="math inline">\(H_0\)</span> than our <span class="math inline">\(\hat\theta\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-1533818238.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:45.0%"></p>
</figure>
</div>
<p>For example, above, our sample estimate <span class="math inline">\(\hat\theta = -2.228\)</span>, and our null hypothesis is <span class="math inline">\(\theta = 0\)</span>. The yellow parts highlighted are the probability of getting an sample estimate <span class="math inline">\(\hat\theta_n\)</span> as extreme or more extreme than our <span class="math inline">\(\hat\theta = -2.228\)</span>.</p>
<p>If this probability of getting our estimate <span class="math inline">\(\hat\theta\)</span> or more extreme is very small (less than 5% typically), we can believe either one of two things:</p>
<ol type="1">
<li>We got extremely lucky (less than 5% chance) and got a estimate very far from the null hypothesis.</li>
<li>Or, we <u><strong>typically</strong></u> believe that we were not actually lucky, but instead, the null hypothesis value of <span class="math inline">\(\theta\)</span> is incorrect.</li>
</ol>
<p>Thus, conclusion 2 essentially means we reject the null hypothesis, and conclude our alternative hypothesis.</p>
<p><br></p>
</section>
<section id="mechanics-of-hypothesis-testing" class="level3">
<h3 class="anchored" data-anchor-id="mechanics-of-hypothesis-testing">Mechanics of Hypothesis Testing</h3>
<p>To start a hypothesis test, you will need to define a status-quo null hypothesis <span class="math inline">\(H_0\)</span> and an alternative hypothesis <span class="math inline">\(H_1\)</span>.</p>
<p>Then, you should take a sample from the population, and compute a sample estimate <span class="math inline">\(\hat\theta\)</span> with your estimator. You can then derive a <strong>standard error</strong> (square root of the variance of the sampling distribution <span class="math inline">\(\V \hat\theta_n\)</span>). This standard error formula will differ depending on the statistical model, and we will introduce several of these for regression.</p>
<p>Then, you can compute a test statistic. The name of the test-statistic will differ based on the statistical model, but the formula is always the same:</p>
<p><span class="math display">\[
\text{test statistic} = \frac{\hat\theta - H_0}{\text{standard error of } \hat\theta}
\]</span></p>
<p>This formula might look familiar - we are essentially turning our normal distribution of the estimator into the <a href="#the-standard-normal-distribution">standard normal distribution</a>. The test statistic is essentially measuring how many standard deviations away from the null hypothesis our sample estimate <span class="math inline">\(\hat\theta\)</span> is.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/img_standard_normal.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%"></p>
</figure>
</div>
<p>Now, go onto the x-axis of our standard normal distribution, and start at the mean (at 0). Now, move in both directions by the distance specified by the test statistic. Mark these two points on either side of the mean. Highlight the are under the sampling distribution beyond these points. The figure below shows a test statistic of 2.228:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-1533818238.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:35.0%"></p>
</figure>
</div>
<p>The highlighted area is the probability (<strong>p-value</strong>) of getting a sample estimate <span class="math inline">\(\hat\theta\)</span> equal or more extreme than the one we got, assuming the null hypothesis is true. If the p-value is below 0.05 (5%), we have sufficient evidence to reject the null hypothesis. If the p-value is above 0.05 (5%), we cannot reject the null hypothesis.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note on Distributions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Here, we have stated you use the standard normal distribution.</p>
<p>However, with different tests, you might actually use some different distribution. Most often, you will use the t-distribution.</p>
<p>The reason we use the t-distribution is because we often cannot actually calculate the variance of our sampling distribution. Instead, we estimate it with some uncertainty, so the t-distribution accounts for this uncertainty.</p>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="confidence-intervals" class="level3">
<h3 class="anchored" data-anchor-id="confidence-intervals">Confidence Intervals</h3>
<p>Sometimes, we are not just interested in proving a null hypothesis wrong. We often are interested in the actual parameter value of <span class="math inline">\(\theta\)</span>. As we know already, there is variation in our estimates <span class="math inline">\(\hat\theta_n\)</span> between samples.</p>
<p>Thus to account for this uncertainty, we want to create some range around our sample estimate <span class="math inline">\(\hat\theta\)</span> that is likely to contain the true value of <span class="math inline">\(\theta\)</span>. This is called a <strong>confidence interval</strong>.</p>
<p>According the the central limit theorem, we know that any sample estimate <span class="math inline">\(\hat\theta_n\)</span> is 95% likely to be within 2 standard deviations of the true population value <span class="math inline">\(\theta\)</span>.</p>
<p>Thus, we can construct an interval of 2 standard errors (square deviations) on both sides of our sample estimate <span class="math inline">\(\hat\theta_n\)</span>. This means that 95% of the time, our interval will include the true population value of <span class="math inline">\(\theta\)</span>. Thus, our 95% confidence interval is:</p>
<p><span class="math display">\[
(\hat\theta - 1.96se(\hat\theta), \ \ \hat\theta+1.96se(\hat\theta)
\]</span></p>
<ul>
<li>Why 1.96? Because exactly 95% of a normal distribution is within 1.96 standard deviations of the mean.</li>
</ul>
<p>This interval means that under repeated sampling and estimating <span class="math inline">\(\hat\theta_n\)</span>, 95% of the confidence intervals we construct will include the true population <span class="math inline">\(\theta\)</span> value.</p>
<p><br></p>
</section>
<section id="nonparametric-bootstrap" class="level3">
<h3 class="anchored" data-anchor-id="nonparametric-bootstrap">Nonparametric Bootstrap</h3>
<p>Most traditional statistical tests rely on asymptotic normality established by the central limit theorem. However, asymptotic normality can only be satisfied if we have a large enough sample size. When we are dealing with small samples, we cannot invoke central limit theorem.</p>
<p>Nonparametric Bootstrap, instead of assuming some sampling distribution, is a method to simulate the sampling distribution. This is done by re-sampling from the sample with replacement. The procedure is as follows:</p>
<ol type="1">
<li>You take the sample you observe (with sample size <span class="math inline">\(n\)</span>), and randomly re-sample <span class="math inline">\(n\)</span> observations from that sample with replacement (so allowing observations to repeat in our re-sample).</li>
<li>Continue to do this over and over again to get <span class="math inline">\(B\)</span> number of re-samples.</li>
<li>For each re-sample <span class="math inline">\(b\)</span>, you should calculate the <span class="math inline">\(\widehat{\theta_b}\)</span>. Plot all of the sample <span class="math inline">\(\widehat{\theta_b}\)</span> in a distribution.</li>
</ol>
<p>You can also estimate the standard error of <span class="math inline">\(\hat\theta\)</span> using the standard deviation of the distribution. However, do not use these standard errors for confidence intervals or tests unless you are confident the sampling distribution is approximately normal.</p>
<p>Nonparametric Bootstrap is also used in some more complex estimators where it is very difficult to calculate or estimate the standard errors.</p>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="correlations-between-variables" class="level1">
<h1><strong>Correlations Between Variables</strong></h1>
<section id="correlated-variables" class="level3">
<h3 class="anchored" data-anchor-id="correlated-variables">Correlated Variables</h3>
<p>In political science and most social sciences, our primary concern is relationships between variables. How does education correlate with voter turnout? Are less educated citizens more anti-immigrant? How do inflation levels correlate with incumbent popularity?</p>
<p>We describe a relationship between two variables <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span> as a correlation (from now on, all random variables are denoted with a subscript).</p>
<ul>
<li>If we are more likely to observe higher values of <span class="math inline">\(Y_i\)</span> when we also observe higher values of <span class="math inline">\(X_i\)</span>, that means we have a positive correlation.</li>
<li>If we are more likely to observe lower values of <span class="math inline">\(Y_i\)</span> when we also observe higher values of <span class="math inline">\(X_i\)</span>, that means we have a negative correlation.</li>
<li>If the value of <span class="math inline">\(Y_i\)</span> does not change no matter the value of <span class="math inline">\(X\)</span>, then the two variables are uncorrelated.</li>
</ul>
<p><u>Important Note: correlation is <strong>not</strong> causation!</u> We can view correlations between variables graphically:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-3489268418.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
</section>
<section id="quantifying-correlations" class="level3">
<h3 class="anchored" data-anchor-id="quantifying-correlations">Quantifying Correlations</h3>
<p>There are two primary ways to quantify correlations between variables. The first is called <strong>covariance</strong>:</p>
<p><span class="math display">\[
cov(X_i,Y_i)  = \E((X_i - \E X_i)(Y_i - \E Y_i)) \\
\]</span></p>
<p>If covariance is negative, that means we have a negative correlation. If covariance is positive, we have a positive correlation. If covariance is 0, we have no correlation.</p>
<p>However, we cannot interpret the actual number of covariance, only the sign. This is because covariance is sensitive to measurement scale: if we change something measured in feet to inches, covariance increases.</p>
<p>Obviously, we do not want a measure that is affected by measurement scale. We want some measure of correlation that can be compared across different scales. This is where the <strong>correlation coefficient</strong> comes in:</p>
<p><span class="math display">\[
\rho = \frac{cov(X_i, Y_i)}{\sqrt{\V X \V Y}}
\]</span></p>
<p>Essentially, we are “normalising” the covariance metric by dividing by the variances, which gets rid of the impact of scale.</p>
<p>Correlation coefficients are always between -1 and 1. Values closer to -1 and 1 are stronger correlations, and values closer to 0 are weaker correlations.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-3774741322.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>We can see here that stronger correlations have points that fit closer to a straight line.</p>
<p><br></p>
</section>
<section id="best-fit-lines-and-magnitude" class="level3">
<h3 class="anchored" data-anchor-id="best-fit-lines-and-magnitude">Best-Fit Lines and Magnitude</h3>
<p>Correlation coefficients have a huge weakness: they measure the “strength” of correlation, but not the magnitude.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-1623227249.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%"></p>
</figure>
</div>
<p>We can see above that these two relationships have the same strength of correlation. However, the slope of the left graph is much higher. Why is the slope important? Well, the slope is essentially how much <span class="math inline">\(Y_i\)</span> changes when <span class="math inline">\(X_i\)</span> increases by 1. It is the <strong>magnitude</strong> of our relationship.</p>
<p>Thus, we might wish to instead look at relationships with a best-fit straight line. Take this figure below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-3708176954.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:45.0%"></p>
</figure>
</div>
<p>We can express this line mathematically as <span class="math inline">\(y = mx + b\)</span>, where <span class="math inline">\(m\)</span> is the slope - the amount <span class="math inline">\(y\)</span> change for every one unit increase in <span class="math inline">\(x\)</span>. Thus, the slope <span class="math inline">\(m\)</span> is a measurement of the magnitude of correlation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p>
<p><br></p>
</section>
<section id="simple-linear-regression" class="level3">
<h3 class="anchored" data-anchor-id="simple-linear-regression">Simple Linear Regression</h3>
<p>Simple Linear Regression is a mathematical model of a best fit line. It takes the following form:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1X_i + \eps_i
\]</span></p>
<ul>
<li>Where <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span> are random variables.</li>
<li>Where <span class="math inline">\(\beta_1\)</span> is the slope - the measurement of the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. <span class="math inline">\(\beta_0\)</span> is the intercept (y-intercept).</li>
<li><span class="math inline">\(\eps_i\)</span> is the error term: the vertical distance between the actual points and the best-fit line.</li>
</ul>
<p>We have some special terminology for the parts of the simple linear regression. The <strong>independent variable</strong>, also called the explanatory variable or the regressor, is labelled <span class="math inline">\(X_i\)</span>. This is the variable multiplied to the slope parameter <span class="math inline">\(\beta_1\)</span>. The <strong>dependent variable</strong>, also called the response or outcome variable, is labelled <span class="math inline">\(Y_i\)</span>.</p>
<p>We can also write regression as the <a href="#conditional-distributions">conditional expectation</a> <span class="math inline">\(\E(Y_i | X_i)\)</span> of the conditional distribution.</p>
<p><span class="math display">\[
\E(Y_i|X_i) = \beta_0 + \beta_1 X_i
\]</span></p>
<p>Throughout this chapter, we have talked about statistical inference and estimators. This is no different in linear regression. The above equation is what we call our population model (also called the data generating process):</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1X_i + \eps_i
\]</span></p>
<p>However, once again, we cannot know the true population values of our intercept and slope. So, we will have a sample model of the following:</p>
<p><span class="math display">\[
Y_i = \hat\beta_0 + \hat\beta_1X_i + \hat\eps_i
\]</span></p>
<p>We will discuss the details of this estimation process of <span class="math inline">\(\hat\beta_0, \hat\beta_1, \hat\eps_i\)</span> in the next chapter, and how we can conduct statistical inference and tests. The points on our predicted best-fit line (so excluding error <span class="math inline">\(\widehat{u_i}\)</span>) are called our <strong>fitted values</strong>:</p>
<p><span class="math display">\[
\hat Y_i = \hat\beta_0 + \hat\beta_1X_i
\]</span></p>
<p>The simple linear regression model is very simple. In the real world, rarely does only one thing cause another. For example, changes in income could be caused by age, but also education. Instead of just one explanatory variable <span class="math inline">\(X_i\)</span>, we can create regression models with multiple explanatory variables <span class="math inline">\(X_{i1}, X_{i2}, \dots, X_{ip}\)</span>. This is the multiple linear regression that we will introduce in the quantitative methods section.</p>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>