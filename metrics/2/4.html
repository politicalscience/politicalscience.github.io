<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Chapter 4: Multiple Linear Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="4_files/libs/clipboard/clipboard.min.js"></script>
<script src="4_files/libs/quarto-html/quarto.js"></script>
<script src="4_files/libs/quarto-html/popper.min.js"></script>
<script src="4_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="4_files/libs/quarto-html/anchor.min.js"></script>
<link href="4_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="4_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="4_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="4_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="4_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Chapter 4: Multiple Linear Regression</h1>
            <p class="subtitle lead">Econometric Methods (for Social Scientists)</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Chapter 4: Multiple Linear Regression</h2>
   
  <ul class="collapse">
  <li><a href="#the-multiple-linear-regression-model" id="toc-the-multiple-linear-regression-model" class="nav-link active" data-scroll-target="#the-multiple-linear-regression-model">4.1: The Multiple Linear Regression Model</a></li>
  <li><a href="#multiple-linear-regression-with-linear-algebra" id="toc-multiple-linear-regression-with-linear-algebra" class="nav-link" data-scroll-target="#multiple-linear-regression-with-linear-algebra">4.2: Multiple Linear Regression with Linear Algebra</a></li>
  <li><a href="#ordinary-least-squares-estimator-for-multiple-regression" id="toc-ordinary-least-squares-estimator-for-multiple-regression" class="nav-link" data-scroll-target="#ordinary-least-squares-estimator-for-multiple-regression">4.3: Ordinary Least Squares Estimator for Multiple Regression</a></li>
  <li><a href="#interpretation-of-coefficients-in-multiple-linear-regression" id="toc-interpretation-of-coefficients-in-multiple-linear-regression" class="nav-link" data-scroll-target="#interpretation-of-coefficients-in-multiple-linear-regression">4.4: Interpretation of Coefficients in Multiple Linear Regression</a></li>
  <li><a href="#model-summary-statistics" id="toc-model-summary-statistics" class="nav-link" data-scroll-target="#model-summary-statistics">4.5: Model Summary Statistics</a></li>
  <li><a href="#standard-errors-and-hypothesis-testing" id="toc-standard-errors-and-hypothesis-testing" class="nav-link" data-scroll-target="#standard-errors-and-hypothesis-testing">4.6: Standard Errors and Hypothesis Testing</a></li>
  <li><a href="#joint-coefficient-hypotheses-tests" id="toc-joint-coefficient-hypotheses-tests" class="nav-link" data-scroll-target="#joint-coefficient-hypotheses-tests">4.7: Joint Coefficient Hypotheses Tests</a></li>
  <li><a href="#implementation-in-r" id="toc-implementation-in-r" class="nav-link" data-scroll-target="#implementation-in-r">Implementation in R</a>
  <ul class="collapse">
  <li><a href="#simple-linear-regression-in-r" id="toc-simple-linear-regression-in-r" class="nav-link" data-scroll-target="#simple-linear-regression-in-r">Simple Linear Regression in R</a></li>
  <li><a href="#confidence-intervals" id="toc-confidence-intervals" class="nav-link" data-scroll-target="#confidence-intervals">Confidence Intervals</a></li>
  <li><a href="#creating-regression-tables" id="toc-creating-regression-tables" class="nav-link" data-scroll-target="#creating-regression-tables">Creating Regression Tables</a></li>
  <li><a href="#f-tests-of-nested-models" id="toc-f-tests-of-nested-models" class="nav-link" data-scroll-target="#f-tests-of-nested-models">F-Tests of Nested Models</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>In this chapter, we build on the last chapter and introduce the multiple linear regression model, and how it can be a useful tool to address the impact of confounding variables.</p>
<p>Topics: Multiple Linear Regression, OLS Estimator, Standard Errors and Hypothesis Testing, R-Squared and F-tests.</p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
<section id="the-multiple-linear-regression-model" class="level1">
<h1>4.1: The Multiple Linear Regression Model</h1>
<p>Let us refresh on the goals of econometrics in order to find a causal effect between <span class="math inline">x</span> and <span class="math inline">y</span>:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Goal of Econometrics
</div>
</div>
<div class="callout-body-container callout-body">
<p>Our goal in econometrics is as follows:</p>
<ol type="1">
<li>Find the correlation between <span class="math inline">x</span> and <span class="math inline">y</span>.</li>
<li>Then, remove the effect of confounders <span class="math inline">w</span> from our correlation.</li>
<li>Then, remove the effect of <span class="math inline">y</span> causing <span class="math inline">x</span> (reverse causality) from our correlation.</li>
</ol>
<p>Then, we are left with the causal effect of <span class="math inline">x \rightarrow y</span>.</p>
</div>
</div>
<p>In the last chapter, we discussed simple linear regression, which addresses the correlation part between <span class="math inline">x</span> and <span class="math inline">y</span>.</p>
<ul>
<li>But we know from above, that correlation is not enough to find a causal effect.</li>
</ul>
<p>In this chapter, we cover the second goal: removing the effect of confounders.</p>
<ul>
<li>This chapter <strong>will not discuss causal inference</strong>. This chapter (and the next) only discusses multiple linear regression as a possible way to control for confounding variables.</li>
<li>We will return to causal inference in Chapter 6.</li>
</ul>
<p><br></p>
<p>Multiple linear regression is an extension of simple linear regression, that can help us deal with confounding variables.</p>
<ul>
<li>Multiple linear regression allows us to “control for the effect” of confounders. We will discuss a little about this in section 4.5, and much more in depth in chapter 6.</li>
</ul>
<p>The <strong>response variable</strong> (outcome variable) is notated <span class="math inline">y</span>, just like in single linear regression.</p>
<p>The <strong>explanatory variable</strong>s are <span class="math inline">x_1, x_2, ..., x_k</span>. We sometimes also denote all explanatory variables as the vector <span class="math inline">\overrightarrow{x}</span>.</p>
<ul>
<li>Our treatment variable of interest <span class="math inline">D</span> or <span class="math inline">x</span> is considered one of the explanatory variables <span class="math inline">\overrightarrow{x}</span> (most often <span class="math inline">x_1</span>).</li>
<li>The other explanatory variables <span class="math inline">x_2, ..., x_k</span> are confounding variables that we have chosen to include in the model.</li>
</ul>
<p>A linear regression model is the specification of the conditional distribution of <span class="math inline">Y</span>, given <span class="math inline">\overrightarrow{x}</span>. The linear regression model focuses on the <strong>expected value</strong> of the conditional distribution, notated <span class="math inline">\mathbb{E}[y_i|\overrightarrow{x}_i]</span>.</p>
<p><br></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Multiple Linear Regression Model
</div>
</div>
<div class="callout-body-container callout-body">
<p>Take a set of observed data with <span class="math inline">n</span> number of pairs of <span class="math inline">(\overrightarrow{x}_i, y_i)</span> observations. The linear model takes the following form:</p>
<p><span class="math display">
\mathbb{E}[y_i|\overrightarrow{x}_i] = \beta_0 + \beta_1 x_{1i} + ... + \beta_k x_{ki}
</span></p>
<ul>
<li>Where the coefficients (that need to be estimated) are vector<span class="math inline">\overrightarrow{\beta} = \beta_0, \beta_1, ..., \beta_k</span>.</li>
</ul>
<p>We can also write the linear model for the value of any point <span class="math inline">y_i</span> in our data:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1x_{1i} + ... + \beta_k x_{ki} + u_i
</span></p>
<ul>
<li>Where <span class="math inline">u_i</span> is the error term function - that determines the error for each unit <span class="math inline">i</span>. Error <span class="math inline">u_i</span> has a variance of <span class="math inline">\sigma^2</span>, and expectation <span class="math inline">\mathbb{E}[u_i] = 0</span>.</li>
</ul>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="multiple-linear-regression-with-linear-algebra" class="level1">
<h1>4.2: Multiple Linear Regression with Linear Algebra</h1>
<p>We can also represent the multiple linear regression model in linear algebra. Let us start with the linear model:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + ... + \beta_k x_{ki} + u_i
</span></p>
<p>The <span class="math inline">i</span>’th observation can be re-written in vector form as following:</p>
<p><span class="math display">
y_i = x_i'\beta + u_i, \text{ where }\beta = \begin{bmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_k\end{bmatrix} \text{ and }x_i = \begin{bmatrix}1 \\x_{1i} \\\vdots \\x_{ki}\end{bmatrix}
</span></p>
<ul>
<li>The <span class="math inline">x_i'</span> in the equation is the transpose of <span class="math inline">x_i</span>, to make matrix multiplication possible.</li>
<li>The first element of the <span class="math inline">x_i</span> matrix is 1, since <span class="math inline">1 \times \beta_0</span> gives us the first parameter (intercept) in the linear model.</li>
<li>Thus, when multiplying out, we get the same equation as the original multiple linear regression.</li>
</ul>
<p><br></p>
<p>Since our data has <span class="math inline">n</span> number of observations <span class="math inline">i</span>, we can express this into vector form, with the <span class="math inline">x_i'</span> and <span class="math inline">\beta</span> being vectors within a vector.</p>
<p><span class="math display">
\begin{split}
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} &amp; = \begin{pmatrix}x_1'\beta + u_1 \\ x_2'\beta + u_2 \\ \vdots \\ x_n'\beta + u_n\end{pmatrix} \\
&amp; \\
&amp; = \begin{pmatrix}x_1'\beta \\ x_2'\beta \\ \vdots \\ x_n'\beta\end{pmatrix} + \begin{pmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{pmatrix}
\end{split}
</span></p>
<p>Since <span class="math inline">\beta</span> vector appears as a common factor for all observations <span class="math inline">i=1,...,n</span>, we can factor it out and have an equation:</p>
<p><span class="math display">
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} = \begin{pmatrix}x_1' \\ x_2' \\ \vdots \\ x_n'\end{pmatrix} \space \beta + \begin{pmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{pmatrix}
</span></p>
<p><br></p>
<p>We can expand the <span class="math inline">x_1',...,x_n'</span> vector into a matrix. Remember that each <span class="math inline">x_1',...,x_n'</span> is already a vector of different explanatory variables. So, we get the following result:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Multiple Linear Regression with Linear Algebra
</div>
</div>
<div class="callout-body-container callout-body">
<p>The multiple linear regression can be expressed in linear algebra as:</p>
<p><span class="math display">
y = X \beta + u, \text{ where } X = \begin{bmatrix}1 &amp; x_{21} &amp; \dots &amp; x_{k1} \\1 &amp; x_{22} &amp; \dots &amp; x_{k2} \\\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\1 &amp; x_{2n} &amp; \dots &amp; x_{kn}\end{bmatrix}
</span></p>
<ul>
<li>Where the notation for elements of <span class="math inline">X</span> is <span class="math inline">x_{ki}</span>, with <span class="math inline">i</span> being the unit of observation <span class="math inline">i = 1, \dots n</span>, and <span class="math inline">k</span> being the explanatory variables index.</li>
<li>Where <span class="math inline">y</span> and <span class="math inline">u</span> are <span class="math inline">n \times 1</span> vectors (as seen above), and <span class="math inline">\beta</span> is a <span class="math inline">k \times 1</span> vector.</li>
<li>The first column of <span class="math inline">X</span> is a vector of 1, which exists because these 1’s are multiplied with <span class="math inline">\beta_0</span> in our model.</li>
</ul>
</div>
</div>
<p>The point of expressing the model in linear algebra is that it makes the estimation process far easier, as we will see in the next section.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="ordinary-least-squares-estimator-for-multiple-regression" class="level1">
<h1>4.3: Ordinary Least Squares Estimator for Multiple Regression</h1>
<p>As we remember from Chapter 1, the goal of Ordinary Least Squares Estimation is to minimise the sum of squared errors. The sum of squared errors in multiple regression is:</p>
<p><span class="math display">
\begin{split}
SSE &amp; = \sum\limits_{i=1}^n (y_i - \hat y_i)^2\\
&amp; = \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i} -  ... - \hat\beta_kx_{ki})^2
\end{split}
</span></p>
<p>Similar to our simple linear regression (but with additional variables), our minimisation condition is:</p>
<p><span class="math display">
\begin{split}
(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...) &amp; = \arg \min\limits_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i} ...)^2 \\
&amp; = \arg \min\limits_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)} S(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)
\end{split}
</span></p>
<p><br></p>
<p>Taking the partial derivatives of each parameter like in simple linear regression, we get first order conditions:</p>
<p><span class="math display">
\begin{split}&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\&amp; \sum\limits_{i=1}^n X_{1i}(y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\
&amp; \sum\limits_{i=1}^n X_{2i} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0
\end{split}
</span></p>
<ul>
<li>and so on for <span class="math inline">x_{3i}, ..., x_{ki}</span>.</li>
</ul>
<p>This system of equations includes <span class="math inline">k+1</span> variables and <span class="math inline">k+1</span> equations, which is way too difficult to solve.</p>
<p><br></p>
<p>Instead, we can use linear algebra. Let us define our estimation vector <span class="math inline">\hat{\beta}</span> as the value of <span class="math inline">\hat\beta</span> that minimises the sum of squared errors:</p>
<p><span class="math display">
\hat{\beta} = \arg \min\limits_{b} (y - Xb)' (y - Xb) = \arg \min\limits_b S(b)
</span></p>
<p>We can expand <span class="math inline">S(b)</span> as follows:</p>
<p><span class="math display">
\begin{split}
S(b) &amp; = y'y - b'X'y - y'Xb + b'X'Xb \\
&amp; = y'y - 2b'X'y + b'X'Xb
\end{split}
</span></p>
<p>Taking the partial derivative in respect to <span class="math inline">b</span>:</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} = \begin{pmatrix}\frac{\partial S(b)}{\partial b_1} \\\vdots \\\frac{\partial S(b)}{\partial b_k}\end{pmatrix}
</span></p>
<p>Differentiating with the vector <span class="math inline">b</span> yields:</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} = -2X'y + 2X'Xb
</span></p>
<p>Evaluted at <span class="math inline">\hat{\beta}</span>, the derivatives should equal zero (since first order condition of finding minimums):</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} \biggr|_{\hat{\beta}} = -2X'y + 2X'X \hat{\beta} = 0
</span></p>
<p>When assuming <span class="math inline">X'X</span> is invertable, we can isolate <span class="math inline">\hat{\beta}</span> to find the solution to OLS:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: OLS Estimate of <span class="math inline">\hat\beta</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Ordinary Least Squares Estimate of vector <span class="math inline">\hat\beta</span> for multiple linear regression is:</p>
<p><span class="math display">
\hat{\beta} = (X'X)^{-1} X'y
</span></p>
</div>
</div>
<p>Once we have estimates of <span class="math inline">\hat{\beta}</span>, we can plug them into our linear model to obtain fitted values:</p>
<p><span class="math display">
\hat{y} = X\hat{\beta} = X(X'X)^{-1} X'y
</span></p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="interpretation-of-coefficients-in-multiple-linear-regression" class="level1">
<h1>4.4: Interpretation of Coefficients in Multiple Linear Regression</h1>
<p>In <a href="https://politicalscience.github.io/metrics/2/3.html#interpretation-of-ols-coefficient-estimates">section 3.6</a>, we discussed the interpretation of coefficients <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span> in the simple linear regression. Things slightly change for multiple linear regression.</p>
<section id="interpretation-of-hatbeta_0" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-of-hatbeta_0">Interpretation of <span class="math inline">\hat\beta_0</span></h3>
<p>The interpretation of <span class="math inline">\hat\beta_0</span> still remains very similar - it is the expected value of <span class="math inline">y</span> when all explanatory variables <span class="math inline">\overrightarrow{x} = 0</span>.</p>
<p>We can prove this mathematically:</p>
<p><span class="math display">
\begin{split}
\hat y_{i, \ \overrightarrow x_i = 0} &amp; = \hat\beta_0 + \hat\beta_1 x_{1i} + ...+\beta_k x_{ki} \\
&amp; = \hat\beta_0 + \hat\beta_1(0) + ... + \beta_k (0) \\
&amp; = \hat\beta_0
\end{split}
</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of <span class="math inline">\hat\beta_0</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>When all explanatory variables equal 0, the expected value of <span class="math inline">y</span> is <span class="math inline">\hat\beta_0</span>.</p>
<ul>
<li>Note: For the linear probability model, <span class="math inline">\hat\beta_0</span> is the expected probability of category <span class="math inline">y=1</span> instead when all explanatory variables equal 0 (<a href="https://politicalscience.github.io/metrics/2/3.html#binary-outcome-variables-and-the-linear-probability-model">section 3.7</a>).</li>
</ul>
</div>
</div>
<p><br></p>
</section>
<section id="interpretation-of-hatbeta_j" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-of-hatbeta_j">Interpretation of <span class="math inline">\hat\beta_j</span></h3>
<p>Let us define <span class="math inline">\hat\beta_j</span> as any coefficient in <span class="math inline">\hat\beta_1, ..., \hat\beta_k</span>, which is multiplied to any variable <span class="math inline">x_j</span> in <span class="math inline">x_1, ..., x_k</span>.</p>
<ul>
<li>Or in other words, <span class="math inline">\hat\beta_j</span> is any coefficient multiplied to any explanatory variable <span class="math inline">x_j</span>.</li>
</ul>
<p>How do we interpret this? We might remember from <a href="https://politicalscience.github.io/metrics/2/3.html#interpretation-of-ols-coefficient-estimates">section 3.6</a> that these coefficients are the slope. But how do the additional explanatory variables change this?</p>
<p>In multiple linear regression, <span class="math inline">\hat\beta_j</span> is simply the expected change in <span class="math inline">y</span> given a one unit increase in <span class="math inline">x_j</span>, <u>holding all other variables constant</u>.</p>
<p>Why is this the case? Let us show this mathematically - the change in <span class="math inline">y</span> given a one unit increase in <span class="math inline">x_j</span> should be the partial derivative of the regression function in respect to <span class="math inline">x_j</span>:</p>
<p><span class="math display">
\begin{split}
\hat y_i &amp; = \hat\beta_0 + \hat\beta_1 x_{1i} + ... + \hat\beta_j x_{ji} + ... + \hat\beta_k x_{ki} \\
\frac{\partial \hat y_i }{\partial x_{ji}} &amp; = \frac{\partial}{\partial x_{ji}}[\hat\beta_0 + \hat\beta_1 x_{1i} + ... + \hat\beta_j x_{ji} + ... + \hat\beta_k x_{ki}] \\
&amp; = 0 + 0 + ... + \hat\beta_j + ... + 0 \\
&amp; = \hat\beta_j
\end{split}
</span></p>
<p>With a partial derivative, we are acting as if the other <span class="math inline">x_1, ... x_k</span> not <span class="math inline">x_j</span> are constant. So, we essentially are holding them constant.</p>
<ul>
<li>This allows us to “control” for confounders - as our <span class="math inline">\beta_j</span> is the relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span> when these confounders are held constant - i.e.&nbsp;the relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span> at every different level of the confounders.</li>
<li>More intuitively - let us say <span class="math inline">x_j</span> is education, <span class="math inline">y</span> is income, and our confounder is <span class="math inline">x_2</span> age. <span class="math inline">\hat\beta_j</span> would be the relationship between education and income, holding constant age. That basically means, if we consider only 25 year olds, what is the relationship between education and income. What about 26 year olds. We hare holding them constant.</li>
<li>We will discuss this in far more detail in chapter 6.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of <span class="math inline">\hat\beta_j</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>When <span class="math inline">x_j</span> increases by one unit, there is an expected <span class="math inline">\hat\beta_j</span> increase in <span class="math inline">y</span>, holding all other explanatory variables constant.</p>
<ul>
<li><p>Note: for binary explanatory variables, <span class="math inline">\hat\beta_j</span> is the expected difference in <span class="math inline">y</span> between categories <span class="math inline">x_j =1</span> and <span class="math inline">x_j = 0</span> (<a href="https://politicalscience.github.io/metrics/2/3.html#interpretation-of-ols-coefficient-estimates">section 3.6</a>).</p></li>
<li><p>Note: for the linear probability model, when <span class="math inline">x_j</span> increases by one unit, there is an expected <span class="math inline">\hat\beta_j</span> increase in the probability of category <span class="math inline">y=1</span> (<a href="https://politicalscience.github.io/metrics/2/3.html#binary-outcome-variables-and-the-linear-probability-model">section 3.7</a>).</p></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Remember, this is the <strong>relationship</strong> between <span class="math inline">x</span> and <span class="math inline">y</span>, <u><strong>not</strong> the causal effect</u>.</p>
</div>
</div>
<p>We can do the same interpretation in terms of standard deviations as shown in <a href="https://politicalscience.github.io/metrics/2/3.html#interpretation-of-ols-coefficient-estimates">section 3.6</a>.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
</section>
<section id="model-summary-statistics" class="level1">
<h1>4.5: Model Summary Statistics</h1>
<section id="estimated-residual-standard-deviations" class="level3">
<h3 class="anchored" data-anchor-id="estimated-residual-standard-deviations">Estimated Residual Standard Deviations</h3>
<p>We can derive the estimate of the <strong>residual variance</strong> <span class="math inline">\sigma^2</span> with this formula:</p>
<p><span class="math display">
\begin{split}
\hat{\sigma}^2 &amp; = \frac{\sum(y_i - \hat{y}_i)^2}{n-k-1} \\
&amp; = \frac{SSE}{df}
\end{split}
</span></p>
<ul>
<li>Where <span class="math inline">SSE</span> is the sum of squared errors (that we used in the OLS estimator)</li>
<li>Where <span class="math inline">df</span> is the degrees of freedom (by definition in regression, number of observations minus number of variables minus 1).</li>
</ul>
<p>But what is the residual variance? Recall our regression model: <span class="math inline">y_i = \beta_0 + \beta_1x_{1i} + ... + \beta_k x_{ki} + u_i</span></p>
<p>Our estimate of the residual variance <span class="math inline">\hat{\sigma}^2</span> is our estimate of the variance of the error term <span class="math inline">\epsilon_i</span>’s variance. More intuitively, it explains how spread out observed values of <span class="math inline">y</span> are from our prediction value <span class="math inline">\hat{y} = \mathbb E(y|x)</span>.</p>
<p>The figure below better showcases this in 2 different models. The red lines are our predicted regression line, and the green lines represent the distribution of our error term <span class="math inline">u_i</span>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/4-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
<p>The residual standard deviation <span class="math inline">\hat{\sigma}</span> (square root of variance) is consistent throughout a model. This is one of the assumptions of the linear regression model - that errors are consistently distributed, no matter the value of <span class="math inline">x</span>. This assumption is called <strong>homoscedasticity</strong>.</p>
<p>If <span class="math inline">\hat{\sigma}</span> varies depending on the value of <span class="math inline">x</span>, then that is called <strong>heteroscedasticity</strong>. When this occurs, it is often a suggestion that our relationship may not be linear - and we perhaps need to try a few transformations. We will get into transformations in chapter 5.</p>
<ul>
<li>Also heteroscedasticity (like briefly mentioned in <a href="https://politicalscience.github.io/metrics/2/3.html#standard-errors-and-hypothesis-testing">section 3.8</a>), requires the use of heteroscedastic-robust standard errors when doing hypothesis testing, which we have already been using.</li>
</ul>
<p>The figure below shows the difference of homoscedasticity vs.&nbsp;heteroscedasticity.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:75.0%"></p>
</figure>
</div>
<p><br></p>
</section>
<section id="total-sum-of-squares" class="level3">
<h3 class="anchored" data-anchor-id="total-sum-of-squares">Total Sum of Squares</h3>
<p>The total sum of squares is the total amount of sample variation in <span class="math inline">y</span>:</p>
<p><span class="math display">
TSS = (n-1)s_y^2
</span></p>
<ul>
<li>Where <span class="math inline">n</span> is the number of observations</li>
<li>Where <span class="math inline">s^2_y</span> is the sample variance of <span class="math inline">y_i</span>.</li>
</ul>
<p>The total sum of squares can also be calculated as:</p>
<p><span class="math display">
\begin{split}TSS &amp; = \sum(y_i - \bar{Y})^2 \\
&amp; = \sum (\hat{y}_i - \bar{y})^2 + \sum (y_i - \hat{y}_i)^2 \\
&amp; = SSM + SSE
\end{split}
</span></p>
<ul>
<li>Where TSS is the total sum of squares, and is formed of the sum of two components.</li>
<li>Where <span class="math inline">\sum (\hat{Y}_i - \bar{Y})^2</span> is the model sum of squares (SSM). This represents the part of the variation of <span class="math inline">y</span> that is explained by the model</li>
<li>Where <span class="math inline">\sum (Y_i - \hat{Y}_i)^2</span> is the sum of squared errors SSE (that we used to fit the model). This represents the part of the variation of <span class="math inline">y</span> that is not explained by the model (hence, why it is called error).</li>
</ul>
<p><br></p>
</section>
<section id="r-squared" class="level3">
<h3 class="anchored" data-anchor-id="r-squared">R-Squared</h3>
<p>R-squared is one of the key summary statistics of our model.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: R-Squared
</div>
</div>
<div class="callout-body-container callout-body">
<p>R-squared <span class="math inline">R^2</span> is a measure of the percentage of variation in <span class="math inline">y</span>, that is explained by our model (with our chosen explanatory variables). The percentage of variation in <span class="math inline">y</span> explained by our model would be:</p>
<p><span class="math display">
R^2 = \frac{SSM}{TSS} = \frac{\sum (\hat{Y}_i - \bar{Y})^2}{\sum(Y_i - \bar{Y})^2}
</span></p>
</div>
</div>
<p>This formula makes sense, since the SSM is the variation in <span class="math inline">y</span> that is explained by our model, and the TSS is the total variation in <span class="math inline">y</span>.</p>
<ul>
<li>Thus naturally, SMM/TSS results in the percentage of variation of <span class="math inline">y</span> explained by our model.</li>
</ul>
<p>For simple linear regression, <span class="math inline">R^2 = r^2_{xy}</span>, where <span class="math inline">r^2_{xy}</span> is the correlation coefficeint between <span class="math inline">x_i</span> and <span class="math inline">y_i</span> squared.</p>
<ul>
<li>This only applies to simple linear regression</li>
</ul>
<p><br></p>
<p><span class="math inline">R^2</span>, as a percentage, must be between 0% and 100% (or 0 and 1).</p>
<ul>
<li>Larger values of <span class="math inline">R^2</span> mean that the explanatory variables in our model explain more of the variation in <span class="math inline">y</span>.</li>
</ul>
<p>Thus, <span class="math inline">R^2</span> is a useful summary measure, because it helps us explain the amount of variance in <span class="math inline">y</span> we have explained with our explanatory variables.</p>
<p>However, <span class="math inline">R^2</span> is often <u>focused on too much</u>. You can have a good model with a low <span class="math inline">R^2</span>, and a bad model with a high <span class="math inline">R^2</span>.</p>
<ul>
<li>In fact, one of the issues of <span class="math inline">R^2</span> is that it never goes down - by including more and more explanatory variables, your <span class="math inline">R^2</span> will always increase.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
</section>
<section id="standard-errors-and-hypothesis-testing" class="level1">
<h1>4.6: Standard Errors and Hypothesis Testing</h1>
<p>We previously discussed the idea of standard errors and hypothesis testing in <a href="https://politicalscience.github.io/metrics/1/2.html#uncertainty-in-causal-estimates-and-standard-errors">section 2.3</a> and <a href="https://politicalscience.github.io/metrics/1/2.html#causal-inference-and-hypothesis-testing">section 2.4</a>, and for linear regression in <a href="https://politicalscience.github.io/metrics/2/3.html#standard-errors-and-hypothesis-testing">section 3.8</a>.</p>
<ul>
<li>The intuition behind the ideas remains the same for simple linear regression.</li>
<li>The main difference is that the tests we are doing for multiple linear regression <u>are not causal inference tests</u> (same for simple linear). These are descriptive inference tests (about the true correlation in the population).</li>
<li>As mentioned previously, we will only discuss causation again in chapter 6.</li>
</ul>
<p><br></p>
<p>There is one slight technical difference in hypothesis testing for multiple linear regression, compared to simple linear regression and randomised controlled experiments.</p>
<p>The standard error formula (used in confidence intervals and hypothesis tests) are slightly different.</p>
<ul>
<li>The robust standard error is too complicated to calculate by hand, so we will have a software calculate it for us.</li>
</ul>
<p><br></p>
<p>Confidence intervals remain the same, with upper and lower bounds:</p>
<p><span class="math display">
\hat\beta_j ± 1.96 \times rse(\hat\beta_1)
</span></p>
<p><br></p>
<p>With hypothesis testing, our typical hypotheses with simple linear regression are:</p>
<p><span class="math display">
\begin{split}
&amp; H_0 : \beta_j = 0 \\
&amp; H_1 : \beta_j ≠ 0
\end{split}
</span></p>
<p>Our t-test statistic is:</p>
<p><span class="math display">
t=\frac{\hat\beta_j - \mu_0}{rse(\hat\beta_j)}
</span></p>
<p>And our degrees of freedom is <span class="math inline">n-k-1</span>.</p>
<p>Using this, we can find the p-value (see <a href="https://politicalscience.github.io/metrics/1/2.html#causal-inference-and-hypothesis-testing">section 2.4</a> for a refresher).</p>
<p><br></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: P-Value
</div>
</div>
<div class="callout-body-container callout-body">
<p>The p-value is the probability of a t-test statistic equal to or even more extreme could occur, given the null hypothesis is true.</p>
<p>If this is less than 0.05 (5%), that means the null hypothesis has a very low chance of being true, so we reject the null hypothesis as no longer true.</p>
<ul>
<li><p>So, if the p-value is above 0.05, there is a above 5% chance that the null hypothesis is true. This is too high for our liking, so we cannot reject the null hypothesis, and we cannot conclude any significant relationship between <span class="math inline">x_j</span> and <span class="math inline">y</span>.</p></li>
<li><p>If the p-value is less than 0.05, there is less than a 5% chance that the null hypothesis is true. In econometrics, we thus reject the null hypothesis, and conclude that there is a significant relationships between <span class="math inline">x_j</span> and <span class="math inline">y</span></p></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Remember, this is the <strong>relationship</strong> between <span class="math inline">x_j</span> and <span class="math inline">y</span>, <u><strong>not</strong> the causal effect</u>.</p>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="joint-coefficient-hypotheses-tests" class="level1">
<h1>4.7: Joint Coefficient Hypotheses Tests</h1>
<section id="f-test-of-nested-models" class="level3">
<h3 class="anchored" data-anchor-id="f-test-of-nested-models">F-Test of Nested Models</h3>
<p>Sometimes, we want to test multiple coefficients and their significance at the same time.</p>
<ul>
<li>This could be because we want to compare two different models, and see which one is better.</li>
<li>Or, as we will cover in the next chapter, there are several situations where we will have multiple coefficients for a single variable, and thus, we would need a test for those coefficients to see if the variable is significant.</li>
</ul>
<p>In that case, our null hypotheses might look like:</p>
<p><span class="math display">
\begin{split}
&amp; H_0 : \beta_1 = 0 \ \text{and} \ \beta_2 = 0 \\
&amp; H_1 : \beta_1 ≠ 0 \ \text{or} \ \beta_2 ≠ 0
\end{split}
</span></p>
<p>In this situation, we can use a F-test.</p>
<p><br></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: F-test of Nested Models
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>F-test of Nested Models</strong> allows us to compare different regression models.</p>
<ul>
<li>We use a smaller model as our null hypothesis, and a larger model (containing the smaller model) as our alternative hypothesis. More mathematically:</li>
</ul>
<p><span class="math display">
\begin{split}
M_0 : &amp; \ \hat y = \hat\beta_0 + \hat\beta_1 X_1 + ... + \hat\beta_g X_g \\
M_a : &amp;  \ \hat y = \hat\beta_0 + \hat\beta_1 X_1 + ... + \hat\beta_g X_g + \hat\beta_{g+1} X_{g+1} + ... + \hat\beta_k X_k
\end{split}
</span></p>
<p>For example, if you wanted to test the statistical significance of a polytomous explanatory variable, <span class="math inline">M_0</span> would be the model without it, and <span class="math inline">M_a</span> would be the model with the variable (and its multiple coefficients).</p>
<p>Importantly, all explanatory variables in model <span class="math inline">M_0</span> must also be in <span class="math inline">M_a</span> (hence “nested”).</p>
</div>
</div>
<p><br></p>
<p>The F-test uses the F-test statistic.</p>
<ul>
<li>This statistic compared the <span class="math inline">R^2</span> values of the two models.</li>
<li>Let us say the <span class="math inline">R^2</span> value of <span class="math inline">M_0</span> is notated <span class="math inline">R^2_0</span>, and the <span class="math inline">R^2</span> value of <span class="math inline">M_a</span> is notated as <span class="math inline">R^2_a</span>.</li>
<li>The F-test statistic essentially measures the difference <span class="math inline">R^2_a - R^2_0</span>. If the difference is sufficiently large, that means the <span class="math inline">M_a</span> model has significantly more explanatory power than <span class="math inline">M_0</span>.</li>
</ul>
<p>Mathematically, the F-test statistic is as follows, with <span class="math inline">k_a</span> being the number of explanatory variables in the alternate hypothesis:</p>
<p><span class="math display">
F = \frac{ R^2_{\text{change}} / df_{\text{change}} }{  (1 - R^2_a ) / [n - (k_a + 1)]}
</span></p>
<p>The sampling distribution of the F-statistic is the F distribution with parameters <span class="math inline">k-a - k_0</span> and <span class="math inline">n-(k_a + 1)</span> degrees of freedom. We then obtain the p-value from this distribution.</p>
<p>The p-values of the F-statistic show the following:</p>
<ul>
<li>If the p-value is very small, that means <span class="math inline">R^2_a</span> is significantly larger than <span class="math inline">R^2_0</span>. This is evidence against model <span class="math inline">M_0</span>, and in favour of the larger model <span class="math inline">M_a</span>.</li>
<li>If the p-value is large, that means <span class="math inline">R^2_a</span> is not much larger than <span class="math inline">R^2_0</span>. This means there is no evidence against <span class="math inline">M_0</span>, and <span class="math inline">M_a</span> is not the statistically significantly better model.</li>
</ul>
<p>F-tests of nested models can help us test the significance of multiple coefficients at once.</p>
<ul>
<li><p>The null model <span class="math inline">M_0</span> will be the model without our coefficients</p></li>
<li><p>The alternate model <span class="math inline">M_a</span> will be the model with all the coefficients we want to test together (so all of the coefficients of a polytomous variable, or polynomial variable, etc.).</p></li>
</ul>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>F-Tests only work under the assumption of homoscedasticity and with normal standard errors.</p>
<p>There is a way to use robust standard errors with a modified F-test, but we will not cover that here.</p>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
</section>
<section id="implementation-in-r" class="level1">
<h1>Implementation in R</h1>
<p>The packages we will need are:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(fixest)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(texreg)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>
<section id="simple-linear-regression-in-r" class="level2">
<h2 class="anchored" data-anchor-id="simple-linear-regression-in-r">Simple Linear Regression in R</h2>
<p>To run simple linear regression, we use the <em>feols()</em> function.</p>
<ul>
<li>The argument <em>se = “hetero”</em> tells R to calculate heteroscedasticity-robust standard errors, which will be discussed later in chapter 4. Just know it is standard to do so.#</li>
<li>We can increase the number of explanatory variables by adding more with a + sign. We can reduce the number of explanatory variables down to 1.</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>modelname <span class="ot">&lt;-</span> <span class="fu">feols</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x3, <span class="at">data =</span> mydata, <span class="at">se =</span> <span class="st">"hetero"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(modelname)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For example:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">feols</span>(pct_missing <span class="sc">~</span> treat_invite <span class="sc">+</span> mosques,</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> dta, <span class="at">se =</span> <span class="st">"hetero"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>OLS estimation, Dep. Var.: pct_missing
Observations: 477
Standard-errors: Heteroskedasticity-robust 
              Estimate Std. Error   t value   Pr(&gt;|t|)    
(Intercept)   0.323624   0.039121  8.272443 1.3386e-15 ***
treat_invite -0.025512   0.032588 -0.782873 4.3409e-01    
mosques      -0.048443   0.018455 -2.624960 8.9458e-03 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
RMSE: 0.340455   Adj. R2: 0.010772</code></pre>
</div>
</div>
<p>We can see the output estimate of the intercept and the two coefficients.</p>
<ul>
<li>These rows include the estimate, the standard error, the t-test statistic, and the p-value. This gives all of the information we need to run linear regression and hypothesis tests.</li>
</ul>
<p><br></p>
<p>We can also use the base-R <em>lm()</em> function, however, this does not calculate heteroscedasticity-robust standard errors (once again, will be discussed in chapter 4).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>modelname <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x3, <span class="at">data =</span> mydata)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(modelname)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>
</section>
<section id="confidence-intervals" class="level2">
<h2 class="anchored" data-anchor-id="confidence-intervals">Confidence Intervals</h2>
<p>You probably have noticed that the normal regression output does not give confidence intervals of the coefficients.</p>
<ul>
<li>You could manually calculate them as specified in the formula in the chapter.</li>
</ul>
<p>However, R can also calculate these automatically for us as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(modelname)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For example, let us find the confidence intervals of our <em>model1</em> earlier:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(model1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                   2.5 %      97.5 %
(Intercept)   0.24675226  0.40049513
treat_invite -0.08954705  0.03852255
mosques      -0.08470606 -0.01217972</code></pre>
</div>
</div>
<p>We can see that the model gives both lower and upper bounds of the intercept and coefficient.</p>
</section>
<section id="creating-regression-tables" class="level2">
<h2 class="anchored" data-anchor-id="creating-regression-tables">Creating Regression Tables</h2>
<p>We can create regression tables using the <em>texreg()</em> or <em>screenreg()</em> functions.</p>
<ul>
<li><em>texreg()</em> produces LaTeX code that you can insert into a LaTeX document</li>
<li><em>screenreg()</em> produces something that looks nice in a R document.</li>
</ul>
<p>The syntax is as follows (you can replace <em>screenreg()</em> with <em>texreg()</em> ):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">screenreg</span>(<span class="at">l =</span> modelname,</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">custom.model.names =</span> <span class="fu">c</span>(<span class="st">"Outcome Variable Name"</span>),</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">custom.coef.names =</span> <span class="fu">c</span>(<span class="st">"Intercept"</span>, <span class="st">"X1 Variable Name"</span>, <span class="st">"X2 Variable Name"</span>, <span class="st">"X3 Variable Name"</span>),</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">digits =</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For example:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">screenreg</span>(<span class="at">l =</span> model1,</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">custom.model.names =</span> <span class="fu">c</span>(<span class="st">"Pct_Missing"</span>),</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">custom.coef.names =</span> <span class="fu">c</span>(<span class="st">"Intercept"</span>, <span class="st">"Treatment"</span>, <span class="st">"Mosques"</span>),</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">digits =</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================
                       Pct_Missing
----------------------------------
Intercept                0.324 ***
                        (0.039)   
Treatment               -0.026    
                        (0.033)   
Mosques                 -0.048 ** 
                        (0.018)   
----------------------------------
Num. obs.              477        
R^2 (full model)         0.015    
R^2 (proj model)                  
Adj. R^2 (full model)    0.011    
Adj. R^2 (proj model)             
==================================
*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05</code></pre>
</div>
</div>
<p><br></p>
</section>
<section id="f-tests-of-nested-models" class="level2">
<h2 class="anchored" data-anchor-id="f-tests-of-nested-models">F-Tests of Nested Models</h2>
<p>To do an F-test, we use the <em>anova()</em> command, and include our null hypothesis model first, then our alternate hypothesis model.</p>
<ul>
<li>We must use the base <em>lm()</em> function for regressions when we do this, since the f-test only applies to homoscedastic-normal-standard errors.</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(model_null, model_alternate)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For example:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create models</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>m0 <span class="ot">&lt;-</span> <span class="fu">lm</span>(pct_missing <span class="sc">~</span> treat_invite, <span class="at">data =</span> dta)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(pct_missing <span class="sc">~</span> treat_invite <span class="sc">+</span> mosques <span class="sc">+</span> total_budget, <span class="at">data =</span> dta)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># f-test</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m0, m1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: pct_missing ~ treat_invite
Model 2: pct_missing ~ treat_invite + mosques + total_budget
  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   
1    475 56.070                                
2    473 54.967  2    1.1027 4.7446 0.009117 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>We can see our alternate model is statistically significant.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>