---
title: "Chapter 3: Simple Linear Regression"
subtitle: "Econometric Methods (for Social Scientists)"
format:
    html:
        page-layout: article
        grid:
          sidebar-width: 300px
          body-width: 750px
          margin-width: 250px
          gutter-width: 2em
        fontsize: 12.5pt
        theme: yeti
        toc: TRUE
        toc-depth: 2
        toc-location: left
        toc-expand: true
        toc-title: "Chapter 3: Simple Linear Regression"
        mainfont: "Computer Modern"
        title-block-banner: true
        classoption: fleqn
        html-math-method: katex
---

This chapter is the first chapter where we move away from the "gold standard" of randomisation, and into the dangerous world of observational studies. In this chapter, we first focus on how we can measure correlations (not causation) in an observational setting.

Topics: Correlation, Simple Linear Regression, Ordinary Least Squares Estimator, Interpretation of Regression Coefficients, Multiple Linear Regression.

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 3.7: Binary Outcome Variables and the Linear Probability Model

Very often, we are trying to predict binary outcome variables.

-   For example, we might be interested in how some $x$ causes someone to vote for or against some measure or politician.
-   Or, we might be interested in how some $x$ causes some factor to be true or false.

These are all binary outcome variables, where $y$ can only take two values, $y=1$ and $y=0$.

<br />

The linear model can be "transformed" slightly to deal with binary outcome variables.

-   The difference is now, instead of the fitted value predicting $\hat y_i$, the model now predicts the probability of any observation $i$ being in in category $y=1$.
-   This predicted probability is notated $\hat\pi_i$.

::: callout-tip
## Definition: Linear Probability Model

The linear probability model is a variation of the linear regression model, where the outcome variable $y$ is a binary variable.

$$
\pi_i = \beta_0 + \beta_1x_i + u_i
$$

-   Where $\pi_i$ is the predicted probability of observation $i$ being in category $y=1$. This value is always between 0 and 1.
-   Where $\beta_0$ and $\beta_1$ are coefficients that need to be estimated (in the same way as the standard linear model).
:::

Once we estimate $\beta_0$ and $\beta_1$, we get fitted probabilities:

$$
\hat\pi_i = \hat\beta_0 + \hat\beta_1 x_i
$$

-   We can interpret the coefficients (discussed below)
-   We can also plug in the $x$ value of any observation $i$ to predict the probability $\hat\pi_i$ of that observation $i$ being in category $y=1$. To find the probability of category $y=0$, we simply do $1 - \hat\pi_i$ (because rules of probability).

Final note: The Linear Probability model has a key issue - since a linear line $y=mx+b$ goes to $±∞$ on either side, that means that the predicted probabilities can be below 0 or above 1, which clearly violates probability rules.

-   For relationships between $x$ and $y$, this is not a huge issue, since $\hat\beta_1$ still gives us a relatively good estimate (particularly when $x$ is binary).
-   However, for prediction tasks, a logistic regression model (discussed later in the course) may be more suited for this task.

<br />

### Interpretation of Coefficients

Interpretation of coefficients $\hat\beta_0$ and $\hat\beta_1$ that we estimate differ slightly when we are dealing with the linear probability model.

<br />

$\hat\beta_1$ is still the slope of our linear model. However, remember that our "output" is now $\hat\pi_i$, not $\hat y_i$. Thus, we have to adjust the interpretation as follows:

::: callout-tip
## Interpretation of $\hat\beta_1$

When $x$ increases by one unit, there is an expected $\hat{\beta}_1$ change in the probability of an observation being in category $y=1$.

-   We can also interpret this in terms of a percentage points change, by multiplying $\hat\beta_1$ by 100.
:::

::: callout-warning
## Warning!

Remember, this is the **relationship** between $x$ and $y$, [**not** the causal effect]{.underline}.
:::

<br />

$\hat\beta_0$ is still the intercept of our linear model. However, with the output being $\hat\pi_i$, not $\hat y_i$, we have to adjust our interpretation as follows:

::: callout-tip
## Interpretation of $\hat\beta_0$

When $x=0$, the expected probability of an observation being in category $y=0$ is $\hat{\beta}_0$.

-   We can also interpret this in terms of a percentage, by multiplying $\hat\beta_0$ by 100.
:::

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 3.8: Standard Errors and Hypothesis Testing

We previously discussed the idea of standard errors and hypothesis testing in [section 2.3](https://politicalscience.github.io/metrics/1/2.html#uncertainty-in-causal-estimates-and-standard-errors) and [section 2.4](https://politicalscience.github.io/metrics/1/2.html#causal-inference-and-hypothesis-testing).

-   The intuition behind the ideas remains the same for simple linear regression.
-   The main difference is that the tests we are doing for simple linear regression [are not causal inference tests]{.underline}. These are descriptive inference tests (about the true correlation in the population).
-   As mentioned previously, we will only discuss causation again in chapter 6.

<br />

There is one slight technical differences in hypothesis testing for simple linear regression.

First, the standard error formula (used in confidence intervals and hypothesis tests) are slightly different.

::: callout-tip
## Definition: Robust Standard Errors

With simple linear regression, the robust standard errors of the estimate $\hat\beta_1$ are:

$$
rse(\hat\beta_1) = \sqrt{\frac{Var[(x_i - \bar x) \hat u_i]}{n \times [Var(x_i)]^2}}
$$

The reason I say "robust" standard errors is because in econometrics, we assume heteroscedastic errors unless proven otherwise.

-   This will be discussed in Chapter 6.
:::

<br />

Confidence intervals remain the same, with upper and lower bounds:

$$
\hat\beta_1 ± 1.96 \times rse(\hat\beta_1)
$$

<br />

With hypothesis testing, our typical hypotheses with simple linear regression are:

$$
\begin{split}
& H_0 : \beta_1 = 0 \\
& H_1 : \beta_1 ≠ 0
\end{split}
$$

Our t-test statistic is:

$$
t=\frac{\hat\beta_1 - \mu_0}{rse(\hat\beta_1)}
$$

And our degrees of freedom is still $n-2$.

Using this, we can find the p-value (see [section 2.4](https://politicalscience.github.io/metrics/1/2.html#causal-inference-and-hypothesis-testing) for a refresher).

<br />

::: callout-tip
## Definition: P-Value

The p-value is the probability of a t-test statistic equal to or even more extreme could occur, given the null hypothesis is true.

If this is less than 0.05 (5%), that means the null hypothesis has a very low chance of being true, so we reject the null hypothesis as no longer true.

-   So, if the p-value is above 0.05, there is a above 5% chance that the null hypothesis is true. This is too high for our liking, so we cannot reject the null hypothesis, and we cannot conclude any significant relationship between $x$ and $y$.

-   If the p-value is less than 0.05, there is less than a 5% chance that the null hypothesis is true. In econometrics, we thus reject the null hypothesis, and conclude that there is a significant relationships between $x$ and $y$
:::

::: callout-warning
## Warning!

Remember, this is the **relationship** between $x$ and $y$, [**not** the causal effect]{.underline}.
:::

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# Implementation in R

The packages we will need are:

```{r, message = FALSE}
library(tidyverse)
library(fixest)
library(texreg)
```

```{r, echo = FALSE, message = FALSE}
setwd("/Users/kevinli/Documents/GitHub/politicalscience.github.io/metrics/2")
dta <- read_csv("data/olken_data.csv")
```

<br />

## Simple Linear Regression in R

To run simple linear regression, we use the *feols()* function.

-   The argument *se = "hetero"* tells R to calculate heteroscedasticity-robust standard errors, which will be discussed later in chapter 4. Just know it is standard to do so.

```{r, eval = FALSE}
modelname <- feols(y ~ x, data = mydata, se = "hetero")
summary(modelname)
```

For example:

```{r, message = FALSE}
model1 <- feols(pct_missing ~ treat_invite,
                data = dta, se = "hetero")
summary(model1)
```

We can see the output estimate of the intercept and the coefficient.

-   These rows include the estimate, the standard error, the t-test statistic, and the p-value. This gives all of the information we need to run linear regression and hypothesis tests.

<br >

We can also use the base-R *lm()* function, however, this does not calculate heteroscedasticity-robust standard errors (once again, will be discussed in chapter 4).

```{r, eval = FALSE}
modelname <- lm(y ~ x, data = mydata)
summary(modelname)
```

<br />

## Binary Variables

If you want to turn $x$ or $y$ into a binary variable, you can use the *as.factor()* command within the regression.

-   R automatically turns some data types into binary/categorical variables, like double and character variables. However, if you binary variable is coded numerically, you need to do *as.factor()*

```{r, eval = FALSE}
modelname <- feols(y ~ as.factor(x), data = mydata)
```

<br />

## Confidence Intervals

You probably have noticed that the normal regression output does not give confidence intervals of the coefficients.

-   You could manually calculate them as specified in the formula in the chapter.

However, R can also calculate these automatically for us as follows:

```{r, eval = FALSE}
confint(modelname)
```

For example, let us find the confidence intervals of our *model1* earlier:

```{r}
confint(model1)
```

We can see that the model gives both lower and upper bounds of the intercept and coefficient.

## Creating Regression Tables

We can create regression tables using the *texreg()* or *screenreg()* functions.

-   *texreg()* produces LaTeX code that you can insert into a LaTeX document
-   *screenreg()* produces something that looks nice in a R document.

The syntax is as follows (you can replace *screenreg()* with *texreg()* ):

```{r, eval = FALSE}
screenreg(l = modelname,
  custom.model.names = c("Outcome Variable Name"),
  custom.coef.names = c("Intercept", "X Variable Name"),
  digits = 3)
```

For example:

```{r}
screenreg(l = model1,
  custom.model.names = c("Pct_Missing"),
  custom.coef.names = c("Intercept", "Treatment"),
  digits = 3)
```

<br />

<br />
