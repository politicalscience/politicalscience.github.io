---
title: "Chapter 1: Introduction to Causal Estimation and Inference"
subtitle: "Econometric Methods (for Social Scientists)"
format:
    html:
        page-layout: article
        grid:
          sidebar-width: 300px
          body-width: 750px
          margin-width: 250px
          gutter-width: 2.5em
        fontsize: 11.5pt
        theme: yeti
        toc: TRUE
        toc-depth: 1
        toc-location: left
        toc-expand: true
        toc-title: "Chapter 1: Introduction to Causal Estimation and Inference"
        mainfont: "Computer Modern"
---

This chapter will introduce the idea of causal effects and causation. We will explore what a causal effect is, how to theoretically identify causal effects, and the dangers of using correlation as causation.

Topics: Correlation and Causation, Potential Outcomes and Causal Effects

<br />

# 1.1: Correlation and Causation

Econometrics is the field of using data to answer economic and social science questions. While econometric was initially developed for economic questions, it has become widely applied to other social sciences.

Econometrics is often concerned with causal questions:

-   What is the effect of years of education on income?
-   What is the effect of democracy/dictatorship on economic growth and development?
-   What is the effect of income on the likelihood of someone turning out to vote?

<br />

### Correlation is not Causation

Variables are **correlated** with each other, if one variable changing is associated with another variable changing on average.

-   For example, if $x$ increases, the average $y$ value increases, then $x$ and $y$ are correlated.
-   The opposite is also true: if $x$ increases, the average $y$ value decreases, then $x$ and $y$ are correlated.

::: callout-tip
## Definition: Sources of Correlation

There are three main reasons why two variables $x$ and $y$ may be correlated:

1.  There is a causal effect of $x$ on $y$
2.  A third variable, $w$, causes both $x$ and $y$ to change (so there is no direct effect of $x$ on $y$)
3.  There is a causal effect of $y$ on $x$

These causes can occur simultaneously at the same time. Thus, correlation is not causation, as correlation can be caused by other factors.
:::

<br />

If our goal in econometrics is to find the causal effect of $x \rightarrow y$ (source #1 of correlation), we must eliminate the effects of the third variable $w$ (source #2 of correlation), and the reverse causality effect $y \rightarrow x$ (source #3 of correlation).

-   Thus, we need some methods to eliminate sources #2 and #3 of correlation in order to accurately estimate the causal effect of $x \rightarrow y$.

::: callout-note
## Example: Correlation is not Causation

In the United States, *Ice Cream Sales* and *Number of Fatal Shark Attacks* are highly correlated variables. Does this mean that selling ice cream **causes** fatal shark attacks? No!

The reason this relationship exists is because of another variable - the *weather*. The weather (when sunny), causes both ice cream sales to rise, as well as causing more people to go to the beach, which then increases the number of fatal shark attacks.

This is a clear example of how correlation is not causation.
:::

<br />

### Key Terminology

Before we start discussing methods to eliminate the effect of third variables and reverse causality, we need to first understand what even is a causal effect. Let us define some key terminology:

-   The variable which causes a causal effect is called the **treatment**, labelled $D$.
-   The variable which changes as a result of the change in $D$ is the **outcome**, labelled $y$.
-   Thus, our causal effect of interest is $D \rightarrow y$.
-   A third variable which causes both $D$ and $y$ is called a **confounder** or **confounding variable**, and is often labelled $w$ or $c$.

<br />

------------------------------------------------------------------------

# 1.2: Potential Outcomes and Causal Effects

### Potential Outcomes Framework

A **causal effect** is a change in some feature of the world $D$ that directly causes a change in some feature of the world $y$.

Causal effects imply the existence of **potential outcomes**.

::: callout-tip
## Definition: Potential Outcomes

Imagine that there are 2 parallel worlds that are exactly the same, and there is one unit/individual $i$ who exists in both worlds.

Then, in one of the two worlds, unit $i$ gets the treatment $D$, which causes a change in the outcome $y$. In the other world, unit $i$ does not get the treatment $D$, so there is no change in outcome $y$.

Since these 2 parallel worlds are identical **except** for the fact one world gets the treatment $D$ and the other does not, the difference in the world's $y$ outcomes can only be the effect of treatment $D$.

The difference in the two world's outcome $y$ is thus the **causal effect** of $D$ on $y$.
:::

<br />

Let us represent these two parallel worlds more formally:

-   The world where unit $i$ gets the treatment will be called the **treatment state**, and will take a value of $D=1$.
-   The world where unit $i$ does not get the treatment will be called the **control state**, and will take a value of $D=0$.

Thus, the outcomes of unit $i$ in these 2 parallel worlds can be notated as $y_{Di}$, where $D$ is the state of the world.

-   In the **treatment state** $D=1$, we will label the outcome of unit $i$ as $y_{1i}$
-   In the **control state** $D=0$, we will label the outcome of unit $i$ as $y_{0i}$.

<br />

::: callout-note
## Example: Democracy and Economic Growth

Let us say we want to find if democracy causes better economic growth. Democracy is our treatment $D$, and economic growth is our outcome $y$.

Country $i$ (for simplicity, let us say Canada) is one of the units. There are two parallel worlds:

-   In one parallel world, Canada is in the **treatment state** $D=1$, meaning it is a democracy. Its outcome in this world would be its economic growth $y_{1i}$.
-   In the other parallel world, Canada is in the **control state** $D=0$, meaning it is not a democracy. Its outcome in this world would be its economic growth $y_{0i}$.
:::

<br />

### Causal Effects

As we mentioned in the potential outcomes framework, the difference in the outcomes in the two parallel worlds must be the causal effect of $D$, since everything else in the parallel worlds are identical.

::: callout-tip
## Definition: Individual Causal Effects

Thus, the individual causal effect of $D$ on unit $i$ is the difference of outcomes $y$ in the treatment state and the control state. Mathematically:

$$
\tau_i = y_{1i} - y_{0i}
$$
:::

Thus, the treatment $D$ will have $\tau$ effect on outcome $y$ for unit $i$

<br />

------------------------------------------------------------------------

# 1.3: Observed Outcomes and the Stable Unit Treatment Value Assumption

### Observed Outcomes

Of course, in real life, we do not have parallel worlds. We only observe one of these parallel worlds.

For example, in the real world, we know Canada is a democracy. We do not observe the parallel world that Canada is a dictatorship. Thus, we also only have one of Canada's potential outcomes, while the other is unobservable.

::: callout-tip
## Definition: Observed Outcomes

Observed outcomes are a subset of potential outcomes that actually come true in the real world. Observed outcomes are given by the formula:

$$
y_i = D_i \times y_{1i} + (1-D_i) \times y_{0i}
$$

-   Where $y_i$ is the observed outcome, $y_{1i}$ is the potential outcome of the treatment state, and $y_{0i}$ is the potential outcome of the control state.
-   Where $D_i$ represents what state unit $i$ is in, where $D_i = 1$ indicates unit $i$ is in the treatment state, and $D_i = 0$ indicates unit $i$ is in the control state.
:::

This formula might seem unintuitive. But we can plug in values of $D_i$ to understand what this formula means.

What if unit $i$ is in the treatment state, $D_i = 1$. Let us plug in $D_i = 1$ into the equation:

$$
\begin{split}
y_i & = D_i \times y_{1i} + (1-D_i) \times y_{0i} \\
y_i & = 1 \times y_{1i} + (1-1) \times y_{0i} \\
y_i & = y_{1i}
\end{split}
$$

Thus, the observed outcome when unit $i$ is in the treatment state is $y_{1i}$, which makes sense, since that is the potential outcome of the treatment state.

<br />

Similarly, what if unit $i$ is in the control state $D_i = 0$. Let us plug in $D_i = 0$ into the equation:

$$
\begin{split}
y_i & = D_i \times y_{1i} + (1-D_i) \times y_{0i} \\
y_i & = 0 \times y_{1i} + (1-0) \times y_{0i} \\
y_i & = y_{0i}
\end{split}
$$

Thus, the observed outcome when unit $i$ is in the control state is $y_{0i}$, which makes sense, since that is the potential outcome of the control state.

::: callout-tip
## Definition: Counterfactuals

A counterfactual is the potential outcome that is not observed. For example, if in the real world, unit $i$ is observed to have received the treatment, then its counterfactual is the world where unit $i$ did not receive the treatment.

Counterfactuals are important, because, if we remember, the individual causal effect is $\tau_i = y_{1i} = y_{0i}$, which implies that we must know the counterfactual to find the causal effect of $D$ on $y$.
:::

<br />

### Stable Unit Treatment Value Observation

The stable unit treatment value assumption (SUTVA) is arguably the key assumption of causal inference.

SUTVA states that given two units $i$ and $j$, unit $i$ is not affected by the treatment assignment of unit $j$. More intuitively, if unit $j$ is assigned to treatment, that has no effect on the outcomes of unit $i$.

Why is this important? Well, if unit $j$'s treatment status affects unit $i$, then unit $i$ would have more than 2 potential outcomes:

-   This is because unit $i$'s outcomes would now not only depend on itself being assigned to treatment or control, but also unit $j$ being assigned to treatment or control.
-   That means unit $i$ now has 4 potential outcomes. If unit $i$ is affected by other units other than $j$, this will quickly multiply to an unmanageable number of potential outcomes.

Thus, violating this assumption will basically make it impossible to calculate treatment effects.

<br />

In what situations is SUTVA typically violated?

1.  Peer effects that result from contact between units $i$ and $j$. For example, if you are testing the treatment of some new curriculum on student educational outcomes, and unit $j$ is assigned to treatment, unit $j$ might teach their friend unit $i$ some new things, even if unit $i$ was in the control group.
2.  Dilution/concentration effects that arise from a prevalence of a treatment. The best example is vaccines - if enough people get a vaccine for a disease, even people who do not get the vaccine are safer, since transmission is much more difficult.

<br />

------------------------------------------------------------------------

# 1.4: Causal Estimands

We introduced how the individual treatment effect of $D \rightarrow y$ for any unit $i$ is $\tau_i = y_{1i} - y_{0i}$. However, there is an issue - we do not know the two parallel worlds.

-   As we discussed in section 1.3, we can only observe one of the two parallel worlds.

Thus, we can never actually find (or estimate) the individual treatment effect. We need another **estimand**. An **estimand** is the true value of something in the real world, that we will try to estimate with an estimator.

This course will focus on introducing many new estimators to try to estimate these estimands. However, before we start estimating, we have to know what we are trying to estimate. What are the estimands we can use, if individual treatment effect is not possible?

<br />

The "key" estimand in causal inference is the Average Treatment Effect:

::: callout-tip
## Definition: Average Treatment Effect

The **Average Treatment Effect (ATE)** is the average of all individual treatment effects:

$$
\tau_{ATE} \ = \ \mathbb{E}[\tau_i] \ = \ \mathbb{E}[y_{1i} - y_{0i}] \ = \ \frac{1}{n} \left( \sum y_{1i} - \sum y_{0i} \right)
$$
:::

However, the ATE is not the only estimand we are interested in.

<br />

Sometimes, our treatment variables are continuous. For example, if we want to estimate how GDP affects democratisation, our treatment variable GDP is not binary.

In cases of non-continuous treatment variables, we want to estimate the **average causal effect** (ACE)- the expected change in $y$ resulting from a one unit increase in $D$:

$$
\tau_{ACE} = \mathbb{E}[y'(D)] \quad \text{where } y'(D) \text{ is the derivative of } y \text{ in respect to } D
$$

<br />

Sometimes, we have reason to expect that the treatment effect may be different accross different categories of units.

For example, maybe we think that a certain educational treatment will benefit women more than men, so the treatment effect will be higher for women than men.

The **Conditional Average Treatment Effect (CATE)** is the treatment effect of units, given they have some other variable $x$ value.

$$
\tau_{CATE} = \mathbb{E}[y_{1i} - y_{0i} \ | \ x \ ]
$$

<br />

We can also estimate the causal effect on only the treatment or control groups.

The **average treatment effect on the treated (ATT)** is the average treatment effect of only units who received the treatment $D_i = 1$:

$$
\tau_{ATT} = \mathbb{E} [y_{1i} - y_{0i} \ | \ D_i = 1]
$$

The **average treatment effect on the controls (ATC)** is the average treatment effect of only units who did not receive the treatment $D_i = 0$:

$$
\tau_{ATC} = \mathbb{E} [y_{1i} - y_{0i} \ | \ D_i = 0]
$$

<br />

------------------------------------------------------------------------

# 1.4: Properties of Estimators - Unbiasdness and Consistency

The above causal estimands are not directly calculable, since we cannot observe both potential outcomes. Thus, we need an **estimator** to estimate the causal estimands.

Estimators have two key properties: unbiasdness and consistency.

<br />

### Unbiasedness

An estimator is **unbiased** if its estimates $\hat{\theta}$ are on average equal to the true estimand $\theta$.

$$
\mathbb{E}[ \ \hat{\theta} \ ] = \theta
$$

Note: I use $\theta$ to represent any possible estimand, including the causal estimands from above.

To better understand unbiasdness, take this example of archery. We have two archers, archer $A$ and archer $B$.

-   Both archers are very very accurate - as in they hit the same spot on the target every time they shoot.
-   Archer $A$ gets the bullseye every single time.
-   However, archer $B$, while very consistently hitting the same spot, for some reason, keeps hitting 5 inches to the right of the bullseye.

Thus, archer $B$ is biased - he is consistently and systematically hitting the wrong spot on the target. Similarly, if an estimate is consistently off by a certain value, then it is biased.

<br />

### Consistency

However, an unbiased estimator is not good enough. Why? Take this example of another archer $C$.

-   Archer $C$ has good aim - he is aiming directly at the bullseye.
-   However, archer $C$ is very inconsistent - he first hits a shot 5 inches to the left of the bullseye, then hits a shot 5 inches to the right of the bullseye. He keeps doing both of these things over and over again.
-   Archer $C$ on average is actually hitting the bullseye, since his leftward and rightward errors cancel each other out.
-   However, archer $C$ never actually hits the bullseye on any specific shot.

Archer $C$ is an example of an unbiased, but very inconsistent estimator. On average, he is getting the right value of the estimate, but he is never that close on any specific estimate.

-   More technically, an inconsistent estimator is one who has high **variance** in their estimates.

<br />

Ideally, we want an estimator that is both **unbiased** and **consistent**. This would mean that their average estimates are equal to the true estimand, and that each estimate is relatively close to the true estimand with low variance.

-   So, an archer than on average, hits the bullseye, and is generally not too far off from the bullseye on any sepcific shot.

<br />

------------------------------------------------------------------------

# 1.5: The Naive Estimator and Proof Correlation is not Causation

The **naive estimator** is an estimator that only compares the observed outcomes, without any comparison to the counterfactual potential outcomes.
