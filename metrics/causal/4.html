<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Chapter 4: Causal Inference through Multiple Linear Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="4_files/libs/clipboard/clipboard.min.js"></script>
<script src="4_files/libs/quarto-html/quarto.js"></script>
<script src="4_files/libs/quarto-html/popper.min.js"></script>
<script src="4_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="4_files/libs/quarto-html/anchor.min.js"></script>
<link href="4_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="4_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="4_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="4_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="4_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Chapter 4: Causal Inference through Multiple Linear Regression</h1>
            <p class="subtitle lead">Econometric Methods (for Social Scientists)</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Chapter 1: Relationships Between Variables and Basics of Regression</h2>
   
  <ul class="collapse">
  <li><a href="#the-multiple-linear-regression-model" id="toc-the-multiple-linear-regression-model" class="nav-link active" data-scroll-target="#the-multiple-linear-regression-model">4.1: The Multiple Linear Regression Model</a></li>
  <li><a href="#multiple-linear-regression-model-with-linear-algebra" id="toc-multiple-linear-regression-model-with-linear-algebra" class="nav-link" data-scroll-target="#multiple-linear-regression-model-with-linear-algebra">4.2: Multiple Linear Regression Model with Linear Algebra</a></li>
  <li><a href="#ordinary-least-squares-estimator-for-multiple-regression" id="toc-ordinary-least-squares-estimator-for-multiple-regression" class="nav-link" data-scroll-target="#ordinary-least-squares-estimator-for-multiple-regression">4.3: Ordinary Least Squares Estimator for Multiple Regression</a></li>
  <li><a href="#regression-anatomy-theory-and-coefficient-interpretation" id="toc-regression-anatomy-theory-and-coefficient-interpretation" class="nav-link" data-scroll-target="#regression-anatomy-theory-and-coefficient-interpretation">4.4: Regression Anatomy Theory and Coefficient Interpretation</a></li>
  <li><a href="#gauss-markov-and-unbiasedness-of-the-ols-estimator" id="toc-gauss-markov-and-unbiasedness-of-the-ols-estimator" class="nav-link" data-scroll-target="#gauss-markov-and-unbiasedness-of-the-ols-estimator">4.5: Gauss-Markov and Unbiasedness of the OLS Estimator</a></li>
  <li><a href="#omitted-variable-bias-endogeneity-and-limitations-of-ols-for-causal-estimation" id="toc-omitted-variable-bias-endogeneity-and-limitations-of-ols-for-causal-estimation" class="nav-link" data-scroll-target="#omitted-variable-bias-endogeneity-and-limitations-of-ols-for-causal-estimation">4.6: Omitted Variable Bias, Endogeneity, and Limitations of OLS for Causal Estimation</a></li>
  <li><a href="#model-selection-for-causal-estimation" id="toc-model-selection-for-causal-estimation" class="nav-link" data-scroll-target="#model-selection-for-causal-estimation">4.7: Model Selection for Causal Estimation</a></li>
  <li><a href="#causal-inference-and-hypothesis-testing-with-multiple-linear-regression" id="toc-causal-inference-and-hypothesis-testing-with-multiple-linear-regression" class="nav-link" data-scroll-target="#causal-inference-and-hypothesis-testing-with-multiple-linear-regression">4.8: Causal Inference and Hypothesis Testing with Multiple Linear Regression</a></li>
  <li><a href="#interaction-terms-and-conditional-average-treatment-effect" id="toc-interaction-terms-and-conditional-average-treatment-effect" class="nav-link" data-scroll-target="#interaction-terms-and-conditional-average-treatment-effect">4.9: Interaction Terms and Conditional Average Treatment Effect</a></li>
  <li><a href="#implementation-in-r" id="toc-implementation-in-r" class="nav-link" data-scroll-target="#implementation-in-r">Implementation in R</a>
  <ul class="collapse">
  <li><a href="#multiple-linear-regression-in-r" id="toc-multiple-linear-regression-in-r" class="nav-link" data-scroll-target="#multiple-linear-regression-in-r">Multiple Linear Regression in R</a></li>
  <li><a href="#confidence-intervals" id="toc-confidence-intervals" class="nav-link" data-scroll-target="#confidence-intervals">Confidence Intervals</a></li>
  <li><a href="#creating-regression-tables" id="toc-creating-regression-tables" class="nav-link" data-scroll-target="#creating-regression-tables">Creating Regression Tables</a></li>
  <li><a href="#f-tests-of-nested-models" id="toc-f-tests-of-nested-models" class="nav-link" data-scroll-target="#f-tests-of-nested-models">F-Tests of Nested Models</a></li>
  <li><a href="#interaction-effects" id="toc-interaction-effects" class="nav-link" data-scroll-target="#interaction-effects">Interaction Effects</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>This chapter introduces multiple linear regression, which allows us to control for confounding variables. We will explore the multiple regression model, and what it means to control for a variable. Then, we will focus heavily on the Ordinary Least Squares (OLS) estimator, why it is a good estimator, and issues when it comes to causal inference.</p>
<p>Topics: Multiple Linear Regression, OLS with Linear Algebra, Regression Anatomy, Gauss-Markov, Unbiasedness and Consistnecy of OLS, Omitted Variable Bias and Endogeneity.</p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
<section id="the-multiple-linear-regression-model" class="level1">
<h1>4.1: The Multiple Linear Regression Model</h1>
<p>Before we start, make sure you have a strong understanding of all aspects of the simple linear regression model covered in <a href="https://politicalscience.github.io/metrics/causal/1.html">chapter 1</a>.</p>
<ul>
<li>Every topic covered in that chapter can be applied to Multiple Linear Regression, including categorical/polytomous explanatory variables, the linear probability model, and polynomial and logarithmic transformations.</li>
</ul>
<p>One of the issues with simple linear regression is that it often only measures the correlation/relationship between <span class="math inline">x</span> and <span class="math inline">y</span>. Simple linear regression can only be used for causal estimation when used in randomised controlled experiments.</p>
<p><br></p>
<p>However, in the social sciences, randomisation is often not possible.</p>
<p>Multiple linear regression allows us to estimate a model with both our treatment and outcome variables, as well as a series of <strong>control</strong> variables.</p>
<ul>
<li>We will explore the idea of “controlling” in later in <a href="https://politicalscience.github.io/metrics/causal/4.html#regression-anatomy-theory-and-coefficient-interpretation">section 4.4</a>.</li>
</ul>
<p>In theory, by including every single confounding variable as a control variable in our regression model, we can partial out the effect of confounders and find the average treatment effect.</p>
<ul>
<li>In reality, as we will cover later, it is often impossible to account for all confounding variables, so we may need additional techniques to account for those situations.</li>
</ul>
<p><br></p>
<p>The <strong>response variable</strong> (outcome variable) is notated <span class="math inline">y</span>, just like in single linear regression.</p>
<p>The <strong>explanatory variable</strong>s are <span class="math inline">x_1, x_2, ..., x_k</span>. We sometimes also denote all explanatory variables as the vector <span class="math inline">\overrightarrow{x}</span>. Our treatment variable <span class="math inline">D</span> is considered one of the explanatory variables <span class="math inline">\overrightarrow{x}</span> (most often <span class="math inline">x_1</span>).</p>
<p>A linear regression model is the specification of the conditional distribution of <span class="math inline">Y</span>, given <span class="math inline">\overrightarrow{x}</span> (if you do not understand this, see <a href="https://politicalscience.github.io/metrics/causal/1.html">chapter 1</a>). The linear regression model focuses on the <strong>expected value</strong> of the conditional distribution, notated <span class="math inline">\mathbb{E}[y_i|\overrightarrow{x}_i]</span>.</p>
<p><br></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Multiple Linear Regression Model
</div>
</div>
<div class="callout-body-container callout-body">
<p>Take a set of observed data with <span class="math inline">n</span> number of pairs of <span class="math inline">(\overrightarrow{x}_i, y_i)</span> observations. The linear model takes the following form:</p>
<p><span class="math display">
\mathbb{E}[y_i|\overrightarrow{x}_i] = \beta_0 + \beta_1 x_{1i} + ... + \beta_k x_{ki}
</span></p>
<ul>
<li>Where the coefficients (that need to be estimated) are vector<span class="math inline">\overrightarrow{\beta} = \beta_0, \beta_1, ..., \beta_k</span>.</li>
</ul>
<p>We can also write the linear model for the value of any point <span class="math inline">y_i</span> in our data:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1x_{1i} + ... + \beta_k x_{ki} + u_i
</span></p>
<ul>
<li>Where <span class="math inline">u_i</span> is the error term function - that determines the error for each unit <span class="math inline">i</span>. Error <span class="math inline">u_i</span> has a variance of <span class="math inline">\sigma^2</span>, and expectation <span class="math inline">\mathbb{E}[u_i] = 0</span>.</li>
</ul>
</div>
</div>
<p><br></p>
<p>In our model, <span class="math inline">x_1</span> is typically the treatment variable we are concerned with.</p>
<ul>
<li>We want to show some sort of causal relationship between <span class="math inline">x_1</span> and <span class="math inline">y</span></li>
</ul>
<p>The other explanatory variables <span class="math inline">x_2, ..., x_k</span> are the confounding variables we want to control for.</p>
<ul>
<li>We will go into detail what “controlling for” means in <a href="https://politicalscience.github.io/metrics/causal/4.html#regression-anatomy-theory-and-coefficient-interpretation">section 4.4</a>.</li>
<li>We will also discuss how to select control variables in <a href="https://politicalscience.github.io/metrics/causal/4.html#validity-of-using-ols-for-causal-estimation">section 4.8</a>.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="multiple-linear-regression-model-with-linear-algebra" class="level1">
<h1>4.2: Multiple Linear Regression Model with Linear Algebra</h1>
<p>We can also represent the multiple linear regression model in linear algebra.</p>
<p>Let us start with the linear model:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + ... + \beta_k x_{ki} + u_i
</span></p>
<p>The <span class="math inline">i</span>’th observation can be re-written in vector form as following:</p>
<p><span class="math display">
y_i = x_i'\beta + u_i, \text{ where }\beta = \begin{bmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_k\end{bmatrix} \text{ and }x_i = \begin{bmatrix}1 \\x_{1i} \\\vdots \\x_{ki}\end{bmatrix}
</span></p>
<ul>
<li><p>The <span class="math inline">x_i'</span> in the equation is the transpose of <span class="math inline">x_i</span>, to make matrix multiplication possible.</p></li>
<li><p>The first element of the <span class="math inline">x_i</span> matrix is 1, since <span class="math inline">1 \times \beta_0</span> gives us the first parameter (intercept) in the linear model.</p></li>
</ul>
<p><br></p>
<p>Since our data has <span class="math inline">n</span> number of observations <span class="math inline">i</span>, we can express this into vector form, with the <span class="math inline">x_i'</span> and <span class="math inline">\beta</span> being vectors within a vector.</p>
<p><span class="math display">
\begin{split}
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} &amp; = \begin{pmatrix}x_1'\beta + u_1 \\ x_2'\beta + u_2 \\ \vdots \\ x_n'\beta + u_n\end{pmatrix} \\
&amp; \\
&amp; = \begin{pmatrix}x_1'\beta \\ x_2'\beta \\ \vdots \\ x_n'\beta\end{pmatrix} + \begin{pmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{pmatrix}
\end{split}
</span></p>
<p>Since <span class="math inline">\beta</span> vector appears as a common factor for all observations <span class="math inline">i=1,...,n</span>, we can factor it out and have an equation:</p>
<p><span class="math display">
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} = \begin{pmatrix}x_1' \\ x_2' \\ \vdots \\ x_n'\end{pmatrix} \space \beta + \begin{pmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{pmatrix}
</span></p>
<p><br></p>
<p>We can expand the <span class="math inline">x_1',...,x_n'</span> vector into a matrix. Remember that each <span class="math inline">x_1',...,x_n'</span> is already a vector of different explanatory variables. So, we get the following result:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Multiple Linear Regression with Linear Algebra
</div>
</div>
<div class="callout-body-container callout-body">
<p>The multiple linear regression can be expressed in linear algebra as:</p>
<p><span class="math display">
y = X \beta + u, \text{ where } X = \begin{bmatrix}1 &amp; x_{21} &amp; \dots &amp; x_{k1} \\1 &amp; x_{22} &amp; \dots &amp; x_{k2} \\\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\1 &amp; x_{2n} &amp; \dots &amp; x_{kn}\end{bmatrix}
</span></p>
<ul>
<li>Where the notation for elements of <span class="math inline">X</span> is <span class="math inline">x_{ki}</span>, with <span class="math inline">i</span> being the unit of observation <span class="math inline">i = 1, \dots n</span>, and <span class="math inline">k</span> being the explanatory variables index.</li>
<li>Where <span class="math inline">y</span> and <span class="math inline">u</span> are <span class="math inline">n \times 1</span> vectors (as seen above), and <span class="math inline">\beta</span> is a <span class="math inline">k \times 1</span> vector.</li>
<li>The first column of <span class="math inline">X</span> is a vector of 1, which exists because these 1’s are multiplied with <span class="math inline">\beta_0</span> in our model.</li>
</ul>
</div>
</div>
<p>The point of expressing the model in linear algebra is that it makes the estimation process far easier, as we will see in the next section.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="ordinary-least-squares-estimator-for-multiple-regression" class="level1">
<h1>4.3: Ordinary Least Squares Estimator for Multiple Regression</h1>
<p>As we remember from Chapter 1, the goal of Ordinary Least Squares Estimation is to minimise the sum of squared errors. The sum of squared errors in multiple regression is:</p>
<p><span class="math display">
\begin{split}
SSE &amp; = \sum\limits_{i=1}^n (y_i - \hat y_i)^2\\
&amp; = \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i} -  ... - \hat\beta_kx_{ki})^2
\end{split}
</span></p>
<p>Similar to our simple linear regression (but with additional variables), our minimisation condition is:</p>
<p><span class="math display">
\begin{split}
(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...) &amp; = \arg \min\limits_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i} ...)^2 \\
&amp; = \arg \min\limits_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)} S(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)
\end{split}
</span></p>
<p><br></p>
<p>Taking the partial derivatives of each parameter like in simple linear regression, we get first order conditions:</p>
<p><span class="math display">
\begin{split}&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\&amp; \sum\limits_{i=1}^n X_{1i}(y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\
&amp; \sum\limits_{i=1}^n X_{2i} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0
\end{split}
</span></p>
<ul>
<li>and so on for <span class="math inline">x_{3i}, ..., x_{ki}</span>.</li>
</ul>
<p>This system of equations includes <span class="math inline">k+1</span> variables and <span class="math inline">k+1</span> equations, which is way too difficult to solve.</p>
<p><br></p>
<p>Instead, we can use linear algebra. Let us define our estimation vector <span class="math inline">\hat{\beta}</span> as the value of <span class="math inline">\hat\beta</span> that minimises the sum of squared errors:</p>
<p><span class="math display">
\hat{\beta} = \arg \min\limits_{b} (y - Xb)' (y - Xb) = \arg \min\limits_b S(b)
</span></p>
<p>We can expand <span class="math inline">S(b)</span> as follows:</p>
<p><span class="math display">
\begin{split}
S(b) &amp; = y'y - b'X'y - y'Xb + b'X'Xb \\
&amp; = y'y - 2b'X'y + b'X'Xb
\end{split}
</span></p>
<p>Taking the partial derivative in respect to <span class="math inline">b</span>:</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} = \begin{pmatrix}\frac{\partial S(b)}{\partial b_1} \\\vdots \\\frac{\partial S(b)}{\partial b_k}\end{pmatrix}
</span></p>
<p>Differentiating with the vector <span class="math inline">b</span> yields:</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} = -2X'y + 2X'Xb
</span></p>
<p>Evaluted at <span class="math inline">\hat{\beta}</span>, the derivatives should equal zero (since first order condition of finding minimums):</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} \biggr|_{\hat{\beta}} = -2X'y + 2X'X \hat{\beta} = 0
</span></p>
<p>When assuming <span class="math inline">X'X</span> is invertable, we can isolate <span class="math inline">\hat{\beta}</span> to find the solution to OLS:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: OLS Estimate of <span class="math inline">\hat\beta</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Ordinary Least Squares Estimate of vector <span class="math inline">\hat\beta</span> for multiple linear regression is:</p>
<p><span class="math display">
\hat{\beta} = (X'X)^{-1} X'y
</span></p>
</div>
</div>
<p>Once we have estimates of <span class="math inline">\hat{\beta}</span>, we can plug them into our linear model to obtain fitted values:</p>
<p><span class="math display">
\hat{y} = X\hat{\beta} = X(X'X)^{-1} X'y
</span></p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="regression-anatomy-theory-and-coefficient-interpretation" class="level1">
<h1>4.4: Regression Anatomy Theory and Coefficient Interpretation</h1>
<p>We talked about how multiple linear regression allows us to control for confounders. But what does that mean? How does it affect our interpretations of coefficients?</p>
<p>The Regression Anatomy Theory, also called the Frisch–Waugh–Lovell (FWL) theorem, illustrates this concept. Take our standard multiple linear regression:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_k x_{ki} + u_i
</span></p>
<p><br></p>
<p>Let us say we are interested in <span class="math inline">x_1</span> (this can be generalised to any explanatory variable). Let us make <span class="math inline">x_1</span> the outcome variable of a regression with <span class="math inline">x_2, ..., x_k</span>:</p>
<p><span class="math display">
x_{1i} = \gamma_0 + \gamma_1 x_{2i} + ... + \gamma_{k-1}x_{ki} + \widetilde{x_{1i}}
</span></p>
<ul>
<li>Where <span class="math inline">\gamma_0, ..., \gamma_{k-1}</span> are coefficients.</li>
</ul>
<p>The error term is <span class="math inline">\widetilde{x_{1i}}</span>, which represents the part of <span class="math inline">x_{1i}</span> that are uncorrelated to <span class="math inline">x_2, ..., x_k</span>.</p>
<ul>
<li>In other words, <span class="math inline">\widetilde{x_{1i}}</span> is the part of <span class="math inline">x_1</span> that cannot be explained by any other explanatory variable. (uncorrelated with them)</li>
</ul>
<p><br></p>
<p>Now, take the regression of with outcome variable <span class="math inline">y</span>, with all explanatory variables except <span class="math inline">x_1</span>:</p>
<p><span class="math display">
y_i = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \widetilde{y_i}
</span></p>
<ul>
<li>Where <span class="math inline">\delta_0, ..., \delta_{k-1}</span> are coefficients.</li>
</ul>
<p>The error term is <span class="math inline">\widetilde{y_i}</span>, which is the part of <span class="math inline">y_i</span> that cannot be explained by <span class="math inline">x_2, ..., x_k</span> (uncorrelated with them).</p>
<p><br></p>
<p>Since <span class="math inline">\widetilde{y_i}</span> is not explained by <span class="math inline">x_2, ..., x_k</span>, variable <span class="math inline">x_1</span> must be the one explaining <span class="math inline">\widetilde{y_i}</span>.</p>
<ul>
<li>But, it is not the whole of <span class="math inline">x_1</span> explaining <span class="math inline">\tilde{y_i}</span> - since <span class="math inline">x_1</span> may also correlated with <span class="math inline">x_2, ..., x_k</span>, and the correlated parts of <span class="math inline">x_1</span> with <span class="math inline">x_2, ..., x_k</span> are already picked up in the regression by the coefficients of <span class="math inline">x_2, ..., x_k</span>.</li>
</ul>
<p>Thus, <span class="math inline">\widetilde{y_i}</span> must be explained by the part of <span class="math inline">x_1</span> that is uncorrelated and not explained by <span class="math inline">x_2, ..., x_k</span>, which we derived earlier as <span class="math inline">\widetilde{x_{1i}}</span>.</p>
<p><br></p>
<p>Thus, we can create another regression with explanatory variable <span class="math inline">\widetilde{x_{1i}}</span> and outcome variable <span class="math inline">\widetilde{y_i}</span>.</p>
<p><span class="math display">
\widetilde{y_i} = \alpha_0 + \alpha_1 \widetilde{x_{1i}} + u_i
</span></p>
<p>We can plug <span class="math inline">\widetilde{y_i}</span> back into our regression of <span class="math inline">y_i</span> with explanatory variables <span class="math inline">x_2 ..., x_k</span>:</p>
<p><span class="math display">
y_i = \delta_0 + \alpha_0 + \alpha_1 \widetilde{x_{1i}} + \delta_1x_{2i} + ... + \delta_{k-1} x_{ki} + u_i
</span></p>
<p>As we can see, this mirrors the original standard multiple linear regression. <u>The estimate of <span class="math inline">\alpha_1</span> will be the same as <span class="math inline">\beta_1</span> in the original regression</u>.</p>
<ul>
<li>The coefficient <span class="math inline">\alpha_1</span> (which is equal to <span class="math inline">\beta_1</span>) explains the expected change in <span class="math inline">y</span>, given an increase in the part of <span class="math inline">x_1</span> uncorrelated with <span class="math inline">x_2, ..., x_k</span>.</li>
<li>So essentially, we have <strong>partialed out</strong> the effect of the other explanatory variables, and only focus on the effect on <span class="math inline">y</span> of the uncorrelated part of <span class="math inline">x_1</span>.</li>
<li><u>This eliminates the effect of confounders on our estimates, and estimates of the effect of <span class="math inline">x_1</span> alone on <span class="math inline">y</span>.</u></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of Coefficients in Multiple Linear Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>Thus, the interpretation of <span class="math inline">\beta_1</span>, or any <span class="math inline">\beta_j</span> multiplied to <span class="math inline">x_j</span>, is:</p>
<ul>
<li>For every one unit increase in <span class="math inline">x_j</span>, there is an expected <span class="math inline">\beta_j</span> increase in <span class="math inline">y</span>, when controlling for all other explanatory variables.</li>
<li>Thus, we have “controlled for” and “partialled out” the effect of confounders on <span class="math inline">x_j</span>.</li>
</ul>
<p>For binary <span class="math inline">x_j</span> variables (such as treatment <span class="math inline">D</span>), <span class="math inline">\beta_j</span> is the expected difference in <span class="math inline">y</span> between categories <span class="math inline">x=1</span> and <span class="math inline">x=0</span>, when controlling for all other explanatory variables.</p>
<p>&nbsp;</p>
<p>Note: the <span class="math inline">\beta_j</span> interpretation is <u>not the same</u> for the linear probability model, polytomous explanatory variables, polynomial transformations, and log transformations.</p>
<ul>
<li>For the interpretation of those, refer back to the interpretation of <a href="https://politicalscience.github.io/metrics/causal/1.html">chapter 1</a>, and add the phrase <em>“when controlling for all other explanatory variables”</em>.</li>
</ul>
<p>Intercept <span class="math inline">\beta_0</span> is the expected value of <span class="math inline">y</span> when all <span class="math inline">\overrightarrow{x} = 0</span>.</p>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="gauss-markov-and-unbiasedness-of-the-ols-estimator" class="level1">
<h1>4.5: Gauss-Markov and Unbiasedness of the OLS Estimator</h1>
<p>An unbiased estimator, if we recall from <a href="https://politicalscience.github.io/metrics/causal/2.html#properties-of-estimators---unbiasedness-and-consistency">section 2.5</a>, means that over many different estimates, the expected value of all the estimates is the true parameter value: <span class="math inline">\mathbb{E}[\hat{\theta}_i] = \theta</span>.</p>
<p>Unbiasedness is desirable property of causal estimators.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Gauss-Markov Theorem for Unbiasdness
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>Gauss-Markov Theorem</strong> states that the ordinary least squares estimator is <strong>unbiased</strong> under 3 conditions:</p>
<ol type="1">
<li><strong>Linearity</strong> of the equation (no coefficients <span class="math inline">\beta_0, ..., \beta_k</span> can be multiplied together. This does not apply to explanatory variables, only the coefficients.</li>
<li>In a simple linear regression, <span class="math inline">x</span> has <strong>variation</strong> (so not all values of <span class="math inline">x</span> are the same). In a multiple linear regression, this condition becomes that there is <strong>no perfect correlation (no perfect multicollinearity)</strong> between any two explanatory variables.</li>
<li><u>And the most important condition - <strong>Zero conditional mean</strong></u>: meaning that no matter the value of <span class="math inline">x</span>, the expected value of the residual <span class="math inline">u_i</span> is always 0. <span class="math inline">\mathbb{E}[u | x] = 0</span> for all <span class="math inline">x</span> (and all explanatory variables in multiple regression).</li>
</ol>
</div>
</div>
<p><br></p>
<section id="proof-of-the-unbiasedness-of-ols" class="level3">
<h3 class="anchored" data-anchor-id="proof-of-the-unbiasedness-of-ols">Proof of the Unbiasedness of OLS</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Properties of Summation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before we start, here are a few key properties of summation</p>
<p><span class="math display">
\begin{split}&amp; \sum\limits_{i=1}^n (x_i - \bar{x}) = 0 \\&amp; \sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y}) \\&amp; \sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2\end{split}
</span></p>
</div>
</div>
<p>We want to show <span class="math inline">\mathbb{E}[ \hat{\beta}_1] = \beta_1</span>. Let us start off with the OLS estimator (we will use simple linear regression for simplicity):</p>
<p><span class="math display">
\hat{\beta}_1 = \frac{\sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum\limits_{i=1}^n(x_i - \bar{x})^2}
</span></p>
<p>We can expand the numerator, and since <span class="math inline">\sum(x_i - \bar{x}) \bar{y} = \bar{y} \sum(x_i - \bar{x}) = 0</span> (see properties of summation above), we can get:</p>
<p><span class="math display">
\hat{\beta}_1 = \frac{\sum\limits_{i=1}^n (x_i - \bar{x})y_i}{\sum\limits_{i=1}^n(x_i - \bar{x})^2}
</span></p>
<ul>
<li>The existence of <span class="math inline">\hat{\beta}_1</span> is guaranteed by condition 2 from above, since the denominator is equal to <span class="math inline">Var(x)</span>, so that must not be 0.</li>
</ul>
<p><br></p>
<p>Now, let us play with the numerator (note the properties of summation introduced earlier):</p>
<p><span class="math display">
\begin{split}\sum\limits_{i=1}^n (x_i - \bar{x})y_i &amp; = \sum\limits_{i=1}^n(x_i - \bar{x})(\beta_0 + \beta_1 x_i + u_i) \\&amp; = \beta_0 \sum\limits_{i=1}^n(x_i - \bar{x}) + \beta_1 \sum\limits_{i=1}^n(x_i - \bar{x}) x_i + \sum\limits_{i=1}^n (x_i - \bar{x}) u_i \\&amp; = 0 + \beta_1 \sum\limits_{i=1}^n(x_i - \bar{x})^2 + \sum\limits_{i=1}^n(x_i - \bar{x})u_i\end{split}
</span></p>
<p>Now, putting the numerator back into the equation, we simplify:</p>
<p><span class="math display">
\begin{split}
\hat{\beta}_1 &amp; = \frac{\beta_1 \sum\limits_{i=1}^n(x_i - \bar{x})^2 + \sum\limits_{i=1}^n(x_i - \bar{x})u_i}{\sum\limits_{i=1}^n(x_i - \bar{x})^2} \\
&amp;  =  \beta_1 + \frac{\sum\limits_{i=1}^n(x_i - \bar{x}) u_i}{\sum\limits_{i=1}^n (x_i - \bar{x})^2} \\
&amp;  = \beta_1 + \sum\limits_{i=1}^n d_i u_i
\end{split}
</span></p>
<ul>
<li>Where <span class="math inline">d_i = \frac{x_i - \bar{x}}{\sum (x_i - \bar{x})^2}</span>, which is a function of random variable <span class="math inline">x</span>.</li>
</ul>
<p><br></p>
<p>Now we need to find the expectation <span class="math inline">\mathbb{E}[\hat{\beta}_1]</span>.</p>
<p>First, do not worry about <span class="math inline">d_i</span>. What should <span class="math inline">u_i</span> be equal to? Naturally, the best estimate of <span class="math inline">u_i</span> is its expected utility.</p>
<p><span class="math display">
\mathbb{E} (\hat{\beta}_1 | x) = \beta_1 + \sum\limits_{i=1}^n d_i \ \mathbb{E}(u_i | x)
</span></p>
<p>We know by the third Gauss-Markov condition (Zero conditional mean), that <span class="math inline">\mathbb{E}[u_i | x_i] = 0</span>. Thus, that makes the entire summation equal to 0:</p>
<p><span class="math display">
\mathbb{E}(\hat{\beta}_1 | x) = \mathbb{E}(\hat{\beta}_1) = \beta_1
</span></p>
<p>Thus, the ordinary least squares estimator is unbiased, given 3 conditions (linearity, variation in <span class="math inline">x</span>, zero-conditional mean) are met.</p>
<ul>
<li>If conditions are met, we have an unbiased estimator in OLS for causal estimation.</li>
<li><u>The key issue is assumption 3: Zero conditional mean <span class="math inline">\mathbb{E}[u | x] = 0</span></u>, which is the most easily violated of these assumptions. We will explore this in detail in the next section.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
</section>
<section id="omitted-variable-bias-endogeneity-and-limitations-of-ols-for-causal-estimation" class="level1">
<h1>4.6: Omitted Variable Bias, Endogeneity, and Limitations of OLS for Causal Estimation</h1>
<section id="omitted-variable-bias" class="level3">
<h3 class="anchored" data-anchor-id="omitted-variable-bias">Omitted Variable Bias</h3>
<p>In the previous section, we discussed how the assumption of <strong>zero-conditional mean</strong> <span class="math inline">\mathbb{E}[u | x] = 0</span> is critical to the unbiasdness of OLS.</p>
<ul>
<li>This assumption is also called <strong>exogeneity</strong>.</li>
</ul>
<p>However, this assumption is frequently violated. The most common reason for this is because of <strong>omitted variable bias</strong>.</p>
<p><br></p>
<p>Consider two regressions, one with only our treatment variable of interest <span class="math inline">D</span>, and, and one with an extra control variable <span class="math inline">x</span> that is omitted from the first regression:</p>
<p><span class="math display">
\begin{split}y_i &amp; = \beta_0^S + \beta_1^SD_i + u_i^S \quad \text{short} \\y_i &amp; = \beta_0 + \beta_1D_i + \beta_2x_i + u_i \quad \text{long}\end{split}
</span></p>
<p>Now consider an auxiliary regression, where the omitted variable <span class="math inline">x</span> is the outcome variable, and <span class="math inline">D_i</span> is the explanatory variable:</p>
<p><span class="math display">
x_i = \delta_0 + \delta_1 D_i + v_i
</span></p>
<ul>
<li>where <span class="math inline">\delta_0, \delta_1</span> are coefficients and <span class="math inline">v_i</span> is the error term</li>
</ul>
<p><br></p>
<p>Now we have <span class="math inline">x_i</span> in terms of <span class="math inline">D_i</span>, let us plug <span class="math inline">x_i</span> into our long regression to “recreate” the short regression:</p>
<p><span class="math display">
\begin{split}y_i &amp; = \beta_0 + \beta_1D_i + \beta_2x_i + u_i \\y_i &amp; = \beta_0 + \beta_1 D_i + \beta_2(\delta_0 + \delta_1D_i + v_i) + u_i \\y_i &amp; = \beta_0 + \beta_1 D_i + \beta_2 \delta_0 + \beta_2 \delta_1 D_i + \beta_2v_i + u_i \\y_i &amp; = \beta_0 + \beta_2 \delta_0 + (\beta_1 + \beta_2 \delta_1)D_i + \beta_2v_i + u_i\end{split}
</span></p>
<p>We have “recreated” the short regression with one variable <span class="math inline">D</span>. We can see in this recreation, the coefficient of <span class="math inline">D_i</span> is <span class="math inline">\beta_1 + \beta_2 \delta_1</span>.</p>
<ul>
<li>Since this is a recreation of the “short” regression, that means coefficient <span class="math inline">\beta_1^S = \beta_1 + \beta_2 \delta_1</span>.</li>
</ul>
<p>The difference between the short regression coefficient <span class="math inline">\beta_1^S</span>, and the original long regression coefficient <span class="math inline">\beta_1</span>, is <span class="math inline">\beta_2 \delta_1</span>.</p>
<ul>
<li>Or in other words, <span class="math inline">\beta_2 \delta_1</span>, which is actually some of the effect of omitted <span class="math inline">x</span> on <span class="math inline">y</span>, is being “tangled up” into the coefficient of the short regression <span class="math inline">\beta_1^S</span>.</li>
</ul>
<p>If <span class="math inline">\beta_2 = 0</span> (meaning no relationship between omitted <span class="math inline">x_i</span> and <span class="math inline">y</span>), or <span class="math inline">\delta_1 = 0</span> (meaning no relationship between omitted <span class="math inline">x_i</span> and <span class="math inline">D_i</span>), then difference <span class="math inline">\beta_2 \delta_1 = 0</span>, thus there is no issue.</p>
<ul>
<li>But if either of those facts are not true, then <span class="math inline">\beta_2 \delta_1 ≠ 0</span>.</li>
</ul>
<p><br></p>
</section>
<section id="endogeneity-and-violation-of-gauss-markov-conditions" class="level3">
<h3 class="anchored" data-anchor-id="endogeneity-and-violation-of-gauss-markov-conditions">Endogeneity and Violation of Gauss-Markov Conditions</h3>
<p>Why is omitted variable bias (if non-zero) an issue?</p>
<ul>
<li>The omitted variable <span class="math inline">x</span>’s effect is mostly subsumed into the error term <span class="math inline">u_i^S</span> of the short regression.</li>
<li>But some bit of <span class="math inline">x</span> (that is correlated with <span class="math inline">D</span>) is included in our coefficient (specifically, <span class="math inline">\beta_2 \delta_1</span>).</li>
<li>That means our treatment variable <span class="math inline">D</span> will be correlated with the error term, <u>violating the Gauss-Markov assumption of zero-conditional mean, and producing biased OLS results</u>.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Endogeneity
</div>
</div>
<div class="callout-body-container callout-body">
<p>Endogeneity is when a regressor <span class="math inline">x</span> is correlated with the error term <span class="math inline">u_i</span>.</p>
<ul>
<li>This frequently occurs due to <strong>omitted variable bias</strong>.</li>
<li>The explanatory variable that is correlated with the error term is called an <strong>endogenous variable</strong>.</li>
</ul>
<p>When endogeneity exists, the assumption of zero-conditional mean/exogeneity <span class="math inline">\mathbb{E}[u | x] = 0</span> is violated.</p>
<ul>
<li>This means that OLS estimates are no longer unbiased.</li>
<li><u>That means, when endoeneity is present, accurate causal estimation with OLS is not possible.</u></li>
</ul>
</div>
</div>
<p><br></p>
<p>Endogeneity can also be caused by other factors, including measurement and reverse causality:</p>
<ul>
<li><p>If we have measurement error, endogeneity may also be introduced, and we will be unable to do causal inference. We will discuss some ways to deal with measurement problems in part 3 of the econometrics course.</p></li>
<li><p>We can eliminiate reverse causality with either a convincing causal “story” that explains why reverse causality is not possible, or with further methods like quasi-experimental methods and instrumental variables.</p></li>
</ul>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="model-selection-for-causal-estimation" class="level1">
<h1>4.7: Model Selection for Causal Estimation</h1>
<p>Recall, that omitted variable bias <span class="math inline">\beta_2 \delta_1</span> is only an issue when <span class="math inline">\beta_2 \delta_1 ≠ 0</span> (since 0 means no bias!). So when does this occur? First, let us figure out when there is no omitted variable bias:</p>
<ul>
<li>When <span class="math inline">\beta_2 = 0</span> (meaning no relationship between omitted <span class="math inline">x</span> and <span class="math inline">y</span>), then <span class="math inline">\beta_2 \delta_1 = 0</span>.</li>
<li>When <span class="math inline">\delta_1 = 0</span> (meaning no relationship between omitted <span class="math inline">x</span> and <span class="math inline">D_i</span>), then <span class="math inline">\beta_2 \delta_1 = 0</span>.</li>
</ul>
<p>Thus, omitted variable bias is only a problem when their is <strong>both</strong> a correlation between omitted <span class="math inline">x</span> and <span class="math inline">y</span>, and a correlation between omitted <span class="math inline">x</span> and treatment <span class="math inline">D</span>.</p>
<ul>
<li>This is the definition of a confounder variable as well! One that affects selection into treatment <span class="math inline">w \rightarrow D</span> and has a backdoor path to outcome <span class="math inline">w \rightarrow y</span>.</li>
</ul>
<p><u>So, omitted variable bias only occurs when we fail to control for confounders!</u></p>
<p><br></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Causal Interpretation of Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p><u>For a causal interpretation of regression and OLS estimates, we must be confident that all confounders are controlled for.</u></p>
<ul>
<li>Often times, this is impossible, since many causal effects have unobservable or impossible to measure confounders.</li>
<li>Thus, to deal with omitted variables and endogeneity when controlling for all confounders is not possible, we will need further techniques (such as instrumental variables).</li>
</ul>
<p>We must also be confident that reverse causality can be ruled out, and measurement error is not an issue.</p>
<p>&nbsp;</p>
<p><u>When this is met, we can interpret <span class="math inline">\beta_j</span> of variable <span class="math inline">x_j</span> as the average causal effect of increasing <span class="math inline">x_j</span> by one unit on <span class="math inline">y</span>.</u></p>
<ul>
<li>And then we can conduct hypothesis tests to confirm our causal inference.</li>
</ul>
</div>
</div>
<p><br></p>
<p>Another important thing is that non-confounders (variables that do not correlate with both <span class="math inline">D</span> and <span class="math inline">y</span> ) do not introduce omitted variable bias or endogeneity.</p>
<ul>
<li>However, including non-confounders as controls increases our standard error, which decreases the precision of our estimates (and makes it harder to reject the null hypothesis, we will discuss this in the next section).</li>
<li>Thus, we should avoid including non-confounders in regression models, as they only make our estimates less consistent, without any benefit.</li>
</ul>
<p>Model selection is one of the most difficult parts of causal inference.</p>
<ul>
<li>Good model selection is not only about econometric and statistical knowledge, but also about knowledge of the content in your discipline.</li>
<li>Only through extensive reading and understanding of the literature, will you be able to make good choices in model selection.</li>
</ul>
<p><br></p>
<p>This chapter has focused on a lot of the limitations on OLS and multiple linear regression for causal estimation.</p>
<ul>
<li>Well then, if we cannot use OLS, what should we use?</li>
<li>The rest of this part of the course on causal estimation will introduce alternative methods to deal with issues regarding OLS estimation.</li>
</ul>
<p>However, <strong>before we move away from linear regression</strong>, we still have two important concepts to cover!</p>
<ol type="1">
<li>Causal Inference - remember, our estimates are just sample estimates, and in order to test if they truly are causal, we need to conduct hypothesis testing.</li>
<li>Interaction and conditional average treatment effects, which will be covered in the final section of the chapter.</li>
</ol>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="causal-inference-and-hypothesis-testing-with-multiple-linear-regression" class="level1">
<h1>4.8: Causal Inference and Hypothesis Testing with Multiple Linear Regression</h1>
<section id="homoscedasticity-and-standard-errors" class="level3">
<h3 class="anchored" data-anchor-id="homoscedasticity-and-standard-errors">Homoscedasticity and Standard Errors</h3>
<p>Homoscedasticity is the property that the variance of errors are constant throughout the model:</p>
<p><span class="math display">
Var(u|x) = \sigma^2
</span></p>
<p>When this is violated, we say our model has <strong>heteroscedasticity</strong>.</p>
<p>That is a little hard to understand. Let us intuitively explore this with a graph. Below are plots of residuals (errors) of the regression - essentially, we take the best fit line, and move it horizontal:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:75.0%"></p>
</figure>
</div>
<p>The heteroscedasticity plot has a clear inconsistency in the variation of the residuals (small variance with lower <span class="math inline">x</span>, larger variance with higher <span class="math inline">x</span>).</p>
<p><br></p>
<p>Homoscedasticity is actually one of the Gauss-Markov conditions.</p>
<ul>
<li>We previously only discussed the Gauss-Markov Theorem in relation to unbiasedness.</li>
<li>However, Gauss-Markov also states that with the unbiasdeness conditions + homoscedasticity, the variance of the OLS estimator is the lowest of all linear unbiased estimators. Thus, OLS is the <strong>best linear unbiased estimator (BLUE)</strong>.</li>
</ul>
<p>However, in reality, we don’t really care about this. OLS is still a estimator with respectable variance (given a large enough sample size) even if this assumption is violated, it just is no longer “the best”.</p>
<p><br></p>
<p>There is a more practical reason we care about homoscedasticity: When we do not meet the assumption, we have to use heteroscedasticity-<strong>robust</strong> standard errors.</p>
<ul>
<li>These account for our estimates less consistent, making the standard errors larger, thus harder to reject the null hypothesis).</li>
<li>These days in econometrics, we assume that homoscedasticity is violated, and almost always use robust standard errors by default.</li>
</ul>
<p><br></p>
</section>
<section id="causal-inference-in-multiple-linear-regression" class="level3">
<h3 class="anchored" data-anchor-id="causal-inference-in-multiple-linear-regression">Causal Inference in Multiple Linear Regression</h3>
<p>We can run hypothesis testing and confidence intervals in almost the same way as in randomised experiments and single linear regression, as explained in <a href="https://politicalscience.github.io/metrics/causal/3.html#uncertainty-in-causal-estimates-and-standard-errors">section 3.4</a> and <a href="https://politicalscience.github.io/metrics/causal/3.html#causal-inference-and-hypothesis-testing">section 3.5</a>.</p>
<p>The only two differences are the standard error and degrees of freedom.</p>
<p>The robust standard error is as follows:</p>
<p><span class="math display">
se(\hat\beta_j) = \sqrt{\frac{Var(\hat{u_i})}{n \times Var(\widetilde{x_{ji}})}}
</span></p>
<ul>
<li>Where <span class="math inline">\widetilde{x_{ji}}</span> are the residuals from a regression of outcome variable <span class="math inline">x_j</span> on all other explanatory variables (as shown during section 4.4 on regression anatomy).</li>
</ul>
<p><br></p>
<p>With the standard error, we can calculate the t-statistic, and reference the t-distribution.</p>
<ul>
<li>However, we have to adjust the degrees of freedom parameter to <span class="math inline">n-k-1</span></li>
<li>Where <span class="math inline">n</span> is the number of observations and <span class="math inline">k</span> is the number of explanatory variables.</li>
<li>NOTE: this is different than the degrees of freedom with difference-in-means.</li>
</ul>
<p>Now, we can calculate the p-values:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of P-Values
</div>
</div>
<div class="callout-body-container callout-body">
<p>If the p-value of <span class="math inline">\beta_j</span> is above 0.05, there is a above 5% chance that the null hypothesis is true. This is too high for our liking, so we cannot reject the null hypothesis, and we cannot conclude any causal effect of <span class="math inline">x_j</span> on <span class="math inline">y</span>.</p>
<p>If the p-value of <span class="math inline">\beta_j</span> is less than 0.05, there is less than a 5% chance that the null hypothesis is true. In econometrics, we thus reject the null hypothesis, and conclude that there is a causal effect of <span class="math inline">x_j</span> on <span class="math inline">y</span>.</p>
<p><u>So very simply, if the p-value of</u> <span class="math inline">\beta_j</span> <u>is less than 0.05, we can conclude that there is a causal effect of <span class="math inline">x_j</span> on <span class="math inline">y</span>. If not, we cannot conclude this.</u></p>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Warnings!
</div>
</div>
<div class="callout-body-container callout-body">
<p>If our causal estimate is biased (often due to Endogeneity), we cannot conclude a causal relationship with a hypothesis test</p>
<p>Furthermore, the same interpretations do not apply to hypotheses tests on polynomial transformations and polytomous categorical variables. We will address this with an F-Test (see below).</p>
</div>
</div>
<p><br></p>
</section>
<section id="r-squared-and-the-f-test-of-multiple-coefficients" class="level3">
<h3 class="anchored" data-anchor-id="r-squared-and-the-f-test-of-multiple-coefficients">R-Squared and the F-Test of Multiple Coefficients</h3>
<p>As I just discussed, polynomial transformations and polytomous explanatory variable coefficients cannot be interpreted as the causal relationship between the variable and <span class="math inline">y</span>.</p>
<ul>
<li>This is because these variables have multiple coefficients for the same variable.</li>
<li>The <span class="math inline">\beta</span> coefficient for the squared term <span class="math inline">x^2</span> (or any other higher polynomial term) only shows if there is a significant non-linear relationship between <span class="math inline">x</span> and <span class="math inline">y</span>, not the significance of the causal effect on <span class="math inline">y</span>.</li>
<li>The <span class="math inline">\beta_j</span> of category <span class="math inline">j</span> of a polytomous categorical variable does not show the significance of the causal effect of the polytomous categorical variable on <span class="math inline">y</span>. It only shows the difference between category <span class="math inline">j</span> and the reference category.</li>
<li>See <a href="https://politicalscience.github.io/metrics/causal/1.html">chapter 1</a> for more detail on these topics.</li>
</ul>
<p>But what if we are interested in if these variables have a causal effect on <span class="math inline">y</span>? How do we conduct statistical inference on these variables with multiple coefficients?</p>
<p><br></p>
<p>To do this, we first need to discuss the idea of the total sum of squares (TSS) and <span class="math inline">R^2</span>.</p>
<p>The total sum of squares is the total amount of sample variation in <span class="math inline">y</span>:</p>
<p><span class="math display">
\begin{split}TSS &amp; = \sum(Y_i - \bar{Y})^2 \\&amp; = \sum (\hat{Y}_i - \bar{Y})^2 + \sum (Y_i - \hat{Y}_i)^2\end{split}
</span></p>
<ul>
<li>Where TSS is the total sum of squares, and is formed of the sum of two components.</li>
<li>Where <span class="math inline">\sum (\hat{Y}_i - \bar{Y})^2</span> is the model sum of squares (SSM). This represents the part of the variation of <span class="math inline">Y</span> that is explained by the model</li>
<li>Where <span class="math inline">\sum (Y_i - \hat{Y}_i)^2</span> is the sum of squared errors SSE (that we used to fit the model). This represents the part of the variation of <span class="math inline">Y</span> that is not explained by the model (hence, why it is called error).</li>
</ul>
<p><br></p>
<p>R-squared is one of the key summary statistics of our model.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: R-Squared
</div>
</div>
<div class="callout-body-container callout-body">
<p>R-squared <span class="math inline">R^2</span> is a measure of the percentage of variation in <span class="math inline">Y</span>, that is explained by our model (with our chosen explanatory variables). The percentage of variation in <span class="math inline">Y</span> explained by our model would be:</p>
<p><span class="math display">
R^2 = \frac{SSM}{TSS} = \frac{\sum (\hat{Y}_i - \bar{Y})^2}{\sum(Y_i - \bar{Y})^2}
</span></p>
</div>
</div>
<p><br></p>
<p>Now that we understand R-squared, we can move to the f-test, which allows us to test multiple coefficients at the same time.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: F-test of Nested Models
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>F-test of Nested Models</strong> allows us to compare different regression models.</p>
<ul>
<li>We use a smaller model as our null hypothesis, and a larger model (containing the smaller model) as our alternative hypothesis. More mathematically:</li>
</ul>
<p><span class="math display">
\begin{split}
M_0 : &amp; \ \hat y = \hat\beta_0 + \hat\beta_1 X_1 + ... + \hat\beta_g X_g \\
M_a : &amp;  \ \hat y = \hat\beta_0 + \hat\beta_1 X_1 + ... + \hat\beta_g X_g + \hat\beta_{g+1} X_{g+1} + ... + \hat\beta_k X_k
\end{split}
</span></p>
<p>For example, if you wanted to test the statistical significance of a polytomous explanatory variable, <span class="math inline">M_0</span> would be the model without it, and <span class="math inline">M_a</span> would be the model with the variable (and its multiple coefficients).</p>
<p>Importantly, all explanatory variables in model <span class="math inline">M_0</span> must also be in <span class="math inline">M_a</span> (hence “nested”).</p>
</div>
</div>
<p><br></p>
<p>The F-test uses the F-test statistic.</p>
<ul>
<li>This statistic compared the <span class="math inline">R^2</span> values of the two models.</li>
<li>Let us say the <span class="math inline">R^2</span> value of <span class="math inline">M_0</span> is notated <span class="math inline">R^2_0</span>, and the <span class="math inline">R^2</span> value of <span class="math inline">M_a</span> is notated as <span class="math inline">R^2_a</span>.</li>
<li>The F-test statistic essentially measures the difference <span class="math inline">R^2_a - R^2_0</span>. If the difference is sufficiently large, that means the <span class="math inline">M_a</span> model has significantly more explanatory power than <span class="math inline">M_0</span>.</li>
</ul>
<p>Mathematically, the F-test statistic is as follows, with <span class="math inline">k_a</span> being the number of explanatory variables in the alternate hypothesis:</p>
<p><span class="math display">
F = \frac{ R^2_{\text{change}} / df_{\text{change}} }{  (1 - R^2_a ) / [n - (k_a + 1)]}
</span></p>
<p>The sampling distribution of the F-statistic is the F distribution with parameters <span class="math inline">k-a - k_0</span> and <span class="math inline">n-(k_a + 1)</span> degrees of freedom. We then obtain the p-value from this distribution.</p>
<p>The p-values of the F-statistic show the following:</p>
<ul>
<li>If the p-value is very small, that means <span class="math inline">R^2_a</span> is significantly larger than <span class="math inline">R^2_0</span>. This is evidence against model <span class="math inline">M_0</span>, and in favour of the larger model <span class="math inline">M_a</span>.</li>
<li>If the p-value is large, that means <span class="math inline">R^2_a</span> is not much larger than <span class="math inline">R^2_0</span>. This means there is no evidence against <span class="math inline">M_0</span>, and <span class="math inline">M_a</span> is not the statistically significantly better model.</li>
</ul>
<p>F-tests of nested models can help us test the significance of multiple coefficients at once.</p>
<ul>
<li><p>The null model <span class="math inline">M_0</span> will be the model without our coefficients</p></li>
<li><p>The alternate model <span class="math inline">M_a</span> will be the model with all the coefficients we want to test together (so all of the coefficients of a polytomous variable, or polynomial variable, etc.).</p></li>
</ul>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="interaction-terms-and-conditional-average-treatment-effect" class="level1">
<h1>4.9: Interaction Terms and Conditional Average Treatment Effect</h1>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="implementation-in-r" class="level1">
<h1>Implementation in R</h1>
<p>The packages we will need are:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(fixest)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(texreg)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>
<section id="multiple-linear-regression-in-r" class="level2">
<h2 class="anchored" data-anchor-id="multiple-linear-regression-in-r">Multiple Linear Regression in R</h2>
<p>To run simple linear regression, we use the <em>feols()</em> function.</p>
<ul>
<li>The argument <em>se = “hetero”</em> tells R to calculate heteroscedasticity-robust standard errors, which will be discussed later in chapter 4. Just know it is standard to do so.</li>
<li>We can add more explanatory variables by using + and then add more. We can reduce the number of explanatory variables down to one.</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>modelname <span class="ot">&lt;-</span> <span class="fu">feols</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> X3, <span class="at">data =</span> mydata, <span class="at">se =</span> <span class="st">"hetero"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(modelname)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For example:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">feols</span>(pct_missing <span class="sc">~</span> treat_invite <span class="sc">+</span> head_edu <span class="sc">+</span> mosques <span class="sc">+</span> pct_poor,</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> dta, <span class="at">se =</span> <span class="st">"hetero"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>OLS estimation, Dep. Var.: pct_missing
Observations: 472
Standard-errors: Heteroskedasticity-robust 
              Estimate Std. Error   t value   Pr(&gt;|t|)    
(Intercept)   0.429443   0.089541  4.796047 2.1799e-06 ***
treat_invite -0.026750   0.032667 -0.818872 4.1328e-01    
head_edu     -0.005320   0.006128 -0.868103 3.8578e-01    
mosques      -0.049943   0.018451 -2.706799 7.0424e-03 ** 
pct_poor     -0.103641   0.073363 -1.412729 1.5840e-01    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
RMSE: 0.340276   Adj. R2: 0.013925</code></pre>
</div>
</div>
<p>We can see the output estimate of the intercept and all the coefficients.</p>
<ul>
<li>These rows include the estimate, the standard error, the t-test statistic, and the p-value. This gives all of the information we need to run linear regression and hypothesis tests.</li>
</ul>
<p>For how to do categorical variables, consult chapter 1.</p>
<p><br></p>
<p>We can also use the base-R <em>lm()</em> function, however, this does not calculate heteroscedasticity-robust standard errors (once again, will be discussed in chapter 4).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>modelname <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> mydata)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(modelname)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>
</section>
<section id="confidence-intervals" class="level2">
<h2 class="anchored" data-anchor-id="confidence-intervals">Confidence Intervals</h2>
<p>To see confidence intervals of all our coefficients, we can use the <em>confint()</em> function on our regression object:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(modelname)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For example, let us find the confidence intervals for our last model:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(model1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                   2.5 %       97.5 %
(Intercept)   0.25348998  0.605396682
treat_invite -0.09094173  0.037442062
head_edu     -0.01736102  0.006721906
mosques      -0.08620059 -0.013685913
pct_poor     -0.24780279  0.040520175</code></pre>
</div>
</div>
<p><br></p>
</section>
<section id="creating-regression-tables" class="level2">
<h2 class="anchored" data-anchor-id="creating-regression-tables">Creating Regression Tables</h2>
<p>We can create regression tables using the <em>texreg()</em> or <em>screenreg()</em> functions.</p>
<ul>
<li><em>texreg()</em> produces LaTeX code that you can insert into a LaTeX document</li>
<li><em>screenreg()</em> produces something that looks nice in a R document.</li>
</ul>
<p>The syntax is as follows (you can replace <em>screenreg()</em> with <em>texreg()</em> ):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">screenreg</span>(<span class="at">l =</span> <span class="fu">list</span>(modelname),</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">custom.model.names =</span> <span class="fu">c</span>(<span class="st">"Outcome Variable Name"</span>),</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">custom.coef.names =</span> <span class="fu">c</span>(<span class="st">"Intercept"</span>, <span class="st">"X1 Variable Name"</span>, <span class="st">"X2 Variable Name"</span>),</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">digits =</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For example:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">screenreg</span>(<span class="at">l =</span> <span class="fu">list</span>(model1),</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">custom.model.names =</span> <span class="fu">c</span>(<span class="st">"Pct_Missing"</span>),</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">custom.coef.names =</span> <span class="fu">c</span>(<span class="st">"Intercept"</span>, <span class="st">"Treatment"</span>, <span class="st">"head_edu"</span>, <span class="st">"mosques"</span>, <span class="st">"pct-poor"</span>),</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">digits =</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================
                       Pct_Missing
----------------------------------
Intercept                0.429 ***
                        (0.090)   
Treatment               -0.027    
                        (0.033)   
head_edu                -0.005    
                        (0.006)   
mosques                 -0.050 ** 
                        (0.018)   
pct-poor                -0.104    
                        (0.073)   
----------------------------------
Num. obs.              472        
R^2 (full model)         0.022    
R^2 (proj model)                  
Adj. R^2 (full model)    0.014    
Adj. R^2 (proj model)             
==================================
*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05</code></pre>
</div>
</div>
<p><br></p>
</section>
<section id="f-tests-of-nested-models" class="level2">
<h2 class="anchored" data-anchor-id="f-tests-of-nested-models">F-Tests of Nested Models</h2>
</section>
<section id="interaction-effects" class="level2">
<h2 class="anchored" data-anchor-id="interaction-effects">Interaction Effects</h2>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>