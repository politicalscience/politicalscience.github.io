<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Chapter 4: Causal Inference through Multiple Linear Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="4_files/libs/clipboard/clipboard.min.js"></script>
<script src="4_files/libs/quarto-html/quarto.js"></script>
<script src="4_files/libs/quarto-html/popper.min.js"></script>
<script src="4_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="4_files/libs/quarto-html/anchor.min.js"></script>
<link href="4_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="4_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="4_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="4_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="4_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Chapter 4: Causal Inference through Multiple Linear Regression</h1>
            <p class="subtitle lead">Econometric Methods (for Social Scientists)</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Chapter 1: Relationships Between Variables and Basics of Regression</h2>
   
  <ul class="collapse">
  <li><a href="#the-multiple-linear-regression-model" id="toc-the-multiple-linear-regression-model" class="nav-link active" data-scroll-target="#the-multiple-linear-regression-model">4.1: The Multiple Linear Regression Model</a></li>
  <li><a href="#multiple-linear-regression-model-with-linear-algebra" id="toc-multiple-linear-regression-model-with-linear-algebra" class="nav-link" data-scroll-target="#multiple-linear-regression-model-with-linear-algebra">4.2: Multiple Linear Regression Model with Linear Algebra</a></li>
  <li><a href="#ordinary-least-squares-estimator-for-multiple-regression" id="toc-ordinary-least-squares-estimator-for-multiple-regression" class="nav-link" data-scroll-target="#ordinary-least-squares-estimator-for-multiple-regression">4.3: Ordinary Least Squares Estimator for Multiple Regression</a></li>
  <li><a href="#regression-anatomy-theory-and-coefficient-interpretation" id="toc-regression-anatomy-theory-and-coefficient-interpretation" class="nav-link" data-scroll-target="#regression-anatomy-theory-and-coefficient-interpretation">4.4: Regression Anatomy Theory and Coefficient Interpretation</a></li>
  <li><a href="#gauss-markov-and-unbiasedness-of-the-ols-estimator" id="toc-gauss-markov-and-unbiasedness-of-the-ols-estimator" class="nav-link" data-scroll-target="#gauss-markov-and-unbiasedness-of-the-ols-estimator">4.5: Gauss-Markov and Unbiasedness of the OLS Estimator</a></li>
  <li><a href="#causal-inference-and-consistency-of-ols-under-gauss-markov" id="toc-causal-inference-and-consistency-of-ols-under-gauss-markov" class="nav-link" data-scroll-target="#causal-inference-and-consistency-of-ols-under-gauss-markov">4.6: Causal Inference and Consistency of OLS under Gauss-Markov</a></li>
  <li><a href="#omitted-variable-bias-and-endogeneity" id="toc-omitted-variable-bias-and-endogeneity" class="nav-link" data-scroll-target="#omitted-variable-bias-and-endogeneity">4.7: Omitted Variable Bias and Endogeneity</a></li>
  <li><a href="#validity-of-using-ols-for-causal-estimation" id="toc-validity-of-using-ols-for-causal-estimation" class="nav-link" data-scroll-target="#validity-of-using-ols-for-causal-estimation">4.8: Validity of Using OLS for Causal Estimation</a></li>
  <li><a href="#implementation-in-r" id="toc-implementation-in-r" class="nav-link" data-scroll-target="#implementation-in-r">Implementation in R</a>
  <ul class="collapse">
  <li><a href="#multiple-linear-regression-in-r" id="toc-multiple-linear-regression-in-r" class="nav-link" data-scroll-target="#multiple-linear-regression-in-r">Multiple Linear Regression in R</a></li>
  <li><a href="#confidence-intervals" id="toc-confidence-intervals" class="nav-link" data-scroll-target="#confidence-intervals">Confidence Intervals</a></li>
  <li><a href="#creating-regression-tables" id="toc-creating-regression-tables" class="nav-link" data-scroll-target="#creating-regression-tables">Creating Regression Tables</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>This chapter introduces multiple linear regression, which allows us to control for confounding variables. We will explore the multiple regression model, and what it means to control for a variable. Then, we will focus heavily on the Ordinary Least Squares (OLS) estimator, why it is a good estimator, and issues when it comes to causal inference.</p>
<p>Topics: Multiple Linear Regression, OLS with Linear Algebra, Regression Anatomy, Gauss-Markov, Unbiasedness and Consistnecy of OLS, Omitted Variable Bias and Endogeneity.</p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
<section id="the-multiple-linear-regression-model" class="level1">
<h1>4.1: The Multiple Linear Regression Model</h1>
<p>Before we start, make sure you have a strong understanding of all aspects of the simple linear regression model covered in <a href="https://politicalscience.github.io/metrics/causal/1.html">chapter 1</a>.</p>
<p>One of the issues with simple linear regression is that it only measures the correlation/relationship between <span class="math inline">x</span> and <span class="math inline">y</span>. Simple linear regression can only be used for causal estimation when used in randomised controlled experiments.</p>
<p><br></p>
<p>However, in the social sciences, randomisation is often not possible.</p>
<p>Linear regression allows us to estimate a model with both our treatment and outcome variables, as well as a series of <strong>control</strong> variables.</p>
<ul>
<li>We will explore the idea of “controlling” in later in section 4.4.</li>
</ul>
<p>In theory, by including every single confounding variable as a control variable in our regression model, we can partial out the effect of confounders and find the average treatment effect.</p>
<ul>
<li>In reality, as we will cover later, it is often impossible to account for all confounding variables, so we may need additional techniques to account for those situations.</li>
</ul>
<p><br></p>
<p>The <strong>response variable</strong> (outcome variable) is notated <span class="math inline">y</span>, just like in single linear regression.</p>
<p>The <strong>explanatory variable</strong>s are <span class="math inline">x_1, x_2, ..., x_k</span>. We sometimes also denote all explanatory variables as the vector <span class="math inline">\overrightarrow{x}</span>. Our treatment variable <span class="math inline">D</span> is considered one of the explanatory variables <span class="math inline">\overrightarrow{x}</span> (most often <span class="math inline">x_1</span>).</p>
<p>A linear regression model is the specification of the conditional distribution of <span class="math inline">Y</span>, given <span class="math inline">\overrightarrow{x}</span> (if you do not understand this, see <a href="https://politicalscience.github.io/metrics/causal/1.html">chapter 1</a>). The linear regression model focuses on the <strong>expected value</strong> of the conditional distribution, notated <span class="math inline">\mathbb{E}[y_i|\overrightarrow{x}_i]</span>.</p>
<p><br></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Multiple Linear Regression Model
</div>
</div>
<div class="callout-body-container callout-body">
<p>Take a set of observed data with <span class="math inline">n</span> number of pairs of <span class="math inline">(\overrightarrow{x}_i, y_i)</span> observations. The linear model takes the following form:</p>
<p><span class="math display">
\mathbb{E}[y_i|\overrightarrow{x}_i] = \beta_0 + \beta_1 x_{1i} + ... + \beta_k x_{ki}
</span></p>
<ul>
<li>Where the coefficients (that need to be estimated) are vector<span class="math inline">\overrightarrow{\beta} = \beta_0, \beta_1, ..., \beta_k</span>.</li>
</ul>
<p>We can also write the linear model for the value of any point <span class="math inline">y_i</span> in our data:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1x_{1i} + ... + \beta_k x_{ki} + u_i
</span></p>
<ul>
<li>Where <span class="math inline">u_i</span> is the error term function - that determines the error for each unit <span class="math inline">i</span>. Error <span class="math inline">u_i</span> has a variance of <span class="math inline">\sigma^2</span>, and expectation <span class="math inline">\mathbb{E}[u_i] = 0</span>.</li>
</ul>
</div>
</div>
<p><br></p>
<p>In our model, <span class="math inline">x_1</span> is typically the treatment variable we are concerned with.</p>
<ul>
<li>We want to show some sort of causal relationship between <span class="math inline">x_1</span> and <span class="math inline">y</span></li>
</ul>
<p>The other explanatory variables <span class="math inline">x_2, ..., x_k</span> are the confounding variables we want to control for.</p>
<ul>
<li><p>We will go into detail what “controlling for” means in section 4.4.</p></li>
<li><p>We will also discuss how to select control variables in section 4.8.</p></li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="multiple-linear-regression-model-with-linear-algebra" class="level1">
<h1>4.2: Multiple Linear Regression Model with Linear Algebra</h1>
<p>We can also represent the multiple linear regression model in linear algebra.</p>
<p>Let us start with the linear model:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + ... + \beta_k x_{ki} + u_i
</span></p>
<p>The <span class="math inline">i</span>’th observation can be re-written in vector form as following:</p>
<p><span class="math display">
y_i = x_i'\beta + u_i, \text{ where }\beta = \begin{bmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_k\end{bmatrix} \text{ and }x_i = \begin{bmatrix}1 \\x_{1i} \\\vdots \\x_{ki}\end{bmatrix}
</span></p>
<ul>
<li><p>The <span class="math inline">x_i'</span> in the equation is the transpose of <span class="math inline">x_i</span>, to make matrix multiplication possible.</p></li>
<li><p>The first element of the <span class="math inline">x_i</span> matrix is 1, since <span class="math inline">1 \times \beta_0</span> gives us the first parameter (intercept) in the linear model.</p></li>
</ul>
<p><br></p>
<p>Since our data has <span class="math inline">n</span> number of observations <span class="math inline">i</span>, we can express this into vector form, with the <span class="math inline">X_i'</span> and <span class="math inline">\beta</span> being vectors within a vector.</p>
<p><span class="math display">
\begin{split}
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} &amp; = \begin{pmatrix}x_1'\beta + u_1 \\ x_2'\beta + u_2 \\ \vdots \\ x_n'\beta + u_n\end{pmatrix} \\
&amp; \\
&amp; = \begin{pmatrix}x_1'\beta \\ x_2'\beta \\ \vdots \\ x_n'\beta\end{pmatrix} + \begin{pmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{pmatrix}
\end{split}
</span></p>
<p>Since <span class="math inline">\beta</span> vector appears as a common factor for all observations <span class="math inline">i=1,...,n</span>, we can factor it out and have an equation:</p>
<p><span class="math display">
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} = \begin{pmatrix}x_1' \\ x_2' \\ \vdots \\ x_n'\end{pmatrix} \space \beta + \begin{pmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{pmatrix}
</span></p>
<p><br></p>
<p>We can expand the <span class="math inline">x_1',...,x_n'</span> vector into a matrix. Remember that each <span class="math inline">x_1',...,x_n'</span> is already a vector of different explanatory variables. So, we get the following result:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Multiple Linear Regression with Linear Algebra
</div>
</div>
<div class="callout-body-container callout-body">
<p>The multiple linear regression can be expressed in linear algebra as:</p>
<p><span class="math display">
y = X \beta + u, \text{ where } X = \begin{bmatrix}1 &amp; x_{21} &amp; \dots &amp; x_{k1} \\1 &amp; x_{22} &amp; \dots &amp; x_{k2} \\\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\1 &amp; x_{2n} &amp; \dots &amp; x_{kn}\end{bmatrix}
</span></p>
<ul>
<li><p>Where the notation for elements of <span class="math inline">X</span> is <span class="math inline">x_{ki}</span>, with <span class="math inline">i</span> being the unit of observation <span class="math inline">i = 1, \dots n</span>, and <span class="math inline">k</span> being the explanatory variables index.</p></li>
<li><p>Where <span class="math inline">y</span> and <span class="math inline">u</span> are <span class="math inline">n \times 1</span> vectors (as seen above), and <span class="math inline">\beta</span> is a <span class="math inline">k \times 1</span> vector.</p></li>
<li><p>The first row of <span class="math inline">X</span> is a vector of 1, which exists because these 1’s are multiplied with <span class="math inline">\beta_0</span> in our model.</p></li>
</ul>
</div>
</div>
<p>The point of expressing the model in linear algebra is that it makes the estimation process far easier, as we will see in the next section.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="ordinary-least-squares-estimator-for-multiple-regression" class="level1">
<h1>4.3: Ordinary Least Squares Estimator for Multiple Regression</h1>
<p>As we remember from Chapter 1, the goal of Ordinary Least Squares Estimation is to minimise the sum of squared errors. The sum of squared errors in multiple regression is:</p>
<p><span class="math display">
\begin{split}
SSE &amp; = \sum\limits_{i=1}^n (y_i - \hat y_i)^2\\
&amp; = \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i} -  ... - \hat\beta_kx_{ki})^2
\end{split}
</span></p>
<p>Similar to our simple linear regression (but with additional variables), our minimisation condition is:</p>
<p><span class="math display">
\begin{split}
(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...) &amp; = \arg \min\limits_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i} ...)^2 \\
&amp; = \arg \min\limits_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)} S(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)
\end{split}
</span></p>
<p><br></p>
<p>Taking the partial derivatives of each parameter like in simple linear regression, we get first order conditions:</p>
<p><span class="math display">
\begin{split}&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\&amp; \sum\limits_{i=1}^n X_{1i}(y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\
&amp; \sum\limits_{i=1}^n X_{2i} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0
\end{split}
</span></p>
<ul>
<li>and so on for <span class="math inline">x_{3i}, ..., x_{ki}</span>.</li>
</ul>
<p>This system of equations includes <span class="math inline">k+1</span> variables and <span class="math inline">k+1</span> equations, which is way too difficult to solve.</p>
<p><br></p>
<p>Instead, we can use linear algebra. Let us define our estimation vector <span class="math inline">\hat{\beta}</span> as the value of <span class="math inline">\hat\beta</span> that minimises the sum of squared errors:</p>
<p><span class="math display">
\hat{\beta} = \arg \min\limits_{b} (y - Xb)' (y - Xb) = \arg \min\limits_b S(b)
</span></p>
<p>We can expand <span class="math inline">S(b)</span> as follows:</p>
<p><span class="math display">
\begin{split}
S(b) &amp; = y'y - b'X'y - y'Xb + b'X'Xb \\
&amp; = y'y - 2b'X'y + b'X'Xb
\end{split}
</span></p>
<p>Taking the partial derivative in respect to <span class="math inline">b</span>:</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} = \begin{pmatrix}\frac{\partial S(b)}{\partial b_1} \\\vdots \\\frac{\partial S(b)}{\partial b_k}\end{pmatrix}
</span></p>
<p>Differentiating with the vector <span class="math inline">b</span> yields:</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} = -2X'y + 2X'Xb
</span></p>
<p>Evaluted at <span class="math inline">\hat{\beta}</span>, the derivatives should equal zero (since first order condition of finding minimums):</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} \biggr|_{\hat{\beta}} = -2X'y + 2X'X \hat{\beta} = 0
</span></p>
<p>When assuming <span class="math inline">X'X</span> is invertable, we can isolate <span class="math inline">\hat{\beta}</span> to find the solution to OLS:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: OLS Estimate of <span class="math inline">\hat\beta</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Ordinary Least Squares Estimate of vector <span class="math inline">\hat\beta</span> for multiple linear regression is:</p>
<p><span class="math display">
\hat{\beta} = (X'X)^{-1} X'y
</span></p>
</div>
</div>
<p>Once we have estimates of <span class="math inline">\hat{\beta}</span>, we can plug them into our linear model to obtain fitted values:</p>
<p><span class="math display">
\hat{y} = X\hat{\beta} = X(X'X)^{-1} X'y
</span></p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="regression-anatomy-theory-and-coefficient-interpretation" class="level1">
<h1>4.4: Regression Anatomy Theory and Coefficient Interpretation</h1>
<p>We talked about how multiple linear regression allows us to control for confounders. But what does that mean? How does it affect our interpretations of coefficients?</p>
<p>The Regression Anatomy Theory, also called the Frisch–Waugh–Lovell (FWL) theorem, illustrates this concept. Take our standard multivariate regression:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_k x_{ki} + u_i
</span></p>
<p><br></p>
<p>Let us say we are interested in <span class="math inline">x_1</span> (this can be generalised to any explanatory variable). Let us make <span class="math inline">x_1</span> the outcome variable of a regression with <span class="math inline">x_2, ..., x_k</span>:</p>
<p><span class="math display">
x_{1i} = \gamma_0 + \gamma_1 x_{2i} + ... + \gamma_{k-1}x_{ki} + \widetilde{x_{1i}}
</span></p>
<ul>
<li>Where <span class="math inline">\gamma_0, ..., \gamma_{k-1}</span> are coefficients.</li>
</ul>
<p>The error term is <span class="math inline">\widetilde{x_{1i}}</span>, which represents the part of <span class="math inline">x_{1i}</span> that are uncorrelated to <span class="math inline">x_2, ..., x_k</span>.</p>
<ul>
<li>In other words, <span class="math inline">\widetilde{x_{1i}}</span> is the part of <span class="math inline">x_1</span> that cannot be explained by any other explanatory variable. (uncorrelated with them)</li>
</ul>
<p><br></p>
<p>Now, take the regression of with outcome variable <span class="math inline">y</span>, with all explanatory variables except <span class="math inline">x_1</span>:</p>
<p><span class="math display">
y_i = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \widetilde{y_i}
</span></p>
<ul>
<li>Where <span class="math inline">\delta_0, ..., \delta_{k-1}</span> are coefficients.</li>
</ul>
<p>The error term is <span class="math inline">\widetilde{y_i}</span>, which is the part of <span class="math inline">y_i</span> that cannot be explained by <span class="math inline">x_2, ..., x_k</span> (uncorrelated with them).</p>
<p><br></p>
<p>Since <span class="math inline">\widetilde{y_i}</span> is not explained by <span class="math inline">x_2, ..., x_k</span>, variable <span class="math inline">x_1</span> must be the one explaining <span class="math inline">\widetilde{y_i}</span>.</p>
<ul>
<li>But, it is not the whole of <span class="math inline">x_1</span> explaining <span class="math inline">\tilde{y_i}</span> - since <span class="math inline">x_1</span> may also correlated with <span class="math inline">x_2, ..., x_k</span>, and the correlated parts of <span class="math inline">x_1</span> with <span class="math inline">x_2, ..., x_k</span> are already picked up in the regression by the coefficients of <span class="math inline">x_2, ..., x_k</span>.</li>
</ul>
<p>Thus, <span class="math inline">\widetilde{y_i}</span> must be explained by the part of <span class="math inline">x_1</span> that is uncorrelated and not explained by <span class="math inline">x_2, ..., x_k</span>, which we derived earlier as <span class="math inline">\widetilde{x_{1i}}</span>.</p>
<p><br></p>
<p>Thus, we can create another regression with explanatory variable <span class="math inline">\widetilde{x_{1i}}</span> and outcome variable <span class="math inline">\widetilde{y_i}</span>.</p>
<p><span class="math display">
\widetilde{y_i} = \alpha_0 + \alpha_1 \widetilde{x_{1i}} + u_i
</span></p>
<p>We can plug <span class="math inline">\widetilde{y_i}</span> back into our regression of <span class="math inline">y_i</span> with explanatory variables <span class="math inline">x_2 ..., x_k</span>:</p>
<p><span class="math display">
y_i = \delta_0 + \alpha_0 + \alpha_1 \widetilde{x_{1i}} + \delta_1x_{2i} + ... + \delta_{k-1} x_{ki} + u_i
</span></p>
<p>As we can see, this mirrors the original standard multiple linear regression. <u>The estimate of <span class="math inline">\alpha_1</span> will be the same as <span class="math inline">\beta_1</span> in the original regression</u>.</p>
<ul>
<li>The coefficient <span class="math inline">\alpha_1</span> (which is equal to <span class="math inline">\beta_1</span>) explains the expected change in <span class="math inline">y</span>, given an increase in the part of <span class="math inline">x_1</span> uncorrelated with <span class="math inline">x_2, ..., x_k</span>.</li>
<li>So essentially, we have <strong>partialed out</strong> the effect of the other explanatory variables, and only focus on the effect on <span class="math inline">y</span> of the uncorrelated part of <span class="math inline">x_1</span>.</li>
<li><u>This eliminates the effect of confounders on our estimates, and estimates of the effect of <span class="math inline">x_1</span> alone on <span class="math inline">y</span>.</u></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of Coefficients in Multiple Linear Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>Thus, the interpretation of <span class="math inline">\beta_1</span>, or any <span class="math inline">\beta_j</span> multiplied to <span class="math inline">x_j</span>, is:</p>
<ul>
<li>For every one unit increase in <span class="math inline">x_j</span>, there is an expected <span class="math inline">\beta_j</span> increase in <span class="math inline">y</span>, when controlling for all other explanatory variables.</li>
<li>Thus, we have “controlled for” and “partialled out” the effect of confounders on <span class="math inline">x_j</span>.</li>
</ul>
<p>Intecept <span class="math inline">\beta_0</span> is the expected value of <span class="math inline">y</span> when all <span class="math inline">\overrightarrow{x} = 0</span>.</p>
</div>
</div>
<p>Using the same procedure as in simple linear regression, we can run hypothesis tests on each coefficient for inference purposes (we will discuss causal inference with OLS in the next sections).</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="gauss-markov-and-unbiasedness-of-the-ols-estimator" class="level1">
<h1>4.5: Gauss-Markov and Unbiasedness of the OLS Estimator</h1>
<p>An unbiased estimator, if we recall from section 1.3, means that over many different estimates, the expected value of all the estimates is the true parameter value: <span class="math inline">\mathbb{E}[\hat{\theta}_i] = \theta</span>.</p>
<p>Unbiasedness is desirable property of causal estimators.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Gauss-Markov Theorem for Unbiasdness
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>Gauss-Markov Theorem</strong> states that the ordinary least squares estimator is <strong>unbiased</strong> under 3 conditions:</p>
<ol type="1">
<li><strong>Linearity</strong> of the equation (no coefficients <span class="math inline">\beta_0, ..., \beta_k</span> can be multiplied together. This does not apply to explanatory variables, only the coefficients.</li>
<li>In a simple linear regression, <span class="math inline">x</span> has <strong>variation</strong> (so not all values of <span class="math inline">x</span> are the same). In a multiple linear regression, this condition becomes that there is <strong>no perfect correlation (no perfect multicollinearity)</strong> between any two explanatory variables.</li>
<li><u>And the most important condition - <strong>Zero conditional mean</strong></u>: meaning that no matter the value of <span class="math inline">x</span>, the expected value of the residual <span class="math inline">u_i</span> is always 0. <span class="math inline">\mathbb{E}[u | x] = 0</span> for all <span class="math inline">x</span> (and all explanatory variables in multiple regression).</li>
</ol>
</div>
</div>
<p><br></p>
<section id="proof-of-the-unbiasedness-of-ols" class="level3">
<h3 class="anchored" data-anchor-id="proof-of-the-unbiasedness-of-ols">Proof of the Unbiasedness of OLS</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Properties of Summation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before we start, here are a few key properties of summation</p>
<p><span class="math display">
\begin{split}&amp; \sum\limits_{i=1}^n (x_i - \bar{x}) = 0 \\&amp; \sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y}) \\&amp; \sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2\end{split}
</span></p>
</div>
</div>
<p>We want to show <span class="math inline">\mathbb{E}[ \hat{\beta}_1] = \beta_1</span>. Let us start off with the OLS estimator (we will use simple linear regression for simplicity):</p>
<p><span class="math display">
\hat{\beta}_1 = \frac{\sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum\limits_{i=1}^n(x_i - \bar{x})^2}
</span></p>
<p>We can expand the numerator, and since <span class="math inline">\sum(x_i - \bar{x}) \bar{y} = \bar{y} \sum(x_i - \bar{x}) = 0</span> (see properties of summation above), we can get:</p>
<p><span class="math display">
\hat{\beta}_1 = \frac{\sum\limits_{i=1}^n (x_i - \bar{x})y_i}{\sum\limits_{i=1}^n(x_i - \bar{x})^2}
</span></p>
<ul>
<li>The existence of <span class="math inline">\hat{\beta}_1</span> is guaranteed by condition 2 from above, since the denominator is equal to <span class="math inline">Var(x)</span>, so that must not be 0.</li>
</ul>
<p><br></p>
<p>Now, let us play with the numerator (note the properties of summation introduced earlier):</p>
<p><span class="math display">
\begin{split}\sum\limits_{i=1}^n (x_i - \bar{x})y_i &amp; = \sum\limits_{i=1}^n(x_i - \bar{x})(\beta_0 + \beta_1 x_i + u_i) \\&amp; = \beta_0 \sum\limits_{i=1}^n(x_i - \bar{x}) + \beta_1 \sum\limits_{i=1}^n(x_i - \bar{x}) x_i + \sum\limits_{i=1}^n (x_i - \bar{x}) u_i \\&amp; = 0 + \beta_1 \sum\limits_{i=1}^n(x_i - \bar{x})^2 + \sum\limits_{i=1}^n(x_i - \bar{x})u_i\end{split}
</span></p>
<p>Now, putting the numerator back into the equation, we simplify:</p>
<p><span class="math display">
\begin{split}
\hat{\beta}_1 &amp; = \frac{\beta_1 \sum\limits_{i=1}^n(x_i - \bar{x})^2 + \sum\limits_{i=1}^n(x_i - \bar{x})u_i}{\sum\limits_{i=1}^n(x_i - \bar{x})^2} \\
&amp;  =  \beta_1 + \frac{\sum\limits_{i=1}^n(x_i - \bar{x}) u_i}{\sum\limits_{i=1}^n (x_i - \bar{x})^2} \\
&amp;  = \beta_1 + \sum\limits_{i=1}^n d_i u_i
\end{split}
</span></p>
<ul>
<li>Where <span class="math inline">d_i = \frac{x_i - \bar{x}}{\sum (x_i - \bar{x})^2}</span>, which is a function of random variable <span class="math inline">x</span>.</li>
</ul>
<p><br></p>
<p>Now we need to find the expectation <span class="math inline">\mathbb{E}[\hat{\beta}_1]</span>.</p>
<p>First, do not worry about <span class="math inline">d_i</span>. What should <span class="math inline">u_i</span> be equal to? Naturally, the best estimate of <span class="math inline">u_i</span> is its expected utility.</p>
<p><span class="math display">
\mathbb{E} (\hat{\beta}_1 | x) = \beta_1 + \sum\limits_{i=1}^n d_i \ \mathbb{E}(u_i | x)
</span></p>
<p>We know by the third Gauss-Markov condition (Zero conditional mean), that <span class="math inline">\mathbb{E}[u_i | x_i] = 0</span>. Thus, that makes the entire summation equal to 0:</p>
<p><span class="math display">
\mathbb{E}(\hat{\beta}_1 | x) = \mathbb{E}(\hat{\beta}_1) = \beta_1
</span></p>
<p>Thus, the ordinary least squares estimator is unbiased, given 3 conditions (linearity, variation in <span class="math inline">x</span>, zero-conditional mean) are met.</p>
<ul>
<li><u>If conditions are met, we have an unbiased estimator in OLS for causal estimation</u>.</li>
<li><u>The key issue is assumption 3: Zero conditional mean <span class="math inline">\mathbb{E}[u | x] = 0</span></u>, which is the most easily violated of these assumptions. We will explore this in detail in section 4.7.</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
</section>
<section id="causal-inference-and-consistency-of-ols-under-gauss-markov" class="level1">
<h1>4.6: Causal Inference and Consistency of OLS under Gauss-Markov</h1>
<section id="causal-inference-in-multiple-linear-regression" class="level3">
<h3 class="anchored" data-anchor-id="causal-inference-in-multiple-linear-regression">Causal Inference in Multiple Linear Regression</h3>
<p>We can run hypothesis testing and confidence intervals in almost the same way as simple linear regression, as explained in <a href="https://politicalscience.github.io/metrics/causal/1.html#hypothesis-testing-and-statistical-inference">section 1.7</a> (confidence intervals in <a href="https://politicalscience.github.io/metrics/causal/3.html#causal-inference-hypothesis-tests-and-confidence-intervals">section 3.5</a>).</p>
<p>The only difference is the standard error:</p>
<p><span class="math display">
se(\hat\beta_j) = \sqrt{\frac{Var(\hat{u_i})}{n \times Var(\widetilde{x_{ji}})}}
</span></p>
<ul>
<li>Where <span class="math inline">\widetilde{x_{ji}}</span> are the residuals from a regression of outcome variable <span class="math inline">x_j</span> on all other explanatory variables (as shown during section 4.4 on regression anatomy).</li>
</ul>
<p><br></p>
</section>
<section id="gauss-markov-and-the-consistency-of-ols" class="level3">
<h3 class="anchored" data-anchor-id="gauss-markov-and-the-consistency-of-ols">Gauss-Markov and the Consistency of OLS</h3>
<p>We talked about unbiasedness of OLS under certain conditions. But as you probably remember, unbiasedness is not the only property of estimators that is important. Another is consistency.</p>
<ul>
<li>Consistency is important, since low variance in estimates means smaller standard errors, which means higher precision and power in hypothesis testing.</li>
</ul>
<p>We previously introduced the Gauss-Markov Theorem, which said that estimates of OLS are unbiased given three assumptions are met.</p>
<ul>
<li>However, there is an additional assumption, that when added to the previous assumptions, makes OLS the best linear unbiased estimator in terms of <strong>consistency</strong> (lowest variation in estimates):</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Gauss-Markov Theorem for Consistency
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Gauss-Markov Theorem states that under 4 conditions, OLS is not only unbiased, but also the <strong>most consistent linear estimator</strong>. Thus, OLS is deemed BLUE (best linear unbiased estimator):</p>
<ol type="1">
<li>Linearity of Parameters.</li>
<li>Variation in <span class="math inline">x</span> (for simple regression) and non-perfect multicollinearity (for multiple regression).</li>
<li>Zero Conditional Mean</li>
<li>Homoscedasticity: variance of errors constant throughout the model: <span class="math inline">Var(u|x) = \sigma^2</span>.</li>
</ol>
</div>
</div>
<p><br></p>
<p>The additional assumption <strong>homoscedasticity</strong> is that the variance of the error term <span class="math inline">u_i</span> is constant, no matter the value of <span class="math inline">x</span>.</p>
<p>To evaluate this assumption, look at the plot of residuals/errors.</p>
<ul>
<li>If errors show a pattern as <span class="math inline">x</span> changes (such as becoming larger or smaller), then homoscedasticity is violated, and we have <strong>heteroscedasticity</strong>.</li>
</ul>
<p>For example, see the figure below. The heteroscedasticity plot has a clear inconsistency in the variation of the residuals (small variance with lower <span class="math inline">x</span>, larger variance with higher <span class="math inline">x</span>).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:75.0%"></p>
</figure>
</div>
<p>When we do not meet the assumption of homoscedasticity, we have to use heteroscedasticity-<strong>robust</strong> standard errors.</p>
<ul>
<li>These account for our estimates less consistent, making the standard errors larger, thus harder to reject the null hypothesis).</li>
<li>These days in econometrics, we assume that homoscedasticity is violated, and almost always use robust standard errors by default (which we have been doing so far).</li>
</ul>
<p>Note: not meeting homoscedasticity <strong>does not bias</strong> OLS estimates. It only makes them less consistent.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
</section>
<section id="omitted-variable-bias-and-endogeneity" class="level1">
<h1>4.7: Omitted Variable Bias and Endogeneity</h1>
<p>In the last section, we discussed how the assumption of <strong>zero-conditional mean</strong> <span class="math inline">\mathbb{E}[u | x] = 0</span> is critical to the unbiasdness of OLS.</p>
<ul>
<li>This assumption is also called <strong>exogeneity</strong>.</li>
</ul>
<p>However, this assumption is frequently violated. The most common reason for this is because of <strong>omitted variable bias</strong>.</p>
<p><br></p>
<p>Consider two regressions, one with only our treatment variable of interest <span class="math inline">D</span>, and, and one with an extra control variable <span class="math inline">x</span> that is omitted from the first regression:</p>
<p><span class="math display">
\begin{split}y_i &amp; = \beta_0^S + \beta_1^SD_i + u_i^S \quad \text{short} \\y_i &amp; = \beta_0 + \beta_1D_i + \beta_2x_i + u_i \quad \text{long}\end{split}
</span></p>
<p>Now consider an auxiliary regression, where the omitted variable <span class="math inline">X</span> is the outcome variable, and <span class="math inline">D_i</span> is the explanatory variable:</p>
<p><span class="math display">
x_i = \delta_0 + \delta_1 D_i + v_i
</span></p>
<ul>
<li>where <span class="math inline">\delta_0, \delta_1</span> are coefficients and <span class="math inline">v_i</span> is the error term</li>
</ul>
<p><br></p>
<p>Now we have <span class="math inline">x_i</span> in terms of <span class="math inline">D_i</span>, let us plug <span class="math inline">x_i</span> into our long regression to “recreate” the short regression:</p>
<p><span class="math display">
\begin{split}y_i &amp; = \beta_0 + \beta_1D_i + \beta_2x_i + u_i \\y_i &amp; = \beta_0 + \beta_1 D_i + \beta_2(\delta_0 + \delta_1D_i + v_i) + u_i \\y_i &amp; = \beta_0 + \beta_1 D_i + \beta_2 \delta_0 + \beta_2 \delta_1 D_i + \beta_2v_i + u_i \\y_i &amp; = \beta_0 + \beta_2 \delta_0 + (\beta_1 + \beta_2 \delta_1)D_i + \beta_2v_i + u_i\end{split}
</span></p>
<p>We have “recreated” the short regression with one variable <span class="math inline">D</span>.</p>
<ul>
<li>That means coefficient <span class="math inline">\beta_1^S = \beta_1 + \beta_2 \delta_1</span>.</li>
</ul>
<p>The difference between the short regression coefficient <span class="math inline">\beta_1^S</span>, and the original long regression coefficient <span class="math inline">\beta_1</span>, is <span class="math inline">\beta_2 \delta_1</span>.</p>
<ul>
<li>Or in other words, <span class="math inline">\beta_2 \delta_1</span>, which is actually the effect of omitted <span class="math inline">x</span> on <span class="math inline">y</span>, is being “tangled up” into the coefficient of the short regression <span class="math inline">\beta_1^S</span>.</li>
</ul>
<p>If <span class="math inline">\beta_2 = 0</span> (meaning no relationship between omitted <span class="math inline">x_i</span> and <span class="math inline">y</span>), or <span class="math inline">\delta_1 = 0</span> (meaning no relationship between omitted <span class="math inline">x_i</span> and <span class="math inline">D_i</span>), then difference <span class="math inline">\beta_2 \delta_1 = 0</span>, thus there is no issue.</p>
<ul>
<li>But if either of those facts are not true, then <span class="math inline">\beta_2 \delta_1 ≠ 0</span>.</li>
</ul>
<p><br></p>
<p>Why is this an issue?</p>
<ul>
<li>The omitted variable <span class="math inline">x</span>’s effect is mostly subsumed into the error term <span class="math inline">u_i^S</span> of the short regression.</li>
<li>But some bit of <span class="math inline">x</span> (that is correlated with <span class="math inline">D</span>) is included in our coefficient (specifically, <span class="math inline">\beta_2 \delta_1</span>).</li>
<li>That means our treatment variable <span class="math inline">D</span> will be correlated with the error term, <u>violating the Gauss-Markov assumption of zero-conditional mean, and producing biased OLS results</u>.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Endogeneity
</div>
</div>
<div class="callout-body-container callout-body">
<p>Endogeneity is when a regressor <span class="math inline">x</span> is correlated with the error term <span class="math inline">u_i</span>.</p>
<ul>
<li><p>This frequently occurs due to <strong>omitted variable bias</strong>.</p></li>
<li><p>The explanatory variable that is correlated with the error term is called an <strong>endogenous variable</strong>.</p></li>
</ul>
<p>When endogeneity exists, the assumption of zero-conditional mean/exogeneity <span class="math inline">\mathbb{E}[u | x] = 0</span> is violated.</p>
<ul>
<li><p>This means that OLS estimates are no longer unbiased.</p></li>
<li><p><u>That means, when endoeneity is present, accurate causal estimation with OLS is not possible.</u></p></li>
</ul>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="validity-of-using-ols-for-causal-estimation" class="level1">
<h1>4.8: Validity of Using OLS for Causal Estimation</h1>
<p>In the last section, we established that omitted variable bias will often cause endogeneity, thus causing OLS to be biased. How do we prevent this from happening?</p>
<p>Recall, that omitted variable bias <span class="math inline">\beta_2 \delta_1</span> is only an issue when <span class="math inline">\beta_2 \delta_1 ≠ 0</span> (since 0 means no bias!). So when does this occur? First, let us figure out when there is no omitted variable bias:</p>
<ul>
<li>When <span class="math inline">\beta_2 = 0</span> (meaning no relationship between omitted <span class="math inline">x</span> and <span class="math inline">y</span>), then <span class="math inline">\beta_2 \delta_1 = 0</span>.</li>
<li>When <span class="math inline">\delta_1 = 0</span> (meaning no relationship between omitted <span class="math inline">x</span> and <span class="math inline">D_i</span>), then <span class="math inline">\beta_2 \delta_1 = 0</span>.</li>
</ul>
<p>Thus, omitted variable bias is only a problem when their is <strong>both</strong> a correlation between omitted <span class="math inline">x</span> and <span class="math inline">y</span>, and a correlation between omitted <span class="math inline">x</span> and treatment <span class="math inline">D</span>.</p>
<ul>
<li>This is the definition of a confounder variable as well! One that affects selection into treatment <span class="math inline">w \rightarrow D</span> and has a backdoor path to outcome <span class="math inline">w \rightarrow y</span>.</li>
</ul>
<p><u>So, omitted variable bias only occurs when we fail to control for confounders!</u></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Causal Interpretation of Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p><u>For a causal interpretation of regression and OLS estimates, we must be confident that all confounders are controlled for.</u></p>
<ul>
<li>Often times, this is impossible, since many causal effects have unobservable or impossible to measure confounders.</li>
<li>Thus, to deal with omitted variables and endogeneity when controlling for all confounders is not possible, we will need further techniques (such as instrumental variables).</li>
</ul>
<p><u>When this is met, we can interpret <span class="math inline">\beta_j</span> of variable <span class="math inline">x_j</span> as the average causal effect of increasing <span class="math inline">x_j</span> by one unit on <span class="math inline">y</span>.</u></p>
<ul>
<li>And we can conduct hypothesis tests to confirm our causal inference.</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warnings!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Everything described above assumes the following:</p>
<p>We do not have any measurement issues with our variables (everything is accurately measured).</p>
<ul>
<li>If we have measurement error, endogeneity may also be introduced, and we will be unable to do causal inference.</li>
<li>We will discuss some ways to deal with measurement problems in part 3 of the econometrics course.</li>
</ul>
<p>Reverse causality can be ruled out. Multiple regression does not eliminate possible causal effect <span class="math inline">y \rightarrow x</span>.</p>
<ul>
<li>We can eliminiate reverse causality with either a strong causal theory that explains why reverse causality is not possible, or with further methods like quasi-experimental methods and instrumental variables.</li>
</ul>
</div>
</div>
<p><br></p>
<p>Another important thing is that non-confounders (variables that do not correlate with both <span class="math inline">D</span> and <span class="math inline">y</span> ) do not introduce omitted variable bias or endogeneity.</p>
<ul>
<li>However, including non-confounders as controls increases our standard error, which decreases the precision of our estimates (and makes it harder to reject the null hypothesis).</li>
<li>Thus, we should avoid including non-confounders in regression models, as they only make our estimates less consistent, without any benefit.</li>
</ul>
<p>Model selection (selecting what variables to include in our regression) is one of the most difficult things in causal inference.</p>
<ul>
<li>We must be confident in selecting confounding variables as controls, without selecting non-confounding variables.</li>
<li>This is where domain knowledge - expertise of the field you are doing research in - becomes so important.</li>
<li>Only through domain knowledge, literature reviews, and intuition, will you be able to create good causal regression models.</li>
</ul>
<p><br></p>
<p>The remainder of this part will introduce variations on the regression models we have seen, in order to deal with some of the limitations of causal inference with multiple linear regression.</p>
<ul>
<li><p>Extensions of linear regression models, including transformations, moderating effects, and fixed effects, can solve some small issues with linear regression.</p></li>
<li><p>The important <strong>instrumental variables estimator</strong> deals with the issue of endogeneity biasing OLS estimates, and reverse causality.</p></li>
<li><p>We will also explore quasi-experimental methods which can solve some issues relating to endogeneity and reverse causality, especially when instrumental variables is too difficult to implement.</p></li>
<li><p>Finally, we have some less common methods in casual inference these-days, including alternate regression models and selection on observables methods.</p></li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="implementation-in-r" class="level1">
<h1>Implementation in R</h1>
<section id="multiple-linear-regression-in-r" class="level2">
<h2 class="anchored" data-anchor-id="multiple-linear-regression-in-r">Multiple Linear Regression in R</h2>
</section>
<section id="confidence-intervals" class="level2">
<h2 class="anchored" data-anchor-id="confidence-intervals">Confidence Intervals</h2>
</section>
<section id="creating-regression-tables" class="level2">
<h2 class="anchored" data-anchor-id="creating-regression-tables">Creating Regression Tables</h2>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>