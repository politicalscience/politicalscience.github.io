---
title: "Chapter 4: Causal Inference through Multiple Linear Regression"
subtitle: "Econometric Methods (for Social Scientists)"
format:
    html:
        page-layout: article
        grid:
          sidebar-width: 300px
          body-width: 750px
          margin-width: 250px
          gutter-width: 2em
        fontsize: 12.5pt
        theme: yeti
        toc: TRUE
        toc-depth: 2
        toc-location: left
        toc-expand: true
        toc-title: "Chapter 4: Causal Inference through Multiple Linear Regression"
        mainfont: "Computer Modern"
        title-block-banner: true
        classoption: fleqn
        html-math-method: katex
---

This chapter introduces multiple linear regression, which allows us to control for confounding variables. We will explore the multiple regression model, and what it means to control for a variable. Then, we will focus heavily on the Ordinary Least Squares (OLS) estimator, why it is a good estimator, and issues when it comes to causal inference.

Topics: Multiple Linear Regression, OLS with Linear Algebra, Regression Anatomy, Gauss-Markov, Unbiasedness and Consistnecy of OLS, Omitted Variable Bias and Endogeneity, Hypothesis Testing and F-tests of Nested Models, Interaction Effects and Conditional Average Treatment Effect.

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 4.4: Regression Anatomy Theory and Coefficient Interpretation

We talked about how multiple linear regression allows us to control for confounders. But what does that mean? How does it affect our interpretations of coefficients?

The Regression Anatomy Theory, also called the Frisch–Waugh–Lovell (FWL) theorem, illustrates this concept. Take our standard multiple linear regression:

$$
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_k x_{ki} + u_i
$$

<br />

Let us say we are interested in $x_1$ (this can be generalised to any explanatory variable). Let us make $x_1$ the outcome variable of a regression with $x_2, ..., x_k$:

$$
x_{1i} = \gamma_0 + \gamma_1 x_{2i} + ... + \gamma_{k-1}x_{ki} + \widetilde{x_{1i}}
$$

-   Where $\gamma_0, ..., \gamma_{k-1}$ are coefficients.

The error term is $\widetilde{x_{1i}}$, which represents the part of $x_{1i}$ that are uncorrelated to $x_2, ..., x_k$.

-   In other words, $\widetilde{x_{1i}}$ is the part of $x_1$ that cannot be explained by any other explanatory variable. (uncorrelated with them)

<br />

Now, take the regression of with outcome variable $y$, with all explanatory variables except $x_1$:

$$
y_i = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \widetilde{y_i}
$$

-   Where $\delta_0, ..., \delta_{k-1}$ are coefficients.

The error term is $\widetilde{y_i}$, which is the part of $y_i$ that cannot be explained by $x_2, ..., x_k$ (uncorrelated with them).

<br />

Since $\widetilde{y_i}$ is not explained by $x_2, ..., x_k$, variable $x_1$ must be the one explaining $\widetilde{y_i}$.

-   But, it is not the whole of $x_1$ explaining $\tilde{y_i}$ - since $x_1$ may also correlated with $x_2, ..., x_k$, and the correlated parts of $x_1$ with $x_2, ..., x_k$ are already picked up in the regression by the coefficients of $x_2, ..., x_k$.

Thus, $\widetilde{y_i}$ must be explained by the part of $x_1$ that is uncorrelated and not explained by $x_2, ..., x_k$, which we derived earlier as $\widetilde{x_{1i}}$.

<br />

Thus, we can create another regression with explanatory variable $\widetilde{x_{1i}}$ and outcome variable $\widetilde{y_i}$.

$$
\widetilde{y_i} = \alpha_0 + \alpha_1 \widetilde{x_{1i}} + u_i
$$

We can plug $\widetilde{y_i}$ back into our regression of $y_i$ with explanatory variables $x_2 ..., x_k$:

$$
y_i = \delta_0 + \alpha_0 + \alpha_1 \widetilde{x_{1i}} + \delta_1x_{2i} + ... + \delta_{k-1} x_{ki} + u_i
$$

As we can see, this mirrors the original standard multiple linear regression. [The estimate of $\alpha_1$ will be the same as $\beta_1$ in the original regression]{.underline}.

-   The coefficient $\alpha_1$ (which is equal to $\beta_1$) explains the expected change in $y$, given an increase in the part of $x_1$ uncorrelated with $x_2, ..., x_k$.
-   So essentially, we have **partialed out** the effect of the other explanatory variables, and only focus on the effect on $y$ of the uncorrelated part of $x_1$.
-   [This eliminates the effect of confounders on our estimates, and estimates of the effect of $x_1$ alone on $y$.]{.underline}

::: callout-tip
## Interpretation of Coefficients in Multiple Linear Regression

Thus, the interpretation of $\beta_1$, or any $\beta_j$ multiplied to $x_j$, is:

-   For every one unit increase in $x_j$, there is an expected $\beta_j$ increase in $y$, when controlling for all other explanatory variables.
-   Thus, we have "controlled for" and "partialled out" the effect of confounders on $x_j$.

For binary $x_j$ variables (such as treatment $D$), $\beta_j$ is the expected difference in $y$ between categories $x=1$ and $x=0$, when controlling for all other explanatory variables.

 

Note: the $\beta_j$ interpretation is [not the same]{.underline} for the linear probability model, polytomous explanatory variables, polynomial transformations, and log transformations.

-   For the interpretation of those, refer back to the interpretation of [chapter 1](https://politicalscience.github.io/metrics/causal/1.html), and add the phrase *"when controlling for all other explanatory variables"*.

Intercept $\beta_0$ is the expected value of $y$ when all $\overrightarrow{x} = 0$.
:::

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 4.5: Gauss-Markov and Unbiasedness of the OLS Estimator

An unbiased estimator, if we recall from [section 2.5](https://politicalscience.github.io/metrics/causal/2.html#properties-of-estimators---unbiasedness-and-consistency), means that over many different estimates, the expected value of all the estimates is the true parameter value: $\mathbb{E}[\hat{\theta}_i] = \theta$.

Unbiasedness is desirable property of causal estimators.

::: callout-tip
## Definition: Gauss-Markov Theorem for Unbiasdness

The **Gauss-Markov Theorem** states that the ordinary least squares estimator is **unbiased** under 3 conditions:

1.  **Linearity** of the equation (no coefficients $\beta_0, ..., \beta_k$ can be multiplied together. This does not apply to explanatory variables, only the coefficients.
2.  In a simple linear regression, $x$ has **variation** (so not all values of $x$ are the same). In a multiple linear regression, this condition becomes that there is **no perfect correlation (no perfect multicollinearity)** between any two explanatory variables.
3.  [And the most important condition - **Zero conditional mean**]{.underline}: meaning that no matter the value of $x$, the expected value of the residual $u_i$ is always 0. $\mathbb{E}[u | x] = 0$ for all $x$ (and all explanatory variables in multiple regression).
:::

<br />

### Proof of the Unbiasedness of OLS

::: callout-note
## Properties of Summation

Before we start, here are a few key properties of summation

$$
\begin{split}& \sum\limits_{i=1}^n (x_i - \bar{x}) = 0 \\& \sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y}) \\& \sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2\end{split}
$$
:::

We want to show $\mathbb{E}[ \hat{\beta}_1] = \beta_1$. Let us start off with the OLS estimator (we will use simple linear regression for simplicity):

$$
\hat{\beta}_1 = \frac{\sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum\limits_{i=1}^n(x_i - \bar{x})^2}
$$

We can expand the numerator, and since $\sum(x_i - \bar{x}) \bar{y} = \bar{y} \sum(x_i - \bar{x}) = 0$ (see properties of summation above), we can get:

$$
\hat{\beta}_1 = \frac{\sum\limits_{i=1}^n (x_i - \bar{x})y_i}{\sum\limits_{i=1}^n(x_i - \bar{x})^2}
$$

-   The existence of $\hat{\beta}_1$ is guaranteed by condition 2 from above, since the denominator is equal to $Var(x)$, so that must not be 0.

<br />

Now, let us play with the numerator (note the properties of summation introduced earlier):

$$
\begin{split}\sum\limits_{i=1}^n (x_i - \bar{x})y_i & = \sum\limits_{i=1}^n(x_i - \bar{x})(\beta_0 + \beta_1 x_i + u_i) \\& = \beta_0 \sum\limits_{i=1}^n(x_i - \bar{x}) + \beta_1 \sum\limits_{i=1}^n(x_i - \bar{x}) x_i + \sum\limits_{i=1}^n (x_i - \bar{x}) u_i \\& = 0 + \beta_1 \sum\limits_{i=1}^n(x_i - \bar{x})^2 + \sum\limits_{i=1}^n(x_i - \bar{x})u_i\end{split}
$$

Now, putting the numerator back into the equation, we simplify:

$$
\begin{split}
\hat{\beta}_1 & = \frac{\beta_1 \sum\limits_{i=1}^n(x_i - \bar{x})^2 + \sum\limits_{i=1}^n(x_i - \bar{x})u_i}{\sum\limits_{i=1}^n(x_i - \bar{x})^2} \\
&  =  \beta_1 + \frac{\sum\limits_{i=1}^n(x_i - \bar{x}) u_i}{\sum\limits_{i=1}^n (x_i - \bar{x})^2} \\
&  = \beta_1 + \sum\limits_{i=1}^n d_i u_i
\end{split}
$$

-   Where $d_i = \frac{x_i - \bar{x}}{\sum (x_i - \bar{x})^2}$, which is a function of random variable $x$.

<br />

Now we need to find the expectation $\mathbb{E}[\hat{\beta}_1]$.

First, do not worry about $d_i$. What should $u_i$ be equal to? Naturally, the best estimate of $u_i$ is its expected utility.

$$
\mathbb{E} (\hat{\beta}_1 | x) = \beta_1 + \sum\limits_{i=1}^n d_i \ \mathbb{E}(u_i | x)
$$

We know by the third Gauss-Markov condition (Zero conditional mean), that $\mathbb{E}[u_i | x_i] = 0$. Thus, that makes the entire summation equal to 0:

$$
\mathbb{E}(\hat{\beta}_1 | x) = \mathbb{E}(\hat{\beta}_1) = \beta_1
$$

Thus, the ordinary least squares estimator is unbiased, given 3 conditions (linearity, variation in $x$, zero-conditional mean) are met.

-   If conditions are met, we have an unbiased estimator in OLS for causal estimation.
-   [The key issue is assumption 3: Zero conditional mean $\mathbb{E}[u | x] = 0$]{.underline}, which is the most easily violated of these assumptions. We will explore this in detail in the next section.

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 4.6: Omitted Variable Bias and Endogeneity

### Omitted Variable Bias

In the previous section, we discussed how the assumption of **zero-conditional mean** $\mathbb{E}[u | x] = 0$ is critical to the unbiasdness of OLS.

-   This assumption is also called **exogeneity**.

However, this assumption is frequently violated. The most common reason for this is because of **omitted variable bias**.

<br />

Consider two regressions, one with only our treatment variable of interest $D$, and, and one with an extra control variable $x$ that is omitted from the first regression:

$$
\begin{split}y_i & = \beta_0^S + \beta_1^SD_i + u_i^S \quad \text{short} \\y_i & = \beta_0 + \beta_1D_i + \beta_2x_i + u_i \quad \text{long}\end{split}
$$

Now consider an auxiliary regression, where the omitted variable $x$ is the outcome variable, and $D_i$ is the explanatory variable:

$$
x_i = \delta_0 + \delta_1 D_i + v_i
$$

-   where $\delta_0, \delta_1$ are coefficients and $v_i$ is the error term

<br />

Now we have $x_i$ in terms of $D_i$, let us plug $x_i$ into our long regression to "recreate" the short regression:

$$
\begin{split}y_i & = \beta_0 + \beta_1D_i + \beta_2x_i + u_i \\y_i & = \beta_0 + \beta_1 D_i + \beta_2(\delta_0 + \delta_1D_i + v_i) + u_i \\y_i & = \beta_0 + \beta_1 D_i + \beta_2 \delta_0 + \beta_2 \delta_1 D_i + \beta_2v_i + u_i \\y_i & = \beta_0 + \beta_2 \delta_0 + (\beta_1 + \beta_2 \delta_1)D_i + \beta_2v_i + u_i\end{split}
$$

We have "recreated" the short regression with one variable $D$. We can see in this recreation, the coefficient of $D_i$ is $\beta_1 + \beta_2 \delta_1$.

-   Since this is a recreation of the "short" regression, that means coefficient $\beta_1^S = \beta_1 + \beta_2 \delta_1$.

The difference between the short regression coefficient $\beta_1^S$, and the original long regression coefficient $\beta_1$, is $\beta_2 \delta_1$.

-   Or in other words, $\beta_2 \delta_1$, which is actually some of the effect of omitted $x$ on $y$, is being "tangled up" into the coefficient of the short regression $\beta_1^S$.

If $\beta_2 = 0$ (meaning no relationship between omitted $x_i$ and $y$), or $\delta_1 = 0$ (meaning no relationship between omitted $x_i$ and $D_i$), then difference $\beta_2 \delta_1 = 0$, thus there is no issue.

-   But if either of those facts are not true, then $\beta_2 \delta_1 ≠ 0$.

<br />

### Endogeneity and Violation of Gauss-Markov Conditions

Why is omitted variable bias (if non-zero) an issue?

-   The omitted variable $x$'s effect is mostly subsumed into the error term $u_i^S$ of the short regression.
-   But some bit of $x$ (that is correlated with $D$) is included in our coefficient (specifically, $\beta_2 \delta_1$).
-   That means our treatment variable $D$ will be correlated with the error term, [violating the Gauss-Markov assumption of zero-conditional mean, and producing biased OLS results]{.underline}.

::: callout-tip
## Definition: Endogeneity

Endogeneity is when a regressor $x$ is correlated with the error term $u_i$.

-   This frequently occurs due to **omitted variable bias**.
-   The explanatory variable that is correlated with the error term is called an **endogenous variable**.

When endogeneity exists, the assumption of zero-conditional mean/exogeneity $\mathbb{E}[u | x] = 0$ is violated.

-   This means that OLS estimates are no longer unbiased.
-   [That means, when endoeneity is present, accurate causal estimation with OLS is not possible.]{.underline}
:::

<br />

Endogeneity can also be caused by other factors, including measurement and reverse causality:

-   If we have measurement error, endogeneity may also be introduced, and we will be unable to do causal inference. We will discuss some ways to deal with measurement problems in part 3 of the econometrics course.

-   We can eliminiate reverse causality with either a convincing causal "story" that explains why reverse causality is not possible, or with further methods like quasi-experimental methods and instrumental variables.

<br />

<br />

------------------------------------------------------------------------

# 4.7: Model Selection for Causal Estimation

Recall, that omitted variable bias $\beta_2 \delta_1$ is only an issue when $\beta_2 \delta_1 ≠ 0$ (since 0 means no bias!). So when does this occur? First, let us figure out when there is no omitted variable bias:

-   When $\beta_2 = 0$ (meaning no relationship between omitted $x$ and $y$), then $\beta_2 \delta_1 = 0$.
-   When $\delta_1 = 0$ (meaning no relationship between omitted $x$ and $D_i$), then $\beta_2 \delta_1 = 0$.

Thus, omitted variable bias is only a problem when their is **both** a correlation between omitted $x$ and $y$, and a correlation between omitted $x$ and treatment $D$.

-   This is the definition of a confounder variable as well! One that affects selection into treatment $w \rightarrow D$ and has a backdoor path to outcome $w \rightarrow y$.

[So, omitted variable bias only occurs when we fail to control for confounders!]{.underline}

<br />

::: callout-tip
## Causal Interpretation of Regression

[For a causal interpretation of regression and OLS estimates, we must be confident that all confounders are controlled for.]{.underline}

-   Often times, this is impossible, since many causal effects have unobservable or impossible to measure confounders.
-   Thus, to deal with omitted variables and endogeneity when controlling for all confounders is not possible, we will need further techniques (such as instrumental variables).

We must also be confident that reverse causality can be ruled out, and measurement error is not an issue.

 

[When this is met, we can interpret $\beta_j$ of variable $x_j$ as the average causal effect of increasing $x_j$ by one unit on $y$.]{.underline}

-   And then we can conduct hypothesis tests to confirm our causal inference.
:::

<br />

Another important thing is that non-confounders (variables that do not correlate with both $D$ and $y$ ) do not introduce omitted variable bias or endogeneity.

-   However, including non-confounders as controls increases our standard error, which decreases the precision of our estimates (and makes it harder to reject the null hypothesis, we will discuss this in the next section).
-   Thus, we should avoid including non-confounders in regression models, as they only make our estimates less consistent, without any benefit.

Model selection is one of the most difficult parts of causal inference.

-   Good model selection is not only about econometric and statistical knowledge, but also about knowledge of the content in your discipline.
-   Only through extensive reading and understanding of the literature, will you be able to make good choices in model selection.

<br />

This chapter has focused on a lot of the limitations on OLS and multiple linear regression for causal estimation.

-   Well then, if we cannot use OLS, what should we use?
-   The rest of this part of the course on causal estimation will introduce alternative methods to deal with issues regarding OLS estimation.

However, **before we move away from linear regression**, we still have two important concepts to cover!

1.  Causal Inference - remember, our estimates are just sample estimates, and in order to test if they truly are causal, we need to conduct hypothesis testing.
2.  Interaction and conditional average treatment effects, which will be covered in the final section of the chapter.

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 4.8: Causal Inference and Hypothesis Testing with Multiple Linear Regression

### Homoscedasticity and Standard Errors

Homoscedasticity is the property that the variance of errors are constant throughout the model:

$$
Var(u|x) = \sigma^2
$$

When this is violated, we say our model has **heteroscedasticity**.

That is a little hard to understand. Let us intuitively explore this with a graph. Below are plots of residuals (errors) of the regression - essentially, we take the best fit line, and move it horizontal:

![](figures/4-1.png){fig-align="center" width="75%"}

The heteroscedasticity plot has a clear inconsistency in the variation of the residuals (small variance with lower $x$, larger variance with higher $x$).

<br />

Homoscedasticity is actually one of the Gauss-Markov conditions.

-   We previously only discussed the Gauss-Markov Theorem in relation to unbiasedness.
-   However, Gauss-Markov also states that with the unbiasdeness conditions + homoscedasticity, the variance of the OLS estimator is the lowest of all linear unbiased estimators. Thus, OLS is the **best linear unbiased estimator (BLUE)**.

However, in reality, we don't really care about this. OLS is still a estimator with respectable variance (given a large enough sample size) even if this assumption is violated, it just is no longer "the best".

<br />

There is a more practical reason we care about homoscedasticity: When we do not meet the assumption, we have to use heteroscedasticity-**robust** standard errors.

-   These account for our estimates less consistent, making the standard errors larger, thus harder to reject the null hypothesis).
-   These days in econometrics, we assume that homoscedasticity is violated, and almost always use robust standard errors by default.

<br />

### Causal Inference in Multiple Linear Regression

We can run hypothesis testing and confidence intervals in almost the same way as in randomised experiments and single linear regression, as explained in [section 3.4](https://politicalscience.github.io/metrics/causal/3.html#uncertainty-in-causal-estimates-and-standard-errors) and [section 3.5](https://politicalscience.github.io/metrics/causal/3.html#causal-inference-and-hypothesis-testing).

The only two differences are the standard error and degrees of freedom.

The robust standard error is as follows:

$$
se(\hat\beta_j) = \sqrt{\frac{Var(\hat{u_i})}{n \times Var(\widetilde{x_{ji}})}}
$$

-   Where $\widetilde{x_{ji}}$ are the residuals from a regression of outcome variable $x_j$ on all other explanatory variables (as shown during section 4.4 on regression anatomy).

<br />

With the standard error, we can calculate the t-statistic, and reference the t-distribution.

-   However, we have to adjust the degrees of freedom parameter to $n-k-1$
-   Where $n$ is the number of observations and $k$ is the number of explanatory variables.
-   NOTE: this is different than the degrees of freedom with difference-in-means.

Now, we can calculate the p-values:

::: callout-tip
## Interpretation of P-Values

If the p-value of $\beta_j$ is above 0.05, there is a above 5% chance that the null hypothesis is true. This is too high for our liking, so we cannot reject the null hypothesis, and we cannot conclude any causal effect of $x_j$ on $y$.

If the p-value of $\beta_j$ is less than 0.05, there is less than a 5% chance that the null hypothesis is true. In econometrics, we thus reject the null hypothesis, and conclude that there is a causal effect of $x_j$ on $y$.

[So very simply, if the p-value of]{.underline} $\beta_j$ [is less than 0.05, we can conclude that there is a causal effect of $x_j$ on $y$. If not, we cannot conclude this.]{.underline}
:::

::: callout-warning
## Key Warnings!

If our causal estimate is biased (often due to Endogeneity), we cannot conclude a causal relationship with a hypothesis test

Furthermore, the same interpretations do not apply to hypotheses tests on polynomial transformations and polytomous categorical variables. We will address this with an F-Test (see below).
:::

<br />

### R-Squared and the F-Test of Multiple Coefficients

As I just discussed, polynomial transformations and polytomous explanatory variable coefficients cannot be interpreted as the causal relationship between the variable and $y$.

-   This is because these variables have multiple coefficients for the same variable.
-   The $\beta$ coefficient for the squared term $x^2$ (or any other higher polynomial term) only shows if there is a significant non-linear relationship between $x$ and $y$, not the significance of the causal effect on $y$.
-   The $\beta_j$ of category $j$ of a polytomous categorical variable does not show the significance of the causal effect of the polytomous categorical variable on $y$. It only shows the difference between category $j$ and the reference category.
-   See [chapter 1](https://politicalscience.github.io/metrics/causal/1.html) for more detail on these topics.

But what if we are interested in if these variables have a causal effect on $y$? How do we conduct statistical inference on these variables with multiple coefficients?

<br />

# 4.9: Interaction Terms and Conditional Average Treatment Effect

### Interaction Effect

Interactions, also called moderating effects, means that the effect of $D$ on $y$ is not constant, and depends on some third variable $x_j$.

-   Essentially, $x_j$'s value changes the relationship between $D$ and $y$.

For example, $y$ could be the chance of a civil war occurring, $D$ is the severity of an economic crash, and $x$ is the development level of a country.

-   We could quite reasonably expect that in the effect of a economic crash on a chance of civil war would be significantly higher in developing nations rather than developed.

-   Or in other words, the chance that a civil war occurs due to a economic crash is higher in countries like Venezuela, North Korea and Eritrea, compared to the relationship in Norway, Switzerland, and Denmark.

-   Essentially, $D$'s effect on $y$ is affected by the value of $x$.

::: callout-tip
## Definition: Interaction Effects

Interaction effects are represented by two variables being multiplied together in a regression equation. In the model below, $D$ and $x$ are interacting with each other:

$$
y_i = \beta_0 + \beta_1 D_i + \beta_2 x_i + \beta_3 D_i x_i
$$

Note: you can see in the above equation, $X_1$ and $X_2$ both are interacted, as well as have their own separate coefficients.

-   We should always include both variables independently along with the interaction effect.

Note: You can add other control variables to this regression.
:::

<br />

We can mathematically show that the effect of $D$ on $y$ is not constant - and varies due to the value of $x$, by taking the partial derivative of $y$ in respect to $D$:

$$
\begin{split}
\hat{y}_i  & = \hat{\beta}_0 + \hat{\beta}_1 D_i + \hat{\beta}_2 x_i + \hat{\beta}_3 D_i x_i \\
 \frac{\partial \hat{y}_i}{\partial D_i} &= 0 + \hat{\beta}_1 + 0 + \hat{\beta}_3x_i \\
& = \hat{\beta}_1 + \hat{\beta}_3x_i\end{split}
$$

Thus, the effect of $D$ on $y$ is $\hat\beta_1 + \hat\beta_3 x_i$, which is a function of $x_i$.

<br />

With these equations, we can interpret the coefficients of our model more generally.

::: callout-tip
## Interpretation of Interaction Effects

$\hat{\beta}_1$ is the relationship between $D$ and $y$, given $x = 0$.

$\hat{\beta}_2$ is the relationship between $x$ and $Y$, given $D = 0$.

$\hat{\beta}_3$ represents two things.

1.  For every one unit increase of $x$, the magnitude of the relationship between $D$ and $y$ changes by $\hat{\beta}_3$.
2.  For every one unit increase of $D$, the magnitude of the relationship between $x$ and $y$ changes by $\hat{\beta}_3$.

The coefficient $\beta_3$'s significance level tells us if there is a statistically significant interaction.

-   If $\beta_3$ is not statistically significant, we can remove the interaction term.
-   However, if $\beta_3$ is statistically significant, that means we have found two terms that interact.
:::

<br />

### Conditional Average Treatment Effect

If we recall from chapter 2, the **Conditional Average Treatment Effect (CATE)** is the treatment effect of units, given they have some other variable $x$ value.

$$
\text{CATE} = \mathbb{E}[ \tau_i | x_i ] 
$$

Thus, with interaction effects, we know that the Conditional Average Treatment Effect (CATE) of $D$ on $y$ can be calculated as following:

$$
\text{CATE} = \beta_1 + \beta_3 x_i
$$

::: callout-note
## Example of Conditional Average Treatment Effect

Let us take the example from earlier: $y$ is the chance of a civil war occurring, $D$ is the severity of an economic crash, and $x$ is the development level of a country:

-   $x$ is a binary variable - either a country is developed ($x=1$), or developing ($x=0$).

Using the formula:

$\text{CATE} = \beta_1 + \beta_3 x_i$

We can calculate the conditional average treatment effects of $D$ on $y$, given some value of $x$:

When a country is a developing country $x=0$, the conditional average treatment effect of an economic crash $D$ on civil war $y$ is:

$$
\begin{split}
\mathbb{E}[\tau|x_i = 0] & = \beta_1 + \beta_3 x_i \\
& = \beta_1 + \beta_3(0) \\
& = \beta_1
\end{split}
$$

And when a country is a developed country $x=0$, the conditional average treatment effect of an economic crash $D$ on a civil war $y$ is:

$$
\begin{split}
\mathbb{E}[\tau|x_i = 1] & = \beta_1 + \beta_3 x_i \\
& = \beta_1 + \beta_3(1) \\
& = \beta_1 + \beta_3
\end{split}
$$
:::

<br />

## F-Tests of Nested Models

An F-test of nested models is very simple. Run two regressions, storing them into 2 objects. Then, use the *anova()* command:

-   NOTE: You must use the base-r *lm()* command for these regressions, since F-tests only work with homoscedastic error terms.

```{r, eval = FALSE}
anova(null_model_name, alternate_mode_name)
```

```{r, message = FALSE}
m0 <- lm(pct_missing ~ treat_invite + head_edu + pct_poor, data = dta)
m1 <- lm(pct_missing ~ treat_invite + head_edu + mosques + pct_poor, data = dta)

anova(m0, m1)
```

We can see the significance level of the second (alternate) model when compared to the null.

<br />

## Interaction Effects

To run interaction effects, simply add a \* in between the two variables you want to interact:

```{r, eval = FALSE}
modelname <- feols(y ~ x1*x2 + x3 + x4, data = mydata, se = "hetero")
```

For example:

```{r}
model <- feols(pct_missing ~ treat_invite*head_edu + mosques + pct_poor,
                data = dta, se = "hetero")
summary(model)
```

We can see there is an interacted term coefficient in the output.

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)
