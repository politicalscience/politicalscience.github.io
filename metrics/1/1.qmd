---
title: "Chapter 1: Causalility and Counterfactuals"
subtitle: "Econometric Methods (for Social Scientists)"
format:
    html:
        page-layout: article
        grid:
          sidebar-width: 300px
          body-width: 750px
          margin-width: 250px
          gutter-width: 2em
        fontsize: 12.5pt
        theme: yeti
        toc: TRUE
        toc-depth: 2
        toc-location: left
        toc-expand: true
        toc-title: "Chapter 1: Causalility and Counterfactuals"
        mainfont: "Computer Modern"
        title-block-banner: true
        classoption: fleqn
        html-math-method: katex
---

This chapter will introduce the idea of causal effects and causation. We will explore what a causal effect is, how to theoretically identify causal effects, and the dangers of using correlation as causation.

Topics: Potential Outcomes Framework, Causal Estimands, Properties of Estimators, Naive Estimator, Selection Bias, Confounding Variables

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 2.1: Correlation and Causation

Econometrics is the field of using data to answer economic and social science questions. While econometrics was initially developed for economic questions, it has become widely applied to other social sciences.

Econometrics is often concerned with causal questions:

-   What is the effect of years of education on income?
-   What is the effect of democracy/dictatorship on economic growth and development?
-   What is the effect of income on the likelihood of someone turning out to vote?

<br />

### Correlation is not Causation

Variables are **correlated** with each other, if one variable changing is associated with another variable changing on average.

-   For example, if $x$ increases, the average $y$ value increases, then $x$ and $y$ are correlated.
-   The opposite is also true: if $x$ increases, the average $y$ value decreases, then $x$ and $y$ are correlated.

For example, the graphs below show correlations between variables:

![](figures/1-2.webp){fig-align="center" width="70%"}

::: callout-tip
## Definition: Sources of Correlation

There are three main reasons why two variables $x$ and $y$ may be correlated:

1.  There is a causal effect of $x$ on $y$
2.  A third variable, $w$, causes both $x$ and $y$ to change (this $w$ is called a **confounder**).
3.  There is a causal effect of $y$ on $x$ (this is called **reverse causality**).

These causes can occur simultaneously at the same time (this is called **simultaneity**). Thus, correlation is not causation, as correlation can be caused by other factors.
:::

::: callout-note
## Example: Correlation is not Causation

In the United States, *Ice Cream Sales* and *Number of Fatal Shark Attacks* are highly correlated variables. Does this mean that selling ice cream **causes** fatal shark attacks? No!

The reason this relationship exists is because of another variable - the *weather*. The weather (when sunny), causes both ice cream sales to rise, as well as causing more people to go to the beach, which then increases the number of fatal shark attacks.

This is a clear example of how correlation is not causation.
:::

<br />

If our goal in econometrics is to find the causal effect of $x \rightarrow y$ (source #1 of correlation), we must eliminate the effects of the third variable $w$ (source #2 of correlation), and the reverse causality effect $y \rightarrow x$ (source #3 of correlation).

::: callout-tip
## Definition: Goal of Econometrics

Our goal in econometrics is as follows:

1.  Find the correlation between $x$ and $y$.
2.  Then, remove the effect of confounders $w$ from our correlation.
3.  Then, remove the effect of $y$ causing $x$ (reverse causality) from our correlation.

Then, we are left with the causal effect of $x \rightarrow y$.
:::

<br />

### Key Terminology

Before we start discussing methods to eliminate the effect of third variables and reverse causality, we need to first understand what even is a causal effect. Let us define some key terminology:

-   The variable which causes a causal effect is called the **treatment**, labelled $D$ (sometimes $x$, but we often use $D$ in a causal setting).
-   The variable which changes as a result of the change in $D$ is the **outcome**, labelled $y$.
-   Thus, our causal effect of interest is $D \rightarrow y$.
-   A third variable which causes both $D$ and $y$ is called a **confounder** or **confounding variable**, and is often labelled $w$ or $c$.

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 2.2: Potential Outcomes and Causal Effects

### Potential Outcomes Framework

A **causal effect** is a change in some feature of the world $D$ that directly causes a change in some feature of the world $y$. Causal effects imply the existence of **potential outcomes**.

::: callout-tip
## Definition: Potential Outcomes

Imagine that there are 2 parallel worlds that are exactly the same, and there is one unit/individual $i$ who exists in both worlds.

Then, in one of the two worlds, unit $i$ gets the treatment $D$, which causes a change in the outcome $y$. In the other world, unit $i$ does not get the treatment $D$, so there is no change in outcome $y$.

Since these 2 parallel worlds are identical **except** for the fact one world gets the treatment $D$ and the other does not, the difference in the world's $y$ outcomes can only be the effect of treatment $D$.

The difference in the two world's outcome $y$ is thus the **causal effect** of $D$ on $y$.
:::

<br />

Let us represent these two parallel worlds more formally:

-   The parallel world where unit $i$ gets the treatment will be called the **treatment state**, and will take a value of $D=1$.
-   The parallel world where unit $i$ does not get the treatment will be called the **control state**, and will take a value of $D=0$.

Thus, the outcomes of unit $i$ in these 2 parallel worlds can be notated as $y_{Di}$, where $D$ is the state of the world.

-   In the **treatment state** $D=1$, we will label the outcome of unit $i$ as $y_{1i}$
-   In the **control state** $D=0$, we will label the outcome of unit $i$ as $y_{0i}$.

<br />

::: callout-note
## Example: Democracy and Economic Growth

Let us say we want to find if democracy causes better economic growth. Democracy is our treatment $D$, and economic growth is our outcome $y$.

Country $i$ (for simplicity, let us say Canada) is one of the units. There are two parallel worlds:

-   In one parallel world, Canada is in the **treatment state** $D=1$, meaning it is a democracy. Its outcome in this world would be its economic growth $y_{1i}$.
-   In the other parallel world, Canada is in the **control state** $D=0$, meaning it is not a democracy. Its outcome in this world would be its economic growth $y_{0i}$.
:::

<br />

### Causal Effects

As we mentioned in the potential outcomes framework, the difference in the outcomes in the two parallel worlds must be the causal effect of $D$, since everything else in the parallel worlds are identical.

::: callout-tip
## Definition: Individual Causal Effects

Thus, the individual causal effect of $D$ on unit $i$ is the difference of outcomes $y$ in the treatment state and the control state. Mathematically:

$$
\tau_i = y_{1i} - y_{0i}
$$
:::

Thus, the treatment $D$ will have $\tau_i$ effect on outcome $y$ for unit $i$

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 2.3: Observed Outcomes and the Stable Unit Treatment Value Assumption

### Observed Outcomes

Of course, in real life, we do not have parallel worlds. We only observe one of these parallel worlds.

For example, in the real world, we know Canada is a democracy. We do not observe the parallel world that Canada is a dictatorship. Thus, we also only have one of Canada's potential outcomes, while the other is unobservable.

::: callout-tip
## Definition: Observed Outcomes

Observed outcomes are a subset of potential outcomes that actually come true in the real world. Observed outcomes are given by the formula:

$$
y_i = D_i \times y_{1i} + (1-D_i) \times y_{0i}
$$

-   Where $y_i$ is the observed outcome, $y_{1i}$ is the potential outcome of the treatment state, and $y_{0i}$ is the potential outcome of the control state.
-   Where $D_i$ represents what state unit $i$ is in, where $D_i = 1$ indicates unit $i$ is in the treatment state, and $D_i = 0$ indicates unit $i$ is in the control state.
:::

This formula might seem unintuitive. But we can plug in values of $D_i$ to understand what this formula means.

What if unit $i$ is in the treatment state, $D_i = 1$. Let us plug in $D_i = 1$ into the equation:

$$
\begin{split}
y_i & = D_i \times y_{1i} + (1-D_i) \times y_{0i} \\
y_i & = 1 \times y_{1i} + (1-1) \times y_{0i} \\
y_i & = y_{1i}
\end{split}
$$

Thus, the observed outcome when unit $i$ is in the treatment state is $y_{1i}$, which makes sense, since that is the potential outcome of the treatment state.

<br />

Similarly, what if unit $i$ is in the control state $D_i = 0$. Let us plug in $D_i = 0$ into the equation:

$$
\begin{split}
y_i & = D_i \times y_{1i} + (1-D_i) \times y_{0i} \\
y_i & = 0 \times y_{1i} + (1-0) \times y_{0i} \\
y_i & = y_{0i}
\end{split}
$$

Thus, the observed outcome when unit $i$ is in the control state is $y_{0i}$, which makes sense, since that is the potential outcome of the control state.

::: callout-tip
## Definition: Counterfactuals

A counterfactual is the potential outcome that is not observed. For example, if in the real world, unit $i$ is observed to have received the treatment, then its counterfactual is the world where unit $i$ did not receive the treatment.

Counterfactuals are important, because, if we remember, the individual causal effect is $\tau_i = y_{1i} - y_{0i}$, which implies that we must know the counterfactual to find the causal effect of $D$ on $y$.
:::

<br />

### Sampling and Observed Outcomes

Sampling is the process of using a subset of a larger population to make claims on the larger population.

-   For example, pollsters do not ask everyone how they will vote - they ask a small **sample** and, if that sample is representative, use that sample to conclude how everyone in the country will vote.

We can actually think about sampling in terms of observed outcomes.

-   Our population is all of the potential outcomes covered in [section 2.2](https://politicalscience.github.io/metrics/causal/2.html#potential-outcomes-and-causal-effects).
-   Our sample is the observed outcomes that are realised in the real world. This is a subset that we are selecting - and we can only use what we observe to try to estimate causal effects on the entire population of potential outcomes.

This will become useful when we run hypothesis testing for causal inference in section 2.6 and section 2.7

<br />

### Stable Unit Treatment Value Observation

The stable unit treatment value assumption (SUTVA) is arguably the key assumption of causal inference.

SUTVA states that given two units $i$ and $j$, unit $i$ is not affected by the treatment assignment of unit $j$. More intuitively, if unit $j$ is assigned to treatment, that has no effect on the outcomes of unit $i$.

Why is this important? Well, if unit $j$'s treatment status affects unit $i$, then unit $i$ would have more than 2 potential outcomes:

-   This is because unit $i$'s outcomes would now not only depend on itself being assigned to treatment or control, but also unit $j$ being assigned to treatment or control.
-   That means unit $i$ now has 4 potential outcomes. If unit $i$ is affected by other units other than $j$, this will quickly multiply to an unmanageable number of potential outcomes.

Thus, violating this assumption will basically make it impossible to calculate treatment effects.

<br />

In what situations is SUTVA typically violated?

1.  Peer effects that result from contact between units $i$ and $j$. For example, if you are testing the treatment of some new curriculum on student educational outcomes, and unit $j$ is assigned to treatment, unit $j$ might teach their friend unit $i$ some new things, even if unit $i$ was in the control group.
2.  Dilution/concentration effects that arise from a prevalence of a treatment. The best example is vaccines - if enough people get a vaccine for a disease, even people who do not get the vaccine are safer, since transmission is much more difficult.

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 2.4: Causal Estimands

We introduced how the individual treatment effect of $D \rightarrow y$ for any unit $i$ is $\tau_i = y_{1i} - y_{0i}$. However, there is an issue - we do not know the two parallel worlds.

-   As we discussed in [section 2.3](https://politicalscience.github.io/metrics/causal/2.html#observed-outcomes-and-the-stable-unit-treatment-value-assumption), we can only observe one of the two parallel worlds.

Thus, we can never actually find (or estimate) the individual treatment effect. We need another **estimand**. An **estimand** is the true value of something in the real world, that we will try to estimate with an estimator.

This course will focus on introducing many new estimators to try to estimate these estimands. However, before we start estimating, we have to know what we are trying to estimate. What are the estimands we can use, if individual treatment effect is not possible?

<br />

The "key" estimand in causal inference is the Average Treatment Effect:

::: callout-tip
## Definition: Average Treatment Effect

The **Average Treatment Effect (ATE)** is the average of all individual treatment effects:

$$
\text{ATE} \ = \ \mathbb{E}[\tau_i] \ = \ \bar{y}_{1i} - \bar{y}_{0i}
$$

Where the bar over the $y$ represents mean.
:::

However, the ATE is not the only estimand we are interested in.

<br />

Sometimes, our treatment variables are continuous. For example, if we want to estimate how GDP affects democratisation, our treatment variable GDP is not binary.

In cases of non-continuous treatment variables, we want to estimate the **average causal effect** (ACE)- the expected change in $y$ resulting from a one unit increase in $D$:

$$
\text{ACE} = \mathbb{E}[y'(D)]
$$

Where $y'(D)$ is the derivative of $y$ in respect to $D$

<br />

Sometimes, we have reason to expect that the treatment effect may be different accross different categories of units.

For example, maybe we think that a certain educational treatment will benefit women more than men, so the treatment effect will be higher for women than men.

The **Conditional Average Treatment Effect (CATE)** is the treatment effect of units, given they have some other variable $x$ value.

$$
\text{CATE} = \mathbb{E}[ \tau_i | x_i ] 
$$

<br />

We can also estimate the causal effect on only the treatment or control groups.

The **average treatment effect on the treated (ATT)** is the average treatment effect of only units who received the treatment $D_i = 1$:

$$
\text{ATT} = \mathbb{E} [\tau_i | D_i = 1] = \bar{y}_{1i, \ D_i = 1} - \bar{y}_{0i, \ D_i = 1}
$$

The **average treatment effect on the controls (ATC)** is the average treatment effect of only units who did not receive the treatment $D_i = 0$:

$$
\text{ATC} = \mathbb{E} [\tau_i|D_i = 0] = \bar{y}_{1i, \ D_i = 0} - \bar{y}_{0i, \ D_i = 0}
$$

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 2.5: Properties of Estimators - Unbiasedness and Consistency

The above causal estimands are not directly calculable, since we cannot observe both potential outcomes. Thus, we need an **estimator** to estimate the causal estimands.

Estimators have two key properties: unbiasdness and consistency.

<br />

### Unbiasedness

An estimator is **unbiased** if its estimates $\hat{\theta}$ are on average equal to the true estimand $\theta$.

$$
\mathbb{E}[ \ \hat{\theta} \ ] = \theta
$$

Note: I use $\theta$ to represent any possible estimand, including the causal estimands from above.

To better understand unbiasdness, take this example of archery. We have two archers, archer $A$ and archer $B$.

-   Both archers are very very accurate - as in they hit the same spot on the target every time they shoot.
-   Archer $A$ gets the bullseye every single time.
-   However, archer $B$, while very consistently hitting the same spot, for some reason, keeps hitting 5 inches to the right of the bullseye.

Thus, archer $B$ is biased - he is consistently and systematically hitting the wrong spot on the target. Similarly, if an estimate is consistently off by a certain value, then it is biased.

<br />

### Consistency

However, an unbiased estimator is not good enough. Why? Take this example of another archer $C$.

-   Archer $C$ has good aim - he is aiming directly at the bullseye.
-   However, archer $C$ is very inconsistent - he first hits a shot 5 inches to the left of the bullseye, then hits a shot 5 inches to the right of the bullseye. He keeps doing both of these things over and over again.
-   Archer $C$ on average is actually hitting the bullseye, since his leftward and rightward errors cancel each other out.
-   However, archer $C$ never actually hits the bullseye on any specific shot.

Archer $C$ is an example of an unbiased, but very inconsistent estimator. On average, he is getting the right value of the estimate, but he is never that close on any specific estimate.

-   More technically, an inconsistent estimator is one who has high **variance** in their estimates.

<br />

Ideally, [we want an estimator that is both **unbiased** and **consistent**]{.underline}. This would mean that their average estimates are equal to the true estimand, and that each estimate is relatively close to the true estimand with low variance.

-   So, an archer than on average, hits the bullseye, and is generally not too far off from the bullseye on any sepcific shot.

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 2.6: The Naive Estimator and Proof Correlation is not Causation

The **naive estimator** is an estimator that only compares the observed outcomes, without any comparison to the counterfactual potential outcomes.

So essentially, we compare the observed treatment group outcomes and the observed control group outcomes:

$$
\hat{\tau}_{\text{naive}} = \bar{y}_{1i, \ D_i = 1} - \bar{y}_{0i, \ D_i = 0}
$$

This is what many people do when trying to find a causal effect. The naive estimator is also similar to a measure of correlation.

<br />

However, [**the naive estimator is a bad idea**]{.underline}. Remember, treatment effects are supposed to be comparing the two potential outcomes [of the same unit]{.underline}. We are supposed to compare $y_{1i}$ to $y_{0i}$.

-   However, the naive estimator does not do that. Instead, it is comparing the observed outcomes of the treatment group versus the different control group.

So essentially, it is comparing the potential outcomes of [different]{.underline} units.

-   But what is the issue with this?
-   Well, if you compare the potential outcome of two different units (let us call them $A$ and $B$), you are comparing $y_{1A}$ and $y_{1B}$.
-   However, what if units $A$ and $B$ are different in some way even before the treatment? Their outcomes $y$ may differ not due to the treatment $D$, but due to some underlying difference between units $A$ and $B$.
-   Thus, we are introducing some **bias** - the differences between unit $A$ and $B$ are being included in our causal estimate, when they should not be, since the differences between $A$ and $B$ may not be due to $D$.

<br />

We can prove this mathematically. We start with the naive estimator:

$$
\hat{\tau} = \bar{y}_{1i, \ D_i = 1} - \bar{y}_{0i, \ D_i = 0}
$$

Then, we do a little algebra trick - we add a new term to this equation, then subtract that same term. The two new terms thus cancel each other out to 0:

$$
\hat{\tau} = \bar{y}_{1i, \ D_i = 1} - \bar{y}_{0i, \ D_i = 0} + \bar{y}_{0i, \ D_i = 1} - \bar{y}_{0i, \ D_i = 1}
$$

Now, let us re-arrange this equation:

$$
\hat{\tau} = \bar{y}_{1i, \ D_i = 1} - \bar{y}_{0i, \ D_i = 1} + \bar{y}_{0i, \ D_i = 1} - \bar{y}_{0i, \ D_i = 0}
$$

We can see the first two terms are equal to the average treatment effect on the treated (ATT). Thus, we can rewrite the naive estimator:

$$
\hat{\tau} = \text{ATT} + \bar{y}_{0i, \ D_i = 1} - \bar{y}_{0i, \ D_i = 0}
$$

If we look at the final result, we can divide it into two parts:

1.  The ATT (see [section 2.4](https://politicalscience.github.io/metrics/causal/2.html#causal-estimands))
2.  The extra bit: $\bar{y}_{0i, \ D_i = 1} - \bar{y}_{0i, \ D_i = 0}$, which we call **selection bias**. Intuitively, it is the difference [prior to treatment]{.underline} between the control and treatment groups. This results in our naive estimator being biased.

We will go into detail on selection bias in the next section.

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 2.7: Selection Bias and Confounding Variables

### Selection Bias

As we just established, the naive estimator contains **selection bias** $\bar{y}_{0i, \ D_i = 1} - \bar{y}_{0i, \ D_i = 0}$. This is the difference [prior to treatment]{.underline} between the control and treatment groups.

::: callout-note
## Example: Hospital and Health

For example, if we are measuring the question *does going to the hospital make you more healthy*, and we simply measured the outcomes of people who went to the hospital and did not go to the hospital, we might see that in general, people who did not go to the hospital are healthier!

-   Does this mean that going to the hospital makes you unhealthier? No! It is because more unhealthy people choose to go to the hospital in the first place. Thus, the hospital has generally more unhealthy individuals in it. The hospital might perform miracles on these people, but they are still not as healthy as the healthy people who did not need to go to the hospital.

-   The differences between the people who chose to go to the hospital versus the people who did not go to the hospital explains the differences in our outcome, not the actual treatment that the hospital provided.
:::

Thus, selection bias can be thought of as when our treatment and control groups are fundamentally different and unequal prior to treatment.

-   And when we use the naive estimator (or correlation), we pick up this bias in our estimates.

<br />

### Confounding Variables

Selection bias is caused by confounding variables.

::: callout-tip
## Definition: Confounding Variable

A **confounder** or **confounding variable** is a variable that is causes differences in the treatment and control groups.

Confounding variables result in selection bias, and are why correlation does not equal causation. In order to accurately calculate causal effects, we need to find some way to eliminate the effect of confounding variables.
:::

For example, look at the figure below:

![](figures/1-1.png){fig-align="center" width="40%"}

In this figure, $D$ is the treatment, and $C$ is the confounding variable that affects whether or not a unit gets the treatment $D$ or not.

When we calculate the naive estimate (or correlation), our causal estimate captures both the direct effect $D \rightarrow Y$, but also the effect of $D \leftrightarrow C \rightarrow O \rightarrow Y$, since $D$ and $C$ are correlated.

-   This second effect $D \leftrightarrow C \rightarrow O \rightarrow Y$ is called the **backdoor path**.

However, the actual causal effect of treatment $D$ on $Y$ is only the section of $D \rightarrow Y$, and does not include the backdoor path. So, we need to find some way to only look at $D \rightarrow Y$, and eliminate/partial out the effect of the back door.

<br />

To make an accurate causal effect estimate, we must get rid of confounding variables, selection bias, and the backdoor path. How do we do this?

There are many many methods to eliminate the effect of confounding variables.

-   The best method is randomisation, which we will cover in the next chapter.
-   After we cover randomisation, we will cover methods to eliminate confounding variables when randomisation is not possible, including multiple linear regression, instrumental variables, selection on observables, quasi-experimental methods, and many more.

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)
