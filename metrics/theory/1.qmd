---
title: "Chapter 1: Simple Linear Regression and Ordinary Least Squares"
subtitle: "Econometric Methods for Social Scientists"
format:
    html:
        page-layout: article
        grid:
          sidebar-width: 300px
          body-width: 750px
          margin-width: 250px
          gutter-width: 2em
        fontsize: 12.5pt
        theme: yeti
        toc: TRUE
        toc-depth: 2
        toc-location: left
        toc-expand: true
        toc-title: "Chapter 1: Simple Linear Regression and Ordinary Least Squares"
        mainfont: "Computer Modern"
        title-block-banner: true
        classoption: fleqn
        html-math-method: katex
---

This first chapter introduces the goals of econometrics. Then, we dive into the simple linear regression as a way to measure relationships between variables. We discuss the layout of the model, the estimation process of parameters, and the properties of the ordinary least squares estimator.

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 1.2: Simple Linear Regression

### Explaining Relationships Between Variables

How can we explore the relationship between a continuous $x$ variable and a continuous $y$ variable?

### Linear Regression Model

How can we fit a best-fit line in order to find the relationship between $x$ and $y$? The main way to do this is with the **simple linear regression model**.

::: callout-tip
The simple linear regression model takes the following form.

$$
y_i = \beta_0 + \beta_1 x_i + u_i
$$

-   Where we have $n$ number of observations in our data, $i$ being any one of them, and each observation has an $x$ and $y$ value $(x_i, y_i)$.
-   Where $\beta_0$ (intercept) and $\beta_1$ (slope) are coefficients of the model that need to be estimated (since they will differ in value between different samples and data).
-   Where $u_i$ is the error term. Remember, $y_i$ is any value in the conditional distribution $y_i|x_i$, so it may not be exactly at the expected value of the conditional distribution. Thus, we have to add an error term to account for this distribution, where $\mathbb{E}[u_i] = 0$ and $Var[u_i] = \sigma^2$.
:::

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 1.3: Ordinary Least Squares Estimator

### Sum of Squared Errors

We want to fit a best-fit line that is accurate. So, we want to find the $\beta_0$ (intercept) and $\beta_1$ (slope) values that will best fit our observed data.

One way we can fit an accurate line is to find the best-fit line that minimises the sum of squared errors.

::: callout-tip
## Definition: Sum of Squared Errors

The sum of squared errors (SSE) is as follows:

$$
\begin{split}
SSE & = \sum\limits_{i=1}^n (y_i - \hat y_i)^2 \\
& = \sum\limits_{i=1}^n (y_i - \hat\beta_0 - \hat\beta_1 x_i)^2
\end{split}
$$

-   The sum of squared errors is exactly as it sounds. Find the error, the distance between the actual $y_i$ and predicted $\hat y$, which is $y_i - \hat y$, then square that error $(y_i - \hat y_i)^2$, then sum up for all observations $i$ in the data.
-   We get the second equation by substituting in the fitted values model (discussed in the previous section), where $\hat{y} = \hat\beta_0 + \hat\beta_1x_i$.
:::

<br />

### Ordinary Least Squares Estimator

The Ordinary Least Squares (OLS) Estimator estimates the coefficients $\beta_0$ and $\beta_1$ by finding the values of $\beta_0$ and $\beta_1$ that result in the line with the smallest sum of squared errors.

We can describe the goal of OLS in a more mathematical way:

::: callout-tip
## Definition: Ordinary Least Squares (OLS) Estimator

The goal of the Ordinary Least Squares (OLS) Estimator is to find the values of $\beta_0$ and $\beta_1$ that make the following statement true:

$$
\begin{split}
(\hat{\beta}_0, \hat{\beta}_1) & = \arg \min\limits_{\hat{\beta}_0, \hat{\beta}_1} \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2 \\
& = \arg \min\limits_{\hat{\alpha}, \hat{\beta}} S(\hat{\beta}_0, \hat{\beta}_1)
\end{split}
$$

Where function $S$ is the sum of squared errors.
:::

<br />

How do we minimise $S$ (the function of the sum of squared errors)?

-   From calculus, we know that a minimum/maximum of a function is where the derivative of the function is equal to 0.

Thus, let us find the partial derivative of the function $S$ in respect to both $\hat\beta_0$ and $\hat\beta_1$, and set them equal to 0. This is also called the **first-order conditions**.

<br />

### First Order Conditions

First, let us find the partial derivative of $S$ in respect to $\hat\beta_0$:

$$
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial \hat{\beta}_0} = \frac{\partial }{\partial \hat{\beta}_0} \left[ \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2 \right]
$$

First, ignore the summation. The partial derivative of the internal section, using chain rule, is the following:

$$
\frac{\partial}{\partial \hat{\beta}_0} \left[ (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2 \right] = -2(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
$$

But how do we deal with the summation? We know that there is the sum rule of derivatives $[f(x) + g(x)]' = f'(x) + g'(x)$. Thus, we know we just sum up the derivatives to get the derivative:

$$
\begin{split}
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial \hat{\beta}_0} & = \sum\limits_{-i=1}^n \left[ -2(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) \right] \\
& = -2 \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
\end{split}
$$

To find the value of $\hat\beta_0$ that minimises $S$, we set the derivative equal to 0. We can ignore the -2, since if the summation is equal to 0, the whole derivative will equal 0. Thus, the first order condition is:

$$
\sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
$$

<br />

Now, let us do the same for $\hat\beta_1$. Using the same steps as before

$$
\begin{split}
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial \hat{\beta}_1} & = \sum\limits_{i=1}^n \left[ -2x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) \right] \\
& = -2 \sum\limits_{i=1}^n x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
\end{split}
$$

The first order condition for $\hat\beta_1$ will be (again, ignoring the -2 for the same reason as before):

$$
\sum\limits_{i=1}^n x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
$$

<br />

::: callout-tip
## Definition: First Order Conditions of OLS

Thus, the first order conditions of OLS are:

$$
\begin{split}
& \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
& \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
$$
:::

<br />

### Solving the System of Equations

We now have our two first-order conditions. Now, we basically have a 2-equation system of equations, with 2 variables.

-   We can solve this through substitution - in the first equation, solve for $\hat\beta_0$ in terms of $\hat\beta_1$.
-   Then, plug in $\hat\beta_0$ in terms of $\hat\beta_1$ into the second equation, thus making that a one-variable equation. We can solve that equation for $\hat\beta_1$, then find $\hat\beta_0$.

<br />

First, let us solve the first equation for $\hat\beta_0$ in terms of $\hat\beta_1$:

$$
\begin{split}\sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) & =  0 \\
\sum\limits_{i=1}^n y_i - n \hat{\beta}_0 - \hat{\beta}_1 \sum\limits_{i=1}^n x_i & = 0 \\
-n\hat{\beta}_0 &= -\sum\limits_{i=1}^n y_i + \hat{\beta}_1\sum\limits_{i=1}^nx_i \\
\hat{\beta}_0 & = \frac{1}{n} \sum\limits_{i=1}^n y_i - \frac{1}{n}\hat{\beta}_1 \sum\limits_{i=1}^n x_i \\
& = \bar{y} - \hat{\beta}_1 \bar{x}
\end{split}
$$

Now, let us substitute our calculated $\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$ into the $\hat{\beta}_1$ condition and solve for $\hat{\beta}_1$:

$$
\begin{split}0 & = \sum\limits_{i=1}^n \left[ x_i(y_i - [\bar{y} - \hat{\beta}_1\bar{x}] - \hat{\beta}_1x_i) \right] \\
& = \sum\limits_{i=1}^n \left[ x_i(y_i - \bar{y} - \hat{\beta}_1 (x_i - \bar{x})) \right] \\
& = \sum\limits_{i=1}^n \left[ x_i(y_i - \bar{y}) - x_i \hat{\beta}_1(x_i - \bar{x}) \right] \\
& = \sum\limits_{i=1}^n x_i (y_i - \bar{y}) - \hat{\beta}_1 \sum\limits_{i=1}^nx_i (x_i - \bar{x})\end{split}
$$

::: callout-note
## Useful Properties of Summation

To help us solve this problem, note these 3 properties of summation:

$$
\begin{split}& \sum\limits_{i=1}^n (x_i - \bar{x}) = 0 \\& \sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y}) \\& \sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2\end{split}
$$
:::

Knowing these properties of summation, we can transform what we had before into:

$$
\begin{split}
0 & = \sum\limits_{i=1}^n(x_i - \bar{x})(y_i - \bar{y}) - \hat{\beta}_1 \sum\limits_{i=1}^n (x_i - \bar{x})^2 \\
\hat{\beta}_1 \sum\limits_{i=1}^n (x_i - \bar{x})^2 & = \sum\limits_{i=1}^n(x_i - \bar{x})(y_i - \bar{y}) \\
\hat{\beta}_1 & = \frac{\sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum\limits_{i=1}^n(x_i - \bar{x})^2}
\end{split}
$$

::: callout-tip
## Definition: OLS Estimate of Coefficient

Thus, the OLS estimate $\hat\beta_1$ (slope) of the linear regression model is:

$$
\hat{\beta}_1 = \frac{\sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum\limits_{i=1}^n(x_i - \bar{x})^2} = \frac{Cov(x, y)}{Var(x)}
$$

This is the expected change in $y$ given a one unit increase in $x$.

-   Remember, this is the **relationship** between $x$ and $y$, [**not** the causal effect]{.underline}.
:::

<br />

Of course, we still need to find $\hat\beta_0$ (the slope). We found that $\hat\beta_0 = \bar{y} - \hat{\beta}_1 \bar{x}$ earlier, so we just plug that in.

And now, we have our estimates $\hat\beta_0$ and $\hat\beta_1$, and thus we now have a best-fit line and an estimate of the relationship between $x$ and $y$.

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 1.4: Algebraic Properties of the OLS Estimator

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 1.5: Interpretation and Reparamaterisation

We now have estimated $\hat\beta_0$ and $\hat\beta_1$. But what do these actually mean in the context of the relationship between $x$ and $y$?

-   Let us start with $\hat\beta_1$, which is the slope, the more important of the two coefficients.

<br />

### Interpretation of $\hat\beta_1$

We know that in a linear model, $\mathbb{E}[y_i|x_i] = \beta_0 + \beta_1 x_i$, the coefficient $\beta_1$ is the slope.

-   And the slope is the change in $y$ given a one unit increase in $x$.

Using this knowledge, we can interpret estimate $\hat\beta_1$.

::: callout-tip
## Interpretation of $\hat\beta_1$

When $x$ increases by one unit, there is an expected $\hat{\beta}_1$ unit change in $y$.
:::

::: callout-warning
## Warning!

Note how I have been using the word **relationship**, not causal effect.

Causality is not established through estimators, it is established through a strong experimental design (which we will cover in part II of the course).
:::

Note that this interpretation of $\hat\beta_1$ only applies to continuous $x$ variables and continuous/ordinal $y$ variables.

<br />

### Interpretation of $\hat\beta_1$ for Binary $x$

**Binary explanatory** variables are variables with 2 values, 0 and 1.

-   Binary explanatory variables are extremely common in the social sciences. They can include things such as yes/no questions, treatment/control, true/false questions, voted/did not vote, etc.

Binary explanatory variables will change the interpretations of our coefficients.

We can "solve" for these interpretations. Assume $x$ has two categories $x=0$ and $x=1$:

$$
\begin{split}
& \hat y_{i, \ x = 0} = \hat\beta_0 + \hat\beta_1(0) = \hat\beta_0 \\
& \hat y_{i, \ x = 1} = \hat\beta_0 + \hat\beta_1(1) = \hat\beta_0 + \hat\beta_1 \\
& \hat y_{i, \ x = 1} - \hat y_{i, \ x = 0} = (\hat\beta_0 + \hat\beta_1) - \hat\beta_0 = \hat\beta_1
\end{split}
$$

Thus, we can interpret the coefficients as follows:

::: callout-tip
## Interpretation of Coefficient with a Binary Explanatory Variable

When $x$ is a binary explanatory variable:

-   $\hat\beta_0$ is the expected value of $y$ given an observation in category $x = 0$
-   $\hat\beta_0 + \hat\beta_1$ is the expected value of $y$ given an observation in category $x = 1$
-   $\hat\beta_1$ is the expected difference in $y$ between the categories $x=1$ and $x=0$.
:::

::: callout-warning
## Warning!

Note how I have been using the word **relationship**, not causal effect.

Causality is not established through estimators, it is established through a strong experimental design (which we will cover in part II of the course).
:::

<br />

### Interpretation of $\hat\beta_0$

We know that in a linear model, $\mathbb{E}[y_i|x_i] = \beta_0 + \beta_1 x_i$, the coefficient $\beta_0$ is the y-intercept.

-   And the y-intercept is the change value of $y$ given $x=0$.

We can prove this mathematically:

$$
\begin{split}
\hat y_{i, \ x_i = 0} & = \hat\beta_0 + \hat\beta_1 x_i \\
& = \hat\beta_0 + \hat\beta_1(0) \\
& = \hat\beta_0
\end{split}
$$

Thus, knowing this, we can interpret $\hat\beta_0$.

::: callout-tip
## Interpretation of $\hat\beta_0$

When $x=0$, the expected value of $y$ is $\hat{\beta}_0$
:::

<br />

### Reparamatising $\beta_1$ in Terms of Standard Deviations

Sometimes, it is hard to understand what changes in $y$ and $x$ mean in terms of units. For example, if we are measuring "democracy", what does a 5 unit change in democracy mean? Is that a lot?

We can add more relevant detail by expressing the change of $y$ and $x$ in standard deviations.

How do we calculate this? Well, let us solve for the change in $\hat{y}_i/\sigma_y$ given $x_i = x$ and $x = x + \sigma_X$. This will tell us how much $\hat{y}$ changes by given a increase of one standard deviation in $x$:

$$
\begin{split}
\frac{\hat y_{i, \ x_i = x + \sigma_x}}{\sigma_y} - \frac{\hat y_{i, \ x_i = x}}{\sigma_y} & = \frac{\hat\beta_0 + \hat\beta_1 x_i}{\sigma_y} - \frac{\hat\beta_0 + \hat\beta_1 x_i}{\sigma_y} \\
& = \frac{\hat\beta_0 + \hat\beta_1 (x+\sigma_x) - (\hat\beta_0 + \hat\beta_1 (x))}{\sigma_y} \\
& = \frac{\hat\beta_0 - \hat\beta_0 + \hat\beta_1x - \hat\beta_1x+\hat\beta_1\sigma_x}{\sigma_y} \\
& = \frac{\hat\beta_1 \sigma_x}{\sigma_y}
\end{split}
$$

::: callout-tip
## Interpretation in Terms of Standard Deviation

For a one-std. deviation increase in $x$, there is an expected $\hat{\beta}_1 \sigma_x / \sigma_y$-std. deviation change in $Y$.
:::

::: callout-warning
## Warning!

Note how I have been using the word **relationship**, not causal effect.

Causality is not established through estimators, it is established through a strong experimental design (which we will cover in part II of the course).
:::

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 1.6: R-Squared and Goodness of Fit

For each observation, we know that the actual $y_i$ value is the predicted $\hat y_i$ plus the residual term $\hat u_i$. Thus:

$$
y_i = \hat y_i + \hat u_i
$$

Now, let us define these three concepts: the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR):

$$
\begin{split}
& SST = \sum\limits_{i=1}^n (y_i - \bar y)^2 \\
& SSE = \sum\limits_{i=1}^n (\hat y_i - \bar y)^2 \\
& SSR = \sum\limits_{i=1}^n (\hat u_i)^2
\end{split}
$$

-   The SST explains the total amount of variation in $y$
-   The SSE is the amount of variation in $y$ explained by our model
-   The SSR is the amount of variation in $y$ not explained by our model

Let us look at the total sum of squares (SST). We can manipulate it as follows:

$$
\begin{split}
SST & = \sum\limits_{i=1}^n (y_i - \bar y)^2 \\
& = \sum\limits_{i=1}^n(y_i - \hat y_i+ \hat y_i - \bar y)^2 \\
& = \sum\limits_{i=1}^n[\hat u_i + (\hat y_i - \bar y)]^2 \\
\end{split}
$$

And since we know $\sum \hat y_i \hat u_i = 0$, we can further simplify to:

$$
\begin{split}
SST & = \sum\limits_{i=1}^n u_i^2 + \sum\limits_{i=1}^n(\hat y_i - \bar y)^2 \\
& = SSE + SSR
\end{split}
$$

This makes sense: After all, SSE is the squared errors explained by the model, and SSR is the residual (non-explained) parts of the model, so together, they should be equal to the total sum of squares.

<br />

Using these properties, we can create a statistic which explains how well our model explains the variation in $y$. This statistic is called $R^2$:

$$
R^2 = \frac{SSE}{SST} = 1 - \frac{SSR}{SST}
$$

What does this $R^2$ value mean?

-   Well SSE is the amount of variation in $y$ explained by our model, and SST is the total amount of variation in $y$.
-   Thus, $R^2$ is the proportion of variation in $y$ explained by our model.

$R^2$ is always between 0 and 1:

-   This is because it is a proportion, so and $0 ≤ SSE ≤ SST$, so this must be true.
-   Values closer to 1 mean our model explains the variance in $y$ more
-   Values closer to 0 mean our model explains less of the variance in $y$.

However, be careful when using $R^2$. Just because it is high, does not mean we can infer anything from it.

Extra note, $R^2$ is also equal to the correlation coefficient between $y_i$ and $\hat y_i$.

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 1.7: Gauss-Markov and Unbiasedness of the OLS Estimator

An unbiased estimator, if we recall from section 1.1, means that over many different estimates, the expected value of all the estimates is the true parameter value: $E(\hat{\theta}_n) = \theta$.

Unbiasedness is desirable property of causal estimators. OLS (for simple linear regression) is an unbiased estimator under 4 conditions:

::: callout-tip
## Definition: Gauss-Markov Theorem for Unbiasdness

The **Gauss-Markov Theorem** states that the ordinary least squares estimator is **unbiased** under 3 conditions:

1.  **Linearity** of the equation (no coefficients $\beta_0, ..., \beta_k$ can be multiplied together. This does not apply to explanatory variables, only the coefficients.
2.  Random Sampling from the population.
3.  Explanatory variable $x$ has **variation** (so not all values of $x$ are the same).
4.  [And the most important condition - **Zero conditional mean**]{.underline}: meaning that no matter the value of $x$, the expected value of the residual $u_i$ is always 0. $E(u|x) = 0$ for all $x$.
:::

<br />

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)
