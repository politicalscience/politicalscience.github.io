<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Chapter 1: Simple Linear Regression and Ordinary Least Squares</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="1_files/libs/clipboard/clipboard.min.js"></script>
<script src="1_files/libs/quarto-html/quarto.js"></script>
<script src="1_files/libs/quarto-html/popper.min.js"></script>
<script src="1_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="1_files/libs/quarto-html/anchor.min.js"></script>
<link href="1_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="1_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="1_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="1_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="1_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Chapter 1: Simple Linear Regression and Ordinary Least Squares</h1>
            <p class="subtitle lead">Econometric Methods for Social Scientists</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Chapter 1: Simple Linear Regression and Ordinary Least Squares</h2>
   
  <ul class="collapse">
  <li><a href="#introduction-to-econometrics" id="toc-introduction-to-econometrics" class="nav-link active" data-scroll-target="#introduction-to-econometrics">1.1: Introduction to Econometrics</a></li>
  <li><a href="#simple-linear-regression" id="toc-simple-linear-regression" class="nav-link" data-scroll-target="#simple-linear-regression">1.2: Simple Linear Regression</a></li>
  <li><a href="#ordinary-least-squares-estimator" id="toc-ordinary-least-squares-estimator" class="nav-link" data-scroll-target="#ordinary-least-squares-estimator">1.3: Ordinary Least Squares Estimator</a></li>
  <li><a href="#algebraic-properties-of-the-ols-estimator" id="toc-algebraic-properties-of-the-ols-estimator" class="nav-link" data-scroll-target="#algebraic-properties-of-the-ols-estimator">1.4: Algebraic Properties of the OLS Estimator</a></li>
  <li><a href="#interpretation-and-reparamaterisation" id="toc-interpretation-and-reparamaterisation" class="nav-link" data-scroll-target="#interpretation-and-reparamaterisation">1.5: Interpretation and Reparamaterisation</a></li>
  <li><a href="#r-squared-and-goodness-of-fit" id="toc-r-squared-and-goodness-of-fit" class="nav-link" data-scroll-target="#r-squared-and-goodness-of-fit">1.6: R-Squared and Goodness of Fit</a></li>
  <li><a href="#gauss-markov-and-unbiasedness-of-the-ols-estimator" id="toc-gauss-markov-and-unbiasedness-of-the-ols-estimator" class="nav-link" data-scroll-target="#gauss-markov-and-unbiasedness-of-the-ols-estimator">1.7: Gauss-Markov and Unbiasedness of the OLS Estimator</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>This first chapter introduces the goals of econometrics. Then, we dive into the simple linear regression as a way to measure relationships between variables. We discuss the layout of the model, the estimation process of parameters, and the properties of the ordinary least squares estimator.</p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
<section id="introduction-to-econometrics" class="level1">
<h1>1.1: Introduction to Econometrics</h1>
<section id="goal-of-econometrics" class="level3">
<h3 class="anchored" data-anchor-id="goal-of-econometrics">Goal of Econometrics</h3>
<p>What is the goal of econometrics? It is to use real-world data to analyse our hypotheses about topics in the social sciences.</p>
<p>The most common use of econometrics is to find causal relationships between variables:</p>
<ul>
<li>How does education effect individual’s incomes?</li>
<li>How does democracy effect a country’s economic growth?</li>
</ul>
<p>The issue with answering these questions is that we often do not have data for everyone in the population.</p>
<ul>
<li>For example, if our goal is to accurately calculate the effect of education on income in the USA, we would have to have data on every single person (300 million+) in the US.</li>
</ul>
<p>That obviously is far too difficult and costly. But cost is not the only concern - sometimes, as we get more into causal inference, we will realise that the population is not even measurable.</p>
<ul>
<li>The true value of the relationship in the population (that we do not know) is called the population parameter, and is labelled <span class="math inline">\theta</span>.</li>
</ul>
<p><br></p>
<p>So what do we have to do? We do sampling:</p>
<ul>
<li>Sampling is taking a small subset of the population (let us say 1,000 people), and estimating the relationship between two variables there. This is called <strong>estimation</strong></li>
<li>The hope is that our sample is somewhat representative of the population, so we can use our sample estimate to say something about the population. This is called <strong>statistical inference</strong>.</li>
</ul>
<p>But before we even get to the second part on inference, we have to do <strong>estimation</strong> within our sample.</p>
<p><br></p>
</section>
<section id="properties-of-estimators" class="level3">
<h3 class="anchored" data-anchor-id="properties-of-estimators">Properties of Estimators</h3>
<p>We can estimate the relationship between variables with an estimator.</p>
<ul>
<li>An estimator is a procedure for taking sample data to produce an estimate value of the true population relationship.</li>
<li>The outcome of an estimator is an <strong>estimate</strong>, which we notate as <span class="math inline">\hat\theta_n</span>.</li>
<li>Note: if we take another sample, and run the estimator again, we will end up with a different estimate <span class="math inline">\hat\theta_n</span>. This is because another sample is unlikely to contain the same exact same individuals, so our estimate will differ between samples.</li>
</ul>
<p>Estimators have two properties: Unbiasedness and Variance.</p>
<p><br></p>
<p><strong>Unbiasedness</strong> is whether if an estimator is, on average, predicting the correct population value. Mathematically:</p>
<p><span class="math display">
E(\hat\theta_n) = \theta
</span></p>
<p>Basically, under many different samples and estimates, we on average get the population value.</p>
<ul>
<li>We want an unbiased estimator - because that means on average, we are getting the right estimate of the population parameter <span class="math inline">\theta</span>.</li>
</ul>
<p><br></p>
<p><strong>Variance</strong> is how spread out an estimator’s estimates are over different samples. It is defined as:</p>
<p><span class="math display">
Var(\hat\theta_n) = E((\hat\theta_n - E(\hat\theta_n))^2)
</span></p>
<p>Basically, under many different samples and estimates, how far apart are our different estimates?</p>
<p>Ideally, we want an estimator to be both unbiased and have low variance, since that means we are on average estimating the correct population parameter <span class="math inline">\theta</span>, and also have our estimates close together around the true parameter <span class="math inline">\theta</span>.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note how I have been using the word <strong>relationship</strong>, not causal effect.</p>
<p>Causality is not established through estimators, it is established through a strong experimental design (which we will cover in part II of the course).</p>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
</section>
<section id="simple-linear-regression" class="level1">
<h1>1.2: Simple Linear Regression</h1>
<section id="explaining-relationships-between-variables" class="level3">
<h3 class="anchored" data-anchor-id="explaining-relationships-between-variables">Explaining Relationships Between Variables</h3>
<p>How can we explore the relationship between a continuous <span class="math inline">x</span> variable and a continuous <span class="math inline">y</span> variable?</p>
<p>One way to explore the relationship is with a best-fit line. A best-fit line is useful, since the <strong>slope</strong> of the best-fit line represents the change in <span class="math inline">y</span> for every unit change in <span class="math inline">x</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%"></p>
</figure>
</div>
<p>In the figure above, we have plotted a number of observations, and implemented a best-fit line in red. The graph here introduces some common terminology:</p>
<ul>
<li>The <strong>independent variable</strong>, also called the <strong>explanatory variable</strong> or <strong>treatment variable</strong>, is the variable that we believe is doing the causing. It is typically notated <span class="math inline">x</span>.</li>
<li>The <strong>dependent variable</strong>, also called the <strong>response variable</strong> or <strong>outcome variable</strong>, is the variable that is affected by a change in the independent variable. The dependent variable is typically notated with <span class="math inline">y</span>.</li>
</ul>
<p>We know from algebra that a linear equation takes the form <span class="math inline">y=mx+b</span>. The slope <span class="math inline">m</span> describes the rate of change of <span class="math inline">y</span> for a one unit change in <span class="math inline">x</span>.</p>
<ul>
<li>Thus, the slope is a measurement of the relationship between <span class="math inline">x</span> and <span class="math inline">y</span>.</li>
</ul>
<p>Thus, if we can fit a best-fit line to our data, we can look at the slope, and determine the <strong>relationship</strong> between <span class="math inline">x</span> and <span class="math inline">y</span>.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note how I have been using the word <strong>relationship</strong>, not causal effect.</p>
<p>Causality is not established through estimators, it is established through a strong experimental design (which we will cover in part II of the course).</p>
</div>
</div>
<p><br></p>
</section>
<section id="linear-regression-model" class="level3">
<h3 class="anchored" data-anchor-id="linear-regression-model">Linear Regression Model</h3>
<p>How can we fit a best-fit line in order to find the relationship between <span class="math inline">x</span> and <span class="math inline">y</span>? The main way to do this is with the <strong>simple linear regression model</strong>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The simple linear regression model takes the following form.</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1 x_i + u_i
</span></p>
<ul>
<li>Where we have <span class="math inline">n</span> number of observations in our data, <span class="math inline">i</span> being any one of them, and each observation has an <span class="math inline">x</span> and <span class="math inline">y</span> value <span class="math inline">(x_i, y_i)</span>.</li>
<li>Where <span class="math inline">\beta_0</span> (intercept) and <span class="math inline">\beta_1</span> (slope) are coefficients of the model that need to be estimated (since they will differ in value between different samples and data).</li>
<li>Where <span class="math inline">u_i</span> is the error term. Remember, <span class="math inline">y_i</span> is any value in the conditional distribution <span class="math inline">y_i|x_i</span>, so it may not be exactly at the expected value of the conditional distribution. Thus, we have to add an error term to account for this distribution, where <span class="math inline">\mathbb{E}[u_i] = 0</span> and <span class="math inline">Var[u_i] = \sigma^2</span>.</li>
</ul>
</div>
</div>
<p><br></p>
</section>
<section id="fitted-values-and-best-fit-lines" class="level3">
<h3 class="anchored" data-anchor-id="fitted-values-and-best-fit-lines">Fitted Values and Best-Fit Lines</h3>
<p>We have discussed the form a simple linear regression takes. However, this is not the best-fit line: we still need to estimate the coefficients <span class="math inline">\beta_0</span> (intercept) and <span class="math inline">\beta_1</span> (slope) in order to create a best-fit line.</p>
<ul>
<li>The estimates of <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span> that we obtain will be denoted with a hat ^: <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span>.</li>
<li>We will discuss the estimation process in the next section.</li>
</ul>
<p>Once we have obtained our estimates of the coefficients, we will have a <strong>best-fit line</strong>, also called a <strong>fitted-values</strong> model:</p>
<p><span class="math display">
\hat{y} = \hat\beta_0 + \hat\beta_1x_i
</span></p>
<ul>
<li>Where <span class="math inline">\hat{y}</span> are the predicted values of <span class="math inline">y</span> based on our best-fit line.</li>
<li>Where <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span> are our estimates for the true coefficients <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span>.</li>
<li>Note that the error term <span class="math inline">u_i</span> disappears. This is because the average value of <span class="math inline">u_i</span> is <span class="math inline">\mathbb{E} [u_i] = 0</span>, so we do not need to include the term.</li>
</ul>
<p>The term we care most about is <span class="math inline">\hat\beta_1</span>, which is the slope of the best-fit line, which tells us the relationship between <span class="math inline">x</span> and <span class="math inline">y</span>.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
</section>
<section id="ordinary-least-squares-estimator" class="level1">
<h1>1.3: Ordinary Least Squares Estimator</h1>
<section id="sum-of-squared-errors" class="level3">
<h3 class="anchored" data-anchor-id="sum-of-squared-errors">Sum of Squared Errors</h3>
<p>We want to fit a best-fit line that is accurate. So, we want to find the <span class="math inline">\beta_0</span> (intercept) and <span class="math inline">\beta_1</span> (slope) values that will best fit our observed data.</p>
<p>One way we can fit an accurate line is to find the best-fit line that minimises the sum of squared errors.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Sum of Squared Errors
</div>
</div>
<div class="callout-body-container callout-body">
<p>The sum of squared errors (SSE) is as follows:</p>
<p><span class="math display">
\begin{split}
SSE &amp; = \sum\limits_{i=1}^n (y_i - \hat y_i)^2 \\
&amp; = \sum\limits_{i=1}^n (y_i - \hat\beta_0 - \hat\beta_1 x_i)^2
\end{split}
</span></p>
<ul>
<li>The sum of squared errors is exactly as it sounds. Find the error, the distance between the actual <span class="math inline">y_i</span> and predicted <span class="math inline">\hat y</span>, which is <span class="math inline">y_i - \hat y</span>, then square that error <span class="math inline">(y_i - \hat y_i)^2</span>, then sum up for all observations <span class="math inline">i</span> in the data.</li>
<li>We get the second equation by substituting in the fitted values model (discussed in the previous section), where <span class="math inline">\hat{y} = \hat\beta_0 + \hat\beta_1x_i</span>.</li>
</ul>
</div>
</div>
<p><br></p>
</section>
<section id="ordinary-least-squares-estimator-1" class="level3">
<h3 class="anchored" data-anchor-id="ordinary-least-squares-estimator-1">Ordinary Least Squares Estimator</h3>
<p>The Ordinary Least Squares (OLS) Estimator estimates the coefficients <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span> by finding the values of <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span> that result in the line with the smallest sum of squared errors.</p>
<p>We can describe the goal of OLS in a more mathematical way:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Ordinary Least Squares (OLS) Estimator
</div>
</div>
<div class="callout-body-container callout-body">
<p>The goal of the Ordinary Least Squares (OLS) Estimator is to find the values of <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span> that make the following statement true:</p>
<p><span class="math display">
\begin{split}
(\hat{\beta}_0, \hat{\beta}_1) &amp; = \arg \min\limits_{\hat{\beta}_0, \hat{\beta}_1} \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2 \\
&amp; = \arg \min\limits_{\hat{\alpha}, \hat{\beta}} S(\hat{\beta}_0, \hat{\beta}_1)
\end{split}
</span></p>
<p>Where function <span class="math inline">S</span> is the sum of squared errors.</p>
</div>
</div>
<p><br></p>
<p>How do we minimise <span class="math inline">S</span> (the function of the sum of squared errors)?</p>
<ul>
<li>From calculus, we know that a minimum/maximum of a function is where the derivative of the function is equal to 0.</li>
</ul>
<p>Thus, let us find the partial derivative of the function <span class="math inline">S</span> in respect to both <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span>, and set them equal to 0. This is also called the <strong>first-order conditions</strong>.</p>
<p><br></p>
</section>
<section id="first-order-conditions" class="level3">
<h3 class="anchored" data-anchor-id="first-order-conditions">First Order Conditions</h3>
<p>First, let us find the partial derivative of <span class="math inline">S</span> in respect to <span class="math inline">\hat\beta_0</span>:</p>
<p><span class="math display">
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial \hat{\beta}_0} = \frac{\partial }{\partial \hat{\beta}_0} \left[ \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2 \right]
</span></p>
<p>First, ignore the summation. The partial derivative of the internal section, using chain rule, is the following:</p>
<p><span class="math display">
\frac{\partial}{\partial \hat{\beta}_0} \left[ (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2 \right] = -2(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
</span></p>
<p>But how do we deal with the summation? We know that there is the sum rule of derivatives <span class="math inline">[f(x) + g(x)]' = f'(x) + g'(x)</span>. Thus, we know we just sum up the derivatives to get the derivative:</p>
<p><span class="math display">
\begin{split}
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial \hat{\beta}_0} &amp; = \sum\limits_{-i=1}^n \left[ -2(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) \right] \\
&amp; = -2 \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
\end{split}
</span></p>
<p>To find the value of <span class="math inline">\hat\beta_0</span> that minimises <span class="math inline">S</span>, we set the derivative equal to 0. We can ignore the -2, since if the summation is equal to 0, the whole derivative will equal 0. Thus, the first order condition is:</p>
<p><span class="math display">
\sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
</span></p>
<p><br></p>
<p>Now, let us do the same for <span class="math inline">\hat\beta_1</span>. Using the same steps as before</p>
<p><span class="math display">
\begin{split}
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial \hat{\beta}_1} &amp; = \sum\limits_{i=1}^n \left[ -2x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) \right] \\
&amp; = -2 \sum\limits_{i=1}^n x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
\end{split}
</span></p>
<p>The first order condition for <span class="math inline">\hat\beta_1</span> will be (again, ignoring the -2 for the same reason as before):</p>
<p><span class="math display">
\sum\limits_{i=1}^n x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
</span></p>
<p><br></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: First Order Conditions of OLS
</div>
</div>
<div class="callout-body-container callout-body">
<p>Thus, the first order conditions of OLS are:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
&amp; \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
</span></p>
</div>
</div>
<p><br></p>
</section>
<section id="solving-the-system-of-equations" class="level3">
<h3 class="anchored" data-anchor-id="solving-the-system-of-equations">Solving the System of Equations</h3>
<p>We now have our two first-order conditions. Now, we basically have a 2-equation system of equations, with 2 variables.</p>
<ul>
<li>We can solve this through substitution - in the first equation, solve for <span class="math inline">\hat\beta_0</span> in terms of <span class="math inline">\hat\beta_1</span>.</li>
<li>Then, plug in <span class="math inline">\hat\beta_0</span> in terms of <span class="math inline">\hat\beta_1</span> into the second equation, thus making that a one-variable equation. We can solve that equation for <span class="math inline">\hat\beta_1</span>, then find <span class="math inline">\hat\beta_0</span>.</li>
</ul>
<p><br></p>
<p>First, let us solve the first equation for <span class="math inline">\hat\beta_0</span> in terms of <span class="math inline">\hat\beta_1</span>:</p>
<p><span class="math display">
\begin{split}\sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) &amp; =  0 \\
\sum\limits_{i=1}^n y_i - n \hat{\beta}_0 - \hat{\beta}_1 \sum\limits_{i=1}^n x_i &amp; = 0 \\
-n\hat{\beta}_0 &amp;= -\sum\limits_{i=1}^n y_i + \hat{\beta}_1\sum\limits_{i=1}^nx_i \\
\hat{\beta}_0 &amp; = \frac{1}{n} \sum\limits_{i=1}^n y_i - \frac{1}{n}\hat{\beta}_1 \sum\limits_{i=1}^n x_i \\
&amp; = \bar{y} - \hat{\beta}_1 \bar{x}
\end{split}
</span></p>
<p>Now, let us substitute our calculated <span class="math inline">\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}</span> into the <span class="math inline">\hat{\beta}_1</span> condition and solve for <span class="math inline">\hat{\beta}_1</span>:</p>
<p><span class="math display">
\begin{split}0 &amp; = \sum\limits_{i=1}^n \left[ x_i(y_i - [\bar{y} - \hat{\beta}_1\bar{x}] - \hat{\beta}_1x_i) \right] \\
&amp; = \sum\limits_{i=1}^n \left[ x_i(y_i - \bar{y} - \hat{\beta}_1 (x_i - \bar{x})) \right] \\
&amp; = \sum\limits_{i=1}^n \left[ x_i(y_i - \bar{y}) - x_i \hat{\beta}_1(x_i - \bar{x}) \right] \\
&amp; = \sum\limits_{i=1}^n x_i (y_i - \bar{y}) - \hat{\beta}_1 \sum\limits_{i=1}^nx_i (x_i - \bar{x})\end{split}
</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Useful Properties of Summation
</div>
</div>
<div class="callout-body-container callout-body">
<p>To help us solve this problem, note these 3 properties of summation:</p>
<p><span class="math display">
\begin{split}&amp; \sum\limits_{i=1}^n (x_i - \bar{x}) = 0 \\&amp; \sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y}) \\&amp; \sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2\end{split}
</span></p>
</div>
</div>
<p>Knowing these properties of summation, we can transform what we had before into:</p>
<p><span class="math display">
\begin{split}
0 &amp; = \sum\limits_{i=1}^n(x_i - \bar{x})(y_i - \bar{y}) - \hat{\beta}_1 \sum\limits_{i=1}^n (x_i - \bar{x})^2 \\
\hat{\beta}_1 \sum\limits_{i=1}^n (x_i - \bar{x})^2 &amp; = \sum\limits_{i=1}^n(x_i - \bar{x})(y_i - \bar{y}) \\
\hat{\beta}_1 &amp; = \frac{\sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum\limits_{i=1}^n(x_i - \bar{x})^2}
\end{split}
</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: OLS Estimate of Coefficient
</div>
</div>
<div class="callout-body-container callout-body">
<p>Thus, the OLS estimate <span class="math inline">\hat\beta_1</span> (slope) of the linear regression model is:</p>
<p><span class="math display">
\hat{\beta}_1 = \frac{\sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum\limits_{i=1}^n(x_i - \bar{x})^2} = \frac{Cov(x, y)}{Var(x)}
</span></p>
<p>This is the expected change in <span class="math inline">y</span> given a one unit increase in <span class="math inline">x</span>.</p>
<ul>
<li>Remember, this is the <strong>relationship</strong> between <span class="math inline">x</span> and <span class="math inline">y</span>, <u><strong>not</strong> the causal effect</u>.</li>
</ul>
</div>
</div>
<p><br></p>
<p>Of course, we still need to find <span class="math inline">\hat\beta_0</span> (the slope). We found that <span class="math inline">\hat\beta_0 = \bar{y} - \hat{\beta}_1 \bar{x}</span> earlier, so we just plug that in.</p>
<p>And now, we have our estimates <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span>, and thus we now have a best-fit line and an estimate of the relationship between <span class="math inline">x</span> and <span class="math inline">y</span>.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
</section>
<section id="algebraic-properties-of-the-ols-estimator" class="level1">
<h1>1.4: Algebraic Properties of the OLS Estimator</h1>
<p>Once we have our OLS estimates <span class="math inline">\hat\beta_1</span> and <span class="math inline">\hat\beta_1</span>, we can obtain both our fitted values (explained previously), and the OLS residuals <span class="math inline">\hat u_i</span>:</p>
<p><span class="math display">
\hat u_i \equiv y_i - \hat y_i = y_i - \hat\beta_0 - \hat\beta_1 x_i
</span></p>
<ul>
<li>Important note, the residual <span class="math inline">\hat u_i</span> is different from the error <span class="math inline">u_i</span>.</li>
<li>The residual <span class="math inline">\hat u_i</span> is after we have calculated our best-fit line, and found the distance between that and our predicted points.</li>
<li>The error is prior to the estimation, if we recall from the simple linear model, it is the distance of <span class="math inline">y_i</span> to the conditional expected value <span class="math inline">\mathbb{E}[y_i | x_i]</span>.</li>
</ul>
<p>The figure below shows what a residual for a point looks like: the distance between the best fit line</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/1-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:55.0%"></p>
</figure>
</div>
<p><br></p>
<p>Recall that the OLS estimates <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span> are chosen to satisfy the following first order conditions:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
&amp; \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
</span></p>
<p>We can plug in <span class="math inline">\hat u_i = y_i - \hat\beta_0 - \hat\beta_1 x_i</span> to obtain a few properties.</p>
<p>First, OLS residuals always add up to zero, since:</p>
<p><span class="math display">
\sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 = \sum\limits_{i=1}^n \hat u_i
</span></p>
<ul>
<li>This property ensures that the average value of <span class="math inline">y_i</span> in our data is the same as the average value of our predictions <span class="math inline">\hat y_i</span>.</li>
<li>This also means that the OLS best-fit line always goes through the “middle” of the data.</li>
</ul>
<p><br></p>
<p>Second, the sample covariance (and correlation) between regressor and residual is always zero, since:</p>
<p><span class="math display">
\sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 = \sum\limits_{i=1}^n x_i \hat u_i
</span></p>
<p><br></p>
<p>Third, the regression always passes through the means.</p>
<p>Remember our solution for <span class="math inline">\hat\beta_0</span> in OLS was <span class="math inline">\hat\beta_0 = \bar{y} - \hat{\beta}_1 \bar{x}</span>. Rearranging this equation, we get:</p>
<p><span class="math display">
\begin{split}
\hat\beta_0  &amp; = \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat\beta_0 + \hat\beta_1 \bar x &amp; = \bar y \\
\bar y &amp; = \hat\beta_0 + \hat\beta_1 \bar x
\end{split}
</span></p>
<p>Thus, the OLS estimated best-fit line always passes though point <span class="math inline">(\bar x, \bar y)</span>.</p>
<p><br></p>
<p>Finally, recall that our <span class="math inline">\hat\beta_1</span> solution of OLS is:</p>
<p><span class="math display">
\hat{\beta}_1 = \frac{Cov(x, y)}{Var(x)}
</span></p>
<ul>
<li>This tells us that for <span class="math inline">\hat\beta_1</span> to exist, <span class="math inline">Var(x) ≠ 0</span>.</li>
<li>This is the only assumption that must be met in order to calculate OLS estimates (however, other assumptions will be needed for the causal interpreation of OLS estimates, covered in section).</li>
</ul>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="interpretation-and-reparamaterisation" class="level1">
<h1>1.5: Interpretation and Reparamaterisation</h1>
<p>We now have estimated <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span>. But what do these actually mean in the context of the relationship between <span class="math inline">x</span> and <span class="math inline">y</span>?</p>
<ul>
<li>Let us start with <span class="math inline">\hat\beta_1</span>, which is the slope, the more important of the two coefficients.</li>
</ul>
<p><br></p>
<section id="interpretation-of-hatbeta_1" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-of-hatbeta_1">Interpretation of <span class="math inline">\hat\beta_1</span></h3>
<p>We know that in a linear model, <span class="math inline">\mathbb{E}[y_i|x_i] = \beta_0 + \beta_1 x_i</span>, the coefficient <span class="math inline">\beta_1</span> is the slope.</p>
<ul>
<li>And the slope is the change in <span class="math inline">y</span> given a one unit increase in <span class="math inline">x</span>.</li>
</ul>
<p>Using this knowledge, we can interpret estimate <span class="math inline">\hat\beta_1</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of <span class="math inline">\hat\beta_1</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>When <span class="math inline">x</span> increases by one unit, there is an expected <span class="math inline">\hat{\beta}_1</span> unit change in <span class="math inline">y</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note how I have been using the word <strong>relationship</strong>, not causal effect.</p>
<p>Causality is not established through estimators, it is established through a strong experimental design (which we will cover in part II of the course).</p>
</div>
</div>
<p>Note that this interpretation of <span class="math inline">\hat\beta_1</span> only applies to continuous <span class="math inline">x</span> variables and continuous/ordinal <span class="math inline">y</span> variables.</p>
<p><br></p>
</section>
<section id="interpretation-of-hatbeta_1-for-binary-x" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-of-hatbeta_1-for-binary-x">Interpretation of <span class="math inline">\hat\beta_1</span> for Binary <span class="math inline">x</span></h3>
<p><strong>Binary explanatory</strong> variables are variables with 2 values, 0 and 1.</p>
<ul>
<li>Binary explanatory variables are extremely common in the social sciences. They can include things such as yes/no questions, treatment/control, true/false questions, voted/did not vote, etc.</li>
</ul>
<p>Binary explanatory variables will change the interpretations of our coefficients.</p>
<p>We can “solve” for these interpretations. Assume <span class="math inline">x</span> has two categories <span class="math inline">x=0</span> and <span class="math inline">x=1</span>:</p>
<p><span class="math display">
\begin{split}
&amp; \hat y_{i, \ x = 0} = \hat\beta_0 + \hat\beta_1(0) = \hat\beta_0 \\
&amp; \hat y_{i, \ x = 1} = \hat\beta_0 + \hat\beta_1(1) = \hat\beta_0 + \hat\beta_1 \\
&amp; \hat y_{i, \ x = 1} - \hat y_{i, \ x = 0} = (\hat\beta_0 + \hat\beta_1) - \hat\beta_0 = \hat\beta_1
\end{split}
</span></p>
<p>Thus, we can interpret the coefficients as follows:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of Coefficient with a Binary Explanatory Variable
</div>
</div>
<div class="callout-body-container callout-body">
<p>When <span class="math inline">x</span> is a binary explanatory variable:</p>
<ul>
<li><span class="math inline">\hat\beta_0</span> is the expected value of <span class="math inline">y</span> given an observation in category <span class="math inline">x = 0</span></li>
<li><span class="math inline">\hat\beta_0 + \hat\beta_1</span> is the expected value of <span class="math inline">y</span> given an observation in category <span class="math inline">x = 1</span></li>
<li><span class="math inline">\hat\beta_1</span> is the expected difference in <span class="math inline">y</span> between the categories <span class="math inline">x=1</span> and <span class="math inline">x=0</span>.</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note how I have been using the word <strong>relationship</strong>, not causal effect.</p>
<p>Causality is not established through estimators, it is established through a strong experimental design (which we will cover in part II of the course).</p>
</div>
</div>
<p><br></p>
</section>
<section id="interpretation-of-hatbeta_0" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-of-hatbeta_0">Interpretation of <span class="math inline">\hat\beta_0</span></h3>
<p>We know that in a linear model, <span class="math inline">\mathbb{E}[y_i|x_i] = \beta_0 + \beta_1 x_i</span>, the coefficient <span class="math inline">\beta_0</span> is the y-intercept.</p>
<ul>
<li>And the y-intercept is the change value of <span class="math inline">y</span> given <span class="math inline">x=0</span>.</li>
</ul>
<p>We can prove this mathematically:</p>
<p><span class="math display">
\begin{split}
\hat y_{i, \ x_i = 0} &amp; = \hat\beta_0 + \hat\beta_1 x_i \\
&amp; = \hat\beta_0 + \hat\beta_1(0) \\
&amp; = \hat\beta_0
\end{split}
</span></p>
<p>Thus, knowing this, we can interpret <span class="math inline">\hat\beta_0</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of <span class="math inline">\hat\beta_0</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>When <span class="math inline">x=0</span>, the expected value of <span class="math inline">y</span> is <span class="math inline">\hat{\beta}_0</span></p>
</div>
</div>
<p><br></p>
</section>
<section id="reparamatising-beta_1-in-terms-of-standard-deviations" class="level3">
<h3 class="anchored" data-anchor-id="reparamatising-beta_1-in-terms-of-standard-deviations">Reparamatising <span class="math inline">\beta_1</span> in Terms of Standard Deviations</h3>
<p>Sometimes, it is hard to understand what changes in <span class="math inline">y</span> and <span class="math inline">x</span> mean in terms of units. For example, if we are measuring “democracy”, what does a 5 unit change in democracy mean? Is that a lot?</p>
<p>We can add more relevant detail by expressing the change of <span class="math inline">y</span> and <span class="math inline">x</span> in standard deviations.</p>
<p>How do we calculate this? Well, let us solve for the change in <span class="math inline">\hat{y}_i/\sigma_y</span> given <span class="math inline">x_i = x</span> and <span class="math inline">x = x + \sigma_X</span>. This will tell us how much <span class="math inline">\hat{y}</span> changes by given a increase of one standard deviation in <span class="math inline">x</span>:</p>
<p><span class="math display">
\begin{split}
\frac{\hat y_{i, \ x_i = x + \sigma_x}}{\sigma_y} - \frac{\hat y_{i, \ x_i = x}}{\sigma_y} &amp; = \frac{\hat\beta_0 + \hat\beta_1 x_i}{\sigma_y} - \frac{\hat\beta_0 + \hat\beta_1 x_i}{\sigma_y} \\
&amp; = \frac{\hat\beta_0 + \hat\beta_1 (x+\sigma_x) - (\hat\beta_0 + \hat\beta_1 (x))}{\sigma_y} \\
&amp; = \frac{\hat\beta_0 - \hat\beta_0 + \hat\beta_1x - \hat\beta_1x+\hat\beta_1\sigma_x}{\sigma_y} \\
&amp; = \frac{\hat\beta_1 \sigma_x}{\sigma_y}
\end{split}
</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation in Terms of Standard Deviation
</div>
</div>
<div class="callout-body-container callout-body">
<p>For a one-std. deviation increase in <span class="math inline">x</span>, there is an expected <span class="math inline">\hat{\beta}_1 \sigma_x / \sigma_y</span>-std. deviation change in <span class="math inline">Y</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note how I have been using the word <strong>relationship</strong>, not causal effect.</p>
<p>Causality is not established through estimators, it is established through a strong experimental design (which we will cover in part II of the course).</p>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
</section>
<section id="r-squared-and-goodness-of-fit" class="level1">
<h1>1.6: R-Squared and Goodness of Fit</h1>
<p>For each observation, we know that the actual <span class="math inline">y_i</span> value is the predicted <span class="math inline">\hat y_i</span> plus the residual term <span class="math inline">\hat u_i</span>. Thus:</p>
<p><span class="math display">
y_i = \hat y_i + \hat u_i
</span></p>
<p>Now, let us define these three concepts: the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR):</p>
<p><span class="math display">
\begin{split}
&amp; SST = \sum\limits_{i=1}^n (y_i - \bar y)^2 \\
&amp; SSE = \sum\limits_{i=1}^n (\hat y_i - \bar y)^2 \\
&amp; SSR = \sum\limits_{i=1}^n (\hat u_i)^2
\end{split}
</span></p>
<ul>
<li>The SST explains the total amount of variation in <span class="math inline">y</span></li>
<li>The SSE is the amount of variation in <span class="math inline">y</span> explained by our model</li>
<li>The SSR is the amount of variation in <span class="math inline">y</span> not explained by our model</li>
</ul>
<p>Let us look at the total sum of squares (SST). We can manipulate it as follows:</p>
<p><span class="math display">
\begin{split}
SST &amp; = \sum\limits_{i=1}^n (y_i - \bar y)^2 \\
&amp; = \sum\limits_{i=1}^n(y_i - \hat y_i+ \hat y_i - \bar y)^2 \\
&amp; = \sum\limits_{i=1}^n[\hat u_i + (\hat y_i - \bar y)]^2 \\
\end{split}
</span></p>
<p>And since we know <span class="math inline">\sum \hat y_i \hat u_i = 0</span>, we can further simplify to:</p>
<p><span class="math display">
\begin{split}
SST &amp; = \sum\limits_{i=1}^n u_i^2 + \sum\limits_{i=1}^n(\hat y_i - \bar y)^2 \\
&amp; = SSE + SSR
\end{split}
</span></p>
<p>This makes sense: After all, SSE is the squared errors explained by the model, and SSR is the residual (non-explained) parts of the model, so together, they should be equal to the total sum of squares.</p>
<p><br></p>
<p>Using these properties, we can create a statistic which explains how well our model explains the variation in <span class="math inline">y</span>. This statistic is called <span class="math inline">R^2</span>:</p>
<p><span class="math display">
R^2 = \frac{SSE}{SST} = 1 - \frac{SSR}{SST}
</span></p>
<p>What does this <span class="math inline">R^2</span> value mean?</p>
<ul>
<li>Well SSE is the amount of variation in <span class="math inline">y</span> explained by our model, and SST is the total amount of variation in <span class="math inline">y</span>.</li>
<li>Thus, <span class="math inline">R^2</span> is the proportion of variation in <span class="math inline">y</span> explained by our model.</li>
</ul>
<p><span class="math inline">R^2</span> is always between 0 and 1:</p>
<ul>
<li>This is because it is a proportion, so and <span class="math inline">0 ≤ SSE ≤ SST</span>, so this must be true.</li>
<li>Values closer to 1 mean our model explains the variance in <span class="math inline">y</span> more</li>
<li>Values closer to 0 mean our model explains less of the variance in <span class="math inline">y</span>.</li>
</ul>
<p>However, be careful when using <span class="math inline">R^2</span>. Just because it is high, does not mean we can infer anything from it.</p>
<p>Extra note, <span class="math inline">R^2</span> is also equal to the correlation coefficient between <span class="math inline">y_i</span> and <span class="math inline">\hat y_i</span>.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="gauss-markov-and-unbiasedness-of-the-ols-estimator" class="level1">
<h1>1.7: Gauss-Markov and Unbiasedness of the OLS Estimator</h1>
<p>An unbiased estimator, if we recall from section 1.1, means that over many different estimates, the expected value of all the estimates is the true parameter value: <span class="math inline">E(\hat{\theta}_n) = \theta</span>.</p>
<p>Unbiasedness is desirable property of causal estimators. OLS (for simple linear regression) is an unbiased estimator under 4 conditions:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Gauss-Markov Theorem for Unbiasdness
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>Gauss-Markov Theorem</strong> states that the ordinary least squares estimator is <strong>unbiased</strong> under 3 conditions:</p>
<ol type="1">
<li><strong>Linearity</strong> of the equation (no coefficients <span class="math inline">\beta_0, ..., \beta_k</span> can be multiplied together. This does not apply to explanatory variables, only the coefficients.</li>
<li>Random Sampling from the population.</li>
<li>Explanatory variable <span class="math inline">x</span> has <strong>variation</strong> (so not all values of <span class="math inline">x</span> are the same).</li>
<li><u>And the most important condition - <strong>Zero conditional mean</strong></u>: meaning that no matter the value of <span class="math inline">x</span>, the expected value of the residual <span class="math inline">u_i</span> is always 0. <span class="math inline">E(u|x) = 0</span> for all <span class="math inline">x</span>.</li>
</ol>
</div>
</div>
<p><br></p>
<section id="proof-of-the-unbiasedness-of-ols" class="level3">
<h3 class="anchored" data-anchor-id="proof-of-the-unbiasedness-of-ols">Proof of the Unbiasedness of OLS</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Useful Properties of Summation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before we start, here are a few key properties of summation</p>
<p><span class="math display">
\begin{split}&amp; \sum\limits_{i=1}^n (x_i - \bar{x}) = 0 \\&amp; \sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y}) \\&amp; \sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2\end{split}
</span></p>
</div>
</div>
<p>We want to show <span class="math inline">E(\beta_1) = \beta_1</span>. Let us start off with the OLS estimator (we will use simple linear regression for simplicity):</p>
<p><span class="math display">
\hat{\beta}_1 = \frac{\sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum\limits_{i=1}^n(x_i - \bar{x})^2}
</span></p>
<p>We can expand the numerator, and since <span class="math inline">\sum(x_i - \bar{x}) \bar{y} = \bar{y} \sum(x_i - \bar{x}) = 0</span> (see properties of summation above), we can get:</p>
<p><span class="math display">
\hat{\beta}_1 = \frac{\sum\limits_{i=1}^n (x_i - \bar{x})y_i}{\sum\limits_{i=1}^n(x_i - \bar{x})^2}
</span></p>
<ul>
<li>The existence of <span class="math inline">\hat{\beta}_1</span> is guaranteed by condition 2 from above, since the denominator is equal to <span class="math inline">Var(x)</span>, so that must not be 0.</li>
</ul>
<p><br></p>
<p>Now, let us play with the numerator (note the properties of summation introduced earlier):</p>
<p><span class="math display">
\begin{split}\sum\limits_{i=1}^n (x_i - \bar{x})y_i &amp; = \sum\limits_{i=1}^n(x_i - \bar{x})(\beta_0 + \beta_1 x_i + u_i) \\&amp; = \beta_0 \sum\limits_{i=1}^n(x_i - \bar{x}) + \beta_1 \sum\limits_{i=1}^n(x_i - \bar{x}) x_i + \sum\limits_{i=1}^n (x_i - \bar{x}) u_i \\&amp; = 0 + \beta_1 \sum\limits_{i=1}^n(x_i - \bar{x})^2 + \sum\limits_{i=1}^n(x_i - \bar{x})u_i\end{split}
</span></p>
<p>Now, putting the numerator back into the equation, we simplify:</p>
<p><span class="math display">
\begin{split}
\hat{\beta}_1 &amp; = \frac{\beta_1 \sum\limits_{i=1}^n(x_i - \bar{x})^2 + \sum\limits_{i=1}^n(x_i - \bar{x})u_i}{\sum\limits_{i=1}^n(x_i - \bar{x})^2} \\
&amp;  =  \beta_1 + \frac{\sum\limits_{i=1}^n(x_i - \bar{x}) u_i}{\sum\limits_{i=1}^n (x_i - \bar{x})^2} \\
&amp;  = \beta_1 + \sum\limits_{i=1}^n d_i u_i
\end{split}
</span></p>
<ul>
<li>Where <span class="math inline">d_i = \frac{x_i - \bar{x}}{\sum (x_i - \bar{x})^2}</span>, which is a function of random variable <span class="math inline">x</span>.</li>
</ul>
<p><br></p>
<p>Now we need to find the expectation <span class="math inline">E(\beta_1)</span>.</p>
<p>First, do not worry about <span class="math inline">d_i</span>. What should <span class="math inline">u_i</span> be equal to? Naturally, the best estimate of <span class="math inline">u_i</span> is its expected utility.</p>
<p><span class="math display">
\mathbb{E} (\hat{\beta}_1 | x) = \beta_1 + \sum\limits_{i=1}^n d_i \ E(u_i | x)
</span></p>
<p>We know by the fourth Gauss-Markov condition (Zero conditional mean), that <span class="math inline">E(u|x) = 0</span>. Because these are randomly sampled (2nd Gauss-Markov condition), we also know <span class="math inline">E(u|x) = E(u_i | x_i) = E(u_i | x)</span> Thus, that makes the entire summation equal to 0:</p>
<p><span class="math display">
\mathbb{E}(\hat{\beta}_1 | x) = \mathbb{E}(\hat{\beta}_1) = \beta_1
</span></p>
<p>Thus, the ordinary least squares estimator is unbiased, given 4 conditions (linearity, random sampling, variation in <span class="math inline">x</span>, zero-conditional mean) are met.</p>
<ul>
<li>If conditions are met, we have an unbiased estimator in OLS.</li>
<li><u>The key issue is assumption 3: Zero conditional mean <span class="math inline">E(u | x) = 0</span></u>.</li>
</ul>
<p><br></p>
<p>What causes this final assumption, zero conditional mean <span class="math inline">E(u|x) = 0</span>, to be violated?</p>
<ol type="1">
<li>Omitted Variable Bias. This is because if we have any other variable correlated with <span class="math inline">y</span> and <span class="math inline">x</span> in the error term <span class="math inline">u_i</span>, <span class="math inline">E(u|x)</span> will be violated. (we will discuss this more in chapter 2)</li>
<li>Measurement Error in our variables (this is not discussed in this course, but the other course <em>Advanced Quantitative Methods</em> explores measurement).</li>
<li>Sample Selection is not random.</li>
<li>Simultaneity, when <span class="math inline">x</span> has an effect on <span class="math inline">y</span> and <span class="math inline">y</span> has an effect on <span class="math inline">x</span>.</li>
</ol>
<p>What can we do to remedy this assumption so OLS is not biased?</p>
<ol type="1">
<li>Add control variables with multiple linear regression (discussed in chapter 2)</li>
<li>Use instrumental variables estimation to address the entire problem (discussed in chapter 6).</li>
</ol>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>