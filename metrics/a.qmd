---
title: "Chapter 1: Correlations and Basics of Regression"
subtitle: "Econometric Methods (for Social Scientists)"
format:
    html:
        page-layout: article
        grid:
          sidebar-width: 300px
          body-width: 750px
          margin-width: 250px
          gutter-width: 2em
        fontsize: 12.5pt
        theme: yeti
        toc: TRUE
        toc-depth: 2
        toc-location: left
        toc-expand: true
        toc-title: "Chapter 1: Correlations and Basics of Regression"
        mainfont: "Computer Modern"
        title-block-banner: true
        classoption: fleqn
        html-math-method: katex
---

[Course Homepage](https://politicalscience.github.io/metrics/)

# 1.7: The Multiple Linear Regression Model

In our simple linear regression model, we only have one explanatory variable $x$. However, in many circumstances, we might want additional explanatory variables.

1.  More explanatory variables tends to increase the predictive accuracy of $\hat y$. For example, if we were trying to predict someone's debt by only their education level, that will not be very accurate. But if we add other variables like income, age, credit rating, etc., we will get a far better prediction.
2.  Including other explanatory variables allows us to look at the relationship between one of them and $y$, and control for the relationship of other variables. We will cover this in far more detail in [chapter 4](https://politicalscience.github.io/metrics/1.html).

<br />

The **response variable** (outcome variable) is notated $y$, just like in single linear regression.

The **explanatory variable**s are $x_1, x_2, ..., x_k$. We sometimes also denote all explanatory variables as the vector $\overrightarrow{x}$. Our treatment variable $D$ is considered one of the explanatory variables $\overrightarrow{x}$ (most often $x_1$).

A linear regression model is the specification of the conditional distribution of $Y$, given $\overrightarrow{x}$. The linear regression model focuses on the **expected value** of the conditional distribution, notated $\mathbb{E}[y_i|\overrightarrow{x}_i]$.

<br />

::: callout-tip
## Definition: Multiple Linear Regression Model

Take a set of observed data with $n$ number of pairs of $(\overrightarrow{x}_i, y_i)$ observations. The linear model takes the following form:

$$
\mathbb{E}[y_i|\overrightarrow{x}_i] = \beta_0 + \beta_1 x_{1i} + ... + \beta_k x_{ki}
$$

-   Where the coefficients (that need to be estimated) are vector$\overrightarrow{\beta} = \beta_0, \beta_1, ..., \beta_k$.

We can also write the linear model for the value of any point $y_i$ in our data:

$$
y_i = \beta_0 + \beta_1x_{1i} + ... + \beta_k x_{ki} + u_i
$$

-   Where $u_i$ is the error term function - that determines the error for each unit $i$. Error $u_i$ has a variance of $\sigma^2$, and expectation $\mathbb{E}[u_i] = 0$.
:::

<br />

We can also represent the multiple linear regression model in linear algebra. Let us start with the linear model:

$$
y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + ... + \beta_k x_{ki} + u_i
$$

The $i$'th observation can be re-written in vector form as following:

$$
y_i = x_i'\beta + u_i, \text{ where }\beta = \begin{bmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_k\end{bmatrix} \text{ and }x_i = \begin{bmatrix}1 \\x_{1i} \\\vdots \\x_{ki}\end{bmatrix}
$$

-   The $x_i'$ in the equation is the transpose of $x_i$, to make matrix multiplication possible.

-   The first element of the $x_i$ matrix is 1, since $1 \times \beta_0$ gives us the first parameter (intercept) in the linear model.

<br />

Since our data has $n$ number of observations $i$, we can express this into vector form, with the $x_i'$ and $\beta$ being vectors within a vector.

$$
\begin{split}
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} & = \begin{pmatrix}x_1'\beta + u_1 \\ x_2'\beta + u_2 \\ \vdots \\ x_n'\beta + u_n\end{pmatrix} \\
& \\
& = \begin{pmatrix}x_1'\beta \\ x_2'\beta \\ \vdots \\ x_n'\beta\end{pmatrix} + \begin{pmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{pmatrix}
\end{split}
$$

Since $\beta$ vector appears as a common factor for all observations $i=1,...,n$, we can factor it out and have an equation:

$$
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} = \begin{pmatrix}x_1' \\ x_2' \\ \vdots \\ x_n'\end{pmatrix} \space \beta + \begin{pmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{pmatrix}
$$

<br />

We can expand the $x_1',...,x_n'$ vector into a matrix. Remember that each $x_1',...,x_n'$ is already a vector of different explanatory variables. So, we get the following result:

::: callout-tip
## Definition: Multiple Linear Regression with Linear Algebra

The multiple linear regression can be expressed in linear algebra as:

$$
y = X \beta + u, \text{ where } X = \begin{bmatrix}1 & x_{21} & \dots & x_{k1} \\1 & x_{22} & \dots & x_{k2} \\\vdots & \vdots & \vdots & \vdots \\1 & x_{2n} & \dots & x_{kn}\end{bmatrix}
$$

-   Where the notation for elements of $X$ is $x_{ki}$, with $i$ being the unit of observation $i = 1, \dots n$, and $k$ being the explanatory variables index.
-   Where $y$ and $u$ are $n \times 1$ vectors (as seen above), and $\beta$ is a $k \times 1$ vector.
-   The first column of $X$ is a vector of 1, which exists because these 1's are multiplied with $\beta_0$ in our model.
:::

The point of expressing the model in linear algebra is that it makes the estimation process far easier, as we will see in the next section.

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 1.8: Ordinary Least Squares Estimator for Multiple Regression

As we remember from Chapter 1, the goal of Ordinary Least Squares Estimation is to minimise the sum of squared errors. The sum of squared errors in multiple regression is:

$$
\begin{split}
SSE & = \sum\limits_{i=1}^n (y_i - \hat y_i)^2\\
& = \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i} -  ... - \hat\beta_kx_{ki})^2
\end{split}
$$

Similar to our simple linear regression (but with additional variables), our minimisation condition is:

$$
\begin{split}
(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...) & = \arg \min\limits_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i} ...)^2 \\
& = \arg \min\limits_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)} S(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)
\end{split}
$$

<br />

Taking the partial derivatives of each parameter like in simple linear regression, we get first order conditions:

$$
\begin{split}& \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\& \sum\limits_{i=1}^n X_{1i}(y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\
& \sum\limits_{i=1}^n X_{2i} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0
\end{split}
$$

-   and so on for $x_{3i}, ..., x_{ki}$.

This system of equations includes $k+1$ variables and $k+1$ equations, which is way too difficult to solve.

<br />

Instead, we can use linear algebra. Let us define our estimation vector $\hat{\beta}$ as the value of $\hat\beta$ that minimises the sum of squared errors:

$$
\hat{\beta} = \arg \min\limits_{b} (y - Xb)' (y - Xb) = \arg \min\limits_b S(b)
$$

We can expand $S(b)$ as follows:

$$
\begin{split}
S(b) & = y'y - b'X'y - y'Xb + b'X'Xb \\ 
& = y'y - 2b'X'y + b'X'Xb
\end{split}
$$

Taking the partial derivative in respect to $b$:

$$
\frac{\partial S(b)}{\partial b} = \begin{pmatrix}\frac{\partial S(b)}{\partial b_1} \\\vdots \\\frac{\partial S(b)}{\partial b_k}\end{pmatrix}
$$

Differentiating with the vector $b$ yields:

$$
\frac{\partial S(b)}{\partial b} = -2X'y + 2X'Xb
$$

Evaluted at $\hat{\beta}$, the derivatives should equal zero (since first order condition of finding minimums):

$$
\frac{\partial S(b)}{\partial b} \biggr|_{\hat{\beta}} = -2X'y + 2X'X \hat{\beta} = 0
$$

When assuming $X'X$ is invertable, we can isolate $\hat{\beta}$ to find the solution to OLS:

::: callout-tip
## Definition: OLS Estimate of $\hat\beta$

The Ordinary Least Squares Estimate of vector $\hat\beta$ for multiple linear regression is:

$$
\hat{\beta} = (X'X)^{-1} X'y
$$
:::

Once we have estimates of $\hat{\beta}$, we can plug them into our linear model to obtain fitted values:

$$
\hat{y} = X\hat{\beta} = X(X'X)^{-1} X'y
$$

<br />

::: callout-tip
## Interpretation of Coefficients in Multiple Regression

Our interpretations of all $\hat\beta_1 , ... , \hat\beta_k$ that are multipled to variables $x_1, ..., x_k$ are the same as in single linear regression - except we add the phrase: *"When controlling for all other explanatory variables*".

-   We will explore what this means in more detail in [chapter 4](https://politicalscience.github.io/metrics/1.html).

The $\hat\beta_0$ is the expected value of $y$ when all explanatory variables equal 0.
:::

::: callout-warning
## Warning!

Remember, this is the **relationship** between $x$ and $y$, [**not** the causal effect]{.underline}.
:::

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# Implementation in R

The packages we will need are:

```{r, message = FALSE}
library(tidyverse)
library(fixest)
library(texreg)
```

```{r, echo = FALSE, message = FALSE}
setwd("/Users/kevinli/Documents/GitHub/politicalscience.github.io/metrics")
dta <- read_csv("data/olken_data.csv")
```

<br />

## Multiple Linear Regression in R

To run simple linear regression, we use the *feols()* function.

-   The argument *se = "hetero"* tells R to calculate heteroscedasticity-robust standard errors, which will be discussed later in chapter 4. Just know it is standard to do so.
-   We can add more explanatory variables by using + and then add more. We can reduce the number of explanatory variables down to one.

```{r, eval = FALSE}
modelname <- feols(Y ~ X1 + X2 + X3, data = mydata, se = "hetero")
summary(modelname)
```

For example:

```{r, message = FALSE}
model1 <- feols(pct_missing ~ treat_invite + head_edu + mosques + pct_poor,
                data = dta, se = "hetero")
summary(model1)
```

We can see the output estimate of the intercept and all the coefficients.

-   These rows include the estimate, the standard error, the t-test statistic, and the p-value. This gives all of the information we need to run linear regression and hypothesis tests.

For how to do categorical variables, consult chapter 1.

<br >

We can also use the base-R *lm()* function, however, this does not calculate heteroscedasticity-robust standard errors (once again, will be discussed in chapter 4).

```{r, eval = FALSE}
modelname <- lm(y ~ x1 + x2, data = mydata)
summary(modelname)
```

<br />

## Creating Regression Tables

We can create regression tables using the *texreg()* or *screenreg()* functions.

-   *texreg()* produces LaTeX code that you can insert into a LaTeX document
-   *screenreg()* produces something that looks nice in a R document.

The syntax is as follows (you can replace *screenreg()* with *texreg()* ):

```{r, eval = FALSE}
screenreg(l = list(modelname),
  custom.model.names = c("Outcome Variable Name"),
  custom.coef.names = c("Intercept", "X1 Variable Name", "X2 Variable Name"),
  digits = 3)
```

For example:

```{r}
screenreg(l = list(model1),
  custom.model.names = c("Pct_Missing"),
  custom.coef.names = c("Intercept", "Treatment", "head_edu", "mosques", "pct-poor"),
  digits = 3)
```

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)
