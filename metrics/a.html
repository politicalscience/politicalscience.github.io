<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Chapter 1: Correlations and Basics of Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="a_files/libs/clipboard/clipboard.min.js"></script>
<script src="a_files/libs/quarto-html/quarto.js"></script>
<script src="a_files/libs/quarto-html/popper.min.js"></script>
<script src="a_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="a_files/libs/quarto-html/anchor.min.js"></script>
<link href="a_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="a_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="a_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="a_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="a_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Chapter 1: Correlations and Basics of Regression</h1>
            <p class="subtitle lead">Econometric Methods (for Social Scientists)</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Chapter 1: Correlations and Basics of Regression</h2>
   
  <ul class="collapse">
  <li><a href="#correlations-and-measuring-relationships-between-variables" id="toc-correlations-and-measuring-relationships-between-variables" class="nav-link active" data-scroll-target="#correlations-and-measuring-relationships-between-variables">1.1: Correlations and Measuring Relationships Between Variables</a></li>
  <li><a href="#the-simple-linear-regression-model" id="toc-the-simple-linear-regression-model" class="nav-link" data-scroll-target="#the-simple-linear-regression-model">1.2: The Simple Linear Regression Model</a></li>
  <li><a href="#fitted-values-and-the-sum-of-squared-errors" id="toc-fitted-values-and-the-sum-of-squared-errors" class="nav-link" data-scroll-target="#fitted-values-and-the-sum-of-squared-errors">1.3: Fitted Values and the Sum of Squared Errors</a></li>
  <li><a href="#mathematics-of-the-ordinary-least-squares-estimator" id="toc-mathematics-of-the-ordinary-least-squares-estimator" class="nav-link" data-scroll-target="#mathematics-of-the-ordinary-least-squares-estimator">1.4: Mathematics of the Ordinary Least Squares Estimator</a></li>
  <li><a href="#interpretation-of-ols-coefficient-estimates" id="toc-interpretation-of-ols-coefficient-estimates" class="nav-link" data-scroll-target="#interpretation-of-ols-coefficient-estimates">1.5: Interpretation of OLS Coefficient Estimates</a></li>
  <li><a href="#binary-outcome-variables-and-the-linear-probability-model" id="toc-binary-outcome-variables-and-the-linear-probability-model" class="nav-link" data-scroll-target="#binary-outcome-variables-and-the-linear-probability-model">1.6: Binary Outcome Variables and the Linear Probability Model</a></li>
  <li><a href="#the-multiple-linear-regression-model" id="toc-the-multiple-linear-regression-model" class="nav-link" data-scroll-target="#the-multiple-linear-regression-model">1.7: The Multiple Linear Regression Model</a></li>
  <li><a href="#ordinary-least-squares-estimator-for-multiple-regression" id="toc-ordinary-least-squares-estimator-for-multiple-regression" class="nav-link" data-scroll-target="#ordinary-least-squares-estimator-for-multiple-regression">1.8: Ordinary Least Squares Estimator for Multiple Regression</a></li>
  <li><a href="#implementation-in-r" id="toc-implementation-in-r" class="nav-link" data-scroll-target="#implementation-in-r">Implementation in R</a>
  <ul class="collapse">
  <li><a href="#multiple-linear-regression-in-r" id="toc-multiple-linear-regression-in-r" class="nav-link" data-scroll-target="#multiple-linear-regression-in-r">Multiple Linear Regression in R</a></li>
  <li><a href="#creating-regression-tables" id="toc-creating-regression-tables" class="nav-link" data-scroll-target="#creating-regression-tables">Creating Regression Tables</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>This chapter covers how we can quantify the relationships between variables with linear regression. This chapter starts with an analysis of how simple linear regression can be used to measure the correlation between variables. Then, we discuss the idea and mechanics of multiple linear regression.</p>
<p>Topics: Correlation, Simple Linear Regression, Ordinary Least Squares Estimator, Interpretation of Regression Coefficients, Multiple Linear Regression.</p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
<section id="correlations-and-measuring-relationships-between-variables" class="level1">
<h1>1.1: Correlations and Measuring Relationships Between Variables</h1>
<p>Econometrics is the field of using data to answer economic and social science questions. While econometrics was initially developed for economic questions, it has become widely applied to other social sciences.</p>
<p>Econometrics is often concerned with causal questions:</p>
<ul>
<li>What is the effect of years of education on income?</li>
<li>What is the effect of democracy/dictatorship on economic growth and development?</li>
<li>What is the effect of income on the likelihood of someone turning out to vote?</li>
</ul>
<p><br></p>
<section id="correlation-is-not-causation" class="level3">
<h3 class="anchored" data-anchor-id="correlation-is-not-causation">Correlation is not Causation</h3>
<p>Variables are <strong>correlated</strong> with each other, if one variable changing is associated with another variable changing on average.</p>
<ul>
<li>For example, if <span class="math inline">x</span> increases, the average <span class="math inline">y</span> value increases, then <span class="math inline">x</span> and <span class="math inline">y</span> are correlated.</li>
<li>The opposite is also true: if <span class="math inline">x</span> increases, the average <span class="math inline">y</span> value decreases, then <span class="math inline">x</span> and <span class="math inline">y</span> are correlated.</li>
</ul>
<p>For example, the graphs below show correlations between variables:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/1-2.webp" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Sources of Correlation
</div>
</div>
<div class="callout-body-container callout-body">
<p>There are three main reasons why two variables <span class="math inline">x</span> and <span class="math inline">y</span> may be correlated:</p>
<ol type="1">
<li>There is a causal effect of <span class="math inline">x</span> on <span class="math inline">y</span></li>
<li>A third variable, <span class="math inline">w</span>, causes both <span class="math inline">x</span> and <span class="math inline">y</span> to change (so there is no direct effect of <span class="math inline">x</span> on <span class="math inline">y</span>)</li>
<li>There is a causal effect of <span class="math inline">y</span> on <span class="math inline">x</span></li>
</ol>
<p>These causes can occur simultaneously at the same time. Thus, <u>correlation is not causation</u>, as correlation can be caused by other factors.</p>
</div>
</div>
<p>However, while correlation is not causation, we still need to understand correlation before we study causation.</p>
<p><br></p>
</section>
<section id="measuring-relationships-between-variables" class="level3">
<h3 class="anchored" data-anchor-id="measuring-relationships-between-variables">Measuring Relationships Between Variables</h3>
<p>How can we explore the relationship between a continuous <span class="math inline">x</span> variable and a continuous <span class="math inline">y</span> variable?</p>
<p>One way to explore the relationship is with a best-fit line. A best-fit line is useful, since the <strong>slope</strong> of the best-fit line represents the change in <span class="math inline">y</span> for every unit change in <span class="math inline">x</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/3-3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:45.0%"></p>
</figure>
</div>
<p>In the figure above, we have plotted a number of observations, and implemented a best-fit line in red. The graph here introduces some common terminology:</p>
<ul>
<li>The <strong>independent variable</strong>, also called the <strong>explanatory variable</strong> or <strong>treatment variable</strong>, is the variable that we believe is doing the causing. It is typically notated <span class="math inline">x</span>.</li>
<li>The <strong>dependent variable</strong>, also called the <strong>response variable</strong> or <strong>outcome variable</strong>, is the variable that is affected by a change in the independent variable. The dependent variable is typically notated with <span class="math inline">y</span>.</li>
</ul>
<p>We know from algebra that a linear equation takes the form <span class="math inline">y=mx+b</span>. The slope <span class="math inline">m</span> describes the rate of change of <span class="math inline">y</span> for a one unit change in <span class="math inline">x</span>.</p>
<ul>
<li>Thus, the slope is a measurement of the relationship between <span class="math inline">x</span> and <span class="math inline">y</span>.</li>
</ul>
<p>Thus, if we can fit a best-fit line to our data, we can look at the slope, and determine the <strong>relationship</strong> between <span class="math inline">x</span> and <span class="math inline">y</span>.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note how I have been using the word <strong>relationship</strong>, not causal effect.</p>
<p>Relationship is the measure of correlation. Correlation is not causation!</p>
<ul>
<li>This chapter is focused on correlation, and we will explore causation in later chapters.</li>
</ul>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
</section>
<section id="the-simple-linear-regression-model" class="level1">
<h1>1.2: The Simple Linear Regression Model</h1>
<p>How can we fit a best-fit line in order to find the relationship between <span class="math inline">x</span> and <span class="math inline">y</span>? The main way to do this is with the <strong>simple linear regression model</strong>.</p>
<p>First, what is a <strong>regression model</strong>? <u>A regression model is the specification of the conditional distribution of <span class="math inline">y</span>, given <span class="math inline">x</span>.</u></p>
<p>A distribution (random variable) says that there are multiple potential values of <span class="math inline">y</span>.</p>
<ul>
<li>For example, if <span class="math inline">x</span> is age, and <span class="math inline">y</span> is income, a distribution of income <span class="math inline">y</span> says that not all individuals make the same amount of money.</li>
<li>If you select someone at random, the probability of selecting someone with a specific income <span class="math inline">y_i</span> will depend on the probability distribution of income.</li>
</ul>
<p>A conditional distribution says that the distribution of <span class="math inline">y</span> is conditional (depends on) the value of <span class="math inline">x</span>.</p>
<ul>
<li>For example, if <span class="math inline">x</span> is age, and <span class="math inline">y</span> is income, a conditional distribution of income <span class="math inline">y</span> on age <span class="math inline">x</span> says that depending on the age <span class="math inline">x</span> of an individual, the probability distribution of income <span class="math inline">y</span> will change.</li>
<li>This makes sense - if the probability distribution of <span class="math inline">y</span> is not affected at all by the value of <span class="math inline">x</span>, then clearly <span class="math inline">x</span> is not related to <span class="math inline">y</span>.</li>
</ul>
<p><br></p>
<p>The linear regression model focuses on the <strong>expected value</strong> of the conditional distribution, notated <span class="math inline">\mathbb{E}[y_i | x_i]</span>.</p>
<ul>
<li>The expected value, also known as the mean, is the “best guess” of <span class="math inline">y</span> (since <span class="math inline">y</span> is a probability distribution with many values).</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Simple Linear Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>The simple linear regression model takes the following form:</p>
<p><span class="math display">
\mathbb{E}[y_i | x_i] = \beta_0 + \beta_1 x_i
</span></p>
<ul>
<li>Where we have <span class="math inline">n</span> number of observations in our data, <span class="math inline">i</span> being any one of them, and each observation has an <span class="math inline">x</span> and <span class="math inline">y</span> value <span class="math inline">(x_i, y_i)</span>.</li>
<li>Where <span class="math inline">\mathbb{E}[y_i | x_i]</span> is the expected value of the conditional distribution of <span class="math inline">y_i|x_i</span>. That distribution has a variance <span class="math inline">Var(y_i | x_i) = \sigma^2</span>.</li>
<li>Where <span class="math inline">\beta_0</span> (intercept) and <span class="math inline">\beta_1</span> (slope) are coefficients of the model that need to be estimated.</li>
</ul>
<p>We can also write the simple linear regression model in respect to a specific <span class="math inline">y_i</span> value, rather than the expected value:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1 x_i + u_i
</span></p>
<ul>
<li>Where we have <span class="math inline">n</span> number of observations in our data, <span class="math inline">i</span> being any one of them, and each observation has an <span class="math inline">x</span> and <span class="math inline">y</span> value <span class="math inline">(x_i, y_i)</span>.</li>
<li>Where <span class="math inline">\beta_0</span> (intercept) and <span class="math inline">\beta_1</span> (slope) are coefficients of the model that need to be estimated.</li>
<li>Where <span class="math inline">u_i</span> is the error term. Remember, <span class="math inline">y_i</span> is any value in the conditional distribution <span class="math inline">y_i|x_i</span>, so it may not be exactly at the expected value of the conditional distribution. Thus, we have to add an error term to account for this distribution, where <span class="math inline">\mathbb{E}[u_i] = 0</span> and <span class="math inline">Var[u_i] = \sigma^2</span>.</li>
</ul>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="fitted-values-and-the-sum-of-squared-errors" class="level1">
<h1>1.3: Fitted Values and the Sum of Squared Errors</h1>
<section id="fitted-values-and-best-fit-lines" class="level3">
<h3 class="anchored" data-anchor-id="fitted-values-and-best-fit-lines">Fitted Values and Best-Fit Lines</h3>
<p>We have discussed the form a simple linear regression takes. However, this is not the best-fit line: we still need to estimate the coefficients <span class="math inline">\beta_0</span> (intercept) and <span class="math inline">\beta_1</span> (slope) in order to create a best-fit line.</p>
<ul>
<li><p>The estimates of <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span> that we obtain will be denoted with a hat ^: <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span>.</p></li>
<li><p>We will discuss the estimation process in the next section.</p></li>
</ul>
<p>Once we have obtained our estimates of the coefficients, we will have a <strong>best-fit line</strong>, also called a <strong>fitted-values</strong> model:</p>
<p><span class="math display">
\hat{y} = \hat\beta_0 + \hat\beta_1x_i
</span></p>
<ul>
<li>Where <span class="math inline">\hat{y}</span> are the predicted values of <span class="math inline">y</span> based on our best-fit line.</li>
<li>Where <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span> are our estimates for the true coefficients <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span>.</li>
<li>Note that the error term <span class="math inline">u_i</span> disappears. This is because the average value of <span class="math inline">u_i</span> is <span class="math inline">\mathbb{E} [u_i] = 0</span>, so we do not need to include the term.</li>
</ul>
<p>We can use the fitted values in two ways:</p>
<ol type="1">
<li>The <span class="math inline">\hat\beta_1</span>, which is the estimate of <span class="math inline">\beta_1</span> (slope), tells us the relationship between <span class="math inline">x</span> and <span class="math inline">y</span>.</li>
<li>The equation outputs a prediction <span class="math inline">\hat y</span>, so we can use the fitted values to make predictions. We will focus on prediction in later parts of the course.</li>
</ol>
<p><br></p>
</section>
<section id="sum-of-squared-errors" class="level3">
<h3 class="anchored" data-anchor-id="sum-of-squared-errors">Sum of Squared Errors</h3>
<p>We want to fit a best-fit line that is accurate. So, we want to find the <span class="math inline">\beta_0</span> (intercept) and <span class="math inline">\beta_1</span> (slope) values that will best fit our observed dataset.</p>
<p>One way we can fit an accurate line is to find the best-fit line that minimises the sum of squared errors.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Sum of Squared Errors
</div>
</div>
<div class="callout-body-container callout-body">
<p>The sum of squared errors (SSE) is as follows:</p>
<p><span class="math display">
\begin{split}
SSE &amp; = \sum\limits_{i=1}^n (y_i - \hat y_i)^2 \\
&amp; = \sum\limits_{i=1}^n (y_i - \hat\beta_0 - \hat\beta_1 x_i)^2
\end{split}
</span></p>
<ul>
<li><p>The sum of squared errors is exactly as it sounds. Find the error, the distance between the actual <span class="math inline">y_i</span> and predicted <span class="math inline">\hat y</span>, which is <span class="math inline">y_i - \hat y</span>, then square that error <span class="math inline">(y_i - \hat y_i)^2</span>, then sum up for all observations <span class="math inline">i</span> in the data.</p></li>
<li><p>We get the second equation by substituting in the fitted values model (discussed in the previous section), where <span class="math inline">\hat{y} = \hat\beta_0 + \hat\beta_1x_i</span>.</p></li>
</ul>
<p>The <strong>Ordinary Least Squares (OLS) Estimator</strong> estimates the coefficients <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span> by finding the values of <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span> that result in the line with the smallest sum of squared errors.</p>
</div>
</div>
<p><br></p>
<p>A common question is: why are the errors squared?.</p>
<ul>
<li>This is because we are only concerned with the <strong>size/magnitude</strong> of errors, not the direction of errors.</li>
<li>A simple subtraction to obtain errors would include negative errors (where the prediction <span class="math inline">\hat y</span> is higher than the actual <span class="math inline">y_i</span>), and positive errors (where the prediction <span class="math inline">\hat y</span> is lower than the actual <span class="math inline">y_i</span>).</li>
<li>But we do not care if the error is above or below the true <span class="math inline">y_i</span>. We only care about the size.</li>
<li>Thus, by squaring the errors, we get rid of the negatives and everything is positive.</li>
</ul>
<p>Then, why not absolute value? This is for a few reasons:</p>
<ul>
<li><p>Firstly, the absolute value function is not differentiable at the vertex. This is an issue, as we will see in the OLS estimation process, we need to take the derivative of the error function for minimisation purposes.</p></li>
<li><p>Second, the OLS estimator with its squared errors has a few unique properties that make it very consistent and unbiased. We will focus on these in <a href="https://politicalscience.github.io/metrics/causal/4.html">chapter 4</a> when we discuss causal inference with regressions.</p></li>
</ul>
<p>In the next section, we will discuss the mathematics behind the OLS estimator.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
</section>
<section id="mathematics-of-the-ordinary-least-squares-estimator" class="level1">
<h1>1.4: Mathematics of the Ordinary Least Squares Estimator</h1>
<p>As we discussed in the previous section, the Ordinary Least Squares Estimator estimates our coefficients <span class="math inline">\beta_0</span> (slope) and <span class="math inline">\beta_1</span> (intercept) by finding the values of <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span> that minimise the sum of squared errors.</p>
<p>We can describe the goal of OLS in a more mathematical way:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Ordinary Least Squares (OLS) Estimator
</div>
</div>
<div class="callout-body-container callout-body">
<p>The goal of the Ordinary Least Squares (OLS) Estimator is to find the values of <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span> that make the following statement true:</p>
<p><span class="math display">
\begin{split}
(\hat{\beta}_0, \hat{\beta}_1) &amp; = \arg \min\limits_{\hat{\beta}_0, \hat{\beta}_1} \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2 \\
&amp; = \arg \min\limits_{\hat{\alpha}, \hat{\beta}} S(\hat{\beta}_0, \hat{\beta}_1)
\end{split}
</span></p>
<p>Where function <span class="math inline">S</span> is the sum of squared errors.</p>
</div>
</div>
<p><br></p>
<p>How do we minimise <span class="math inline">S</span> (the function of the sum of squared errors)?</p>
<ul>
<li>From calculus, we know that a minimum/maximum of a function is where the derivative of the function is equal to 0.</li>
</ul>
<p>Thus, let us find the partial derivative of the function <span class="math inline">S</span> in respect to both <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span>, and set them equal to 0. This is also called the <strong>first-order conditions</strong>.</p>
<p><br></p>
<section id="first-order-conditions" class="level3">
<h3 class="anchored" data-anchor-id="first-order-conditions">First Order Conditions</h3>
<p>First, let us find the partial derivative of <span class="math inline">S</span> in respect to <span class="math inline">\hat\beta_0</span>:</p>
<p><span class="math display">
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial \hat{\beta}_0} = \frac{\partial }{\partial \hat{\beta}_0} \left[ \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2 \right]
</span></p>
<p>First, ignore the summation. The partial derivative of the internal section, using chain rule, is the following:</p>
<p><span class="math display">
\frac{\partial}{\partial \hat{\beta}_0} \left[ (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2 \right] = -2(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
</span></p>
<p>But how do we deal with the summation? We know that there is the sum rule of derivatives <span class="math inline">[f(x) + g(x)]' = f'(x) + g'(x)</span>. Thus, we know we just sum up the derivatives to get the derivative:</p>
<p><span class="math display">
\begin{split}
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial \hat{\beta}_0} &amp; = \sum\limits_{-i=1}^n \left[ -2(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) \right] \\
&amp; = -2 \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
\end{split}
</span></p>
<p>To find the value of <span class="math inline">\hat\beta_0</span> that minimises <span class="math inline">S</span>, we set the derivative equal to 0. We can ignore the -2, since if the summation is equal to 0, the whole derivative will equal 0. Thus, the first order condition is:</p>
<p><span class="math display">
\sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
</span></p>
<p><br></p>
<p>Now, let us do the same for <span class="math inline">\hat\beta_1</span>. Using the same steps as before</p>
<p><span class="math display">
\begin{split}
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial \hat{\beta}_1} &amp; = \sum\limits_{i=1}^n \left[ -2x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) \right] \\
&amp; = -2 \sum\limits_{i=1}^n x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
\end{split}
</span></p>
<p>The first order condition for <span class="math inline">\hat\beta_1</span> will be (again, ignoring the -2 for the same reason as before):</p>
<p><span class="math display">
\sum\limits_{i=1}^n x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
</span></p>
<p><br></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: First Order Conditions of OLS
</div>
</div>
<div class="callout-body-container callout-body">
<p>Thus, the first order conditions of OLS are:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
&amp; \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
</span></p>
</div>
</div>
<p><br></p>
</section>
<section id="solving-the-system-of-equations" class="level3">
<h3 class="anchored" data-anchor-id="solving-the-system-of-equations">Solving the System of Equations</h3>
<p>We now have our two first-order conditions. Now, we basically have a 2-equation system of equations, with 2 variables.</p>
<ul>
<li>We can solve this through substitution - in the first equation, solve for <span class="math inline">\hat\beta_0</span> in terms of <span class="math inline">\hat\beta_1</span>.</li>
<li>Then, plug in <span class="math inline">\hat\beta_0</span> in terms of <span class="math inline">\hat\beta_1</span> into the second equation, thus making that a one-variable equation. We can solve that equation for <span class="math inline">\hat\beta_1</span>, then find <span class="math inline">\hat\beta_0</span>.</li>
</ul>
<p><br></p>
<p>First, let us solve the first equation for <span class="math inline">\hat\beta_0</span> in terms of <span class="math inline">\hat\beta_1</span>:</p>
<p><span class="math display">
\begin{split}\sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) &amp; =  0 \\
\sum\limits_{i=1}^n y_i - n \hat{\beta}_0 - \hat{\beta}_1 \sum\limits_{i=1}^n x_i &amp; = 0 \\
-n\hat{\beta}_0 &amp;= -\sum\limits_{i=1}^n y_i + \hat{\beta}_1\sum\limits_{i=1}^nx_i \\
\hat{\beta}_0 &amp; = \frac{1}{n} \sum\limits_{i=1}^n y_i - \frac{1}{n}\hat{\beta}_1 \sum\limits_{i=1}^n x_i \\
&amp; = \bar{y} - \hat{\beta}_1 \bar{x}
\end{split}
</span></p>
<p>Now, let us substitute our calculated <span class="math inline">\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}</span> into the <span class="math inline">\hat{\beta}_1</span> condition and solve for <span class="math inline">\hat{\beta}_1</span>:</p>
<p><span class="math display">
\begin{split}0 &amp; = \sum\limits_{i=1}^n \left[ x_i(y_i - [\bar{y} - \hat{\beta}_1\bar{x}] - \hat{\beta}_1x_i) \right] \\
&amp; = \sum\limits_{i=1}^n \left[ x_i(y_i - \bar{y} - \hat{\beta}_1 (x_i - \bar{x})) \right] \\
&amp; = \sum\limits_{i=1}^n \left[ x_i(y_i - \bar{y}) - x_i \hat{\beta}_1(x_i - \bar{x}) \right] \\
&amp; = \sum\limits_{i=1}^n x_i (y_i - \bar{y}) - \hat{\beta}_1 \sum\limits_{i=1}^nx_i (x_i - \bar{x})\end{split}
</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Summation Properties
</div>
</div>
<div class="callout-body-container callout-body">
<p>To help us solve this problem, note these 3 properties of summation:</p>
<p><span class="math display">
\begin{split}&amp; \sum\limits_{i=1}^n (x_i - \bar{x}) = 0 \\&amp; \sum\limits_{i=1}^n x_i(y_i - \bar{y}) = \sum\limits_{i=1}^n(x_i - \bar{x}) (y_i - \bar{y}) \\&amp; \sum\limits_{i=1}^n x_i(x_i - \bar{x}) = \sum\limits_{i=1}^n(x_i - \bar{x})^2\end{split}
</span></p>
</div>
</div>
<p>Knowing these properties of summation, we can transform what we had before into:</p>
<p><span class="math display">
\begin{split}
0 &amp; = \sum\limits_{i=1}^n(x_i - \bar{x})(y_i - \bar{y}) - \hat{\beta}_1 \sum\limits_{i=1}^n (x_i - \bar{x})^2 \\
\hat{\beta}_1 \sum\limits_{i=1}^n (x_i - \bar{x})^2 &amp; = \sum\limits_{i=1}^n(x_i - \bar{x})(y_i - \bar{y}) \\
\hat{\beta}_1 &amp; = \frac{\sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum\limits_{i=1}^n(x_i - \bar{x})^2}
\end{split}
</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: OLS Estimate of Coefficient
</div>
</div>
<div class="callout-body-container callout-body">
<p>Thus, the OLS estimate <span class="math inline">\hat\beta_1</span> (slope) of the linear regression model is:</p>
<p><span class="math display">
\hat{\beta}_1 = \frac{\sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum\limits_{i=1}^n(x_i - \bar{x})^2} = \frac{Cov(x, y)}{Var(x)} = \frac{\sigma_{xy}}{\sigma_x^2}
</span></p>
<p>This is also the expected change in <span class="math inline">y</span> given a one unit increase in <span class="math inline">x</span>.</p>
<ul>
<li>Remember, this is the <strong>relationship</strong> between <span class="math inline">x</span> and <span class="math inline">y</span>, <u><strong>not</strong> the causal effect</u>.</li>
</ul>
</div>
</div>
<p><br></p>
<p>Of course, we still need to find <span class="math inline">\hat\beta_0</span> (the slope). We found that <span class="math inline">\hat\beta_0 = \bar{y} - \hat{\beta}_1 \bar{x}</span> earlier, so we just plug that in.</p>
<p>And now, we have our estimates <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span>, and thus we now have a best-fit line and an estimate of the relationship between <span class="math inline">x</span> and <span class="math inline">y</span>.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
</section>
<section id="interpretation-of-ols-coefficient-estimates" class="level1">
<h1>1.5: Interpretation of OLS Coefficient Estimates</h1>
<p>We now have estimated <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span>. But what do these actually mean in the context of the relationship between <span class="math inline">x</span> and <span class="math inline">y</span>?</p>
<ul>
<li>Let us start with <span class="math inline">\hat\beta_1</span>, which is the slope, the more important of the two coefficients.</li>
</ul>
<p><br></p>
<section id="interpretation-of-hatbeta_1" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-of-hatbeta_1">Interpretation of <span class="math inline">\hat\beta_1</span></h3>
<p>We know that in a linear model, <span class="math inline">\mathbb{E}[y_i|x_i] = \beta_0 + \beta_1 x_i</span>, the coefficient <span class="math inline">\beta_1</span> is the slope.</p>
<ul>
<li>And the slope is the change in <span class="math inline">y</span> given a one unit increase in <span class="math inline">x</span>.</li>
</ul>
<p>Using this knowledge, we can interpret estimate <span class="math inline">\hat\beta_1</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of <span class="math inline">\hat\beta_1</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>When <span class="math inline">x</span> increases by one unit, there is an expected <span class="math inline">\hat{\beta}_1</span> unit change in <span class="math inline">y</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Remember, this is the <strong>relationship</strong> between <span class="math inline">x</span> and <span class="math inline">y</span>, <u><strong>not</strong> the causal effect</u>.</p>
</div>
</div>
<p>Note that this interpretation of <span class="math inline">\hat\beta_1</span> only applies to continuous <span class="math inline">x</span> variables and continuous/ordinal <span class="math inline">y</span> variables.</p>
<p><br></p>
</section>
<section id="interpretation-of-hatbeta_1-for-binary-x" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-of-hatbeta_1-for-binary-x">Interpretation of <span class="math inline">\hat\beta_1</span> for Binary <span class="math inline">x</span></h3>
<p><strong>Binary explanatory</strong> variables are variables with 2 values, 0 and 1.</p>
<ul>
<li>Binary explanatory variables are extremely common in the social sciences. They can include things such as yes/no questions, treatment/control, true/false questions, voted/did not vote, etc.</li>
</ul>
<p>Binary explanatory variables will change the interpretations of our coefficients.</p>
<p>We can “solve” for these interpretations. Assume <span class="math inline">x</span> has two categories <span class="math inline">x=0</span> and <span class="math inline">x=1</span>:</p>
<p><span class="math display">
\begin{split}
&amp; \hat y_{i, \ x = 0} = \hat\beta_0 + \hat\beta_1(0) = \hat\beta_0 \\
&amp; \hat y_{i, \ x = 1} = \hat\beta_0 + \hat\beta_1(1) = \hat\beta_0 + \hat\beta_1 \\
&amp; \hat y_{i, \ x = 1} - \hat y_{i, \ x = 0} = (\hat\beta_0 + \hat\beta_1) - \hat\beta_0 = \hat\beta_1
\end{split}
</span></p>
<p>Thus, we can interpret the coefficients as follows:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of Coefficient with a Binary Explanatory Variable
</div>
</div>
<div class="callout-body-container callout-body">
<p>When <span class="math inline">x</span> is a binary explanatory variable:</p>
<ul>
<li><span class="math inline">\hat\beta_0</span> is the expected value of <span class="math inline">y</span> given an observation in category <span class="math inline">x = 0</span></li>
<li><span class="math inline">\hat\beta_0 + \hat\beta_1</span> is the expected value of <span class="math inline">y</span> given an observation in category <span class="math inline">x = 1</span></li>
<li><span class="math inline">\hat\beta_1</span> is the expected difference in <span class="math inline">y</span> between the categories <span class="math inline">x=1</span> and <span class="math inline">x=0</span>.</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Remember, this is the <strong>relationship</strong> between <span class="math inline">x</span> and <span class="math inline">y</span>, <u><strong>not</strong> the causal effect</u>.</p>
</div>
</div>
<p><br></p>
</section>
<section id="interpretation-of-hatbeta_0" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-of-hatbeta_0">Interpretation of <span class="math inline">\hat\beta_0</span></h3>
<p>We know that in a linear model, <span class="math inline">\mathbb{E}[y_i|x_i] = \beta_0 + \beta_1 x_i</span>, the coefficient <span class="math inline">\beta_0</span> is the y-intercept.</p>
<ul>
<li>And the y-intercept is the change value of <span class="math inline">y</span> given <span class="math inline">x=0</span>.</li>
</ul>
<p>We can prove this mathematically:</p>
<p><span class="math display">
\begin{split}
\hat y_{i, \ x_i = 0} &amp; = \hat\beta_0 + \hat\beta_1 x_i \\
&amp; = \hat\beta_0 + \hat\beta_1(0) \\
&amp; = \hat\beta_0
\end{split}
</span></p>
<p>Thus, knowing this, we can interpret <span class="math inline">\hat\beta_0</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of <span class="math inline">\hat\beta_0</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>When <span class="math inline">x=0</span>, the expected value of <span class="math inline">y</span> is <span class="math inline">\hat{\beta}_0</span></p>
</div>
</div>
<p><br></p>
</section>
<section id="interpreting-beta_1-in-terms-of-standard-deviations" class="level3">
<h3 class="anchored" data-anchor-id="interpreting-beta_1-in-terms-of-standard-deviations">Interpreting <span class="math inline">\beta_1</span> in Terms of Standard Deviations</h3>
<p>Sometimes, it is hard to understand what changes in <span class="math inline">y</span> and <span class="math inline">x</span> mean in terms of units. For example, if we are measuring “democracy”, what does a 5 unit change in democracy mean? Is that a lot?</p>
<p>We can add more relevant detail by expressing the change of <span class="math inline">y</span> and <span class="math inline">x</span> in standard deviations.</p>
<p>How do we calculate this? Well, let us solve for the change in <span class="math inline">\hat{y}_i/\sigma_y</span> given <span class="math inline">x_i = x</span> and <span class="math inline">x = x + \sigma_X</span>. This will tell us how much <span class="math inline">\hat{y}</span> changes by given a increase of one standard deviation in <span class="math inline">x</span>:</p>
<p><span class="math display">
\begin{split}
\frac{\hat y_{i, \ x_i = x + \sigma_x}}{\sigma_y} - \frac{\hat y_{i, \ x_i = x}}{\sigma_y} &amp; = \frac{\hat\beta_0 + \hat\beta_1 x_i}{\sigma_y} - \frac{\hat\beta_0 + \hat\beta_1 x_i}{\sigma_y} \\
&amp; = \frac{\hat\beta_0 + \hat\beta_1 (x+\sigma_x) - (\hat\beta_0 + \hat\beta_1 (x))}{\sigma_y} \\
&amp; = \frac{\hat\beta_0 - \hat\beta_0 + \hat\beta_1x - \hat\beta_1x+\hat\beta_1\sigma_x}{\sigma_y} \\
&amp; = \frac{\hat\beta_1 \sigma_x}{\sigma_y}
\end{split}
</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation in Terms of Standard Deviation
</div>
</div>
<div class="callout-body-container callout-body">
<p>For a one-std. deviation increase in <span class="math inline">x</span>, there is an expected <span class="math inline">\hat{\beta}_1 \sigma_x / \sigma_y</span>-std. deviation change in <span class="math inline">Y</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Remember, this is the <strong>relationship</strong> between <span class="math inline">x</span> and <span class="math inline">y</span>, <u><strong>not</strong> the causal effect</u>.</p>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
</section>
<section id="binary-outcome-variables-and-the-linear-probability-model" class="level1">
<h1>1.6: Binary Outcome Variables and the Linear Probability Model</h1>
<p>Very often, we are trying to predict binary outcome variables.</p>
<ul>
<li>For example, we might be interested in how some <span class="math inline">x</span> causes someone to vote for or against some measure or politician.</li>
<li>Or, we might be interested in how some <span class="math inline">x</span> causes some factor to be true or false.</li>
</ul>
<p>These are all binary outcome variables, where <span class="math inline">y</span> can only take two values, <span class="math inline">y=1</span> and <span class="math inline">y=0</span>.</p>
<p><br></p>
<p>The linear model can be “transformed” slightly to deal with binary outcome variables.</p>
<ul>
<li>The difference is now, instead of the fitted value predicting <span class="math inline">\hat y_i</span>, the model now predicts the probability of any observation <span class="math inline">i</span> being in in category <span class="math inline">y=1</span>.</li>
<li>This predicted probability is notated <span class="math inline">\hat\pi_i</span>.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Linear Probability Model
</div>
</div>
<div class="callout-body-container callout-body">
<p>The linear probability model is a variation of the linear regression model, where the outcome variable <span class="math inline">y</span> is a binary variable.</p>
<p><span class="math display">
\pi_i = \beta_0 + \beta_1x_i + u_i
</span></p>
<ul>
<li>Where <span class="math inline">\pi_i</span> is the predicted probability of observation <span class="math inline">i</span> being in category <span class="math inline">y=1</span>. This value is always between 0 and 1.</li>
<li>Where <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span> are coefficients that need to be estimated (in the same way as the standard linear model).</li>
</ul>
</div>
</div>
<p>Once we estimate <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span>, we get fitted probabilities:</p>
<p><span class="math display">
\hat\pi_i = \hat\beta_0 + \hat\beta_1 x_i
</span></p>
<ul>
<li>We can interpret the coefficients (discussed below)</li>
<li>We can also plug in the <span class="math inline">x</span> value of any observation <span class="math inline">i</span> to predict the probability <span class="math inline">\hat\pi_i</span> of that observation <span class="math inline">i</span> being in category <span class="math inline">y=1</span>. To find the probability of category <span class="math inline">y=0</span>, we simply do <span class="math inline">1 - \hat\pi_i</span> (because rules of probability).</li>
</ul>
<p>Final note: The Linear Probability model has a key issue - since a linear line <span class="math inline">y=mx+b</span> goes to <span class="math inline">±∞</span> on either side, that means that the predicted probabilities can be below 0 or above 1, which clearly violates probability rules.</p>
<ul>
<li>For relationships between <span class="math inline">x</span> and <span class="math inline">y</span>, this is not a huge issue, since <span class="math inline">\hat\beta_1</span> still gives us a relatively good estimate (particularly when <span class="math inline">x</span> is binary).</li>
<li>However, for prediction tasks, a logistic regression model (discussed later in the course) may be more suited for this task.</li>
</ul>
<p><br></p>
<section id="interpretation-of-coefficients" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-of-coefficients">Interpretation of Coefficients</h3>
<p>Interpretation of coefficients <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span> that we estimate differ slightly when we are dealing with the linear probability model.</p>
<p><br></p>
<p><span class="math inline">\hat\beta_1</span> is still the slope of our linear model. However, remember that our “output” is now <span class="math inline">\hat\pi_i</span>, not <span class="math inline">\hat y_i</span>. Thus, we have to adjust the interpretation as follows:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of <span class="math inline">\hat\beta_1</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>When <span class="math inline">x</span> increases by one unit, there is an expected <span class="math inline">\hat{\beta}_1</span> change in the probability of an observation being in category <span class="math inline">y=1</span>.</p>
<ul>
<li>We can also interpret this in terms of a percentage points change, by multiplying <span class="math inline">\hat\beta_1</span> by 100.</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Remember, this is the <strong>relationship</strong> between <span class="math inline">x</span> and <span class="math inline">y</span>, <u><strong>not</strong> the causal effect</u>.</p>
</div>
</div>
<p><br></p>
<p><span class="math inline">\hat\beta_0</span> is still the intercept of our linear model. However, with the output being <span class="math inline">\hat\pi_i</span>, not <span class="math inline">\hat y_i</span>, we have to adjust our interpretation as follows:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of <span class="math inline">\hat\beta_0</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>When <span class="math inline">x=0</span>, the expected probability of an observation being in category <span class="math inline">y=0</span> is <span class="math inline">\hat{\beta}_0</span>.</p>
<ul>
<li>We can also interpret this in terms of a percentage, by multiplying <span class="math inline">\hat\beta_0</span> by 100.</li>
</ul>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
</section>
<section id="the-multiple-linear-regression-model" class="level1">
<h1>1.7: The Multiple Linear Regression Model</h1>
<p>In our simple linear regression model, we only have one explanatory variable <span class="math inline">x</span>. However, in many circumstances, we might want additional explanatory variables.</p>
<ol type="1">
<li>More explanatory variables tends to increase the predictive accuracy of <span class="math inline">\hat y</span>. For example, if we were trying to predict someone’s debt by only their education level, that will not be very accurate. But if we add other variables like income, age, credit rating, etc., we will get a far better prediction.</li>
<li>Including other explanatory variables allows us to look at the relationship between one of them and <span class="math inline">y</span>, and control for the relationship of other variables. We will cover this in far more detail in <a href="https://politicalscience.github.io/metrics/1.html">chapter 4</a>.</li>
</ol>
<p><br></p>
<p>The <strong>response variable</strong> (outcome variable) is notated <span class="math inline">y</span>, just like in single linear regression.</p>
<p>The <strong>explanatory variable</strong>s are <span class="math inline">x_1, x_2, ..., x_k</span>. We sometimes also denote all explanatory variables as the vector <span class="math inline">\overrightarrow{x}</span>. Our treatment variable <span class="math inline">D</span> is considered one of the explanatory variables <span class="math inline">\overrightarrow{x}</span> (most often <span class="math inline">x_1</span>).</p>
<p>A linear regression model is the specification of the conditional distribution of <span class="math inline">Y</span>, given <span class="math inline">\overrightarrow{x}</span>. The linear regression model focuses on the <strong>expected value</strong> of the conditional distribution, notated <span class="math inline">\mathbb{E}[y_i|\overrightarrow{x}_i]</span>.</p>
<p><br></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Multiple Linear Regression Model
</div>
</div>
<div class="callout-body-container callout-body">
<p>Take a set of observed data with <span class="math inline">n</span> number of pairs of <span class="math inline">(\overrightarrow{x}_i, y_i)</span> observations. The linear model takes the following form:</p>
<p><span class="math display">
\mathbb{E}[y_i|\overrightarrow{x}_i] = \beta_0 + \beta_1 x_{1i} + ... + \beta_k x_{ki}
</span></p>
<ul>
<li>Where the coefficients (that need to be estimated) are vector<span class="math inline">\overrightarrow{\beta} = \beta_0, \beta_1, ..., \beta_k</span>.</li>
</ul>
<p>We can also write the linear model for the value of any point <span class="math inline">y_i</span> in our data:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1x_{1i} + ... + \beta_k x_{ki} + u_i
</span></p>
<ul>
<li>Where <span class="math inline">u_i</span> is the error term function - that determines the error for each unit <span class="math inline">i</span>. Error <span class="math inline">u_i</span> has a variance of <span class="math inline">\sigma^2</span>, and expectation <span class="math inline">\mathbb{E}[u_i] = 0</span>.</li>
</ul>
</div>
</div>
<p><br></p>
<p>We can also represent the multiple linear regression model in linear algebra. Let us start with the linear model:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + ... + \beta_k x_{ki} + u_i
</span></p>
<p>The <span class="math inline">i</span>’th observation can be re-written in vector form as following:</p>
<p><span class="math display">
y_i = x_i'\beta + u_i, \text{ where }\beta = \begin{bmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_k\end{bmatrix} \text{ and }x_i = \begin{bmatrix}1 \\x_{1i} \\\vdots \\x_{ki}\end{bmatrix}
</span></p>
<ul>
<li><p>The <span class="math inline">x_i'</span> in the equation is the transpose of <span class="math inline">x_i</span>, to make matrix multiplication possible.</p></li>
<li><p>The first element of the <span class="math inline">x_i</span> matrix is 1, since <span class="math inline">1 \times \beta_0</span> gives us the first parameter (intercept) in the linear model.</p></li>
</ul>
<p><br></p>
<p>Since our data has <span class="math inline">n</span> number of observations <span class="math inline">i</span>, we can express this into vector form, with the <span class="math inline">x_i'</span> and <span class="math inline">\beta</span> being vectors within a vector.</p>
<p><span class="math display">
\begin{split}
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} &amp; = \begin{pmatrix}x_1'\beta + u_1 \\ x_2'\beta + u_2 \\ \vdots \\ x_n'\beta + u_n\end{pmatrix} \\
&amp; \\
&amp; = \begin{pmatrix}x_1'\beta \\ x_2'\beta \\ \vdots \\ x_n'\beta\end{pmatrix} + \begin{pmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{pmatrix}
\end{split}
</span></p>
<p>Since <span class="math inline">\beta</span> vector appears as a common factor for all observations <span class="math inline">i=1,...,n</span>, we can factor it out and have an equation:</p>
<p><span class="math display">
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} = \begin{pmatrix}x_1' \\ x_2' \\ \vdots \\ x_n'\end{pmatrix} \space \beta + \begin{pmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{pmatrix}
</span></p>
<p><br></p>
<p>We can expand the <span class="math inline">x_1',...,x_n'</span> vector into a matrix. Remember that each <span class="math inline">x_1',...,x_n'</span> is already a vector of different explanatory variables. So, we get the following result:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Multiple Linear Regression with Linear Algebra
</div>
</div>
<div class="callout-body-container callout-body">
<p>The multiple linear regression can be expressed in linear algebra as:</p>
<p><span class="math display">
y = X \beta + u, \text{ where } X = \begin{bmatrix}1 &amp; x_{21} &amp; \dots &amp; x_{k1} \\1 &amp; x_{22} &amp; \dots &amp; x_{k2} \\\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\1 &amp; x_{2n} &amp; \dots &amp; x_{kn}\end{bmatrix}
</span></p>
<ul>
<li>Where the notation for elements of <span class="math inline">X</span> is <span class="math inline">x_{ki}</span>, with <span class="math inline">i</span> being the unit of observation <span class="math inline">i = 1, \dots n</span>, and <span class="math inline">k</span> being the explanatory variables index.</li>
<li>Where <span class="math inline">y</span> and <span class="math inline">u</span> are <span class="math inline">n \times 1</span> vectors (as seen above), and <span class="math inline">\beta</span> is a <span class="math inline">k \times 1</span> vector.</li>
<li>The first column of <span class="math inline">X</span> is a vector of 1, which exists because these 1’s are multiplied with <span class="math inline">\beta_0</span> in our model.</li>
</ul>
</div>
</div>
<p>The point of expressing the model in linear algebra is that it makes the estimation process far easier, as we will see in the next section.</p>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="ordinary-least-squares-estimator-for-multiple-regression" class="level1">
<h1>1.8: Ordinary Least Squares Estimator for Multiple Regression</h1>
<p>As we remember from Chapter 1, the goal of Ordinary Least Squares Estimation is to minimise the sum of squared errors. The sum of squared errors in multiple regression is:</p>
<p><span class="math display">
\begin{split}
SSE &amp; = \sum\limits_{i=1}^n (y_i - \hat y_i)^2\\
&amp; = \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i} -  ... - \hat\beta_kx_{ki})^2
\end{split}
</span></p>
<p>Similar to our simple linear regression (but with additional variables), our minimisation condition is:</p>
<p><span class="math display">
\begin{split}
(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...) &amp; = \arg \min\limits_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i} ...)^2 \\
&amp; = \arg \min\limits_{(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)} S(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, ...)
\end{split}
</span></p>
<p><br></p>
<p>Taking the partial derivatives of each parameter like in simple linear regression, we get first order conditions:</p>
<p><span class="math display">
\begin{split}&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\&amp; \sum\limits_{i=1}^n X_{1i}(y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0 \\
&amp; \sum\limits_{i=1}^n X_{2i} (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{1i} -  \hat{\beta}_2x_{2i}...) = 0
\end{split}
</span></p>
<ul>
<li>and so on for <span class="math inline">x_{3i}, ..., x_{ki}</span>.</li>
</ul>
<p>This system of equations includes <span class="math inline">k+1</span> variables and <span class="math inline">k+1</span> equations, which is way too difficult to solve.</p>
<p><br></p>
<p>Instead, we can use linear algebra. Let us define our estimation vector <span class="math inline">\hat{\beta}</span> as the value of <span class="math inline">\hat\beta</span> that minimises the sum of squared errors:</p>
<p><span class="math display">
\hat{\beta} = \arg \min\limits_{b} (y - Xb)' (y - Xb) = \arg \min\limits_b S(b)
</span></p>
<p>We can expand <span class="math inline">S(b)</span> as follows:</p>
<p><span class="math display">
\begin{split}
S(b) &amp; = y'y - b'X'y - y'Xb + b'X'Xb \\
&amp; = y'y - 2b'X'y + b'X'Xb
\end{split}
</span></p>
<p>Taking the partial derivative in respect to <span class="math inline">b</span>:</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} = \begin{pmatrix}\frac{\partial S(b)}{\partial b_1} \\\vdots \\\frac{\partial S(b)}{\partial b_k}\end{pmatrix}
</span></p>
<p>Differentiating with the vector <span class="math inline">b</span> yields:</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} = -2X'y + 2X'Xb
</span></p>
<p>Evaluted at <span class="math inline">\hat{\beta}</span>, the derivatives should equal zero (since first order condition of finding minimums):</p>
<p><span class="math display">
\frac{\partial S(b)}{\partial b} \biggr|_{\hat{\beta}} = -2X'y + 2X'X \hat{\beta} = 0
</span></p>
<p>When assuming <span class="math inline">X'X</span> is invertable, we can isolate <span class="math inline">\hat{\beta}</span> to find the solution to OLS:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: OLS Estimate of <span class="math inline">\hat\beta</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Ordinary Least Squares Estimate of vector <span class="math inline">\hat\beta</span> for multiple linear regression is:</p>
<p><span class="math display">
\hat{\beta} = (X'X)^{-1} X'y
</span></p>
</div>
</div>
<p>Once we have estimates of <span class="math inline">\hat{\beta}</span>, we can plug them into our linear model to obtain fitted values:</p>
<p><span class="math display">
\hat{y} = X\hat{\beta} = X(X'X)^{-1} X'y
</span></p>
<p><br></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpretation of Coefficients in Multiple Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>Our interpretations of all <span class="math inline">\hat\beta_1 , ... , \hat\beta_k</span> that are multipled to variables <span class="math inline">x_1, ..., x_k</span> are the same as in single linear regression - except we add the phrase: <em>“When controlling for all other explanatory variables</em>”.</p>
<ul>
<li>We will explore what this means in more detail in <a href="https://politicalscience.github.io/metrics/1.html">chapter 4</a>.</li>
</ul>
<p>The <span class="math inline">\hat\beta_0</span> is the expected value of <span class="math inline">y</span> when all explanatory variables equal 0.</p>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Remember, this is the <strong>relationship</strong> between <span class="math inline">x</span> and <span class="math inline">y</span>, <u><strong>not</strong> the causal effect</u>.</p>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
<section id="implementation-in-r" class="level1">
<h1>Implementation in R</h1>
<p>The packages we will need are:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(fixest)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(texreg)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>
<section id="multiple-linear-regression-in-r" class="level2">
<h2 class="anchored" data-anchor-id="multiple-linear-regression-in-r">Multiple Linear Regression in R</h2>
<p>To run simple linear regression, we use the <em>feols()</em> function.</p>
<ul>
<li>The argument <em>se = “hetero”</em> tells R to calculate heteroscedasticity-robust standard errors, which will be discussed later in chapter 4. Just know it is standard to do so.</li>
<li>We can add more explanatory variables by using + and then add more. We can reduce the number of explanatory variables down to one.</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>modelname <span class="ot">&lt;-</span> <span class="fu">feols</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> X3, <span class="at">data =</span> mydata, <span class="at">se =</span> <span class="st">"hetero"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(modelname)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For example:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">feols</span>(pct_missing <span class="sc">~</span> treat_invite <span class="sc">+</span> head_edu <span class="sc">+</span> mosques <span class="sc">+</span> pct_poor,</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> dta, <span class="at">se =</span> <span class="st">"hetero"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>OLS estimation, Dep. Var.: pct_missing
Observations: 472
Standard-errors: Heteroskedasticity-robust 
              Estimate Std. Error   t value   Pr(&gt;|t|)    
(Intercept)   0.429443   0.089541  4.796047 2.1799e-06 ***
treat_invite -0.026750   0.032667 -0.818872 4.1328e-01    
head_edu     -0.005320   0.006128 -0.868103 3.8578e-01    
mosques      -0.049943   0.018451 -2.706799 7.0424e-03 ** 
pct_poor     -0.103641   0.073363 -1.412729 1.5840e-01    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
RMSE: 0.340276   Adj. R2: 0.013925</code></pre>
</div>
</div>
<p>We can see the output estimate of the intercept and all the coefficients.</p>
<ul>
<li>These rows include the estimate, the standard error, the t-test statistic, and the p-value. This gives all of the information we need to run linear regression and hypothesis tests.</li>
</ul>
<p>For how to do categorical variables, consult chapter 1.</p>
<p><br></p>
<p>We can also use the base-R <em>lm()</em> function, however, this does not calculate heteroscedasticity-robust standard errors (once again, will be discussed in chapter 4).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>modelname <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> mydata)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(modelname)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>
</section>
<section id="creating-regression-tables" class="level2">
<h2 class="anchored" data-anchor-id="creating-regression-tables">Creating Regression Tables</h2>
<p>We can create regression tables using the <em>texreg()</em> or <em>screenreg()</em> functions.</p>
<ul>
<li><em>texreg()</em> produces LaTeX code that you can insert into a LaTeX document</li>
<li><em>screenreg()</em> produces something that looks nice in a R document.</li>
</ul>
<p>The syntax is as follows (you can replace <em>screenreg()</em> with <em>texreg()</em> ):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">screenreg</span>(<span class="at">l =</span> <span class="fu">list</span>(modelname),</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">custom.model.names =</span> <span class="fu">c</span>(<span class="st">"Outcome Variable Name"</span>),</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">custom.coef.names =</span> <span class="fu">c</span>(<span class="st">"Intercept"</span>, <span class="st">"X1 Variable Name"</span>, <span class="st">"X2 Variable Name"</span>),</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">digits =</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For example:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">screenreg</span>(<span class="at">l =</span> <span class="fu">list</span>(model1),</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">custom.model.names =</span> <span class="fu">c</span>(<span class="st">"Pct_Missing"</span>),</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">custom.coef.names =</span> <span class="fu">c</span>(<span class="st">"Intercept"</span>, <span class="st">"Treatment"</span>, <span class="st">"head_edu"</span>, <span class="st">"mosques"</span>, <span class="st">"pct-poor"</span>),</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">digits =</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================
                       Pct_Missing
----------------------------------
Intercept                0.429 ***
                        (0.090)   
Treatment               -0.027    
                        (0.033)   
head_edu                -0.005    
                        (0.006)   
mosques                 -0.050 ** 
                        (0.018)   
pct-poor                -0.104    
                        (0.073)   
----------------------------------
Num. obs.              472        
R^2 (full model)         0.022    
R^2 (proj model)                  
Adj. R^2 (full model)    0.014    
Adj. R^2 (proj model)             
==================================
*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05</code></pre>
</div>
</div>
<p><br></p>
<p><br></p>
<hr>
<p><a href="https://politicalscience.github.io/metrics/">Course Homepage</a></p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>