---
title: "Chapter 1: Introduction to Statistical Inference"
subtitle: "Econometric Methods for Social Scientists"
format:
    html:
        page-layout: article
        grid:
          sidebar-width: 300px
          body-width: 750px
          margin-width: 250px
          gutter-width: 2em
        fontsize: 12.5pt
        theme: yeti
        toc: TRUE
        toc-depth: 2
        toc-location: left
        toc-expand: true
        toc-title: "Chapter 1: Introduction to Statistical Inference"
        mainfont: "Computer Modern"
        title-block-banner: true
        classoption: fleqn
        html-math-method: katex
---

This chapter introduces the concepts of statistical inference, and the goals of econometrics.

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 1.1: Basics of Statistical Inference

The goal of econometrics and statistical inference is to use a subset of observations (sample) to learn something about a **population**.

For example, we might be interested in how education affects income in the United Kingdom. To find the true effect of education on income, we would have to ask every single individual in the UK (almost 70 million).

Obviously, that is not feasible. Instead, we can use a subset of the 70 million, called a sample. Perhaps our sample is just of 1,000 individuals, which is much more feasible.

The goal of econometrics and statistical inference is to take those 1,000 people in our sample, analyse them, and use them to conclude something about all 70 million people in the UK.

<br />

When we take a sample, it is heavily luck-dependent.

If I randomly select 1,000 people, then put them all back, and then re-select randomly another 1,000 people, I am not going to end up with the same 1,000 people as the first time.

Our sample is like a **random variable**: every time we take a new one, we get different observations. This is called **sampling fluctuation**.

This is an important concept - after all, if re-running our experiment with a new sample produces a slightly different sample, our estimates from the first sample we not be the same in the second sample.

-   For example, if I selected 1,000 people, and measured how much education impacted income, and then I re-ran the experiment with a different sample, I would find that the two samples have different answers for how much education impacted income.

We need some way to adjusted for this sampling fluctuation to account for the differences between our samples, in order to conclude anything about the population.

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 1.2: Estimators and Sampling Distribution

The unknown quantity is the true population value of something, that we are trying to estimate. This is called an **estimand**, often labelled $\theta$.

-   For example, in our above example, the estimand is the true population value of the impact of education on income for all 70 million individuals.

We do not know the true population value. Thus, we use an **estimator** to create an estimate of this true population value. The estimator is a rule for combining data, that produces an estimate of the population parameter.

-   In this first part of the course, we will consider estimators such as the Ordinary Least Squares estimator, Method of Moments estimator, Instrumental Variables estimator, and the Maximum Likelihood Estimator.
-   In the next section, we will discuss the properties of estimators, and what makes an estimator good or bad.

The estimate that is produced by our estimator for a sample is labelled with a hat: $\hat\theta_n$.

Our estimate $\hat\theta_n$ is a random variable:

-   Imagine you took one sample, and got an estimate. Let us label it $\hat\theta_1$.
-   Now, you take another sample, and get a different estimate. Let us label it $\hat\theta_2$.
-   We keep taking $k$ samples, so we have estimates $\hat\theta_1, \dots \hat\theta_k$, where $\hat\theta_n$ is any one of them.

Now we can plot all of our sample estimates $\hat\theta_1, \dots \hat\theta_k$ in a density plot. On the $x$ axis will be the different $\hat\theta_n$ values, and the $y$ axis (height) will be the frequency of each estimate. This is a **sampling distribution**.

This distribution represents the probability of getting a certain $\hat\theta_n$ estimate.

::: callout-note
## Example: Education and Income

Say we are interested in the effect of education on income in the UK.

We take a sample of the UK population, and use an estimator to estimate the relationship between education and income, which we notate as $\hat\theta_1$.

However, if we took another sample, we would get slightly different individuals in our sample, and thus, a slightly different estimate of the relationship between income and education $\hat\theta_2$.

We can keep taking more samples and estimates $\hat\theta_n$, and create a sampling distribution of the likelihood of getting a certain estimate $\hat\theta_n$ from our estimator, given an random sample.
:::

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 1.3: Properties of Estimators

In order to say anything about a population from a sample, we need a way to estimate the true population parameter from just sample data.

-   However, we don't just need an estimator, we need a **good estimator**. After all, if our estimator is just always wrong, why bother with it?

Estimators have a few properties that allow us to compare them.

<br />

**Unbiasdness**: An estimator is unbiased, if on average, its estimates get the correct population value. More mathematically:

$$
E(\hat\theta_n) = \theta
$$

-   Where $\hat\theta$ is the estimator's estimate, and $\theta$ is the true value in the population.

We want an unbiased estimator, since then, we know, on average, we are estimating the population correctly.

-   If our estimate was biased, then we would be on average missing the true population value, which is not good.

We can quantify the bias of an estimator as:

$$
Bias(\hat\theta_n) = E(\hat\theta_n) - \theta
$$

<br />

**Variance**: The variance of an estimator is defined as:

$$
E[(\hat\theta_n - E(\hat\theta_n))^2] = Var(\hat\theta_n)
$$

The variance is essentially how spread out our sampling distribution is.

-   If variance is small, that means our estimates between samples are less spread out, and thus, our estimates are more consistent.
-   If our variance is large, that means our estimates between samples are more spread out, and our estimates are less consistent.

<br />

Bias and Variance are both properties of the sampling distribution of $\hat\theta_n$. Bias describes if the mean of the distribution is equal to the true population parameter, and variance describes how spread out the distribution is.
