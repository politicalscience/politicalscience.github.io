---
title: "Chapter 1: Relationships between Variables with Regression"
subtitle: "Econometric Methods (for Social Scientists)"
format:
    html:
        page-layout: article
        grid:
          sidebar-width: 300px
          body-width: 750px
          margin-width: 250px
          gutter-width: 2em
        fontsize: 12.5pt
        theme: yeti
        toc: TRUE
        toc-depth: 2
        toc-location: left
        toc-expand: true
        toc-title: "Chapter 1: Relationships between Variables with Regression"
        mainfont: "Computer Modern"
        title-block-banner: true
        classoption: fleqn
        html-math-method: katex
---

This chapter introduces regression, the key technique used in econometrics and statistics to measure relationships between variables. We will explore the basics of regression, including how the model is specified, the coefficients of the model, the estimation process, and transformations for non-linear relationships between variables.

Topics: Correlation, Simple Linear Regression, Ordinary Least Squares Estimator, Interpretation of Coefficients, Polynomial and Log Transformations

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 1.6: Binary and Categorical Explanatory Variables



### Polytomous Categorical Variables

A **categorical polytomous** variable is one with 3 or more categories. A classic example is the variable *country*, which has categories USA, Germany, UK, Australia, etc.

How do we run a regression with polytomous explanatory variables?

-   What happens is that we divide the variables into a set of binary variables (also called dummy variables).

-   Dummy binary variables are created for all [except one]{.underline} of the categories in our variable. Each dummy variable has two values - 1 meaning the observation is in the category, and 0 meaning the observation is not in that category.

-   The category without a dummy variable is the **reference/baseline** category. Essentially, when all other dummy variables are equal to 0, that is referring to the reference/baseline category (the intercept)

::: callout-tip
## Definition: Linear Regression with a Polytomous Explanatory Variable

Thus, a **polytomous explanatory variable** with $n$ number of categories in $x$, we would create $n-1$ dummy variables, and input it into a regression equation as follows:

$$
\mathbb{E}[y] = \beta_0 + \beta_{x=1}x_{x=1} + ... + \beta_{x=n-1}x_{x=n-1}
$$

-   Where $\beta_0$ is the mean of the reference category $n$, and the other categories $1, ..., n-1$ get their own dummy variable.
:::

<br />

::: callout-note
## Example of Polytomous Categorical Variables

For example, take the following polytomous variable: *company*, which contains the categories *microsoft, google,* and *apple*. Let us create dummy variables for 2 of the 3 categories:

-   *Google* will become the first dummy variable $x_g$. When $x_g = 1$, that observation is part of the *google* category. When $x_g = 0$, that observation is NOT a part of the *google* category.
-   *Apple* will become the second dummy variable $x_a$. When $x_a = 1$, that observation is part of the *apple* category. When $x_a = 0$, that observation is NOT a part of the *apple* category.
-   *Microsoft* will not get its own dummy variable. This is because when both *apple* and *microsoft* $x_g = x_a = 0$ that is referring to the *microsoft* category (observations not a part of either previous category).

Mathematically, this is how it would be represented in a regression equation:

$$
\mathbb{E}[y] = \beta_0 + \beta_g x_g + \beta_a x_a
$$
:::

Continuing with the example, to find the expected value of each category, we would do the following:

$$
\begin{split}& \mathbb{E}[y|x = \text{Google}] = \mathbb{E}[y|x_g = 1, x_a = 0] = \beta_0 + \beta_g(1) + \beta_a(0) = \beta_0 + \beta_g \\& \mathbb{E}[y|x = \text{Apple}] = \mathbb{E}[y|x_g = 0, x_a = 1] = \beta_0 + \beta_g(0) + \beta_a(1) = \beta_0 + \beta_a \\& \mathbb{E}[y|x = \text{Microsoft}] = \mathbb{E}[y|x_g = 0, x_a = 0] = \beta_0 + \beta_g(0) + \beta_a(0) = \beta_0\end{split}
$$

Thus, from these above equations, we can see the interpretation of the coefficients:

-   $\beta_0$ is the expected value of the reference category, in this case, *microsoft*.
-   $\beta_g$ is the expected $y$ difference between the *google* category and the reference category *microsoft*. The statistical significance of this coefficient would be a difference of means test between the two categories.
-   $\beta_a$ is the expected $y$ difference between the *apple* category and the reference category *microsoft*. The statistical significance of this coefficient would be a difference of means test between the two categories.

<br />

We can generalise this example to all polytomous variables:

::: callout-tip
## Interpretation of Polytomous Categorical Variables

$\beta_j$ is the expected difference in $y$ values between category $j$ and the baseline category.

$\beta_0$ is the expected value of $y$ of the baseline category.
:::

Additional note: The coefficient $p$-values of $\beta_j$ (discussed in [section 3.5](https://politicalscience.github.io/metrics/causal/3.html#causal-inference-and-hypothesis-testing)) are a difference-of-means test between two categories, and not a statistical significance test of the entire categorical variable.

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# 1.8: Polynomial and Logarithmic Transformations

### Quadratic Polynomial Transformations

Sometimes, a linear (straight-line) best-fit line is a poor description of a relationship.

We can model more flexible relationships that are not straight lines, by including a transformation of the variable $x$ that we are interested in.

::: callout-tip
## Definition: Quadratic Transformation

Quadratic transformations of $x$ take the following form:

$$
y_i = \beta_0 + \beta_1 x_i + \beta_2 x^2_i + u_i
$$
:::

<br />

If you recall from high-school algebra, an equation that takes the form of $y = ax^2 + bx + c$ creates a *parabola*.

-   A true parabola has a domain of $(-∞, ∞)$.
-   However, our model often does not need to do this. The best-fit parabola is only used for the range of plausible $x$ values, given the nature of our explanatory variable.

For example, if $x$ was age, a negative number would make no sense.

-   Because the parabola's domain often exceeds our plausible range of $x$ values, the vertex of the parabola (where it changes directions) may not be in our data.

<br />

We always include lower degree terms in our model.

-   For example, in this quadratic (power 2) model, we also include the $x$ term without the square.

To fit a model like this, we simply do the same process of minimising the sum of squared errors.

<br />

How do we interpret the coefficients $\beta_1$ and $\beta_2$?

::: callout-tip
## Interpretation of Quadratic Transformations

$\beta_1$'s value is no longer directly interpretable.

$\beta_2$'s value also cannot be directly interpreted, but we can interpret some things:

-   [If the coefficient of $\beta_2$ is statistically significant, we can conclude that there is a non-linear relationship between $X$ and $Y$]{.underline}. (we will discuss this in [section 3.5](https://politicalscience.github.io/metrics/causal/3.html#causal-inference-and-hypothesis-testing)).
-   If $\beta_2$ is negative, the best-fit parabola will open downwards, and if $\beta_2$ is positive, the best-fit parabola will open upwards.
:::

If we want to interpret the magnitude of the model, we are best off using predicted values of $y$ (obtained using the model equation above).

<br />

There is one more thing we can interpret with the quadratic transformation: the **vertex** of the best-fit parabola.

-   The vertex, if we remember our algebra, is either the maximum or minimum point of a parabola.

If we remember from calculus and optimisation, we can find the maximum and minimums through setting the derivative equal to 0.

For the quadratic model, this is as follows - we first find the derivative, then set the derivative equal to 0:

$$
\begin{split}
& \hat{y} = \hat{\beta}_0 + \hat{\beta}_1x + \hat{\beta}_2 x^2 \\
& \frac{d \hat{y}}{dx} = 0 + \hat{\beta}_1 + 2\hat{\beta}_2x \\
& 0 = \hat{\beta}_1 + 2 \hat{\beta}_2x \\
& - \hat{\beta}_1 = 2 \hat{\beta}_2 x \\
& x = \frac{-\hat{\beta}_1}{2 \hat{\beta}_2}\end{split}
$$

This point is useful, as it is either the maximum or minimum of our best-fit parabola.

-   This means that at the $x$ value we calculate from this equation, we will either see the highest or lowest expected $y$ value.

<br />

### General Polynomial Transformations

While quadratic models are the most common polynomial transformation, we do not have to stop there.

We can continue to add further polynomials (although anything beyond cubic is exceedingly rare):

-   Cubic: $y_i = \beta_0 + \beta_1 x_i + \beta_2 x^2_i + \beta_3 x^3_i + u_i$
-   Quartic: $y_i = \beta_0 + \beta_1 x_i + \beta_2 x^2_i + \beta_3 x^3_i + \beta_4 x^4_i + u_i$

Each higher order coefficient, if statistically significant (we will discuss this in the [section 3.5](https://politicalscience.github.io/metrics/causal/3.html#causal-inference-and-hypothesis-testing)), indicates that the relationship between $x$ and $y$, is not of the previous highest power.

-   For example, if the cubic term $\beta_3$ is statistically significant, we can reject a quadratic relationship between $x$ and $y$

Remember to always include the lower power monomials within our polynomial model.

-   For example, if you have a quartic transformation, you must also have the linear, quadratic, and cubic terms.

<br />

### Logarithmic Transformations

Logarithmic transformations are another form of non-linear transformations. These are commonly used for heavily skewed variables, such as when the explanatory variable is income, wealth, and so on.

-   Heavily skewed distributions sometimes perform poorly in statistical tests, as we will discuss throughout this course.

::: callout-tip
## Definition: Logarithmic Transformation

The logarithmic transformation of explanatory variable $x$ takes the following form:

$$
y_i = \beta_0 + \beta_1 \log(x_i) + u_i
$$
:::

<br />

**Interpretation** of the $\beta$ coefficient can be a little bit trickier for logarithmic transformations.

-   We could interpret it in the same way we interpret linear regressions: given a one unit increase in the log of $x$, there is an expected $\beta$ change in $y$.

-   However, this issue is that this does not really say much - I mean, who knows what a *one unit increase in the log of* $x$ even means?

With some properties of logarithms, we can actually create a more useful interpretation. Based on logarithm rules, we know the following to be true:

$$
\begin{split}\log(x) + A & = \log(x) + \log(e^A) \\& = \log(e^A \times x)\end{split}
$$

Now, let us plug this into our original regression model:

$$
\begin{split}
y_{i, \ x_i = x} & = \beta_0 + \beta_1\log(x) \\
y_{i, \ x_i = e^A + x} & = \beta_0 + \beta_1 \log(e^A \times x) \\
& = \beta_0 + \beta_1[\log(x) + A] \\
& = \beta_0 + \beta_1 A + \beta_1 \log(x)
\end{split}
$$

Now find the difference between $y_{i, \ x_i = e^A + x}$ and $y_{i, \ x_i = x}$:

$$
\begin{split}
y_{i, \ x_i = e^A + x} - y_{i, \ x_i = x} & = [\beta_0 + \beta_1 A + \beta_1\log(x)] - [\beta_0 + \beta_1\log(x)] \\
& = \beta A\end{split}
$$

Thus, we can seen that when we multiply $x$ by $e^A$, we get an expected $\beta A$ change in $y$.

-   We can make this interpretation more useful by purposely choosing some value $A$ that makes $e^A$ make more sense.
-   For example, if $A = 0.095$, then $e^A = 1.1$, and multiplying by 1.1 is a 10% increase.

::: callout-tip
## Interpretation of Logarithmic Transformation

When $x$ increases by 10%, there is a expected $0.095\beta_j$ unit change in $y$
:::

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)

# Implementation in R

The packages we will need are:

```{r, message = FALSE}
library(tidyverse)
library(fixest)
library(texreg)
```

```{r, echo = FALSE, message = FALSE}
setwd("/Users/kevinli/Documents/GitHub/politicalscience.github.io/metrics/causal")
dta <- read_csv("data/olken_data.csv")
```

<br />

## Simple Linear Regression

To run simple linear regression, we use the *feols()* function.

-   The argument *se = "hetero"* tells R to calculate heteroscedasticity-robust standard errors, which will be discussed later in chapter 4. Just know it is standard to do so.

```{r, eval = FALSE}
modelname <- feols(y ~ x, data = mydata, se = "hetero")
summary(modelname)
```

For example:

```{r, message = FALSE}
model1 <- feols(pct_missing ~ treat_invite, data = dta, se = "hetero")
summary(model1)
```

We can see the output estimate of *treat_invite*, our $x$ variable

-   The row includes the estimate, the standard error, the t-test statistic, and the p-value. This gives all of the information we need to run linear regression and hypothesis tests.

<br >

We can also use the base-R *lm()* function, however, this does not calculate heteroscedasticity-robust standard errors (once again, will be discussed in chapter 4).

```{r, eval = FALSE}
modelname <- lm(y ~ x, data = mydata)
summary(modelname)
```

<br />

## Categorical Explanatory Variables

To run a regression with a categorical explanatory variable, we must first look in R at the categorical variable.

-   R treats *string* and *double* variables automatically as categorical variables, so we can just directly insert them into our regression like above.
-   However, if our categorical variable is coded numerically, we must use the *as.factor()* function to coerce the variable into a categorical variable

For example, below, variable $x$ is being coerced into a categorical variable

```{r, eval = FALSE}
modelname <- feols(y ~ as.factor(x), data = mydata, se = "hetero")
```

<br />

## Polynomial Transformations

For polynomial transformations, we use the *I()* function within the *feols()* function.

-   The first parameter of *I()* is the variable name, and the second parameter is the number of the highest polynomial.

Below is the syntax for a cubic polynomial:

```{r, eval = FALSE}
modelname <- feols(y ~ x + I(x^2) + I(x^3), data = mydata, se = "hetero")
```

For example:

```{r}
model <- feols(pct_missing ~ mosques + I(mosques^2) + I(mosques^3),
               data = dta, se = "hetero")
summary(model)
```

We can see the 3 coefficients for the variable *mosques*, each for $x$, $x^2$, and $x^3$.

<br />

## Logarithmic Transformations

For logarithmic transformations, we use the *log()* function within the *feols()* function.

-   You can also create a separate variable of the *log()* of the original variable, and include it in the regression.

The syntax is as follows:

```{r, eval = FALSE}
modelname <- feols(y ~ log(x), data = mydata, se = "hetero")
```

For example:

```{r}
model <- feols(pct_missing ~ log(mosques), data = dta, se = "hetero")
summary(model)
```

<br />

## Creating Regression Tables

We can create regression tables using the *texreg()* or *screenreg()* functions.

-   *texreg()* produces LaTeX code that you can insert into a LaTeX document
-   *screenreg()* produces something that looks nice in a R document.

The syntax is as follows (you can replace *screenreg()* with *texreg()* ):

```{r, eval = FALSE}
screenreg(l = list(modelname),
  custom.model.names = c("Outcome Variable Name"),
  custom.coef.names = c("Intercept", "X Variable Name"),
  digits = 3)
```

For example:

```{r}
screenreg(l = list(model1),
  custom.model.names = c("Pct_Missing"),
  custom.coef.names = c("Intercept", "Treatment"),
  digits = 3)
```

<br />

<br />

------------------------------------------------------------------------

[Course Homepage](https://politicalscience.github.io/metrics/)
