---
title: "Multiple Linear Regression"
subtitle: "Chapter 2, Quantitative Methods (Causal Inference)"
sidebar: side
---

In the last chapter, we discussed the basics of statistics, and briefly introduced regression as a way to find correlations. This chapter dives deep into multiple linear regression, the foundational model for all of statistics. We cover the specification of the model, estimation and statistical inference, as well as extensions.

Use the right sidebar for quick navigation. R-code is provided at the bottom.

------------------------------------------------------------------------

# **Basics of the Model**

### Model Specification

Let us say we have some outcome variable $y$, and several explanatory variables $x_1, x_2, \dots, x_k$. We have data on $n$ number of observations $i = 1, \dots n$. We assume these observations $i = 1, \dots, n$ are independent of each other. The linear model can be specified for any specific outcome value $y_i$ for unit $i$:

$$
y_i = \beta_0 + \beta_1 x_{1i} + \dots + \beta_kx_{ki} + u_i
$$

::: {.callout-note collapse="true" appearance="simple"}
## More Info on the Error Term $u_i$

The $u_i$ is called the error term. This indicates that not every value of $y_i$ in our data will be exactly on the linear best-fit line.

Graphically, it is the highlighted part:

![](images/clipboard-1210742477.png){fig-align="center" width="65%"}

In social science terms, the $u_i$ is the effect of any other variable not included in our model on $y$.

For example, if $x$ is age, and $y$ is income, we will have the following relationship:

$$
\text{income}_i = \beta_0 + \beta_1 \text{age}_i + u_i
$$

However, not every individual lies perfectly on this linear line. This is because there are other factors outside of age that affect $y$ (income), and these other factors are bundled into the error term.
:::

We can condense this above with the use of vectors:

$$
y_i = \mathbf x_i^\mathsf{T}\boldsymbol\beta + u_i, \quad \text{where } \mathbf x_i = \begin{pmatrix} 1 \\ x_{i1} \\ \vdots \\ x_{ki} \end{pmatrix}, \quad \boldsymbol\beta = \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_k\end{pmatrix}
$$

We can specify all observations $y_1, \dots, y_n$ in a vector:

$$
\begin{align}
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} & = \begin{pmatrix} \mathbf x_1^\mathsf{T}\boldsymbol\beta + u_1 \\ \mathbf x_2^\mathsf{T}\boldsymbol\beta + u_2 \\ \vdots \\ \mathbf x_n^\mathsf{T}\boldsymbol\beta + u_n\end{pmatrix}\\
\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{pmatrix} & =
\begin{pmatrix}1 & x_{11} & \dots & x_{k1} \\1 & x_{12} & \dots & x_{k2} \\\vdots & \vdots & \vdots & \vdots \\1 & x_{1n} & \dots & x_{kn}\end{pmatrix}
\begin{pmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_k\end{pmatrix}
+ \begin{pmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{pmatrix}
\end{align}
$$

We typically denote the above model as:

$$
\mathbf y = \mathbf X \boldsymbol\beta + \mathbf u
$$

::: {.callout-note collapse="true" appearance="simple"}
## Visualising the Geometry of Regression

Our linear model is essentially a hyperplane space $\mathbb R^k$. The figure below shows this:

![](images/clipboard-365376575.png){fig-align="center" width="60%"}
:::

<br />

### Estimation Process

To estimate the population parameters $\beta_0, \dots, \beta_k$, we use our sample data, and try to find the values $\widehat{\beta_0}, \dots, \widehat{\beta_k}$ that **minimise the square sum of residuals** (SSR):

$$
\begin{align}
SSR = S(\widehat{\beta_0}, \dots, \widehat{\beta_k})& = \sum\limits_{i=1}^n(y_i - \hat y_i)^2 \\
& = \sum\limits_{i=1}^n(y_i - \color{blue}{(\widehat{\beta_0} + \widehat{\beta_1}x_{1i} + \dots + \widehat{\beta_k}x_{ki})}\color{black})^2 && (\text{plug in } \color{blue}{\hat y = \widehat{\beta_0} + \widehat{\beta_1}x_{1i} + \dots }\color{black}) \\
& = \sum\limits_{i=1}^n(y_i - \widehat{\beta_0} - \widehat{\beta_1}x_{1i} - \dots - \widehat{\beta_k}x_{ki})^2 &&(\text{distribute negative sign})
\end{align}
$$

In the linear algebra representation (where $\mathbf b$ is the vector of estimated parameters $\widehat{\beta_0}, \dots, \widehat{\beta_k}$):

$$
\begin{align}
SSR = S(\hat{\boldsymbol\beta}) & = (\mathbf y - \hat{\mathbf y})^\mathsf{T} (\mathbf y - \hat{\mathbf y})\\
& = (\mathbf y - \color{blue}{\mathbf X \hat{\boldsymbol\beta}}\color{black})^\mathsf{T} (\mathbf y - \color{blue}{\mathbf{X} \hat{\boldsymbol\beta}}\color{black}) && (\text{plug in } \color{blue}{\hat{\mathbf y}  = \mathbf X \hat{\boldsymbol\beta}}\color{black}) \\
\end{align}
$$

::: {.callout-note collapse="true" appearance="simple"}
## Intuitive Visualisation of SSR

The residuals are the difference from our predicted best-fit line result $\widehat{y_i}$, and the actual value of $y_i$ in the data. Below highlighted in red are the residuals.

![](images/clipboard-846785636.png){fig-align="center" width="70%"}

After we have the residual values, we simply square each of them, then sum all of them together. That is the sum of squared residuals.
:::

This estimation is called the **ordinary least squares (OLS) estimator**. The solutions to the OLS estimator can be derived mathematically.

<br />

### Deriving OLS Estimates

OLS wants to minimise the sum of squared residuals $S(\hat{\boldsymbol\beta})$ - the differences between the actual $\mathbf y$ and our predicted $\hat{\mathbf y}$:

$$
\begin{align}
S(\hat{\boldsymbol\beta}) & = (\mathbf y - \hat{\mathbf y})^\mathsf{T} (\mathbf y - \hat{\mathbf y})\\
& = (\mathbf y - \color{blue}{\mathbf X \hat{\boldsymbol\beta}}\color{black})^\mathsf{T} (\mathbf y - \color{blue}{\mathbf{X} \hat{\boldsymbol\beta}}\color{black}) && (\text{plug in } \color{blue}{\hat{\mathbf y}  = \mathbf X \hat{\boldsymbol\beta}}\color{black}) \\
& = \mathbf y^\mathsf{T} \mathbf y - \hat{\boldsymbol\beta}^\mathsf{T} \mathbf X^\mathsf{T} \mathbf y - \mathbf y^\mathsf{T} \mathbf{X}\hat{\boldsymbol\beta} + \hat{\boldsymbol\beta}^\mathsf{T} \mathbf X^\mathsf{T} \mathbf{Xb} && (\text{distribute out)} \\ 
& = \mathbf y^\mathsf{T} \mathbf y - \color{blue}{2\hat{\boldsymbol\beta}^\mathsf{T} \mathbf X^\mathsf{T} \mathbf y}\color{black} + \hat{\boldsymbol\beta}^\mathsf{T} \mathbf X^\mathsf{T} \mathbf{X} \hat{\boldsymbol\beta} &&(\text{combine } \color{blue}{- \hat{\boldsymbol\beta}^\mathsf{T} \mathbf X^\mathsf{T} \mathbf y - \mathbf y^\mathsf{T} \mathbf{X}\hat{\boldsymbol\beta}}\color{black})
\end{align}
$$

Now, let us find the first order condition:

$$
\frac{\partial S(\hat{\boldsymbol\beta})}{\partial \hat{\boldsymbol\beta}} = -2\mathbf X^\mathsf{T} \mathbf y + 2 \mathbf X^\mathsf{T} \mathbf{X} \hat{\boldsymbol\beta} = 0
$$

::: {.callout-note collapse="true" appearance="simple"}
## First Order Conditions for Simple Linear Regression

Currently, we are deriving the first order conditions for multiple linear regression using linear algebra.

For simple linear regression (with one explanatory variable), we can use summation notation. Recall our sum of squared residuals in summation form:

$$
SSR = S(\widehat{\beta_0}, \widehat{\beta_1})= \sum\limits_{i=1}^n(y_i - \widehat{\beta_0} - \widehat{\beta_1}x_i)^2
$$

We want to minimise the SSR in respect to both $\widehat{\beta_0}$ and $\widehat{\beta_1}$. We can do this by taking the partial derivative in respect to both, and setting them equal to 0. We can find the partial derivative with chain rule and sum rule:

$$
\begin{align}
\frac{\partial S(\widehat{\beta_0}, \widehat{\beta_1})}{\partial \widehat{\beta_0}} & = \sum\limits_{i=1}^n(y_i - \widehat{\beta_0} - \widehat{\beta_1}x_i) = 0 \\
\frac{\partial S(\widehat{\beta_0}, \widehat{\beta_1})}{\partial \widehat{\beta_1}} & = \sum\limits_{i=1}^nx_i(y_i - \widehat{\beta_0} - \widehat{\beta_1}x_i) = 0
\end{align}
$$

These conditions create a system of equations, which you can solve for the OLS solutions of $\widehat{\beta_0}$ and $\widehat{\beta_1}$. I will not do that here, since this method is more tedious than the linear algebra method, and can only apply to simple linear regression.

However, while we are not going to solve for our OLS solutions with summations, it is still useful to know these first order conditions, since we will use them in many proofs that show other estimators are the same as OLS because they have the same first order conditions.
:::

When assuming $\mathbf X^\mathsf{T} \mathbf X$ is invertable (which is true if $\mathbf X$ is full rank), we can isolate $\hat{\beta}$ to find the solution to OLS:

$$
\begin{align}
-2\mathbf X^T\mathbf y + 2 \mathbf X^\mathsf{T} \mathbf X \boldsymbol{\hat{\beta}} & = 0 \\
2 \mathbf X^\mathsf{T} \mathbf X \boldsymbol{\hat\beta} & = 2\mathbf X^\mathsf{T} \mathbf y && (+ 2\mathbf X^\mathsf{T} \mathbf y \text{ to both sides}) \\
\boldsymbol{\hat\beta} & = (2\mathbf X^\mathsf{T} \mathbf X)^{-1} 2 \mathbf X^\mathsf{T} \mathbf y && (\times (2\mathbf X^\mathsf{T} \mathbf X)^{-1} \text{ to both sides})\\
\boldsymbol{\hat\beta} & = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf y &&(\text{cancel out } 2^{-1}\times 2)
\end{align}
$$

Those are our coefficient solutions to OLS. With the estimated parameters $\widehat{\beta_0}, \dots, \widehat{\beta_k}$, we now have a best-fit line, called the **fitted values**.

We will discuss the OLS estimator in far more detail in the [next chapter](quant3.qmd), discussing its properties, and its strengths/weaknesses.

<br />

### Conditional Expectation Function {#conditional-expectation-function}

Previously, we wrote the linear regression model in respect to $y_i$. However, we can also write the linear regression model as the best linear approximation of a [conditional expectation](quant1.qmd#conditional-distributions) $E(y_i |x_i)$:

$$
E(y_i|x_i) = \beta_0 + \beta_1 x_1 + \dots + \beta_k x_k
$$

Best-approximation is defined by the lowest mean-squared error (MSE). Let us prove OLS on $y_i$ gets the same $\beta_0, \dots, \beta_k$ as the best linear approximation of $E(y_i|x_i)$. Take this very simple CEF and its MSE:

$$
\begin{align}
E(y_i|x_i) & = b_0 + b_1x_i \\
MSE & = E(y_i - E(y_i|x_i))^2 \\
& =  E(y_i - b_0 - b_1x_i)^2
\end{align}
$$

The first order conditions are (using chain rule and partial derivatives):

$$
\begin{split}
& E(y_i - b_0 - b_1x_i) = 0 \\
& E(x_i(y_i - b_0 - b_1x_i) = 0
\end{split}
$$

Now, recall our OLS minimisation conditions (simple linear regression, presented in the extra info box above). Since by definition, average/expectation is $E(x) = \frac{1}{n} \sum x_i$, we can rewrite as:

$$
\begin{split}
& \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = \ n \times E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
& \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = \  n \times E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
$$

And since anything multiplied to a zero turns into zero, we can ignore the $n$ in the first order condition. Thus, we can see, are the exact same minimisation conditions as the conditional expectation function. [Thus, OLS is the best approximation of the conditional expectation function]{.underline}.

This property is very useful for interpreting our regression results. You will hear "expected changes in $y$ given an increase in $x$" very frequently when it comes to regression and its applications.

<br />

<br />

------------------------------------------------------------------------

# **Interpretation**

### Interpretation of Parameters

I define $\widehat{\beta_j} \in \{\widehat{\beta_1}, \dots, \widehat{\beta_k}\}$, multiplied to $x_j \in \{x_1, \dots, x_k\}$. $\widehat{\beta_0}$ is the intercept. We assume a continuous $y$ variable - for a binary $y$, see the below section on the [linear probability model](#linear-probability-model).

+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                     | **Continuous** $x_j$                                                                                                                                 | **Binary** $x_j$                                                                                                                                           |
+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+
| $\widehat{\beta_j}$ | For every one unit increase in $x_j$, there is an expected $\widehat{\beta_j}$ unit change in $y$, holding all other explanatory variables constant. | There is a $\widehat{\beta_j}$ unit difference in $y$ between category $x_j = 1$ and category $x_j = 0$, holding all other explanatory variables constant. |
+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+
| $\widehat{\beta_0}$ | When all explanatory variables equal 0, the expected value of $y$ is $\widehat{\beta_0}$.                                                            | For category $x_j = 0$, the expected value of $y$ is $\widehat{\beta_0}$ (when all other explanatory variables equal 0).                                   |
+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+

Sometimes, these interpretations don't make much sense. For example, if $y$ is *democracy*, what does a 5 unit increase in democracy actually mean? Instead, w can express the change of $y$ and $x$ in terms of standard deviations. Or in other words, we want to find the change in $\frac{\hat y_i}{\sigma_y}$ for every one standard deviation $\sigma_x$ increase in $x$. For simplicity, let us use a simple linear regression $E(y_i|x_i) = \beta_0 + \beta_1 x_i$:

$$
\begin{align}
& E \left(\frac{y_i}{\sigma_y} | x_i = x + \sigma_x \right ) - E \left(\frac{y_i}{\sigma_y} | x_i = x \right ) \\
& = \frac{E(y_i|x_i = x+ \sigma_x)}{\sigma_y} - \frac{E(y_i|x_i = x)}{\sigma_y} &&(\text{property of expectation}) \\
& = \frac{E(y_i|x_i = x+ \sigma_x) - E(y_i|x_i = x)}{\sigma_y} && (\text{combine into 1 fraction})\\
& = \frac{\beta_0 + \beta_1(x+\sigma_x) - [\beta_0 + \beta_1(x)]}{\sigma_y} && (\text{plug in regression models})\\
& = \frac{\beta_1\sigma_x}{\sigma_y} && (\text{cancel and simplify})
\end{align}
$$

Thus, for a one standard deviation $\sigma_x$ increase in $x_j$, there is an expected $\frac{\beta_j\sigma_x}{\sigma_y}$-standard deviation change in $y$.

<br />

### R-Squared

Our fitted values equation takes the following form:

$$
\begin{align}
\hat{\mathbf y} & = \mathbf X \hat{\boldsymbol\beta}  \\
\hat{\mathbf y} & = \mathbf X\color{blue}{(\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf y} && \color{black}(\text{plug in } \color{blue}{\boldsymbol{\hat\beta} = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf y}\color{black}) \\
\hat{\mathbf y} &= \color{blue}{\mathbf P}\color{black}{\mathbf y}  && (\text{plug in } \color{blue}{\mathbf P : = \mathbf X(\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T}} \color{black})
\end{align}
$$

We can see that $\mathbf P$ is a matrix that turns $\mathbf y \rightarrow \hat{\mathbf y}$. Matrix $\mathbf P$ is our linear model that projects the true values $\mathbf y$ into a space of our regressors $\mathbf X$. We will not go too into depth here on this, but see the [next chapter](quant3.qmd#ols-orthogonal-projection) for more details.

One thing we might be interested in is how well our model $\mathbf{Py}$ explains the actual $\mathbf y$. One way we can do this is the scalar product of two vectors. We know that the scalar/dot product calculates the project/shadow of one vector on another. Thus, the scalar product $\mathbf y^\mathsf{T}\mathbf{Py}$ describes the shadow the actual $y$ casts on our projected model.

However, this value will change based on the scale of our $y$ variable. Thus, we will divide it by $\mathbf y^\mathsf{T}\mathbf y$, which is the "maximum" shadow possible (perfect shadow). This ratio is called $R^2$.

$$
R^2 = \frac{\mathbf y^\mathsf{T}\mathbf{Py}}{\mathbf y^\mathsf{T}\mathbf y}
$$

We can also reason about $R^2$ in a another way. The total amount of variation in $y$ is called the total sum of squares (SST). The part of $y$ we cannot explain is the Sum of Squared Residuals (SSR) that we minimised for OLS esimtation. That implies that the remaining part SST-SSR is the variation in $y$ that our model explains, called the sum of explained squares (SSE). $R^2$ can be though of the ratio of explained variation in $y$ by our model to the total variation in $y$:

$$
R^2 = \frac{SSE}{SST} = \frac{SST - SSR}{SST} = 1 - \frac{SSR}{SST} = 1 - \frac{\sum \hat u_i^2}{\sum(y_i - \bar y)^2}
$$

R-Squared ($R^2$) measures the proportion of variation in $y$ that is explained by our explanatory variables. R-Squared is always between 0 and 1 (or 0-100 as a percentage). Higher values indicate our model better explains the variation in $y$.

<br />

<br />

------------------------------------------------------------------------

# **Statistical Inference**

### T-Tests

In the last chapter, we discussed the [basics of hypothesis testing](quant1.qmd#intuition-of-hypothesis-testing). In regression, our typical hypotheses are:

-   $H_0 : \beta_j = 0$ (i.e. there is no relationship between $x_j$ and $y$).
-   $H_1:\beta_j ≠ 0$ (i.e. there is a relationship between $x_j$ and $y$).

Using the robust standard errors, we calculate the $t$-statistic, and using the $t$-statistic and a [t-distribution](quant1.qmd#the-t-distribution), we calculate a p-value.

-   Why a t-distribution and not a standard normal distribution? The reason will be explained in the next chapter, but has to deal with the fact we cannot observe the variance of the error term $u_i$, so we have to estimate it.

::: {.callout-note collapse="true" appearance="simple"}
## Details of Running a Hypothesis Test

First, we calculate the t-test statistic:

$$
t = \frac{\widehat{\beta_1} - H_0}{\widehat{se}(\widehat{\beta_1})}
$$

-   Where $H_0$ is typically 0, but if you do decide to alter the null hypothesis, you would plug it in.

Now, we consult a t-distribution of $n-k-1$ degrees of freedom. We use a t-distribution because the standard error calculation used in OLS is slightly imprecise.

-   Note: we can only do this step if we believe the central limit theorem is met (that our errors are asymptotically normal). We need a large enough sample size.

We start from the middle of the t-distribution, and move *t-test-statstic* number of standard deviations from both sides of the middle.

Then, we find the probability of getting a t-test statistic even further from the middle than the one we got. The area highlighted in the figure below showcases this. In the figure, the t-test statistic is 2.228.

![](images/clipboard-1533818238.png){fig-align="center" width="60%"}

The area highlighted, divided by the entire area under the curve, is the p-value.
:::

::: {.callout-note collapse="true" appearance="simple"}
## Robust Standard Errors

In modern econometrics, we tend to use **robust** standard errors, not normal standard errors. To understand why, we need to look at homoscedasticity.

[Homoscedasticity]{.underline} is the idea that no matter the values of any explanatory variable, the error term variance is **constant**. If this is false, then we have [heteroscedasticity]{.underline}. An easy way to identify homoscedasticity is to look at a residual plot (just the plot of all $\widehat{u_i}$):

![](images/clipboard-1713529842.png){fig-align="center" width="80%"}

Notice how the homoscedasticity residuals seem to have the same up-down variance, no matter the value of $x$. The heteroscedasticity residuals have a clear pattern - the up-down variance is smaller when $x$ is smaller, and the up-down variance is larger when $x$ is larger.

Essentially, if you see a pattern in the residual plot, it is likely heteroscedasticity.

If you have homoscedasticity, you should use normal OLS standard errors. However, it is often hard to prove your data is homoscedastic. Thus, [we generally default to heteroscedasticity-robust standard errors]{.underline}, unless we can prove we have homoscedasticity.

We will discuss homoscedasticity and heteroscedasticity in more detail in the [next chapter](quant3.qmd), as well as derive the standard errors.
:::

The p-value we get is the probability of getting a test statistic equally or more extreme than the one we got, given the null hypothesis is true.

-   If $p<0.05$, we believe the probability of a null hypothesis is low enough, such that we reject the null hypothesis (that there is no relationship between $x$ and $y$), and conclude our alternate hypothesis (that there is a relationship between $x$ and $y$).

-   If $p > 0.05$, we cannot reject the null hypothesis, and cannot reject there is no relationship between $x$ and $y$.

NOTE: this is not causality - we are only looking at the relationship. Causality needs to be established with an adequate research design, which we will explore in later chapters.

<br />

### Confidence Intervals

In the last chapter, we discussed the idea of [confidence intervals](quant1.qmd#confidence-intervals). The 95% confidence intervals of coefficients have the following bounds:

$$
\widehat{\beta_j} - 1.96 \widehat{se}(\widehat{\beta_j}), \ \ \widehat{\beta_j} + 1.96 \widehat{se}(\widehat{\beta_j})
$$

-   The 1.96 is an approximation assuming a normal distribution. The actual confidence intervals (calculated by computers) will use a t-distribution of $n-k-1$, which will result in a slightly different multiplicative factor.

The confidence interval means that under repeated sampling and estimating $\widehat{\beta_j}$, 95% of the confidence intervals that we construct will include the true $\beta_j$ value in the population.

If the confidence interval contains 0, we cannot conclude a relationship between $x_j$ and $y$, as 0 is a plausible value of $\beta_j$. These results will always match those of the t-test.

<br />

### F-Tests {#f-tests}

F-tests are used to test more than one coefficient at a time. For example, you might want to test if two variables or coefficients are jointly significant (this will become more clear in the extensions of regression).

Our hypotheses will be:

-   $M_0 : y = \beta_0 + \beta_1 x_1 + \dots + \beta_g x_g + u_i$ (the smaller null model).
-   $M_a : y = \beta_0 + \beta_1x_1 + \dots + \beta_g x_g + \dots + \beta_kx_k + u_i$ (the bigger model with additional variables).

::: {.callout-note collapse="true" appearance="simple"}
## Details of the F-test

F-tests compare the $R^2$ of the two models through the F-statistic:

$$
F = \frac{(SSR_0 - SSR_a) / (k_a - k_0)}{SSR_a /(n - k_a - 1)}
$$

We then consult a F-distribution with $k_a - k_0$ and $n-k_a - 1$ degrees of freedom, obtaining a p-value (in the same way as the t-test).
:::

The p-value we get is the probability of getting a test statistic equally or more extreme than the one we got, given the null hypothesis is true.

-   If $p<0.05$, the we believe the probability of the null hypothesis is low enough, such that we reject the null hypothesis (that $M_0$ is the better model), and conclude our alternate hypothesis (that $M_a$ is a better model). This also means the extra coefficients in $M_a$ are jointly statistically significant.
-   If $p > 0.05$, we cannot reject the null hypothesis, and cannot reject that $M_0$ is a better model. Thus, the extra coefficients in $M_a$ are jointly not statistically significant.

<br />

### Predictive Inference

In political science, we are mostly concerned with the relationship between variables.

But sometimes, prediction is also a useful tool. We can predict using the linear regression by plugging in explanatory variable values, and finding the predicted $\widehat{y_i}$.

$$
\begin{align}
\hat{\mathbf y} & = \mathbf X \hat{\boldsymbol\beta}  \\
\hat{\mathbf y} & = \mathbf X\color{blue}{(\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf y} && \color{black}(\text{plug in } \color{blue}{\boldsymbol{\hat\beta} = (\mathbf X^\mathsf{T} \mathbf X)^{-1} \mathbf X^\mathsf{T} \mathbf y}\color{black}) \\
\end{align}
$$

We also have confidence intervals for every predicted $\widehat{y_i}$. These intervals are calculated with the residual standard deviation (covered in the [next chapter](quant3.qmd)):

$$
\widehat{y_i} - 1.96 \hat\sigma, \ \widehat{y_i} + 1.96 \hat\sigma
$$

Note: It is pretty rare to just use linear regression for predictions, since most relationships are not linear. In the *further statistical models* part of this collection, there are some more accurate predictive methods.

<br />

<br />

------------------------------------------------------------------------

# **Extension: Different Variables**

### Linear Probability Model {#linear-probability-model}

The standard linear model assumes a continuous $y$ variable. However, we can adapt the linear model to fit binary $y$ variables. When $y$ is binary (i.e. only has values $y_i \in \{0, 1\}$, our linear model is actually no longer a predictor of $y_i$, since our regression will output values that are not 0 and 1.

Instead, our linear model will now predict the probability of unit $i$ having $y_i = 1$. The is due to the [conditional expectation interpretation of regression](#conditional-expectation-function), and the expectation of the [bernoulli distribution](quant1.qmd#bernoulli-and-binomial-distribution):

$$
\begin{align}
E(y_i|x_i) & = \underbrace{0 \times Pr(y_i = 0|x_i) \ + \ \times Pr(y_i = 1|x_i)}_{\text{a weighted avg. formula}} \\
& = Pr(y_i=1|x_i)
\end{align}
$$

Thus, we can rewrite our linear model with the primary outcome being $Pr(y_i = 1|x_i)$. This model is called the **linear probability model**:

$$
Pr(y_i = 1|x_i) = \beta_0 + \beta_1x_{1i} + \dots + \beta_kx_{ki} + u_i
$$

Our interpretations of coefficients also slightly change.

+---------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                     | **Continuous** $x_j$                                                                                                                                                                                                      | **Binary** $x_j$                                                                                                                                                                                                               |
+---------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| $\widehat{\beta_j}$ | For every one unit increase in $x_j$, there is an expected $\widehat{\beta_j} \times 100$ percentage point change in the probability of a unit being in category $y=1$, holding all other explanatory variables constant. | There is a $\widehat{\beta_j}\times 100$ percentage point difference in the probability of a unit being in category $y=1$ between category $x_j = 1$ and category $x_j = 0$, holding all other explanatory variables constant. |
+---------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| $\widehat{\beta_0}$ | When all explanatory variables equal 0, the expected probability of a unit being in category $y=1$ is $\widehat{\beta_0} \times 100$                                                                                      | For category $x_j = 0$, the expected probability of a unit being in category $y=1$ is $\widehat{\beta_0} \times 100$ (when all other explanatory variables equal 0).                                                           |
+---------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

Note: we cannot do linear regression with a categorical (more than 2 categories and undordered) $y$. We will need to use a multinomial logistic regression. We can do a linear regression with an ordinal $y$ (more than 2 categories with an order) by pretending that the ordinal $y$ is continuous. We can also is a ordinal logistic regression for that.

<br />

### Categorical Explanatory Variables

Take an explanatory variable $x$, which has $n$ number of categories $1, \dots, n$. To include $x$ in our regression, we would create $n-1$ dummy (binary) variables, to create the following regression model:

$$
E(y_i|x_i) = \beta_0 + \beta_1x_{1i} + \dots + \beta_k x_{n-1 \ i}
$$

-   Categories $1, \dots, n-1$ get there own binary variable $x_1, \dots, x_{n-1}$.
-   Category $n$ (the reference category) does not get its own variable. We can change which category we wish to be the reference.

Interpretation is as follows (category $j$ is any one of category $1, \dots, n-1$).

-   $\beta_j$ is the difference in expected $y$ between category $j$ and the reference category.
-   $\beta_0$ is the expected $y$ of the reference category.
-   Thus, category $j$ has an expected $y$ of $\beta_0 + \beta_j$.

::: {.callout-note collapse="true" appearance="simple"}
## Example of a Categorical Explanatory Variable

Let us say that $x$ is the variable *development level of a country*, with 3 categories: low (L), medium (M), and high (H). $y$ will be the crime rate of the country.

Let us set *low development (L)* as our reference category. Our regression will be:

$$
E(y|x) = \beta_0 + \beta_1x_M + \beta_2 x_H
$$

Now let us interpret the coefficients:

-   $\beta_0$ is the expected crime rate for a country of *low (L)* development.
-   $\beta_1$ is the difference in expected crime rate between a *medium (M)* developed country and a *low (L) developed country* (since low is the reference category).
-   $\beta_2$ is the difference in expected crime rate between a *high (H)* developed country and a *low (L) developed country* (since low is the reference category).

The expected/predicted $y$ (crime rate) for each category is:

-   Low (L): $\beta_0$
-   Medium (M): $\beta_0 + \beta_1$
-   High (H): $\beta_0 + \beta_2$.
:::

Each coefficient $\beta_j$'s statistical significance is a difference-in-means significance test, not the significance of the categorical variable as a whole. To find if the entire categorical variable is significant, you should use a [F-test](#f-tests).

<br />

### Interaction Effects

An interaction between two variables means they are multiplied in the regression equation:

$$
y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + \beta_3 x_{1i} x_{2i}
$$

Interpretation of the relationship between $x_1$ and $y$ is as follows:

+----------------------+-------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
|                      | **Binary** $x_2$                                                                                                                    | **Continuous** $x_2$                                                                                                     |
+----------------------+-------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| **Binary** $x_1$     | When $x_2 = 0$, the effect of $x_1$ (going from 0 to 1) on $y$ is $\widehat{\beta_1}$.                                              | The effect of $x_1$ (going from 0 to 1) on $y$ is $\widehat{\beta_1} + \widehat{\beta_3} x_2$.                           |
|                      |                                                                                                                                     |                                                                                                                          |
|                      | When $x_2 = 1$, the effect of $x_1$ (going from 0 to 1) on $y$ is $\widehat{\beta_1} + \widehat{ \beta_3}$.                         |                                                                                                                          |
+----------------------+-------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| **Continuous** $x_1$ | When $x_2 = 0$, for every increase in one unit of $x_1$, there is an expected $\widehat{\beta_1}$ unit change in $y$.               | For every increase of one unit in $x_1$, there is an expected $\widehat{\beta_1} + \widehat{\beta_3} x_2$ change in $y$. |
|                      |                                                                                                                                     |                                                                                                                          |
|                      | When $x_2 = 1$, for every increase in one unit of $x_1$, there is an expected $\widehat{\beta_1}+ \widehat{\beta_3}$ change in $y$. |                                                                                                                          |
+----------------------+-------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+

::: {.callout-note collapse="true" appearance="simple"}
## Proof of Interpretations of Interactions

We can solve for the change of $x_1$ on $y$ using a partial derivative of $y$ in respect to $x_1$:

$$
\begin{split}
\frac{\partial \widehat{y_i}}{\partial x_{1i}} & = \frac{\partial}{\partial x_{1i}} \left[ \widehat{\beta_0} + \widehat{\beta_1}x_{1i} + \widehat{\beta_2}x_{2i} + \widehat{\beta_3}x_{1i}x_{2i}\right] \\
\frac{\partial \widehat{y_i}}{\partial x_{1i}} & = \widehat{\beta_1} + \widehat{\beta_3}x_2
\end{split}
$$

This gives us the effect of $x_1$ on $y$.
:::

$\widehat{\beta_0}$ is still the expected $y$ when all explanatory variables equal 0.

The coefficient of the interaction $\widehat{\beta_3}$, when statistically significant, indicates a statistically significant interaction effect. If it is not statistically significant, then the interaction effect is not statistically significant (and can be dropped).

<br />

### Polynomial Transformations

Sometimes the relationship between two variables is not a straight line - we can add more flexibility with polynomials. The most common form of polynomial transformation is the quadratic transformation:

$$
y_i = \beta_0 + \beta_1x_{i} + \beta_2 x_{i}^2 + u_i
$$

Our estimated $\widehat{\beta_0}$ remains the expected value of $y$ when all explanatory variables equal 0.

Unfortunately, the $\widehat{\beta_1}$ and $\widehat{\beta_2}$ coefficients are not directly interpretable.

-   $\widehat{\beta_2}$'s sign can tell us if the best-fit parabola opens upward or downward.
-   The significance of $\widehat{\beta_2}$ also indicates if the quadratic term is statistically significant. If it is not, we can remove the transformation.

We can interpret two things about the quadratic transformation:

-   For every one unit increase in $x$, there is an expected $\widehat{\beta_1} + 2 \widehat{\beta_2}x$ unit increase in $y$.
-   The minimum/maximum point in the best-fit parabola occurs at $x_i = - \widehat{\beta_1}/2 \widehat{\beta_2}$

::: {.callout-note collapse="true" appearance="simple"}
## Proof of Polynomial Interpretations

We can derive the change in $y$ given a one unit increase in $x$ by finding the partial derivative of $y$ in respect to $x$:

$$
\begin{split}
\frac{\partial \widehat{y_i}}{\partial x} & = \frac{\partial}{\partial x} \left[ \widehat{\beta_0} + \widehat{\beta_1}x_i + \widehat{\beta_2}x_i^2 \right] \\
\frac{\partial \widehat{y_i}}{\partial x} & = \widehat{\beta_1} + 2 \widehat{\beta_2}x_i
\end{split}
$$

We can also solve for the $x_i$ that results in the minimum/maximum of the best-fit parabola by setting the partial derivative equal to 0:

$$
\begin{split}
0 & = \widehat{\beta_1} + 2 \widehat{\beta_2}x_i \\
x_i & = -\widehat{\beta_1}/2 \widehat{\beta_2}
\end{split}
$$
:::

We can go beyond quadratic - as long as we always include lower degree terms in our model:

-   Cubic: $y_i = \beta_0 + \beta_1x_{i} + \beta_2 x_{i}^2 + \beta_3 x_i^3 + u_i$
-   Quartic: $y_i = \beta_0 + \beta_1x_{i} + \beta_2 x_{i}^2 + \beta_3 x_i^3 + \beta_4 x_i^4 + u_i$

<br />

### Logarithmic Transformations

Logarithmic transformations are often used to change skewed variables into normally distributed variables. These are not as common in political science as compared to economics, but can be useful in certain situations.

::: {.callout-note collapse="true" appearance="simple"}
## Logging a Skewed Variable

Many monetary variables are heavily skewed. Natural logging these variables can turn them into normal distributions. This is useful, since skewed variables tend to have heteroscedasticity, and by making them normal, we can use the smaller normal standard errors.

For example, take this variable called *expenses* with a significant right skew:

![](images/clipboard-4184932163.png){fig-align="center" width="60%"}

If we take the log of this variable, we get the following distribution that is almost normal:

![](images/clipboard-296612325.png){fig-align="center" width="60%"}
:::

We have 3 types of logarithmic transformations:

+--------------+-------------------------------------+-----------------------------------------+
|              | $x$                                 | $\log (x)$                              |
+--------------+-------------------------------------+-----------------------------------------+
| $y$          | Linear Model:                       | Linear-Log Model:                       |
|              |                                     |                                         |
|              | $y = \beta_0 + \beta_1 x + u$       | $y = \beta_0 + \beta_1 \log x + u$      |
+--------------+-------------------------------------+-----------------------------------------+
| $\log (y)$   | Log-Linear Model:                   | Log-Log Model:                          |
|              |                                     |                                         |
|              | $\log(y) = \beta_0 + \beta_1 x + u$ | $\log y = \beta_0 + \beta_1 \log x + u$ |
+--------------+-------------------------------------+-----------------------------------------+

<br />

Interpreting the models:

+------------+----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+
|            | $x$                                                                                          | $\log (x)$                                                                                    |
+------------+----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+
| $y$        | Linear Model:                                                                                | Linear-Log Model:                                                                             |
|            |                                                                                              |                                                                                               |
|            | When $x$ increases by one unit, there is an expected $\widehat{\beta_1}$ unit change in $y$. | When $x$ increases by 10%, there is an expected $0.096 \widehat{\beta_1}$ unit change in $y$. |
+------------+----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+
| $\log (y)$ | Log-Linear Model:                                                                            | Log-Log Model:                                                                                |
|            |                                                                                              |                                                                                               |
|            | For every one unit increase in $x$, $y$ is multiplied by $e^{\widehat{\beta_1}}$.            | Multiplying $x$ by $e$ will multiply the expected value of $y$ by $e^{\widehat{\beta_1}}$.    |
+------------+----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+

::: {.callout-note collapse="true" appearance="simple"}
## Proof of Interpretations for Log Transformations

Proof of Linear-Log Model:

$$
\begin{split}
& E(y_i|x_i = x) = \beta_0 + \beta_1 \log x \\
& E(y_i | x_i = e^A x) = \beta_0 + \beta_1 \log(e^A x) \\
& = \beta_0 + \beta_1 (\log(e^A) + \log x) \\
& = \beta_0 + \beta_1 (A + \log x) \\
& = \beta_0 + \beta_1A + \beta_1 \log x
\end{split}
$$

$$
\begin{split}
E(y_i|x_i = \alpha x) - E(y_i|x_i = x) & = \beta_0 + \beta_1 A + \beta_1 \log (x) - (\beta_0 + \beta_1 \log x) \\
& = \beta_1 A
\end{split}
$$

-   When $A = 0.095$, then $e^A = 1.1$. Thus, a 1.1 times increase of $x$ results in a $0.095 \widehat{\beta_1}$ change in $y$.

<br />

Proof of Log-Linear Model:

$$
\begin{split}
E(\log y_i | x_i = x) =  \log y_i & = \beta_0 + \beta_1 x \\
y_i & = e^{\beta_0 + \beta_1 x} \\
y_i & = e^{\beta_0}e^{\beta_1 x} \\
E(\log y_i|x_i = x+1) = \log y_i & = \beta_0 + \beta_1(x+1) \\
y_i & = e^{\beta_0 + \beta_1 + \beta_1 x} \\
y_i & = e^{\beta_0}e^{\beta_1}e^{\beta_1x}
\end{split}
$$

$$
\begin{split}
\frac{E(\log y_i|x_i = x+1)}{E(\log y_i | x_i = x)} & = \frac{e^{\beta_0}e^{\beta_1}e^{\beta_1x}}{e^{\beta_0}e^{\beta_1x}} \\
& = e^{\beta_1}
\end{split}
$$

-   Thus, when $x$ increases by one, there is a multiplicative increase of $e^{\beta_1}$.

<br />

Proof of Log-Log model:

$$
\begin{split}
E(\log y_i | x_i = x) =  \log y_i & = \beta_0 + \beta_1 \log x \\
y_i & = e^{\beta_0 + \beta_1 \log x} \\
y_i & = e^{\beta_0}e^{\beta_1 \log x} \\
E(\log y_i|x_i = ex) = \log y_i & = \beta_0 + \beta_1 \log (ex) \\
y_i & = e^{\beta_0 + \beta_1 \log e + \beta_1 \log x} \\
y_i & = e^{\beta_0}e^{\beta_1}e^{\beta_1 \log x}
\end{split}
$$

$$
\begin{split}
\frac{E(\log y_i|x_i = ex)}{E(\log y_i | x_i = x)} & = \frac{e^{\beta_0}e^{\beta_1}e^{\beta_1 \log x}}{e^{\beta_0}e^{\beta_1 \log x}} \\
& = e^{\beta_1}
\end{split}
$$

-   Thus, when $x$ is multiplied by $e$, there is a multiplicative increase of $e^{\beta_1}$.
:::

<br />

<br />

------------------------------------------------------------------------

# **Extension: Types of Data**

### Clustered and Panel Data

When we have hierarchical or panel data, we need to control for differences between clusters. We essentially include the cluster variable as a categorical variable in our regression.

::: {.callout-note collapse="true" appearance="simple"}
## Hierarchical/Clustered Data

Hierarchical data is data where the basic units of analysis $i$ are clustered, grouped, or nested into clusters.

For example, let us say we are measuring how income affects voter turnout in european countries. We have observations from France, Switzerland, Germany, and many other countries. However, these observations can be grouped by the country they came from.

Why is this grouping important? This is because there may be something in common between observations within the same cluster. For example, Switzerland might just have higher voter turnout in general due to something about Swiss institutions or culture.

This means that observations aren't random - i.e. we know that if we select from switzerland, it is likely to have higher turnout - observations from the same country are correlated. Thus, we need some way to account for this clustering of observations. We will explore this below.
:::

::: {.callout-note collapse="true" appearance="simple"}
## Panel Data

Panel data is data that can be clustered in two ways - by unit, and by time. For example, let us say we have a dataset on all countries and their GDP between 1960-2020.

-   We will have clusters based on country: Germany will have an observation in 1960, in 1961, ..., to 2020. Same for every other country. These observations are grouped by the unit (country in this case).
-   We will also have clusters based on time: We will have all GDP observations for all countries in 1960, in 1961, etc. These observations are grouped by the time (year in this case).
:::

Let us say we have $m$ number of clusters $i = 1, \dots, m$. Within each cluster, we will have units $t = 1, \dots, n$. Our cluster fixed effects model will take the form:

$$
\begin{split}
y_{it} & = \alpha_i + \beta_1x_1 + \dots + \beta_kx_k + u_{it} \\
& \text{where } \alpha_i = \beta_{00} + \underbrace{\beta_{02}D_{i2} + \beta_{03}D_{i3} + \dots + \beta_{0m}D_{im}}_{\text{unique intercepts for each cluster}}
\end{split}
$$

-   Where $D_{i2}, D_{i3}, \dots, D_{im}$ are dummy variables for clusters $2, \dots, m$. Cluster 1 is the reference category.

For panel data, we use two-way fixed effects, which is basically just two fixed effects for different clustering. Let us say we have $i = 1, \dots, m$ units with $t = 1, \dots, n$ different numbers of time periods. Our two way fixed effects model takes the form:

$$
\begin{split}
y_{it} & = \alpha_i + \gamma_t + \beta_1x_1 + \dots + \beta_kx_k + u_{it} \\
& \text{where } \alpha_i =  \alpha_{00} + \underbrace{\alpha_{02}D_{i2} + \alpha_{03}D_{i3} + \dots + \alpha_{0m}D_{im}}_{\text{unique intercepts for each unit}} \\
& \text{where } \gamma_t =  \gamma_{00} + \underbrace{\gamma_{02}T_{i2} + \gamma_{03}D_{t3} + \dots + \gamma_{0n}T_{in}}_{\text{unique intercepts for each time}} \\
\end{split}
$$

-   Where $D_{i2}, D_{i3}, \dots, D_{im}$ are dummy variables for units $2, \dots, m$., and $T_{i2}, T_{i3}, \dots, T_{in}$ are dummy variables for time periods $2, \dots, n$.

::: {.callout-note collapse="true" appearance="simple"}
## Intuitive Explanation of Fixed Effects

For one-way fixed effects, we essentially add a unique intercept term for every cluster, accounting for the average differences in $y$ between each category.

-   $\beta_{00}$ is the intercept for the reference category 1.
-   $\beta_{00} + \beta_{0i}$ is the intercept for the $i$th category.

For two-way fixed effects, we add a unique intercept term for every year and country, accounting for the average differences in $y$ between each country, and the average differences in $y$ between each year.
:::

<br />

### Spatial Regression Models

::: {.callout-note collapse="true" appearance="simple"}
## Spatial Relationships
:::

::: {.callout-note collapse="true" appearance="simple"}
## Weights Matrix
:::

Big model with all factors goes here.

Explain smaller models

Interpretations of coefficients.

<br />

### Time Series Models

<br />

<br />

------------------------------------------------------------------------

# **Implementation in R**

You will need package *fixest* and *estimatr*.

```{r, eval = FALSE}
library(fixest)
library(estimatr)
```

Regression with normal standard errors can be done with the *lm()* function:

```{r, eval = FALSE}
model <- lm(y ~ x1 + x2 + x3, data = mydata)
summary(model)
```

Regression with robust standard errors can be done with the *feols()* function or *lm_robust()* function:

```{r, eval = FALSE}
# feols
model <- feols(y ~ x1 + x2 + x3, data = mydata, se = "hetero")
summary(model)

# lm robust
model <- lm_robust(y ~ x1 + x2 + x3, data = mydata)
```

Output will include coefficients, standard errors, p-values, and more.

::: {.callout-note collapse="true" appearance="simple"}
## Binary and Categorical Variables

You can include binary and categorical variables by using the *as.factor()* function:

```{r, eval = FALSE}
feols(y ~ x1 + as.factor(x2) + x3, data = mydata, se = "hetero")
```

You can do the same for $y$ or $x$. Just remember, $y$ cannot be a categorical variable (use multinomial logsitic regression instead).
:::

::: {.callout-note collapse="true" appearance="simple"}
## Fixed Effects

You can include one-way fixed effects by adding a \| after your regression formula in *feols()*:

```{r, eval = FALSE}
model <- feols(y ~ x1 + x2 + x3 | cluster,
               data = mydata, se = "hetero")
summary(model)
```

You can add two-way fixed effects as follows:

```{r, eval = FALSE}
model <- feols(y ~ x1 + x2 + x3 | unit + year,
               data = mydata, se = "hetero")
summary(model)
```
:::

::: {.callout-note collapse="true" appearance="simple"}
## Interaction Effects

Two interact two variables, use \* between them. This will automatically include both the interaction term, and the two variables by themselves.

```{r, eval = FALSE}
feols(y ~ x1 + x2*x3, data = mydata, se = "hetero")
```

If for some reason, you only want the interaction term, but not the variables by themselves, you can use a colon : between the two variables:

```{r, eval = FALSE}
feols(y ~ x1 + x2:x3, data = mydata, se = "hetero")
```
:::

::: {.callout-note collapse="true" appearance="simple"}
## Polynomial Transformations

To conduct a polynomial transformation, you can use the *I()* function. The second argument is the degree of the polynomial:

```{r, eval = FALSE}
feols(y ~ x1 + I(x2, 3), data = mydata, se = "hetero") #cubic for x2
```
:::

::: {.callout-note collapse="true" appearance="simple"}
## Logarithmic Transformations

The best way to do a logarithmic transformation is to create a new variable that is the log of the variable you want to transform using the *log()* function, before you even start the regression:

```{r, eval = FALSE}
mydata$x1_log <- log(mydata$x1)
```
:::

::: {.callout-note collapse="true" appearance="simple"}
## Confidence Intervals

To find the confidence intervals for coefficients, first estimate the model with *lm()* or *feols()* as shown previously, then use the *confint()* command:

```{r, eval = FALSE}
confint(model)
```
:::

::: {.callout-note collapse="true" appearance="simple"}
## F-Tests

To run a f-test, use the *anova()* command, and input your two different models, with the null model going first.

```{r, eval = FALSE}
anova(model1, model2)
```

Note: F-tests only work with models that are run with homoscedastic standard errors. Robust standard errors will not work.
:::

::: {.callout-note collapse="true" appearance="simple"}
## LaTeX Regression Tables

You can use the *texreg* package to make nice regression tables automatically.

```{r, eval = FALSE}
library(texreg)
```

The syntax for *texreg()* is as follows:

```{r, eval = FALSE}
texreg(l = list(model1, model2, model3),
       custom.model.names = c("model 1", "model 2", "model 3"),
       custom.coef.names = c("intercept", "x1", "x2"),
       digits = 3)
```

You can replace *texreg()* with *screenreg()* if you want a nicer regression table in the R-console.

Note: you must have the same amount of model names as total models in your texreg, and you must have the same amount of coeficient names as the total amount of coefficients in all of your models.
:::

::: {.callout-note collapse="true" appearance="simple"}
## Prediction

We can use the *predict()* function to generate fitted value predictions in R:

```{r, eval = FALSE}
my_predictions <- predict(model, newdata = my_new_data)
```

*my_new_data* is a dataframe with a bunch of explanatory variable values (for every explanatory variable) for a collection of observations, that you wish to predict $\hat y$ for.
:::
