---
title: "Tree-Based Classifiers"
format:
    html:
        theme: darkly
        max-width: 800px
        fontsize: 12pt
subtitle: "Lesson 2.2, Applied Machine Learning"
---

[Course Homepage](https://politicalscience.github.io/ml)

## Table of Contents {#contents}

1.  [Tree-Based Classifiers](#trees)

2.  [Tree-Based Classifiers in R](#r)

------------------------------------------------------------------------

Remember to load tidyverse. We will also need the package **randomForest**.

```{r, message = FALSE}
library(randomForest)
library(tidyverse)
```

Let us also load the dataset we will be using for these examples (feel free to load your own dataset)

```{r, message = FALSE}
df <- read_csv("voctaxdata.csv")
```

------------------------------------------------------------------------

[Table of Contents](#contents) \| [Course Homepage](https://politicalscience.github.io/ml)

# Tree-Based Classifiers {#trees}

In the previous lesson on [Bagging and Random Forest](https://politicalscience.github.io/ml/1-6.html), we covered the prediction methods of Bagging and Random Forest.

In that lesson, we focused on how Bagging and Random Forest could be used to predict continuous $y$ variables. However, Bagging and Random Forest can also be applied to classification with a few small modifications.

<br />

Bagging and Random Forest work in almost the same way for classification (If you need a refresher on how they work, check out the lessons [Regression Trees](https://politicalscience.github.io/ml/1-5.html) and [Bagging and Random Forest](https://politicalscience.github.io/ml/1-6.html)).

In Tree-Based Prediction Models, to determine at which threshold to split $x$, the computer will find the optimal $x$ threshold for splitting based on which value reduces the residual sum of squares (RSS) the most.

$$
RSS = \sum\limits_{i=1}^{n} (y_i - \hat{y}_i)
$$

<br />

However, with classification, this isn't possible! Why? Well, there isn't really a "residual" in classification as there is on a mis-prediction.

Instead of finding the optimal $x$ splitting threshold based on Residual Sum of Squares (RSS), we instead use the **Classification Error Rate**.

The classification error rate is the fraction of the observations, that were predicted in a certain category, but don't belong to that category.

So, in Tree-Based Classifiers, to determine at which threshold to split $x$, the computer will find the optimal $x$ threshold for splitting based on which value reduces the Clasification Error Rate the most

Aside from this, classification trees work the same way.

<br />

------------------------------------------------------------------------

[Table of Contents](#contents) \| [Course Homepage](https://politicalscience.github.io/ml)

# Tree-Based Classifiers in R {#r}

To conduct Bagging and Random Forest, wee need the package **randomForest**:

```{r, message = FALSE, eval = FALSE}
library(randomForest)
```

Remember to load tidyverse.

```{r, message = FALSE, eval = FALSE}
library(tidyverse)
```

Let us also load the dataset we will be using for these examples (feel free to load your own dataset)

```{r, message = FALSE, eval = FALSE}
df <- read_csv("voctaxdata.csv")
```

<br />

#### Bagging Classifier in R

Bagging in R is conducted using the **randomForest()** function. We can view the summary by simply printing the regression variable.

```{r, eval = FALSE}

# Remember to install and load package randomForest

# set a seed to keep the same results when we re-run
set.seed(100)

bagging <- randomForest(Y ~ .,
                        data = df,
                        na.action = na.omit,
                        mtry = 9, # set to number of IV
                        importance = TRUE)

# call model variable to see output
bagging
```

These are the parts of the syntax that can be altered:

-   **bagging** is the variable I am saving my regression model to. *You can name this anything you want to.*

-   **Y** is the Y variable (Dependent variable) you are trying to predict. *Replace this with the variables you want to use.*

-   "**.**" after the tilda **\~** tells R to include all other variables in the data frame as independent variables. This is very common for Bagging since you will typically include all variables.

    -   Note: Make sure to de-select variables you don't want to include in the model, such as ID, date, etc.

-   **df** is the name of the data frame that I am drawing these X and Y variables from. *Replace this with the name of your data frame.*

-   NOTE: Remember to include the sections **na.action = na.omit**, and **importance = TRUE**.

<br />

#### Random Forest Classifier in R

To create a Random Forest model, the syntax is the exact same, except for the fact we set **mtry** to the square root of the number of independent variables in your regression.

```{r, eval = FALSE}
# Remember to install and load package randomForest

# set a seed to keep the same results when we re-run
set.seed(100)

randomforest <- randomForest(Y ~ .,
                        data = df,
                        na.action = na.omit,
                        mtry = 3, # square root of number of IV
                        importance = TRUE)

# call model variable to see output
randomforest
```

<br />

### Predictions in R

We can generate predictions with the **predict()** function:

```{r, eval = FALSE}
#create new df for comparison of actual and prediction
df_results <- df %>%
  select(Y) #optional, may help with readability

# newdata is what values of X1, X2... to predict for.
df_results$prediction <- predict(bagging, newdata = df)

# brief glimpse of the results
head(df_results)
```

These are the parts of the syntax that can be altered:

-   **df_results** is the results data frame I am creating. *You can name this anything you want to.*

-   **Y** is the Y variable I am trying to predict. *Replace this with the name of your Y variable.*

-   **bagging** is the variable I am saved my prior model to. *Rename this to what your prior model was named.*

-   **df** is the name of the data frame that houses the $x$ values I want to predict for. *Replace this with the name of your data frame with the* $x$ *values you want to predict for.*

<br />

#### Example in R

Take the following examples, where I predict whether a country is a **Liberal Market Economy (0) or Coordinated Market Economy (1)**. (These are topics from Comparative Political Economy).

```{r}
# Remember to install and load package randomForest

# set a seed to keep the same results when we re-run
set.seed(100)

bagging <- randomForest(as.factor(voc) ~ taxpercent + gini + econglobal,
                        data = df,
                        na.action = na.omit,
                        mtry = 3, # set to number of IV
                        importance = TRUE)

# call model variable to see output
bagging

```

We will discuss the output confusion matrix in the later section on [Evaluating Classification Models](https://politicalscience.github.io/ml/2-3.html).

<br />

Let us generate in-sample predictions for this model:

```{r}
#create new df for comparison of actual and prediction
df_results <- df %>%
  select(voc) #optional, may help with readability

# newdata is what values of X1, X2... to predict for.
df_results$prediction <- predict(bagging, newdata = df)

# brief glimpse of the results
head(df_results)
```

<br />

------------------------------------------------------------------------

[Table of Contents](#contents) \| [Course Homepage](https://politicalscience.github.io/ml)
