---
title: "Naive Bayes Classifier"
format:
    html:
        theme: darkly
        max-width: 800px
        fontsize: 12pt
subtitle: "Lesson 2.1, Applied Machine Learning"
---

[Course Homepage](https://politicalscience.github.io/ml)

## Table of Contents {#contents}

1.  [Introduction to Classification](#intro)
2.  [Naive Bayes Classifier](#bayes)
3.  [Naive Bayes in R](#r)

------------------------------------------------------------------------

Remember to load tidyverse. We will also need the package **e1071**.

```{r, message = FALSE}
library(e1071)
library(tidyverse)
```

Let us also load the dataset we will be using for these examples (feel free to load your own dataset)

```{r, message = FALSE}
df <- read_csv("voctaxdata.csv")
```

------------------------------------------------------------------------

[Table of Contents](#contents) \| [Course Homepage](https://politicalscience.github.io/ml)

# Introduction to Classification {#intro}

So far in this course, we have focused on predicting $y$ values. However, what about when we have a categorical or binary $y$, and our goal is to predict which category $k$ a specific observation falls into? This is called classification.

An example of a binary $y$ variable in which classification would be relevant is the following:

$$
y = \left\{    \begin{array}{lr}        
k = 0, & \text{Not a Democracy }\\        k=1, & \text{Democracy}    \end{array}\right\}
$$

Where we would try to predict what category of $y$ an observation falls into based on its $x$ values, by first predicting the probability of being in $y=k$, $\hat{\pi}$, and the assigning observations based on $\hat{\pi}$ to categories.

<br />

In the course Regression Analysis, we covered logistic regression, which is the more "traditional" method of classification prediction. Logistic regression a transformation of the linear regression model to ensure probabilities predicted remain between 0 and 1.

$$
\text{Logistic transformation function} \space f(m) = \frac{e^m}{1+e^m}
$$

$$
\text{where the linear model is the input:} \space m = \hat{\mathbf{\beta}}_0 + \hat{\mathbf{\beta}}_1 x_i
$$

$$
\text{and the resulting model is: } \hat{Ï€_i} = \frac{e^{\hat{\mathbf{\beta}}_0 + \hat{\mathbf{\beta}}_1 x}}{1 + e^{\hat{\mathbf{\beta}}_0 + \hat{\mathbf{\beta}}_1 x}}
$$

<br />

------------------------------------------------------------------------

[Table of Contents](#contents) \| [Course Homepage](https://politicalscience.github.io/ml)

# Naive Bayes Classifier {#bayes}

#### Bayes' Rule

However, there is an alternative approach to estimate the probabilities of being in a category of $y$: the Naive Bayes Classifier.

Naive Bayes, as the name suggests, relies on Bayes' Rule, which states the following:

$$
P(A|B) = \frac{P(B|A) P(A)}{P(B)}
$$

-   Where $P(A|B)$ is the probability of event A occurring, given $B$ is true.

-   Where $P(B|A)$ is the probability of event B occurring, given B is true.

-   Where $P(A)$ and $P(B)$ are the probabilities of observing $A$ and $B$ without any conditions (prior probabilities).

<br />

When we are making classifications, we are trying to predict the probability $\hat{\pi}$ of being in a certain category $y=k$, given the condition that our input values of our independent variables $X_i$ equals $x$. Thus:

$$
\hat{\pi} = P(Y=k|X_i = x)
$$

<br />

The equation above has the form of $P(A|B)$. Thus, using Bayes' rule, we know:

$$
P(Y=k|X_i=x) \propto P(Y)P(X_i=x|Y=k)
$$

Where:

-   $Y = k$ is when the output variable $Y$ has the value of category $k$

-   $X_i$ is the vector of covariates containing our independent variables.

-   $P(Y)$ is the probability that a randomly chosen observation is in class $k$ (called the prior). We can estimate this from the proportions of the sample

-   $P(X_i=x|Y=k)$ is the probability of a randomly chosen observation in class $k$, has the independent variable vector $X_i$ equal the value of $x$, our input values (This is called the likelihood).

    -   Basically, if we look at category $k$, what are the chances we get the input values that we are inputting into the model?

Note: I use $\propto$ (proportional) instead of $=$ (equal), because I have omitted the denominator to simplify the explanation.

-   The denominator does not depend on the category, so we will worry about it later.

<br />

#### Assumption of Independence Simplification

Because $X_i$ is a vector of covariates, we, in theory, would need to work out $P(X_i=x|Y=k)$ from a multivariate probability distribution.

But, we can use a simplification step: assuming that features are independent.

-   This assumption is often not true, but it does simplify the estimation process.

When we make this simplification, since independent probabilities are simply multiplied, we know the second part of the equation (the likelihood) is:

$$
\text{Let us first shorthand } P(X_i = x | Y = k) \text{ to } P(x|k)
$$

$$
P(x|k) = P(x_1|k) * P(x_2 |k) *P(x_3 | k) ...
$$

$$
P(x|k) = \prod\limits_{j=1}^J P (x_j|k)
$$

<br />

Now, we can plug this likelihood back into the Bayes' Rule equation, replacing $P(X_i = x|Y = k)$ to get:

$$
P(Y_i = k|X_i) \propto P(k) \prod\limits_{j=1}^J P(x_j|k)
$$

<br />

#### Turning Probabilities into Classification

But this equation we derived above (when we include the denominator) gives us probabilities. But, we want to classify - actually assign observations to categories.

-   Thus, we need to not only calculate probabilities, but assign each observation to a category depending on which category it is most probable to belong to.

<br />

To do this, we assign an algorithm to the function. This algorithm will return the class $k$, of the observation, which has the maximum probability. We actually can just ignore the denominator, since it is always constant within the same sample.

We introduce the the **argmax** function to assign the category, where the $k$ value with the highest result for the function is assigned the category. The result will be the prediction of which category of $y$ the observation belongs to:

$$
\hat{y}=argmax_{k \in \{ 1, ... k\}} P(k) \prod\limits_{j=1}^J P(x_j|k) 
$$

<br />

Despite the very strong (and often not met) assumptions that Naive Bayes makes, it still performs especially well.

-   Naive Bayes performs especially well when there are a lot of independent variables.

Thus, it is often the favoured technique for large models aiming to make classification predictions.

<br />

------------------------------------------------------------------------

[Table of Contents](#contents) \| [Course Homepage](https://politicalscience.github.io/ml)

# Naive Bayes in R {#r}

To run Naive Bayes, we need the library **e1071**:

```{r, message = FALSE, eval = FALSE}
library(e1071)
```

Remember to load tidyverse.

```{r, message = FALSE, eval = FALSE}
library(tidyverse)
```

Let us also load the dataset we will be using for these examples (feel free to load your own dataset)

```{r, message = FALSE, eval = FALSE}
df <- read_csv("voctaxdata.csv")
```

<br />

#### Creating the Model in R

We can run a Naive Bayes with the function **naiveBayes()**. Let us first generate the model, then generate the predictions. The syntax is as follows:

```{r, eval = FALSE}

nb_model <- naiveBayes(Y ~ X1 + X2, data = df)
```

These are the parts of the syntax that can be altered:

-   **nb_model** is the variable I am saving my model to. *You can name this anything you want to.*

-   **Y** is the Y variable (Dependent variable) you are trying to predict, and **X1, X2** are the X variables (independent variable) you are using to get your prediction. *Replace these with the variables you want to use.*

    -   NOTE: Always put the Y variable before the X variable. Separate the two with a tilda **\~**
    -   NOTE: You can add more simply by using a **+** sign and adding another variable.

-   **df** is the name of the data frame that I am drawing these X and Y variables from. *Replace this with the name of your data frame.*

<br />

#### Predictions in R

Now, let us generate predictions for our sample. You can predict in-sample data by setting **newdata** = the data frame you used for regression. You can predict out-of-sample data by using a dataframe with the same variables but new values. The syntax is as follows:

```{r, eval = FALSE}
#create new df for comparison of actual and prediction
nb_results <- df %>%
  select(Y) #optional, may help with readability

# newdata is what values of X1, X2... to predict for.
nb_results$prediction <- predict(nb_model, newdata = df)

# brief glimpse of the results
head(nb_results)
```

These are the parts of the syntax that can be altered:

-   **nb_results** is the results data frame I am creating. *You can name this anything you want to.*

-   **Y** is the Y variable I am trying to predict. *Replace this with the name of your Y variable.*

-   **nb_model** is the variable I am saved my prior model to. *Rename this to what your prior model was named.*

-   **df** is the name of the data frame that houses the $x$ values I want to predict for. *Replace this with the name of your data frame with the* $x$ *values you want to predict with.*

<br />

#### Example in R

Take the following examples, where I predict whether a country is a **Liberal Market Economy (0) or Coordinated Market Economy (1)**. (These are topics from Comparative Political Economy).

```{r}

#Naive Bayes model
nb_model <- naiveBayes(as.factor(voc) ~ taxpercent + gini + econglobal, data = df)

#Add predictions to new data frame
nb_results <- df %>%
  select(voc) #optional, may help with readability

# newdata is what values of X1, X2... to predict for.
nb_results$prediction <- predict(nb_model, newdata = df)

# brief glimpse of the results
head(nb_results)
```

We will talk about prediction accuracy metrics in the later section on [Evaluating Classification Models](https://politicalscience.github.io/ml/2-3.html).

<br />

------------------------------------------------------------------------

[Table of Contents](#contents) \| [Course Homepage](https://politicalscience.github.io/ml)
