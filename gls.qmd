# Generalised Least Squares {#generalised-least-squares}

In the last chapter, we discussed the Ordinary Least Squares estimator, which can only be used in a linear model with specific assumptions met.

In this chapter, we weaken the assumption of [spherical errors](ols.qmd#assumption-5-spherical-errors) and [i.i.d.](ols.qmd#assumption-2-independent-and-identically-distributed-i.i.d.), and explore a new estimator called the Generalised Least Squares. We first discuss the consequences of heteroscedasticity and introduce the weighted least squares estimator. Then, we discuss the consequences of autocorrelation, and discuss the generalised least squares estimator.

<br />

## Conditional Heteroscedasticity

For the classical linear model, one of the assumptions was spherical errors (@def-sphericalerrors). This was an assumption made on the variance-covariance matrix of error term $\eps_t$. However, this assumption is often violated - in fact, in many fields like econometrics, we assume it is violated by default. What occurs when the other 4 assumptions of the classical model are met, but spherical errors is violated? This chapter explains how we can deal with this scenario.

Spherical Errors was broken into two parts. No autocorrelation implied that the covariance elements were all equal to 0, and homoscedasticity implied that all the variances were equal to some constant $\sigma^2$.

For the first part of this chapter, we will keep the no autocorrelation assumption, but weaken homoscedasticity. Instead of assuming all observations $t$ have the same error variable $\sigma^2$, we will now assume that each different observation has different error variances $\sigma^2_t$.

::: {#def-heteroscedasticity style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;"}
## Conditional Heteroscedasticity

Conditional heteroscedasticity says that the covariance matrix of errors $\eps_t$ takes the form:

$$
\V(\b\eps|\b X) = \b\Omega= \begin{pmatrix}
\sigma^2_1 & 0 & 0 & \dots \\
0 & \sigma^2_2 & 0 & \dots  \\
0 & 0& \sigma^2_3 & \vdots \\
\vdots & \vdots & \dots & \ddots
\end{pmatrix}
$$

This implies that the error variance $\sigma^2_i$ depends on observation $i$, and specifically, the values of the explanatory variables $X_{i1}, \dots, X_{ip}$ for that observation $i$.
:::

![](images/clipboard-1713529842.png){fig-align="center" width="90%"}

The above is a residual plot of OLS residuals $\hat\eps_i$ against some explanatory variable $X$. Notice how for homoscedasticity, the variance of the error terms (how spread out they are up-down wise) is constant for any value of $X$.

For heteroscedasticity, we can clearly see that the residual variance is smaller for some $X$ values, and larger for other $X$ values. If you see a pattern in your residual plot, it is likely heteroscedasticity.

When heteroscedasticity is present, that means our spherical errors assumption of the classical linear model is violated. What implications does this have?

Under heteroscedasticity, OLS is still unbiased, since by @thm-olsunbiased, we see that the unbiasedness of OLS does not require spherical errors.

However, OLS now has inaccurate standard error estimates since OLS variance (@thm-varols) used the condition of spherical errors. OLS is also no longer the best linear unbiased estimator (the linear unbiased estimator with the least variance), since the Gauss-Markov Theorem (@thm-gaussmarkov) requires spherical errors.

In this chapter, we will introduce two ways to deal with heteroscedasticity. First, we will discuss ways to "correct" the incorrect OLS standard errors, while sticking with the OLS estimator. Then, we will introduce a new estimator that is BLUE under Gauss-Markov when heteroscedasticity is present.

<br />

## Robust Standard Errors {#robust-standard-errors}

If conditional heteroscedasticity (@def-heteroscedasticity) is present in our population model, then the standard OLS variance (@thm-varols) is inaccurate, since that variance calculation used homoscedasticity.

Thus, we need to find the "true" standard errors under homoscedasticity.

::: {#thm-heterovar style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;"}
## Heteroscedasticity Variance of OLS

The variance of the OLS estimator under heteroscedasticity is given by

$$
\V(\hat{\b\beta}|\b X) = (\b{X^\top X})^{-1}\b X^\top \begin{pmatrix}
\sigma^2_1 & 0 & 0 & \dots \\
0 & \sigma^2_2 & 0 & \dots  \\
0 & 0& \sigma^2_3 & \vdots \\
\vdots & \vdots & \dots & \ddots
\end{pmatrix} \b X(\b{X^\top X})^{-1}
$$
:::

<br />

**Proof:** Let us start where we left off from @eq-olsimplify during the proof of unbiasedness. This tells us the variance of the OLS estimator is:

$$
\V(\hat{\b\beta}|\b X) = \V(\b\beta + (\b{X^\top X})^{-1} \b{X^\top \eps})
$$

We know that $\b\beta$ is a vector of fixed true population values. $(\b{X^\top X})^{-1} \b X^\top$ can also be considered a fixed constant matrix because we are conditioning our variance on $\b X$. Thus, we can use @thm-variance to rewrite the above as

$$
\V(\hat{\b\beta}|\b X) = (\b{X^\top X})^{-1}\b X^\top \V(\b\eps| \b X)[(\b{X^\top X})^{-1}\b X^\top]^{-1}
$$

With the properties of matrix inverses and transposes, we can determine that $[(\b{X^\top X})^{-1}\b X^\top]^{-1}$ is equivalent to $\b X(\b{X^\top X})^{-1}$. Thus, plugging this in, we get

$$
\V(\hat{\b\beta}|\b X) = (\b{X^\top X})^{-1}\b X^\top \V(\b\eps| \b X) \b X(\b{X^\top X})^{-1}
$$

Now, by conditional heteroscedasticity (@def-heteroscedasticity), we can replace $\V(\b\eps|\b X)$ with the heteroscedasticity variance matrix of errors:

$$
\V(\hat{\b\beta}|\b X) = (\b{X^\top X})^{-1}\b X^\top \begin{pmatrix}
\sigma^2_1 & 0 & 0 & \dots \\
0 & \sigma^2_2 & 0 & \dots  \\
0 & 0& \sigma^2_3 & \vdots \\
\vdots & \vdots & \dots & \ddots
\end{pmatrix} \b X(\b{X^\top X})^{-1}
$$

And thus, we have proven the OLS variance under heteroscedasticity. When we actually write the formula, we will typically replace the matrix with $\b\Omega$.

However, we of course do not know what the population values of $\sigma^2_1, \dots, \sigma^2_n$ are. We can estimate them with OLS residuals as follows:

$$
\sigma_t^2 \approx s_t^2 = \eps^2_t
$$

And our robust standard errors for any parameter $\hat\beta_j$ are simply

$$
se(\hat\beta_j) = \sqrt{[(\b{X^\top X})^{-1}\b X^\top \hat{\b\Omega} \b X(\b{X^\top X})^{-1}]}_{jj}
$$

We can conduct statistical inference in the same way as we did for OLS, but replacing the standard errors with our new robust standard errors.

<br />

## Heteroscedasticity Transformation

When heteroscedasticity (@def-heteroscedasticity) is present, we could just use the OLS estimator with robust standard errors. However, OLS with robust standard errors is no longer the best linear unbiased estimator, as the Gauss-Markov theorem (@thm-gaussmarkov) depends on spherical errors.

The weighted least squares (WLS) estimator is an alternative of OLS (and is considered a special case of the generalised least squares estimator we will see later in the chapter).

Suppose that our heteroscedasticity is of the form:

$$
\V(\eps_t|\b X) = \sigma^2 \  \Omega(X_t)
$$ {#eq-wlsvariance}

Where $\sigma^2$ is some constant (can be 1) and $\Omega(X_t)$ is some function of $X_t$ that explained the difference in error variance between individuals.

Now, consider the variance of this (modified) error term that is the original error $\eps_t$ divided by the square root $\Omega(X_t)$, which is a function of $X_t$:

$$
\V \left( \frac{1}{\sqrt{\Omega(X_t)}}\eps_t \biggr| X_t \right)
$$

Using @thm-variance, we know $\V(cu) = c^2 \V(u)$ if $c$ is a constant and $u$ is a random variable. This function also applies to a function $a(x)$ where $\V(a(x) u) = a(x)^2 \V(u)$. Using this, we can determine the variance of the modified error term is equal to

$$
\V \left( \frac{1}{\sqrt{\Omega(X_t)}}\eps_t \biggr| X_t \right) = \left(\frac{1}{\sqrt{\Omega(X_t)}}\right)^2 \V(\eps_t | X_t)
$$

And from @eq-wlsvariance, we can plug in $\V(\eps_t|X_t) = \sigma^2 \ \Omega(X_t)$ to get

$$
\V \left( \frac{1}{\sqrt{\Omega(X_t)}}\eps_t \biggr| X_t \right) = \frac{1}{\Omega(X_t)}\sigma^2 \ \Omega(X_t) = \sigma^2
$$

What does this tell us? Well it tells us the modified error term $\frac{1}{\sqrt{\Omega(X_i)}} \eps_t$ has a variance of constant $\sigma^2$ for all units $i$, which does not dependent on $X_i$. What does this mean? Well, our modified error term is now meeting homoscedasticity (@def-sphericalerrors)!

However, we obviously cannot just divide the error term by $1/\sqrt{\Omega(X_t)}$ - that changes our linear model. What we can though do is divide every term of our linear model by $1/\sqrt{\Omega(X_t)}$:

$$
\frac{Y_t}{\sqrt{\Omega(X_t)}} = \beta_0\left(\frac{1}{\sqrt{\Omega(X_t)}}\right) + \beta_1 \left(\frac{X_{t1}}{\sqrt{\Omega(X_t)}}\right) + \dots +  \frac{\eps_t}{\sqrt{\Omega(X_t)}}
$$ {#eq-wlstransformed}

And since we divide both side by $1/\sqrt{\Omega(X_t)}$, and our model is conditional on individual $t$ (see all the subscripts), that means this model is still "equivalent" to our original linear model.

Thus, the idea of weighted least squares is to "transform" our heteroscedastic linear model into one that meets homoscedasitcity. We can then just use OLS on our new homoscedastic regression, and since homoscedsaticity is met, Gauss-Markov (@thm-gaussmarkov) is met, and our estimator is once again the best linear unbiased estimator.

<br />

## Weighted Least Squares {#weighted-least-squares}

In the last section and @eq-wlstransformed, we showed that dividing every term in the linear model by $1/\sqrt{\Omega(X_t)}$, where $\Omega(X_t)$ is some function of $X_t$, can get rid of heteroscedasticity, allowing us to use OLS to estimate the regression.

Let us formalise this idea with our linear algebra representation of the linear model. First, let us define a matrix $\b\Omega^{-1/2}$, which will be the inverse of the square root of the heteroscedastic covariance matrix given in @def-heteroscedasticity:

$$
\b\Omega^{-1/2} = \begin{pmatrix}
1/\sigma_1 & 0 & \dots & 0 \\
0 & 1/\sigma_2 & \vdots & 0 \\
\vdots & \dots & \ddots & \vdots \\
0 & 0 & \dots & 1/\sigma_n
\end{pmatrix}
$$

Note that by this definition, $\b\Omega^{-1/2} \b\Omega^{-1/2} = \b\Omega^{-1}$, since $\b\Omega^{-1/2}$ is a square root.

We can rewrite our transformed linear model from @eq-wlstransformed in linear algebra form as:

$$
\underbrace{\b\Omega^{-1/2}}_{\b y^*} \b y = \underbrace{\b\Omega^{-1/2} \b X}_{\b X^*} \b \beta + \underbrace{\b\Omega^{-1/2} \b \eps}_{\b \eps^*}
$$ {#eq-wlstransformedmatrix}

We can see that this model is a transformed linear model $\b y^* = \b X^* \b\beta + \b\eps^*$. Using our OLS estimator from @def-ols, we know the OLS solution for $\hat{\b\beta}$ for this model is

$$
\hat{\b\beta} = (\b X^{*\top} \b X^*)^{-1} \b X^{*\top} \b y^*
$$

And if we plug in our definitions of $\b y^*$, and $\b X^*$ from @eq-wlstransformedmatrix, we can get

$$
\hat{\b\beta} = \left[(\b\Omega^{-1/2} \b X)^\top (\b\Omega^{-1/2} \b X) \right]^{-1} (\b\Omega^{-1/2} \b X) (\b\Omega^{-1/2} \b y)
$$

And using the properties of matrix transposes, and that $\b\Omega^{-1/2} \b\Omega^{-1/2} = \b\Omega^{-1}$, we can get

$$
\begin{align}
\hat{\b\beta} & = [\b X^\top \b\Omega^{-1/2} \b\Omega^{-1/2} \b X]^{-1} \b X^\top \b\Omega^{-1/2} \b\Omega^{-1/2} \b y \\
& = (\b X^\top \b\Omega^{-1} \b X)^{-1} \b X^\top \b\Omega^{-1} \b y
\end{align}
$$

::: {#def-wls style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;"}
## Weighted Least Squares Estimator

The $\hat{\b\beta}$ estimates produced by the weighted least squares estimator is

$$
\hat{\b\beta} = (\b X^\top \b\Omega^{-1} \b X)^{-1} \b X^\top \b\Omega^{-1} \b y
$$
:::

::: {.callout-note collapse="true" appearance="simple"}
## Alternative Way of Thinking about WLS

We focused on weighted least squares as first transforming a linear model to meet spherical errors, then performing OLS.

However, we can also think about weighted least squares in terms of minimising a weighted sum of squared residuals. The normal OLS estimator minimises the standard sum of squared residuals:

$$
SSR = \sum (Y_t - \hat Y_t)^2
$$

The weighted least squares minimises a weighted version of the sum of squared residuals:

$$
WSSR = \sum\frac{1}{w_i}(Y_t - \hat Y_t)^2
$$

When we set our weights equal to $\b\Omega^{-1/2}$, both the weighted sum of squared residuals solution and the transformed-model solution get the same result.
:::

Since the weighted least squares is essentially an OLS estimator on a transformed linear model that has homoscedasticity, it is also covered by the Gauss-Markov theorem (@thm-gaussmarkov). Thus, the Weighted Least Squares estimator is the linear unbiased estimator with the least variance, if heteroscedasticity is present in our original linear model.

For statistical inference with this estimator, see the [generalised least squares](#generalised-least-squares-estimator) section.

Of course, the weighted least squares estimator requires us to know the structure of $\b\Omega$, the covariance-variance matrix of the error term. This is a big limitation as we typically do not know the structure of this. We will explore weights to estimate $\hat{\b\Omega}$ in the [feasible generalised least squares](#feasible-generalised-least-squares) estimator later in the chapter.

<br />

## Autocorrelation

So far, we have weakened the spherical errors assumption (@def-sphericalerrors) only through weakening homoscedasticity to heteroscedasticity.

However, the spherical errors assumption also states another assumption - no autocorrelation - i.e. the covariance between two error terms $Cov(\eps_i, \eps_j) = 0$. This assumption is typically implied by the [i.i.d](ols.qmd#assumption-2-independent-and-identically-distributed-i.i.d.) assumption.

However, there are settings where we cannot assume i.i.d. and no autocorrelation. For example, in time series settings of one variable $Y$ over a set of time periods $t$, we would expect a previous value in time $t-1$ to have some effect on $t$, which implies that values from $t-1$ and $t$ are not independent random variables, and might have some covariance. We will discuss time series in far more detail later in the course.

Similarly, in datasets with spatial characteristics, we might expect some neighbouring areas to have correlated errors. For example, if we are modelling $Y$ as unemployment rate, and we have a bunch of $X_1, \dots, X_p$, we might be missing out on some regional trends, like a regional natural disaster that is causing some neighbouring counties to have higher than expected unemployment rates $Y$.

The presence of autocorrelation means that our error-covariance matrix will not be a diagonal matrix, as some $Cov(\eps_i, \eps_j) ≠ 0$:

$$
\V(\b\eps | \b X) = \b\Omega = \begin{pmatrix}
\V \eps_1 & Cov(\eps_1, \eps_2) & Cov(\eps_1, \eps_3) & \dots \\
Cov(\eps_2, \eps_1) & \V \eps_2 & Cov(\eps_2, \eps_3) & \dots \\
Cov(\eps_3, \eps_1) & Cov(\eps_3, \eps_2) & \V\eps_3 & \vdots \\
\vdots & \vdots & \dots & \ddots
\end{pmatrix}
$$

Just like with conditional heteroscedasticity, the presence of autocorrelation alone does not cause bias in OLS, since OLS unbiasedness (@thm-olsunbiased) does not depend on spherical errors. However, OLS will no longer be the best-linear unbiased estimator, and (as we will cover more in detail in the time-series chapters later), autocorrelation is often associated with violations of exogeneity (@def-strictexog).

If we assume that only autocorrelation is present (and no violation of exogeneity is occurring), then we have two approaches, just like in heteroscedasticity.

We could stick with the OLS estimator, but use **Autocorrelation and Heteroscedasticity Robust (HAC) standard errors**. If we are sampling with clustered sampling, we can use clustered standard errors (see below). I will not derive them here, since these standard errors aren't too commonly used, and are very difficult to derive.

::: {.callout-note collapse="true" appearance="simple"}
## Clustered Standard Errors

Clustered standard errors are when you have done clustered sampling for your observations. For example, you randomly sample 100 people from 100 different villages.

The errors of observations belonging to the same cluster (say village) might exhibit correlation, while errors of observtations from distinct clusters are assumed to be uncorrelated.

Our covariance matrix might take the form:

$$
\b\Omega = \begin{pmatrix}
\b\Sigma_1 & 0 & \dots & 0 \\
0 & \b\Sigma_2 & & 0 \\
\vdots & 0 & \ddots & 0 \\
0 & 0& \dots & \b\Sigma_G
\end{pmatrix}
$$

Where $\b\Sigma_1, \dots \b\Sigma_G$ are intracluster covariance-variance error matrices, that exhibit autocorrelation. We just insert this matrix where we would insert it for [robust standard errors](#robust-standard-errors).
:::

Or, we could use another estimator, such as the Cochrane-Orcutt Estimator or the Generalised Least Squares estimator, which we will introduce later in the chapter.

<br />

## Cochrane-Orcutt Estimator {#cochrane-orcutt-estimator}

Let us start off with a linear model, but in time-series form, so instead of observation $i$, each variable/observation will be from a point in time $t$. Our linear model will be

$$
Y_t = \beta_0 + \beta_1 X_{t}  + \eps_t
$$

Let us say some autocorrelation is present - which means the error term $\eps_t$ is related to some other error term of another observation. More specifically, let us assume an **autoregressive order 1** autocorrelation, which means that $\eps_t$ is correlated with the error term of the time period before, $\eps_{t-1}$. We can model this as:

$$
\eps_t = \rho \eps_{t-1} + u_t
$$

Where $\rho$ is the coefficient describing the correlation between $\eps_t$ and $\eps_{t-1}$, and $u_t$ is the error term of this smaller model that is the part of $\eps_t$ that is not explained by $\eps_{t-1}$.

Thus, our true linear model is:

$$
Y_t = \beta_0 + \beta_1 X_{t} + \rho \eps_{t-1} + u_t
$$

If we could get the $\rho\eps_{t-1}$ term out of this equation, we will no longer have any autocorrelation, since $u_t$ is not correlated/explained by past error terms. How do we do this?

Consider the linear model for $Y_{t-1}$:

$$
Y_{t-1} = \beta_0 + \beta_1 X_{t-1}+ \eps_{t-1}
$$

Now, let us multiply both sides (every term) with parameter $\rho$:

$$
\rho Y_{t-1} = \rho\beta_0 + \rho\beta_1 X_{t-1} + \rho\eps_{t-1}
$$

Now, let us subtract our model for $\rho Y_{t-1}$ from our original $Y_t$:

$$
\begin{array}{ccccccc}
Y_t & = & \beta_0 & + & \beta_1X_t & + & \rho\eps_{t-1} + u_t \\
\rho Y_{t-1} & = & \rho\beta_0 & + & \rho\beta_1X_{t-1} & + & \rho\eps_{t-1} \\
\hline 
Y_t - \rho Y_{t-1} & = & \beta_0(1-\rho) & + & \beta_1(X_t - \rho X_{t-1}) & + & u_t
\end{array}
$$

Now we can see we have a new transformed model with only error term $u_t$ which is not autocorrelated with $t-1$.

$$
\underbrace{Y_t - \rho Y_{t-1}}_{Y_t^*} = \underbrace{\beta_0(1-\rho)}_{\beta_0^*} + \beta_1 \underbrace{(X_t - \rho X_{t-1})}_{X_t^*} + \underbrace{u_t}_{\eps_t^*}
$$

Which we can rewrite more simply as:

$$
Y_t^* = \beta_0^* + \beta_1 X_t^* + \eps_t^* 
$$

Since this model no longer has autocorrelation and now meets spherical errors, we can use the OLS estimator on this transformed model, and this will be the best linear unbiased estimator.

Do not that since this model depends on subtracting time $t-1$ from time $t$, the first time period in a time-series must be thrown out to complete this transformation. In small sample sizes, this can lead to efficiency losses.

<br />

## Generalised Least Squares Estimator {#generalised-least-squares-estimator}

Both the Cochrane-Ocrutt Estimation and the Weighted Least Squares estimator can be thought of as specific versions of the generalised least squares estimator.

In spherical errors, we assumed that the population variance-covariance matrix of errors had the following form:

$$
\V(\b\eps | \b X) = \E(\b{\eps\eps^\top}) =  \sigma^2 \b I_n
$$

And the variance is equivalent to $\E(\b{\eps\eps^\top})$ because we assume by strict exogeneity (@def-strictexog) that $\E(\b\eps) = 0$. So the formula for variance (@def-var) with strict exogeneity equals the variance of the error term.

In the generalised least squares estimator, we assume that the variance-covariance matrix of errors has the form:

$$
\V(\b\eps | \b X) = \E(\b{\eps\eps^\top}) = \sigma^2 \b\Omega
$$

Where $\sigma^2$ is an unknown scalar constant, but $\b\Omega$ is a known matrix that is equivalent to the population variance-covariance matrix of errors (up to a scalar factor).

Let us define a matrix $\b\Omega^{-1/2}$, which will be the inverse of the square root of $\b\Omega$. This means that the following should be true:

$$
\b\Omega^{-1/2} \ \b\Omega \ {\b\Omega^{-1/2}}^\top = \b I
$$ {#eq-glsproperty}

We can use $\b\Omega^{-1/2}$ to transform our original model $\b y = \b{X\beta} + \b\eps$ to get:

$$
\underbrace{\b\Omega^{-1/2}}_{\b y^*} \b y = \underbrace{\b\Omega^{-1/2} \b X}_{\b X^*} \b \beta + \underbrace{\b\Omega^{-1/2} \b \eps}_{\b \eps^*}
$$ {#eq-glstransformation}

This transformed model should have spherical errors. Let us calculate the variance of $\b\eps^*$ by plugging in the definition of $\b\eps^*$:

$$
\begin{align}
\V (\b\eps^* | \b X) & = \E(\b\eps^* \b\eps^{*\top}) \\
& = \E(\b\Omega^{-1/2} \b \eps \b\eps^\top {\b\Omega^{-1/2}}^\top) \\
& = \b\Omega^{-1/2} \E(\b{\eps \eps^\top}) \b\Omega^{-1/2} \\
& = \b\Omega^{-1/2} \sigma^2 \b\Omega \b\Omega^{-1/2}
\end{align}
$$

And by moving scalar $\sigma^2$ to the front, and using the property from @eq-glsproperty, we get:

$$
\V (\b\eps^* | \b X) = \sigma^2 \underbrace{\b\Omega^{-1/2} \b\Omega \b\Omega^{-1/2}}_{\b I} = \sigma^2 \b I
$$

Thus proving this transformed model meets the spherical errors assumption (@def-sphericalerrors). Thus, we can use OLS on this transformed model, and it will be the best linear unbiased estimator. Our OLS estimator (@def-ols) of the transformed model will be:

$$
\hat{\b\beta} = (\b X^{*\top} \b X^*)^{-1} \b X^{*\top} \b y^*
$$

And if we plug in our definitions of $\b y^*$, and $\b X^*$ from @eq-glstransformation, we can get

$$
\hat{\b\beta} = \left[(\b\Omega^{-1/2} \b X)^\top (\b\Omega^{-1/2} \b X) \right]^{-1} (\b\Omega^{-1/2} \b X) (\b\Omega^{-1/2} \b y)
$$

And using the properties of matrix transposes, and that $\b\Omega^{-1/2} \b\Omega^{-1/2} = \b\Omega^{-1}$, we can get

$$
\begin{align}
\hat{\b\beta} & = [\b X^\top \b\Omega^{-1/2} \b\Omega^{-1/2} \b X]^{-1} \b X^\top \b\Omega^{-1/2} \b\Omega^{-1/2} \b y \\
& = (\b X^\top \b\Omega^{-1} \b X)^{-1} \b X^\top \b\Omega^{-1} \b y
\end{align}
$$

::: {#def-gls style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;"}
## Generalised Least Squares Estimator

The GLS estimator is

$$
\hat{\b\beta} = (\b X^\top \b\Omega^{-1} \b X)^{-1} \b X^\top \b\Omega^{-1} \b y
$$

Where $\b\Omega$ is the population variance-covariance matrix of errors. The variance is

$$
\V\hat{\b\beta} = (\b X^\top \b\Omega^{-1} \b X)^{-1}
$$
:::

<br />

We can see that OLS is a GLS estimator, when $\b\Omega = \b I$.

<br />

## Feasible Generalised Least Squares {#feasible-generalised-least-squares}

We have derived the generalised least squares (GLS) estimator's parameter estimates and variance. However, we do not know what $\b\Omega$ is, as it is made up of the population variance and covariances of error terms $\eps_i$. Thus, we need some estimator of $\hat{\b\Omega}$ to actually implement generalised least squares. When combining this estimator of $\hat{\b\Omega}$ with GLS, we call this new estimator the feasible generalised least squares (FGLS) estimator.

We have two ways to get a consistent estimator of $\b\Omega$. First, if we have some strong idea of the form of heteroscedasticity or autocorrelation in our population (perhaps due to past research or some strong internal reasoning), we could estimate $\b\Omega$.

For example, if we believe that there is autocorrelation defined by the Autoregerssive first order process from the [Cochrane-Orcutt Estimator](#cochrane-orcutt-estimator), then we know the structure of $\b\Omega$ is:

$$
\b\Omega = \begin{pmatrix}
1 & \rho & \rho^2 & \dots & \rho^{n-1} \\
\rho & 1 & \rho & \dots & \rho^{n-2} \\
\rho^2 & \rho & 1 & \dots & \rho^{n-3} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\rho^{n-1} & \rho^{n-2} & \rho^{n-3} & \dots & 1
\end{pmatrix}
$$

Other examples of knowing $\b\Omega$ include financial data heteroscedasticity being proportional to some known factor like the market capitalisation of a unit, or spatial data where autocorrelation can be modeled as a \[decaying\] function of distance.

The other option (which is far more common) is to estimate $\b\Omega$. We typically produce a estimate $\hat{\b\Omega}$ by first running an OLS regression, in which we will obtain the residuals $\hat\eps_i$. These can be used to estimate the structure of $\b\Omega$, producing $\hat{\b\Omega}$. Then, using this estimate $\hat{\b\Omega}$, we can run feasible GLS, and obtain an estimator with less error.

::: {#def-fgls style="border-style: solid; border-radius: 8px; border-width: 0.5px; padding-top: 5px;   padding-right: 5px; padding-bottom: 5px; padding-left: 5px;"}
## Feasible Generalised Least Squares Estimator

The feasible generalised least squares estimator produces estimates

$$
\hat{\b\beta} = (\b X^\top \hat{\b\Omega}^{-1} \b X)^{-1} \b X^\top \hat{\b\Omega}^{-1} \b y
$$

Where $\hat{\b\Omega}$ is the estimated variance-covariance matrix of the error term $\eps_i$, frequently through an OLS estimation before conducting feasible generalised least squares. The variance of the estimator is

$$
\V\hat{\b\beta} = (\b X^\top \hat{\b\Omega}^{-1} \b X)^{-1}
$$
:::

<br />

The square root of the variance of the FGLS estimator are the standard errors, and we can conduct t-tests just as we did for OLS.

However, when we estimate $\hat{\b\Omega}$ with OLS (or any other method), we of course have some imprecision in our estimates. Econometricians have shown that the feasible GLS estimator often is far worse than the hypothetical perfect GLS. Very often, feasible GLS will actually result in larger variances of estimates.

There is also some risk with feasible GLS. Often, heteroscedasticity and autocorrelation occur in our estimated OLS models not because the population actually has heteroscedasticity or autocorrelation, but rather, our original linear model is missing some explanatory variables which causes other violations in our classical linear model, such as exogeneity violations. This mispecified nature will not only make FGLS even more imprecise, but also has the potential to bias FGLS estimates.

Thus, FGLS is not super popular in most applied statistician's toolkit, and the default tends to be sticking to OLS with either robust standard errors or Heteroscedasticity-and-autocorrelation (HAC) robust standard errors.
