<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Background: Ordinary Least Squares Theory</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="0_files/libs/clipboard/clipboard.min.js"></script>
<script src="0_files/libs/quarto-html/quarto.js"></script>
<script src="0_files/libs/quarto-html/popper.min.js"></script>
<script src="0_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="0_files/libs/quarto-html/anchor.min.js"></script>
<link href="0_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="0_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="0_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="0_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="0_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Background: Ordinary Least Squares Theory</h1>
            <p class="subtitle lead">Quantitative Methods (Causal Inference)</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Table of Contents</h2>
   
  <ul>
  <li><a href="#ordinary-least-squares-estimator" id="toc-ordinary-least-squares-estimator" class="nav-link active" data-scroll-target="#ordinary-least-squares-estimator"><strong>Ordinary Least Squares Estimator</strong></a>
  <ul class="collapse">
  <li><a href="#deriving-the-estimator" id="toc-deriving-the-estimator" class="nav-link" data-scroll-target="#deriving-the-estimator">Deriving the Estimator</a></li>
  <li><a href="#regression-anatomy-theorem" id="toc-regression-anatomy-theorem" class="nav-link" data-scroll-target="#regression-anatomy-theorem">Regression Anatomy Theorem</a></li>
  </ul></li>
  <li><a href="#ols-as-an-unbiased-estimator" id="toc-ols-as-an-unbiased-estimator" class="nav-link" data-scroll-target="#ols-as-an-unbiased-estimator"><strong>OLS as an Unbiased Estimator</strong></a>
  <ul class="collapse">
  <li><a href="#unbiased-estimators" id="toc-unbiased-estimators" class="nav-link" data-scroll-target="#unbiased-estimators">Unbiased Estimators</a></li>
  <li><a href="#conditions-for-unbiasedness" id="toc-conditions-for-unbiasedness" class="nav-link" data-scroll-target="#conditions-for-unbiasedness">Conditions for Unbiasedness</a></li>
  <li><a href="#proof-of-unbiasedness" id="toc-proof-of-unbiasedness" class="nav-link" data-scroll-target="#proof-of-unbiasedness">Proof of Unbiasedness</a></li>
  </ul></li>
  <li><a href="#ols-as-a-consistent-estimator" id="toc-ols-as-a-consistent-estimator" class="nav-link" data-scroll-target="#ols-as-a-consistent-estimator"><strong>OLS as a Consistent Estimator</strong></a>
  <ul class="collapse">
  <li><a href="#asymptotically-consistent-estimators" id="toc-asymptotically-consistent-estimators" class="nav-link" data-scroll-target="#asymptotically-consistent-estimators">Asymptotically Consistent Estimators</a></li>
  <li><a href="#conditions-for-asymptotic-consistency" id="toc-conditions-for-asymptotic-consistency" class="nav-link" data-scroll-target="#conditions-for-asymptotic-consistency">Conditions for Asymptotic Consistency</a></li>
  <li><a href="#proof-of-asymptotic-consistency" id="toc-proof-of-asymptotic-consistency" class="nav-link" data-scroll-target="#proof-of-asymptotic-consistency">Proof of Asymptotic Consistency</a></li>
  </ul></li>
  <li><a href="#further-properties-of-ols" id="toc-further-properties-of-ols" class="nav-link" data-scroll-target="#further-properties-of-ols"><strong>Further Properties of OLS</strong></a>
  <ul class="collapse">
  <li><a href="#ols-as-a-conditional-expectation-function" id="toc-ols-as-a-conditional-expectation-function" class="nav-link" data-scroll-target="#ols-as-a-conditional-expectation-function">OLS as a Conditional Expectation Function</a></li>
  <li><a href="#ols-as-a-method-of-moments-estimator" id="toc-ols-as-a-method-of-moments-estimator" class="nav-link" data-scroll-target="#ols-as-a-method-of-moments-estimator">OLS as a Method of Moments Estimator</a></li>
  <li><a href="#ols-as-a-maximum-likelihood-estimator" id="toc-ols-as-a-maximum-likelihood-estimator" class="nav-link" data-scroll-target="#ols-as-a-maximum-likelihood-estimator">OLS as a Maximum Likelihood Estimator</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>This background chapter introduces some key theory regarding the ordinary least squares estimator, that will inform the conditions and assumptions we will need to make in order to achieve unbiased causal estimation.</p>
<hr>
<section id="ordinary-least-squares-estimator" class="level1">
<h1><strong>Ordinary Least Squares Estimator</strong></h1>
<section id="deriving-the-estimator" class="level3">
<h3 class="anchored" data-anchor-id="deriving-the-estimator">Deriving the Estimator</h3>
<p>Let us define our estimation vector <span class="math inline">\boldsymbol{\hat{\beta}}</span> as the value of <span class="math inline">\boldsymbol{\hat{\beta}}</span> that minimises the <a href="https://politicalscience.github.io/regression/1.html#estimation-process">sum of squared errors</a>:</p>
<p><span class="math display">
\boldsymbol{\hat{\beta}} = \min\limits_{b} (\mathbf y - \mathbf{Xb})^T (\mathbf y - \mathbf{Xb}) = \min\limits_b S(\mathbf b)
</span></p>
<p>We can expand <span class="math inline">S(\mathbf b)</span> as follows:</p>
<p><span class="math display">
\begin{split}
S(\mathbf b) &amp; = \mathbf y^T \mathbf y \color{red}{ - \mathbf b^T \mathbf X^T \mathbf y - \mathbf y^T \mathbf{Xb}} \color{black} + \mathbf b^T \mathbf X^T \mathbf{Xb} \\
&amp; = \mathbf y^T \mathbf y \color{red}{- 2\mathbf b^T \mathbf X^T \mathbf y} \color{black} + \mathbf b^T \mathbf X^T \mathbf{Xb}
\end{split}
</span></p>
<p>Taking the partial derivative in respect to <span class="math inline">b</span>:</p>
<p><span class="math display">
\frac{\partial S(\mathbf b)}{\partial \mathbf b} = \begin{pmatrix}\frac{\partial S(\mathbf b)}{\partial b_1} \\\vdots \\\frac{\partial S(\mathbf b)}{\partial b_k}\end{pmatrix}
</span></p>
<p>Differentiating with the vector <span class="math inline">b</span> yields:</p>
<p><span class="math display">
\frac{\partial S(\mathbf b)}{\partial b} = -2\mathbf X^T \mathbf y + 2 \mathbf X^T \mathbf{Xb}
</span></p>
<p>Evaluated at <span class="math inline">\hat{\beta}</span>, the derivatives should equal zero (since first order condition of finding minimums):</p>
<p><span class="math display">
\frac{\partial S(\mathbf b)}{\partial b} \biggr|_{\hat{\beta}} = -2\mathbf X^T \mathbf y + 2\mathbf X^T \mathbf X \boldsymbol{\hat{\beta}} = 0
</span></p>
<p>When assuming <span class="math inline">\mathbf X^T \mathbf X</span> is invertable (which is true if <span class="math inline">\mathbf X</span> is full rank), we can isolate <span class="math inline">\hat{\beta}</span> to find the solution to OLS:</p>
<p><span class="math display">
\begin{split}
-2\mathbf X^T\mathbf y + 2 \mathbf X^T \mathbf X \boldsymbol{\hat{\beta}} &amp; = 0 \\
2 \mathbf X^T \mathbf X \boldsymbol{\hat\beta} &amp; = 2\mathbf X^T \mathbf y \\
\boldsymbol{\hat\beta} &amp; = (2\mathbf X^T \mathbf X)^{-1} 2 \mathbf X^T \mathbf y \\
\boldsymbol{\hat\beta} &amp; = (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf y
\end{split}
</span></p>
<p><br></p>
</section>
<section id="regression-anatomy-theorem" class="level3">
<h3 class="anchored" data-anchor-id="regression-anatomy-theorem">Regression Anatomy Theorem</h3>
<p>Take our standard multiple linear regression:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_k x_{ki} + u_i
</span></p>
<p>Let us say we are interested in <span class="math inline">x_1</span> (this can be generalised to any <span class="math inline">x_j</span>). Let us make <span class="math inline">x_1</span> the outcome variable of a regression with explanatory variables <span class="math inline">x_2, ..., x_k</span>:</p>
<p><span class="math display">
x_{1i} = \gamma_0 + \gamma_1 x_{2i} + ... + \gamma_{k-1}x_{ki} + \widetilde{r_{1i}}
</span></p>
<ul>
<li>The error term is <span class="math inline">\widetilde{r_{1i}}</span>, which represents the part of <span class="math inline">x_{1i}</span> that are uncorrelated to <span class="math inline">x_2, ..., x_k</span>. In other words, <span class="math inline">\widetilde{r_{1i}}</span> is the part of <span class="math inline">x_1</span> that cannot be explained by any other explanatory variable <span class="math inline">x_2, ..., x_k</span>. (uncorrelated with them)</li>
</ul>
<p>Now, take the regression of with outcome variable <span class="math inline">y</span>, with all explanatory variables <u>except</u> <span class="math inline">x_1</span>:</p>
<p><span class="math display">
y_i = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \widetilde{y_i}
</span></p>
<ul>
<li>The error term is <span class="math inline">\widetilde{y_i}</span>, which is the part of <span class="math inline">y_i</span> that cannot be explained by <span class="math inline">x_2, ..., x_k</span> (uncorrelated with them). Since <span class="math inline">\widetilde{y_i}</span> is not explained by <span class="math inline">x_2, ..., x_k</span>, variable <span class="math inline">x_1</span> must be the one explaining <span class="math inline">\widetilde{y_i}</span>.</li>
<li>But, it is not the whole of <span class="math inline">x_1</span> explaining <span class="math inline">\tilde{y_i}</span>. This is since <span class="math inline">x_1</span> may also correlated with <span class="math inline">x_2, ..., x_k</span>, and the correlated parts of <span class="math inline">x_1</span> with <span class="math inline">x_2, ..., x_k</span> are already picked up in the regression by the coefficients of <span class="math inline">x_2, ..., x_k</span>.</li>
<li>Thus, <span class="math inline">\widetilde{y_i}</span> must be explained by the part of <span class="math inline">x_1</span> that is uncorrelated and not explained by <span class="math inline">x_2, ..., x_k</span>, which we derived earlier as <span class="math inline">\widetilde{r_{1i}}</span>.</li>
</ul>
<p>Thus, we can create another regression with explanatory variable <span class="math inline">\widetilde{x_{1i}}</span> and outcome variable <span class="math inline">\widetilde{y_i}</span>.</p>
<p><span class="math display">
\widetilde{y_i} = \alpha_0 + \alpha_1 \widetilde{r_{1i}} + u_i
</span></p>
<p>We can plug <span class="math inline">\widetilde{y_i}</span> back into our regression of <span class="math inline">y_i</span> with explanatory variables <span class="math inline">x_2 ..., x_k</span>, and re-arrange:</p>
<p><span class="math display">
\begin{split}
y_i &amp; = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \widetilde{y_i} \\
y_i &amp; = \delta_0 + \delta_1 x_{2i} + ... + \delta_{k-1} x_{ki} + \underbrace{\alpha_0 + \alpha_1 \widetilde{r_{1i}} + u_i}_{\because \text{ plug in } \widetilde{y_i}} \\
y_i  &amp; = \underbrace{(\delta_0 + \alpha_0)}_{\beta_0} + \underbrace{\alpha_1 \widetilde{r_{1i}}}_{\beta_1 x_{1i}} + \underbrace{\delta_1x_{2i}}_{\beta_2 x_{2i}} + ... + \underbrace{\delta_{k-1} x_{ki}}_{\beta_kx_{ki}} + \underbrace{u_i}_{u_i}
\end{split}
</span></p>
<p>As we can see, this new regression mirrors the original standard multiple linear regression: Importantly we know the <span class="math inline">\beta_1 x_{1i}</span> in the original is analogous to <span class="math inline">\alpha_1 \widetilde{r_{1i}}</span>. Thus, <u>the estimate of <span class="math inline">\alpha_1</span> will be the same as <span class="math inline">\beta_1</span> in the original regression</u>.</p>
<p>The coefficient <span class="math inline">\alpha_1</span> (which is equal to <span class="math inline">\beta_1</span>) explains the expected change in <span class="math inline">y</span>, given an increase in the part of <span class="math inline">x_1</span> uncorrelated with <span class="math inline">x_2, ..., x_k</span>. So essentially, <u>we have <strong>partialed out</strong> the effect of the other explanatory variables, and only focus on the effect on <span class="math inline">y</span> of the uncorrelated part of <span class="math inline">x_1</span> (which is <span class="math inline">\widetilde{r_{1i}}</span>)</u></p>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="ols-as-an-unbiased-estimator" class="level1">
<h1><strong>OLS as an Unbiased Estimator</strong></h1>
<section id="unbiased-estimators" class="level3">
<h3 class="anchored" data-anchor-id="unbiased-estimators">Unbiased Estimators</h3>
<p>An estimator is unbiased, if its estimates <span class="math inline">\hat\theta_n</span> are of the following:</p>
<p><span class="math display">
E(\hat\theta_n) = \theta
</span></p>
<p>Or in other words, if we repeatedly sample and use the estimator, on average, the estimates will be equal to the true population value.</p>
<p>We want an estimator that is unbiased. Why?</p>
<ul>
<li>We know that the expectation of a random variable is its “best guess” of its value.</li>
<li>We know that estimates <span class="math inline">\hat\theta_n</span> from an estimator are a random variable called the sampling distribution.</li>
<li>Thus, if <span class="math inline">E(\hat\theta_n) = \theta</span>, that means our “best guess” of the estimator value is the true parameter value <span class="math inline">\theta</span>. That means any one estimate <span class="math inline">\hat\theta_n</span> is on average, correct.</li>
</ul>
<p><br></p>
</section>
<section id="conditions-for-unbiasedness" class="level3">
<h3 class="anchored" data-anchor-id="conditions-for-unbiasedness">Conditions for Unbiasedness</h3>
<p>OLS is an unbiased estimator of the relationship between any <span class="math inline">x_j</span> and <span class="math inline">y</span> under 4 conditions:</p>
<ol type="1">
<li><strong>Linearity</strong> in parameters: the model of the population (data generating process) can be modelled as:</li>
</ol>
<p><span class="math display">
\mathbf y = \mathbf X \boldsymbol\beta + \mathbf u
</span></p>
<ol start="2" type="1">
<li><strong>Random Sampling</strong>: the observations in our sample are randomly sampled from the same population.</li>
<li><strong>No Perfect Multicolinearity</strong>: There is no exact linear relationships between the regressors. This ensures that <span class="math inline">\mathbf X^T \mathbf X</span> is invertible, which is required for the derivation of OLS.</li>
<li><strong>Zero Conditional Mean</strong>: <span class="math inline">E(u|\mathbf X) = 0</span>. This implies that no regressor, and no function of multiple regressors is correlated with the error term.</li>
</ol>
<p>If any of these conditions are violated, OLS is not an unbiased estimator.</p>
<p><br></p>
</section>
<section id="proof-of-unbiasedness" class="level3">
<h3 class="anchored" data-anchor-id="proof-of-unbiasedness">Proof of Unbiasedness</h3>
<p>We need condition 3 to ensure <span class="math inline">\mathbf X^T \mathbf X</span> is invertible, in order to have OLS estimates. Once we have OLS estimates (derivation above), we can manipulate it as following:</p>
<p><span class="math display">
\begin{split}
\boldsymbol{\hat\beta} &amp; = (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf y \\
&amp; = (\mathbf X^T \mathbf X)^{-1} \mathbf X^T\underbrace{(\mathbf X \boldsymbol\beta + \mathbf u)}_{\text{plug in } \mathbf y} \\
&amp; = \underbrace{(\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf X}_{\text{inverses cancel out }} \boldsymbol\beta + (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf u \\
&amp; = \boldsymbol\beta + (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf u \\
\end{split}
</span></p>
<p>Now, let us take the expectation of <span class="math inline">\boldsymbol{\hat\beta}</span> conditional on <span class="math inline">\mathbf X</span>. Remember condition 4 - zero conditional mean, which allows us to simplify this.</p>
<p><span class="math display">
\begin{split}
E(\boldsymbol{\hat\beta}|\mathbf X) &amp; = \boldsymbol\beta + (\mathbf X^T \mathbf X)^{-1} \underbrace{E(\mathbf u | \mathbf X)}_{= \ 0} \\
E(\boldsymbol{\hat\beta}|\mathbf X) &amp; = \boldsymbol\beta
\end{split}
</span></p>
<p>Now, we can use the law of iterated expectations to conclude this proof:</p>
<p><span class="math display">
\begin{split}
E(\boldsymbol{\hat\beta}) &amp; = E(E(\boldsymbol{\hat\beta}|\mathbf X)) \\
&amp; = E(\boldsymbol\beta) \\
&amp; = \boldsymbol\beta
\end{split}
</span></p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Law of Iterated Expectations
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The law of iterated expectations states that:</p>
<p><span class="math display">
E(X) = E(E(X|Y))
</span></p>
</div>
</div>
</div>
<p>Thus, OLS is unbiased under the 4 conditions above.</p>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="ols-as-a-consistent-estimator" class="level1">
<h1><strong>OLS as a Consistent Estimator</strong></h1>
<section id="asymptotically-consistent-estimators" class="level3">
<h3 class="anchored" data-anchor-id="asymptotically-consistent-estimators">Asymptotically Consistent Estimators</h3>
<p>Asymptotic properties are properties of estimators as the sample size becomes larger and larger. Or more mathematically, as the sample size <span class="math inline">n</span> approaches infinity.</p>
<p>An estimator is consistent, if as we increase sample size towards infinity, the estimate will become more and more concentrated around the true population value <span class="math inline">\theta</span>.</p>
<p>Or in other words, as sample size increases indefinitely, we will get closer and closer to the true population value <span class="math inline">\theta</span>, until at infinite sample size, all our estimates will be exactly <span class="math inline">\theta</span>. Mathematically:</p>
<p><span class="math display">
Pr(|\hat\theta_n - \theta|&gt; \epsilon) \rightarrow 0, \text { as } n \rightarrow ∞
</span></p>
<ul>
<li>Or in other words, the proabability that the distance between an estimate <span class="math inline">\hat\theta_n</span> and the true population value <span class="math inline">\theta</span> will be higher than a small close-to-zero value <span class="math inline">\epsilon</span> will be 0, since our estimates <span class="math inline">\hat\theta_n</span> will converge at the <span class="math inline">\theta</span>.</li>
</ul>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Biased but Consistent
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>An estimator can be both biased, but consistent.</p>
<ul>
<li>i.e.&nbsp;in smaller sample sizes, the estimator might not be on average correct, but over a large enough sample size, it will become “unbiased”.</li>
</ul>
<p>For example, in the figure below, we can see that this estimator is biased at small values of <span class="math inline">n</span>, but as <span class="math inline">n</span> increases, it becomes more consistent, collapsing its distribution around the true <span class="math inline">\theta</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-1215503621.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Law of Large Numbers and Consistency
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The law of large numbers states that the sample average of a random sample, is a consistent estimator of the population mean.</p>
<p>For example, let us say we have a random variable <span class="math inline">x</span>. We take a random sample of <span class="math inline">n</span> units, so our sample is <span class="math inline">(x_1, \dots, x_n)</span>.</p>
<ul>
<li>Let us define <span class="math inline">\bar x_n</span> as our sample average.</li>
<li>Let us define <span class="math inline">\mu</span> as the true population mean of variable <span class="math inline">x</span>.</li>
</ul>
<p>The law of large numbers states that:</p>
<p><span class="math display">
\text{plim}( \bar x_n) = \mu
</span></p>
<ul>
<li>Where <span class="math inline">\text{plim}</span> states that as <span class="math inline">n</span> approaches infinity, the probability distribution of <span class="math inline">\bar x_n</span> collapses around <span class="math inline">\mu</span>.</li>
</ul>
<p><br></p>
<p>Why is this the case? This sample mean estimator is calculated simply through the formula for mean:</p>
<p><span class="math display">
\bar x_n = \frac{1}{n}\sum\limits_{i=1}^n x_i
</span></p>
<p>Let us define the variance of our sample of <span class="math inline">x_1, \dots, x_n</span> as <span class="math inline">Var(x_i) = \sigma^2</span>. We can now find the variance of our sampling distribution of estimator <span class="math inline">\bar x_n</span>:</p>
<p><span class="math display">
\begin{split}
Var(\bar x_n) &amp; = Var\left( \frac{1}{n}\sum\limits_{i=1}^n x_i \right) \\
&amp; = \frac{1}{n^2} Var \left(\sum\limits_{i=1}^n x_i\right) \\
&amp; = \frac{1}{n^2} \sum\limits_{i=1}^n Var(x_i) \\
&amp; = \frac{1}{n^2} \sigma^2 \\
&amp; = \frac{\sigma^2}{n}
\end{split}
</span></p>
<p>And as sample size <span class="math inline">n</span> increases to infinity, we get:</p>
<p><span class="math display">
\lim\limits_{n \rightarrow ∞} Var(\bar x_n) = \lim\limits_{n \rightarrow ∞} \frac{\sigma^2}{n} = 0
</span></p>
<p>Thus, the variance of our estimator <span class="math inline">\bar x_n</span> shrinks to zero, so as sample size increases to infinity <span class="math inline">n</span>, the sampling distribution of estimator <span class="math inline">\bar x_n</span> collapses around the true population mean.</p>
</div>
</div>
</div>
<p><br></p>
</section>
<section id="conditions-for-asymptotic-consistency" class="level3">
<h3 class="anchored" data-anchor-id="conditions-for-asymptotic-consistency">Conditions for Asymptotic Consistency</h3>
<p>OLS is an asymptotically consistent estimator of the relationship between any <span class="math inline">x_j</span> and <span class="math inline">y</span> under 4 conditions. These conditions are identical to the unbiasedness conditions EXCEPT condition 4, which is weakened from the original unbiasedness condition.</p>
<ol type="1">
<li><strong>Linearity</strong> in parameters: the model of the population (data generating process) can be modelled as:</li>
</ol>
<p><span class="math display">
\mathbf y = \mathbf X \boldsymbol\beta + \mathbf u
</span></p>
<ol start="2" type="1">
<li><strong>Random Sampling</strong>: the observations in our sample are randomly sampled from the same population.</li>
<li><strong>No Perfect Multicolinearity</strong>: There is no exact linear relationships between the regressors. This ensures that <span class="math inline">\mathbf X^T \mathbf X</span> is invertible, which is required for the derivation of OLS.</li>
<li><strong>Zero Mean and Exogeneity</strong>: <span class="math inline">E(u_i) = 0</span>, and <span class="math inline">Cov(x_i, u_i) = 0</span>. This means that no regressor should be correlated with the error term. This also implies <span class="math inline">E(x_iu_i) = 0</span>.</li>
</ol>
<p>If any of these conditions are violated, OLS is not a consistent estimator.</p>
<p>Note - even if all these conditions are met, OLS is not unbiased, just consistent. For OLS to be unbiased, it must meet the full Zero Conditional Mean assumption as discussed previously.</p>
<p><br></p>
</section>
<section id="proof-of-asymptotic-consistency" class="level3">
<h3 class="anchored" data-anchor-id="proof-of-asymptotic-consistency">Proof of Asymptotic Consistency</h3>
<p>We need condition 3 to ensure <span class="math inline">\mathbf X^T \mathbf X</span> is invertible, in order to have OLS estimates. Once we have OLS estimates (derivation above), we can manipulate it as following:</p>
<p><span class="math display">
\begin{split}
\boldsymbol{\hat\beta} &amp; = (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf y \\
&amp; = (\mathbf X^T \mathbf X)^{-1} \mathbf X^T\underbrace{(\mathbf X \boldsymbol\beta + \mathbf u)}_{\text{plug in } \mathbf y} \\
&amp; = \underbrace{(\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf X}_{\because \text{ inverses cancel}} \boldsymbol\beta + (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf u \\
&amp; = \boldsymbol\beta + (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf u \\
\end{split}
</span></p>
<p>We cannot apply the law of large numbers to sums. Thus, we can convert our matrices into vector notation.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Vector Notation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The following statements are true:</p>
<p><span class="math display">
\begin{split}
&amp; \mathbf X^T \mathbf X = \sum\limits_{i=1}^n \mathbf x_i \mathbf x_i^T \\
&amp; \mathbf X^T \mathbf  u = \sum\limits_{i=1}^n \mathbf x_i \mathbf u_i
\end{split}
</span></p>
</div>
</div>
</div>
<p>Thus, we can simplify the above to:</p>
<p><span class="math display">
\begin{split}
\boldsymbol{\hat\beta} &amp; = \boldsymbol\beta + \left( \sum\limits_{i=1}^n \mathbf x_i \mathbf x_i^T\right)^{-1} \left( \sum\limits_{i=1}^n \mathbf x_i \mathbf u \right) \\
&amp; = \boldsymbol\beta + \underbrace{\left( \frac{1}{n}\sum\limits_{i=1}^n \mathbf x_i \mathbf x_i^T\right)^{-1} \left( \frac{1}{n}\sum\limits_{i=1}^n \mathbf x_i \mathbf u \right)}_{\because \ \left(\frac{1}{n} \right)^{-1} \text{and } \frac{1}{n} \text{ cancel out}}
\end{split}
</span></p>
<p>Now, let us apply the probability limit:</p>
<p><span class="math display">
\text{plim} \boldsymbol{\hat\beta} = \boldsymbol\beta + \left( \text{plim} \frac{1}{n}\sum\limits_{i=1}^n \mathbf x_i \mathbf x_i^T\right)^{-1} \left( \text{plim} \frac{1}{n}\sum\limits_{i=1}^n \mathbf x_i \mathbf u \right)
</span></p>
<p>And using the law of large numbers (see above), and the zero-mean and exogeneity condition, we can simplify to:</p>
<p><span class="math display">
\begin{split}
\text{plim} \boldsymbol{\hat\beta} &amp; = \boldsymbol\beta + (E(\mathbf x_i \mathbf x_i^T))^{-1}\underbrace{E(\mathbf x_i \mathbf u_i)}_{ = \ 0} \\
\text{plim} \boldsymbol{\hat\beta} &amp; = \boldsymbol\beta
\end{split}
</span></p>
<p>Thus, OLS is asymptotically consistent under the 4 conditions above.</p>
<p><br></p>
<p><br></p>
<hr>
</section>
</section>
<section id="further-properties-of-ols" class="level1">
<h1><strong>Further Properties of OLS</strong></h1>
<section id="ols-as-a-conditional-expectation-function" class="level3">
<h3 class="anchored" data-anchor-id="ols-as-a-conditional-expectation-function">OLS as a Conditional Expectation Function</h3>
<p>OLS is a best-linear approximation of the conditional expectation function.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Conditional Expectation Functions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A <strong>conditional expectation function</strong> (CEF) says that the value of <span class="math inline">E(y)</span> depends on the value of <span class="math inline">x</span>. We notate a conditional expectation function as <span class="math inline">E(y|x)</span>. As we noted earlier, the linear regression model can be a conditional expectation function of <span class="math inline">E(y|x)</span>.</p>
<p>A <strong>best linear approximation</strong> of a conditional expectation function can take the following form:</p>
<p><span class="math display">
E(y_i|x_i) = b_0 + b_1x_i
</span></p>
<p>With parameters <span class="math inline">b_0, b_1</span> that minimise the mean squared errors (MSE).</p>
<p><span class="math display">
\begin{split}
MSE &amp; = E(y_i - E(y_i|x_i))^2 \\
&amp; = \frac{1}{n}\sum\limits_{i=1}^n( y_i - E(y_i|x_i))^2
\end{split}
</span></p>
</div>
</div>
</div>
<p>Suppose we have the conditional expectation function:</p>
<p><span class="math display">
E(y_i|x_i) = b_0 + b_1x_i
</span></p>
<p>We also know that our typical regression equation is:</p>
<p><span class="math display">
y_i = \beta_0 + \beta_1 x_i + u_i
</span></p>
<p>We know that <span class="math inline">E(u_i|x_i) = 0</span>. Let us define <span class="math inline">u_i</span> as the following:</p>
<p><span class="math display">
u_i = y_i - E(y_i|x_i)
</span></p>
<p>If the above defined <span class="math inline">u_i</span> is true, <span class="math inline">E(u_i|x_i)</span> should also be equal to 0. So, let us plug in the above <span class="math inline">u_i</span> into <span class="math inline">E(u_i | x_i)</span>.</p>
<p><span class="math display">
\begin{split}
E(u_i|x_i) &amp; = E(y_i - E(y_i|x_i) \ | \ x_i) \\
&amp; = E(y_i|x_i) - E(y_i|x_i) \\
&amp; = 0
\end{split}
</span></p>
<p>Thus, we know <span class="math inline">u_i = y_i - E(y_i|x_i)</span> to be true. Thus, rearranging, we know:</p>
<p><span class="math display">
y_i = E(y_i|x_i) + u_i
</span></p>
<p>We also know that <span class="math inline">y_i = \beta_0 + \beta_1 x_i + u_i</span>. Thus, the following is true:</p>
<p><span class="math display">
\begin{split}
E(y_i|x_i) + u_i &amp; = \beta_0 + \beta_1 + u_i \\
b_0 + b_1 + u_i &amp; = \beta_0 + \beta_1 + u_i \\
b_0 + b_1 &amp; = \beta_0 + \beta_1
\end{split}
</span></p>
<p>Well, you might point out, it is still possible that <span class="math inline">b_1 ≠ \beta_1</span> in this scenario. We can go further. We know that the conditional expectation function minimises the mean of squared errors.</p>
<p><span class="math display">
\begin{split}
MSE &amp; = \min\limits_{b_0, b_1} E(y_i - E(y_i|x_i))^2 \\
&amp; = \min\limits_{b_0, b_1} E(y_i - \beta_0 - \beta_1x_i)^2
\end{split}
</span></p>
<p>The first order conditions are (using chain rule and partial derivatives):</p>
<p><span class="math display">
\begin{split}
&amp; E(y_i - b_0 - b_1x_i) = 0 \\
&amp; E(x_i(y_i - b_0 - b_1x_i) = 0
\end{split}
</span></p>
<p>Now, recall our OLS minimisation conditions (simple linear regression):</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
&amp; \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
</span></p>
<p>Since by definition, average/expectation is <span class="math inline">E(x) = \frac{1}{n} \sum x_i</span>, we can rewrite the OLS minimisation conditions as:</p>
<p><span class="math display">
\begin{split}
&amp; n \times E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
&amp; n \times E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
</span></p>
<p>And since anything multiplied to a zero turns into zero, we can ignore the <span class="math inline">n</span> in the first order condition, and only focus on the expected value part. Thus, our conditions are:</p>
<p><span class="math display">
\begin{split}
&amp; E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
&amp; E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
</span></p>
<p>Which as we can see, are the exact same minimisation conditions as the conditional expectation function. Thus, OLS is the best approximation of the conditional expectation function.</p>
<p>This property is very useful for causal inference, as it means OLS is calculated the expected <span class="math inline">y</span>, which allows us to find causal effects by comparing the expected <span class="math inline">y</span> of the treatment and control groups (assuming the OLS estimator is unbiased).</p>
<p><br></p>
</section>
<section id="ols-as-a-method-of-moments-estimator" class="level3">
<h3 class="anchored" data-anchor-id="ols-as-a-method-of-moments-estimator">OLS as a Method of Moments Estimator</h3>
<p>The Method of Moments Estimator is another estimator of the true value of populations in the parameter. The estimator defines key population <strong>moments</strong> of interest - which are the population parameters written in terms of expected value functions set equal to 0.</p>
<p>Then, the Method of Moments uses the sample equivalents of the population moments to estimate the population parameter. For example, to estimate the population mean, the Method of Moments uses the sample mean.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Details of the Method of Moments Estimator
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In order to define a method of moments for a set of parameters <span class="math inline">\theta_1, \dots, \theta_k</span>, we need to specify at least one population moment per parameter. Or in other words, we must have more than <span class="math inline">k</span> population moments.</p>
<p>Our population moments can be defined as the expected value of some function <span class="math inline">m(\theta; y)</span> that consists of both the variable <span class="math inline">y</span> and our unknown parameter <span class="math inline">\theta</span>. The expectation of the function <span class="math inline">m(\theta; y)</span> should equal 0.</p>
<p><span class="math display">
E(m(\theta; y)) = 0
</span></p>
<p>Our sample moments will be the sample analogues of <span class="math inline">\theta</span> and <span class="math inline">y</span>, which are <span class="math inline">\hat\theta</span> and <span class="math inline">y_i</span>:</p>
<p><span class="math display">
\frac{1}{n}\sum\limits_{i=1}^n m(\hat\theta; y_i) = 0
</span></p>
<ul>
<li>The <span class="math inline">\frac{1}{n} \sum</span> is there because the definition of expectation/mean is that.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example of the Method of Moments Estimator
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let us say that we have some random variable <span class="math inline">y</span>, with a true population mean <span class="math inline">\mu</span>. We want to estimate <span class="math inline">\mu</span>, but we only have a sample of the population.</p>
<p>How can we define our true population parameter <span class="math inline">\mu</span> in an expectation equation of the form: <span class="math inline">E(m(\mu, y)) = 0</span>?</p>
<ul>
<li>Well, what is <span class="math inline">\mu</span>, the mean, intuitively speaking? It is the expectation of <span class="math inline">y</span>, so <span class="math inline">\mu = E(y)</span>.</li>
</ul>
<p>Now that we know that <span class="math inline">\mu = E(y)</span>, since they are equal, <span class="math inline">\mu - E(y) = 0</span>. Thus, we can define the mean as a moment of the following condition:</p>
<p><span class="math display">
E(y - \mu) = 0
</span></p>
<p>The method of moments says we should use the sample equivalent of the population parameter. The sample equivalent of <span class="math inline">\mu</span> (the true mean of the population), is of course, the sample mean <span class="math inline">\bar y</span>.</p>
<p>Thus, our sample estimate of the moment would be:</p>
<p><span class="math display">
E(y_i - \hat\mu) = \frac{1}{n}\sum\limits_{i=1}^n (y_i - \hat\mu) = 0
</span></p>
<p>With this equation, we can then solve for <span class="math inline">\hat\mu</span>:</p>
<p><span class="math display">
\begin{split}
0 &amp; = \frac{1}{n}\sum\limits_{i=1}^n (y_i - \hat\mu) \\
0 &amp; = \frac{1}{n}\sum\limits_{i=1}^ny_i - \frac{1}{n}\sum\limits_{i=1}^n \hat\mu \\
0 &amp; = \frac{1}{n}\sum\limits_{i=1}^ny_i - \frac{1}{n} n \hat\mu\\
0 &amp; = \bar y - \hat \mu \\
\hat\mu &amp; = \bar y
\end{split}
</span></p>
<p>So, we see the method of moments estimates our true population mean <span class="math inline">\mu</span>, with the sample mean <span class="math inline">\bar y</span>.</p>
</div>
</div>
</div>
<p>OLS is a special case of the Method of Moments Estimator. Consider the bivariate regression model:</p>
<p><span class="math display">
y = \beta_0 + \beta_1x + u
</span></p>
<p>The OLS estimator can be derived as a method of moments estimator, with 2 moments (expectation functions set equal to 0), one for each parameter (<span class="math inline">\beta_0, \beta_1</span>):</p>
<p><span class="math display">
\begin{split}
&amp; E(y-\beta_0 -\beta_1x) = 0 \\
&amp; E(x(y - \beta_0 - \beta_1 x)) = 0
\end{split}
</span></p>
<p>Since we know <span class="math inline">u = y - \beta_0 - \beta_1 x</span>, we can rewrite the two moments as:</p>
<p><span class="math display">
\begin{split}
&amp; E(u) = 0 \\
&amp; E(xu) = 0
\end{split}
</span></p>
<p>The estimates of these moments would use the sample equivalents: <span class="math inline">\hat\beta_0</span> and <span class="math inline">\hat\beta_1</span>.</p>
<p><span class="math display">
\begin{split}
&amp; E(y-\hat\beta_0 -\hat\beta_1x) = 0 \\
&amp; E(x(y - \hat\beta_0 - \hat\beta_1 x)) = 0
\end{split}
</span></p>
<p>Remember our OLS minimisation conditions:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
&amp; \sum\limits_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0
\end{split}
</span></p>
<p>Since by definition, average/expectation is <span class="math inline">E(x) = \frac{1}{n} \sum x_i</span>, we can rewrite the OLS minimisation conditions as:</p>
<p><span class="math display">
\begin{split}
&amp; n \times E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
&amp; n \times E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
</span></p>
<p>And since anything multiplied to a zero turns into zero, we can ignore the <span class="math inline">n</span> in the first order condition, and only focus on the expected value part. Thus, our conditions are:</p>
<p><span class="math display">
\begin{split}
&amp; E(y_i - \hat\beta_0 - \hat\beta_1x_i) = 0 \\
&amp; E(x_i(y_i - \hat\beta_0 - \hat\beta_1x_i)) = 0
\end{split}
</span></p>
<p>Which as we can see, are the exact same minimisation conditions as the method of moments estimator. Thus, the OLS estimator is a special case of the Method of Moments estimator, and they produce the same coefficients. This is an important property for the instrumental variables method that will be covered later.</p>
<p><br></p>
</section>
<section id="ols-as-a-maximum-likelihood-estimator" class="level3">
<h3 class="anchored" data-anchor-id="ols-as-a-maximum-likelihood-estimator">OLS as a Maximum Likelihood Estimator</h3>
<p>The Maximum Likelihood Estimator is an estimation procedure that is used for Logistic Regression and Count Regression (and many other statistical concepts). OLS is a special case of the maximum likelihood estimator under two conditions:</p>
<ol type="1">
<li>The error term <span class="math inline">u</span> must be normally distributed, such that <span class="math inline">n \sim \mathcal N(0, \sigma^2)</span>.</li>
<li>We assume homoscedasticity: <span class="math inline">Var(u|x) = \sigma^2</span>.</li>
</ol>
<p>A normal distribution has the following probability density function:</p>
<p><span class="math display">
f(y) = \frac{1}{\sqrt{2 \pi \sigma^2}}e^{\left( \frac{(x- \mu)^2}{2 -\sigma^2}\right)}
</span></p>
<p>Assuming our error term is normally distributed, we know that the conditional probability density function of our linear model <span class="math inline">y = \beta_0 + \beta_1 x + u_i</span> is as follows:</p>
<p><span class="math display">
f(y_i|x_i;\beta_0, \beta_1, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}}
e^{ \left( -\frac{1}{2 \sigma^2}(y_i - \beta_0 - \beta_1 x_i)^2 \right)}</span></p>
<p>By the independence of <span class="math inline">y_1, \dots, y_n</span>, the likelihood function is:</p>
<p><span class="math display">
\begin{split}
L &amp; = \prod_{i=1}^nf(y_i|x_i; \beta_0, \beta_1, \sigma^2) \\
&amp; =  \frac{1}{\sqrt{2 \pi \sigma^2}}
e^{ \left( -\frac{1}{2 \sigma^2}\sum_{i=1}^n(y_i - \beta_0 - \beta_1 x_i)^2 \right)}
\end{split}
</span></p>
<p>The log-likelihood function is thus:</p>
<p><span class="math display">
\log L = -\frac{n}{2} \log (2 \pi) - \frac{n}{2} \log (\sigma^2) -\frac{1}{2 \sigma^2}\sum_{i=1}^n(y_i - \beta_0 - \beta_1 x_i)^2
</span></p>
<p>We can maximise the log-likelihood function (and thus the likelihood function) by finding the first order conditions:</p>
<p><span class="math display">
\begin{split}
\frac{\partial \log L}{\partial\beta_0} : &amp; \ -\frac{1}{\sigma^2} \sum\limits_{i=1}^n(y_i - \hat\beta_0 - \hat\beta_1 x_i) \times -1 =0 \\
\frac{\partial \log L}{\partial\beta_1} : &amp; \ -\frac{1}{\sigma^2} \sum\limits_{i=1}^n(y_i - \hat\beta_0 - \hat\beta_1 x_i) \times -x_i =0 \\
\frac{\partial \log L}{\partial\sigma^2} : &amp; \ -\frac{n}{2\sigma^2}  + \frac{1}{2\sigma^4} \sum\limits_{i=1}^n(y_i - \hat\beta_0 - \hat\beta_1 x_i) =0 \\
\end{split}
</span></p>
<p>We can focus on the first two conditions, since we are interested in the intercept <span class="math inline">\beta_0</span> and coefficient <span class="math inline">\beta_1</span> estimates. We can ignore the initial <span class="math inline">\frac{1}{\sigma^2}</span>, since if the summation equals zero, the whole partial derivative will also equal zero.</p>
<p>Thus, our first order conditions for maximum likelihood estimation are:</p>
<p><span class="math display">
\begin{split}
&amp; \sum\limits_{i=1}^n(y_i - \hat\beta_0 - \hat\beta_1 x_i) = 0 \\
&amp; \sum\limits_{i=1}^n(y_i - \hat\beta_0 - \hat\beta_1 x_i)x_i = 0
\end{split}
</span></p>
<p>These conditions are identical to our OLS conditions. Thus, the maximum likelihood estimator is equivalent to OLS given normality of the error term and homoscedasticity.</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>