---
title: "Causal Frameworks and Estimands"
subtitle: "Quantitative Methods (Causal Inference)"
format:
    html:
        page-layout: article
        grid:
          sidebar-width: 350px
          body-width: 700px
          margin-width: 250px
          gutter-width: 2em
        fontsize: 12pt
        theme: default
        toc: TRUE
        toc-depth: 3
        toc-location: left
        toc-expand: true
        toc-title: "Table of Contents"
        mainfont: "Computer Modern"
        title-block-banner: true
        classoption: fleqn
        html-math-method: katex
editor: visual
---

This chapter introduces the main causal frameworks (potential outcomes, causal graphs), the main causal estimands used in causal inference, and the idea of selection bias and confounders.

------------------------------------------------------------------------

# **Potential Outcomes Framework**

### Treatment Variables

In causal inference, we are interested in how one variable $D$ causes some effect on some outcome variable $Y$.

$D$ is our treatment variable. The indicator of treatment for each unit $i$ is $D_i$.

$$
D_i = \begin{cases}
1 \quad \text{if unit } i \text{ recieved the treatment} \\
0 \quad \text{if unit } i \text{ did not recieve the treatment}
\end{cases}
$$

::: {.callout-note collapse="true" appearance="simple"}
## Further information on Treatment Variables

Causal variables/treatments must occur before the outcome. A variable cannot cause something to occur in the past.

Causal variables/treatments must be able to be manipulated (in order to imagine a world where the treatment did not occur).

-   For example, $D$ cannot be sex assigned at birth, ethnicity, etc.
-   For example, major global events (how did 9/11 cause the Arab spring?)
:::

<br />

### Potential Outcomes

Imagine there are two hypothetical parallel worlds - one where unit $i$ receives the treatment $D$, and one where unit $i$ does not receive the treatment $D$. Everything else in these worlds is identical.

Potential Outcomes for unit $i$ are denoted:

$$
Y_{di} =\begin{cases}
& Y_{1i} \quad \text{Outcome for unit } i \text{ when } D_i = 1 \\
& Y_{0i} \quad \text{Outcome for unit } i \text{ when } D_i = 0 \\
\end{cases}
$$

For example, imagine we are interested in finding the effect of democracy $D$ on GDP growth $Y$. Potential outcome $Y_{1i}$ is the potential GDP growth of country $i$ if they were a democracy, and outcome $Y_{0i}$ is the potential GDP growth of a country $i$ if they were not a democracy.

<br />

### Observed Outcomes and "Missing Data"

Of course, there is not two parallel worlds with 2 potential outcomes. In the real world, each unit $i$ either receives treatment $D$, or does not. We do not observe the other potential outcome.

$Y_i$ is the observed outcome for unit $i$. This is given by formula:

$$
Y_i = D_i \cdot Y_{1i} + (1-D_i) \cdot Y_{0i}
$$

If we plug in $D_i = 0, 1$ to the equation above, we get the observed outcomes:

$$
Y_i = \begin{cases}
Y_{1i} \quad \text{if } D_i = 1 \\
Y_{0i} \quad \text{if } D_i = 0 \\
\end{cases}
$$

Before the treatment (*A priori*), both potential outcomes could be observed. After the treatment, one is **observed**, and the other is **counterfactual**. For any given experiment, only one will ever be seen, and the counterfactual will never be seen (missing data problem).

::: {.callout-note collapse="true" appearance="simple"}
## Neyman Urn Model

Potential Outcomes can be visualised with the Neyman Urn Model.

Before the treatment, we have a box (we cannot see) with both potential outcomes.

![](images/clipboard-802589608.png){fig-align="center" width="55%"}

Then, when we apply treatment, we stick our hand into the box that we cannot see, and pull out one observed outcome.

![](images/clipboard-2975134148.png){fig-align="center" width="60%"}

We are essentially sampling from potential outcomes to get observed outcomes.
:::

This missing data problem is called the **fundamental problem of causal inference**.

<br />

### Stable Unit Treatment Value Assumption

The above given observed and potential outcome frameworks depends on the Stable Unit Treatment Value Assumption (SUTVA).

$$
\underbrace{Y_{(D_1, D_2, \dots, D_N)i}}_{\text{Observed } D_i} = \underbrace{Y_{(D_1', D_2', \dots, D_N')i}}_{\text{All alternative } D_i}
$$

Or more intuitively, the potential outcomes of unit $i$ only depends on their own treatment status, and no other unit's treatment status. The treatment is also the same for everyone (treatment is stable and consistent)

::: {.callout-note collapse="true" appearance="simple"}
## Examples of SUTVA Violations

-   Spill-over effects: If we are testing a new curriculum, one student $j$ getting the new curriculum may teach their friend $i$ the new curriculum, thus affecting the potential outcomes of $i$.

-   Contagion: If we are studying a disease, diseases can spread, so another unit $j$ getting a disease affects the potential outcomes of unit $i$.

-   Dilution: If we are studying vaccines - there is herd immunity - other people getting the vaccine also reduces our chances of getting the disease.

-   Variable levels of treatment: If we are doing a drug trial, if some people got two doses, while others only got one dose. This is not a consistent treatment.

-   Technical errors: If someone who is supposed to be treated accidentally is not treated. This is not a consistent treatment.
:::

When SUTVA is violated, potential outcomes become very messy, and we no longer have the neat framework as before.

<br />

<br />

------------------------------------------------------------------------

# **Causal Estimands**

### Individual Treatment Effect

Remember the potential outcomes from parallel worlds $Y_{1i}$ and $Y_{0i}$.

Since these two parallel worlds are identical except for the fact one receives the treatment $D$ and the other does not, the causal effect of $D$ should be the difference between the potential outcomes of these two worlds. Thus, the individual treatment effect of a unit $i$ is:

$$
\tau_i = Y_{1i} - Y_{0i}
$$

This is the specific treatment effect for a specific unit $i$. This cannot be observed, because we do not see both potential outcomes for the same unit $i$.

This is also very hard to estimate, as we cannot reliably fill in the missing potential outcome for any one unit $i$. Thus, we almost never use individual treatment effects, and use group treatment effects.

<br />

### Average Treatment Effect (ATE)

ATE is a group-level causal estimand.

::: {.callout-note collapse="true" appearance="simple"}
## Group-Level Causal Estimands

Consider a population of units $i = 1, \dots, N$.

The population has potential outcomes represented in two (only partially observed) vectors:

$$
\begin{split}
& Y_1 = (Y_{11}, Y_{12}, \dots, Y_{1N}) \\
& Y_0 = (Y_{01}, Y_{02}, \dots, Y_{0N})
\end{split}
$$

We compare these two vectors of potential outcomes. The most common way to do this is to use their expected values.
:::

The Average Treatment Effect is defined as:

$$
\begin{split}
\tau_{ATE} & = E(Y_{1i} - Y_{0i}) \\
& = \underbrace{\frac{1}{N} \sum\limits_{i=1}^N (Y_{1i} - Y_{0i})}_{\text{a formula for average}}
\end{split}
$$

We cannot calculate this with observed data - since we need all potential outcomes to do this. We can estimate this (covered throughout this course).

<br />

### Average Treatment Effect on the Treated (ATT)

An alternative estimand to the ATE is the **Average Treatment Effect on the Treated (ATT)**:

$$
\begin{split}
\tau_{ATT} & = E(Y_{1i} - Y_{0i} \ | \ D_i = 1) \\
& = \underbrace{\frac{1}{N_1} \sum\limits_{i=1}^N D_i (Y_{1i} - Y_{0i}) \quad  \text{where } N_1 = \sum\limits_{i=1}^ND_i}_{\text{a formula for the average only for units }D_i = 1}
\end{split}
$$

This is the causal effect of [only units who have received the treatment]{.underline}. Note that frequently the ATT is not equal to the ATE, so be aware of which estimand you are trying to estimate/identify.

::: {.callout-note collapse="true" appearance="simple"}
## ATT vs. ATE

When does $\tau_{ATT} = \tau_{ATE}$?

-   When the expectation of the potential outcomes of both the treated and control are the same, then the two equal each other.

The opposite is also true: if the expectation of the potential outcomes of both the treated and control are different, then the two are not equal.
:::

The opposite estimand is the **Average Treatment effect on the Untreated (ATU)**, which only measures the causal effect of units who did not recieve the treatment.

This is not used very often, since it is kind of uninituive to think about treatment effects on individuals who did not recieve treatment. However, it can be useful in understanding identification assumptions.

<br />

### Conditional Average Treatment Effect (CATE)

The conditional average treatment effect is any treatment effect where there is a condition on a characteristic/covariate:

$$
\tau_{CATE}(x) = E(Y_{1i} - Y_{0i} \ | \ \underbrace{X_i = x}_{\text{condition}})
$$

This is the causal effect of only variables who meet the condition of the covariate specified. For example, you could find the conditional average treatment effect of only women (so the covariate which we are conditioning on is *gender*). You can also condition on multiple covariates.

This is often used for tailoring products/medicine/advertising to certain groups of people. It is also frequently used in identification strategies.

This estimand will go by other names, including the Local Average Treatment Effect (LATE).

<br />

<br />

------------------------------------------------------------------------

# **Selection Bias and Confounders**

### Naive Estimator and Selection Bias

A natural way to estimate the ATE is to use a naive estimator: find the average difference of [observed]{.underline} outcomes. This is called the naive estimator:

$$
\begin{split}
\hat\tau_{naive} = \underbrace{E(Y_i|D_i = 1)}_{\text{Observed }Y \text{ for treatment}} - \underbrace{E(Y_i|D_i = 0)}_{\text{Observed }Y \text{ for control}}
\end{split}
$$

However, there is an issue - we can show this with algebra:

$$
\begin{split}
\hat\tau_{naive} & = E(Y_i|D_i = 1) - E(Y_i|D_i = 0) \\
& = \underbrace{E(Y_{1i}|D_i = 1) - E(Y_{0i} | D_i = 0)}_{\because \text{ observed potential outcomes}} \\
& = E(Y_{1i}|D_i = 1) - E(Y_{0i} | D_i = 0) + \underbrace{E(Y_{0i}|D_i = 1) \color{red}{- E(Y_{0i}|D_i = 1)}}_{\because \text{ this equals 0, so we can add it}} \\
& = \underbrace{E(Y_{1i}|D_i = 1) \color{red}{- E(Y_{0i}|D_i = 1)}}_{\tau_{ATT}} + \underbrace{E(Y_{0i}|D_i = 1) - E(Y_{0i} | D_i = 0)}_{\text{Selection Bias}} \\
\end{split}
$$

We can see that our naive estimator produces the $\tau_{ATT}$ **plus** an extra bit (called the selection bias). Thus, our naive estimator is [biased](https://politicalscience.github.io/causal/0.html#unbiased-estimators), so we should be careful about using this naive estimator (correlation does not equal causation).

::: {.callout-note collapse="true" appearance="simple"}
## Naive Estimator Biased for ATU

The proof above shows how the naive estimator is a biased estimator for the $\tau_{ATT}$. We can also prove it is a biased estimator of the ATU:

$$
\begin{split}
\hat\tau_{naive} & = E(Y_i|D_i = 1) - E(Y_i|D_i = 0) \\
& = \underbrace{E(Y_{1i}|D_i = 1) - E(Y_{0i} | D_i = 0)}_{\because \text{ observed potential outcomes}} \\
& = E(Y_{1i}|D_i = 1) - E(Y_{0i} | D_i = 0) + \underbrace{E(Y_{1i}|D_i = 0) - E(Y_{1i}|D_i = 0)}_{\because \text{ this equals 0, so we can add it}} \\
& = \underbrace{E(Y_{1i}|D_i = 0)- E(Y_{0i}|D_i = 0)}_{\tau_{ATU}} + \underbrace{E(Y_{1i}|D_i = 1) - E(Y_{1i} | D_i = 0)}_{\text{Selection Bias}} \\
\end{split}
$$
:::

::: {.callout-note collapse="true" appearance="simple"}
## Naive Estimator Biased for ATE

The proofs above shows how the naive estimator is a biased estimator for the $\tau_{ATT}$ and $\tau_{ATU}$. We can also prove it is a biased estimator of the ATE.

Let us first start with the ATE. Let us call $Y_{1i} - Y_{0i} := \tau_i$ for notation simplicity:

$$
\begin{split}
\tau_{ATE} & = E(Y_{1i} - Y_{0i})  = E(\tau_i)\\
& = \underbrace{E(\tau_i|D_i = 1)Pr(D_i = 1) + E(\tau_i|D_i = 0)Pr(D_i = 0)}_{\because \text{ weighted average of ATE and ATU by proportion in }D_i = 1, 0} \\
& = E(\tau_i|D_i = 1)\underbrace{(1 -Pr(D_i = 0))}_{\because \text{ complement prob.}} + E(\tau_i|D_i = 0)Pr(D_i = 0) \\
\end{split}
$$

Let us call $Pr(D_i = 0) := \pi$ for notation simplicity. Now, continue:

$$
\begin{split}
& = \underbrace{E(\Delta|D_i = 1) - \pi E(\Delta|D_i = 1)}_{\because \text{ distribute out}} + E(\tau_i|D_i = 0)\pi \\
& = E(\tau_i|D_i = 1) + \underbrace{\pi[E(\tau_i|D_i = 0) - E(\tau_i|D_i = 1)]}_{\because \text{ factor out } \pi = Pr(D_i = 0)} \\
& = E(Y_{1i} |D_i = 1) - E(Y_{0i}|D_i = 1) + \pi[E(\tau_i|D_i = 0) - E(\tau_i|D_i = 1)] \\
\end{split}
$$

Let us call the part $\pi[E(\tau_i|D_i = 0) - E(\tau_i|D_i = 1)] := \Pi(\tau_i)$. Now, continue to simplify:

$$
\begin{split}
& = E(Y_{1i} |D_i = 1) - E(Y_{0i}|D_i = 1) + \Pi(\tau_i) + \underbrace{E(Y_{1i} |D_i = 0) - E(Y_{0i}|D_i = 0)}_{\because \text{ these two cancel out so we add 0}}  \\
& = E(Y_{1i} |D_i = 1) - E(Y_{0i}|D_i = 0) + E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \Pi(\tau_i) \\
& = \underbrace{E(Y_i|D_i = 1)}_{\because \text{ observed outcome}} - \underbrace{E(Y_i|D_i = 0)}_{\because \text{ observed outcome}} + E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \Pi(\tau_i) \\
& = \underbrace{E(Y_{i} |D_i = 1) - E(Y_{i}|D_i = 0)}_{\hat\tau_{naive}} + E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \Pi(\tau_i) \\
& = \hat\tau_{naive}+ E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \Pi(\tau_i)
\end{split}
$$

Thus, we can see that $\tau_{ATE}$ is not equivalent to $\hat\tau_{naive}$. Let us isolate $\hat\tau_{naive}$ to identify the selection bias.

$$
\begin{split}
& \tau_{ATE} = \hat\tau_{naive}+ E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \Pi(\tau_i) \\
& -\hat\tau_{naive} = -\tau{ATE} + E(Y_{1i} |D_i = 0)- E(Y_{0i}|D_i = 1) + \Pi(\tau_i) \\
& \hat\tau_{naive} = \tau_{ATE} - E(Y_{1i} |D_i = 0) + E(Y_{0i}|D_i = 1) + \Pi(\tau_i) \\
& \hat\tau_{naive} = \tau_{ATE} + \underbrace{E(Y_{0i}|D_i = 1)- E(Y_{1i} |D_i = 0) + \Pi(\tau_i)}_{\text{selection bias}}
\end{split}
$$
:::

<br />

### Confounders

Take the selection bias formula from above:

$$
\underbrace{E(Y_{0i}|D_i = 1)}_{Y_{0i}\text{ for treatment}} - \underbrace{E(Y_{0i} | D_i = 0)}_{Y_{0i}\text{ for control}}
$$

If selection bias is non-zero, this essentially states that the expected potential outcome before treatment $Y_{0i}$ between the treatment and control groups is not equal.

Or in other words, the treatment and control groups have some other variable causing differences even before treatment has begun. This implies that the differences between the treatment and control group may not be due to treatment $D$, but due to the underlying differences before treatment even occurred.

**Confounders** are variables that cause the differences between treatment and control groups before the treatment has started. [**Confounders correlate with both the treatment variable and the outcome**]{.underline}.

-   If a variable only correlates with $D$ or $Y$, then it is not a confounder. If must correlate with both $D$ **and** $Y$.

This is why correlation does not equal causation - if the treatment and control group are different before we start the experiment, we cannot say the difference between the two is purely a result of treatment $D$.

<br />

### Omitted Variable Bias in Regression

We can demonstrate how confounders cause bias in regression. Suppose there is some confounding variable $z$ that we have not included in a "short" regression. The actual, "true" regression of the population, would include this confounder $z$

$$
\underbrace{\mathbf y = \mathbf X \boldsymbol\beta + \mathbf u}_{\text{short regression}}
 \qquad \underbrace{\mathbf y = \mathbf X \boldsymbol\beta + \mathbf z\boldsymbol\delta + \mathbf u}_{\text{true regression with } z}
$$

The [OLS estimate](https://politicalscience.github.io/causal/0.html#deriving-the-estimator) of the "short regression" excluding confounder $z$ is:

$$
\boldsymbol{\hat\beta} = (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf y
$$

Let us now plug in the "true" model into where $\mathbf y$ is:

$$
\begin{split}
\boldsymbol{\hat\beta} & = (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \underbrace{(\mathbf X \boldsymbol\beta + \mathbf z\boldsymbol\delta + \mathbf u)}_{\because \text{ true model } \mathbf y} \\
& = \underbrace{(\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf X}_{\because \text{ inverses cancel}} \boldsymbol\beta + (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf z\boldsymbol\delta + (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf u \\
& = \boldsymbol\beta + (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf z\boldsymbol\delta + (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf u \\
\end{split}
$$

Now, let us find the expected value of $\boldsymbol{\hat\beta}$, which is conditional on $\mathbf X, \mathbf z$, and simplify (using [zero conditional mean](https://politicalscience.github.io/causal/0.html#conditions-for-unbiasedness)):

$$
\begin{split}
E(\boldsymbol{\hat\beta}|\mathbf X, \mathbf z) & = \boldsymbol\beta + (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf z\boldsymbol\delta + (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \underbrace{E(\mathbf u | \mathbf X, \mathbf z)}_{= \ 0} \\
& = \boldsymbol\beta + (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf z\boldsymbol\delta
\end{split}
$$

Now, what if we had a regression of outcome variable being the confounder $z$, on the explanatory variables $\mathbf X$, such that $\mathbf z = \mathbf X \boldsymbol\pi + \mathbf v$. Our OLS estimate would have the solution:

$$
\boldsymbol{\hat\pi} = (\mathbf X^T\mathbf X)^{-1}\mathbf X^T \mathbf z
$$

Now, we can plug $\boldsymbol{\hat\pi}$ into our expected value of $\boldsymbol{\hat\beta}$:

$$
E(\boldsymbol{\hat\beta}|\mathbf X, \mathbf z)  = \boldsymbol\beta + \boldsymbol{\hat\pi \delta}
$$

Now, using the law of iterated expectations, we get (assuming we are using an unbiased estimator for $\boldsymbol{\hat\pi}$ such that $E(\boldsymbol{\hat\pi}) = \boldsymbol\pi$):

$$
\begin{split}
E(\boldsymbol{\hat\beta}) & = E(E(\boldsymbol{\hat\beta}|\mathbf X, \mathbf z)) \\
& = E(\boldsymbol\beta + \boldsymbol{\hat\pi \delta}) \\
& = \boldsymbol\beta + E(\boldsymbol{\hat\pi}) \boldsymbol \delta \\
& = \boldsymbol\beta + \boldsymbol{\pi \delta}
\end{split}
$$

Thus, we can see by not including confounder $z$ in our "short regression", the estimator is now biased by $\boldsymbol{\hat\pi \delta}$.

<br />

### Assignment Mechanism

The Assignment Mechanism is the procedure that determines the treatment status of each unit. In causal inference, we want to restrict the assignment mechanism, in order to remove the effect of selection bias.

There are two types of studies that use different assignment mechanisms:

1.  Randomisation Experiments: The assignment mechanism is both known, and controlled by the researcher. Generally, the researcher chooses some type of randomisation.
2.  Observational Studies: The assignment mechanism is not known to, or not under the control of the researcher. This means that confounders may be driving selection into treatment and control, inducing bias.

Generally, the most credible studies are randomisation studies, since we can control interventions to parse out the effect of confounders. Observational studies generally rely on more assumptions that need to be met, and need to be defended for the study to be credible.

<br />

<br />

------------------------------------------------------------------------

# **Directed Acyclic Graphs**

### Components of the Graphs

Causal Diagrams are a visual way to represent causal theories and frameworks, which allows us to visualise how variables interact with each other.

Each Directed Acyclic Graph (DAG) has the following components:

1.  Nodes: representing variables (which are also called vertices).
2.  Directed Edges: Arrows that encode one-way causal theories between variables. For example, we might believe $Z$ causes $X$, so we will draw an arrow from $Z$ to $X$. These connections are **observable** (solid) or **unobservable** (dashed).

::: {.callout-note collapse="true" appearance="simple"}
## Example of Directed Acyclic Graphs

Below is an example of a directed acyclic graph:

![](images/clipboard-3648393347.png){fig-align="center" width="50%"}

What does this diagram show?

-   We have two unobserved variables: $Q$ and $Y$
-   We have three observed variables: $Z$, $D$, and $Y$.
-   We can see the causal theories represented by arrows.

What can we learn from this diagram?

-   $Z \rightarrow Y$ is confounded by $W$: $W$ is affecting who gets treatment $Z$, and causing $Y$. Thus, $W$ is affecting who gets selected into treatment $Z$, and selecting your potential outcome $Y$. Thus, this is an example of selection bias.
-   $D \rightarrow Y$ is confounded by $Q$.
-   $Z \rightarrow D$ is not confounded, so we can estimate this causal effect.

Note: All these conclusions are only true if our causal theory is correct (we have specified all the possible variables, and we have specified the correct causal relationships).
:::

Features of a Directed Acyclic Graph:

1.  They must be acyclic: This means that they are not circular - $A$ does not terminate back at $A$.
2.  Non-Connections: The absence of relationships between variables.

<br />

### Representing Interventions

Treatments (interventions by the researcher, for example) are when we determine one variable exogenously (such as by randomisation).

Or in other words, one variable is determined randomly externally, not caused by any variables within the directed acyclic graphs.

Treatments are represented by the **do()** operator. When the treatment is exogenous, we can break all the connections into that variable's node.

This is because we are determining the value of the variable, not any other variables.

::: {.callout-note collapse="true" appearance="simple"}
## Example of Interventions

![](images/clipboard-1438441757.png){fig-align="center" width="50%"}

An intervention here is on variable $D$. That means the value of $D$ is being chosen outside of this graph (by randomisation, or the researcher).

This allows us to delete the arrow between $Q \rightarrow D$ and $Z \rightarrow D$. This is because we are exogenously determining $D$, so $Q$ and $Z$ are not determining the value of $D$.
:::

With exogenously determined variables, we can find the causal effect that variable is causing on another.

<br />

### Blocked Paths

A set of nodes $\{ \mathbf S \}$ blocks a path $p$ if either:

1.  If the path $p$ contains at least one arrow-emitting node included in the set of nodes $S$, or
2.  The path $p$ contains at least one collision node (multiple arrows point into it) that is outside the set of nodes $S$, and the collision node has no descendant within the set of nodes $S$ (no arrows go out of it to another node).

Take this directed acyclic graph:

![](images/clipboard-2373065460.png){fig-align="center" width="40%"}

We can see the following:

1.  The path $D \rightarrow P \rightarrow Y$ is blocked by set $\{P\}$, because the node $P$ is one arrow-emitting node that is in the path $D \rightarrow P \rightarrow Y$.
2.  The path $D \leftarrow M \rightarrow Y$ is blocked by set $\{M\}$, because the node $M$ is one arrow-emitting node in the path $D \leftarrow M \rightarrow Y$.
3.  $D \leftarrow Z \rightarrow M \rightarrow Y$ is blocked by $\{M\}$, $\{Z\}$, or $\{M, Z\}$ - note $D$ and $Y$ do not emit arrows so they cannot block.

Blocking paths is important, since in order to estimate $D \rightarrow Y$, we need to block any other path between $D$ and $Y$ that is not directly $D \rightarrow Y$.

<br />

<br />

------------------------------------------------------------------------

# **Implementation in R**

This section will show how you can create DAGs in R. We will need the *ggdag* and *dagitty* packages.

```{r, eval = FALSE}
library(ggdag)
library(dagitty)
```

::: {.callout-note collapse="true" appearance="simple"}
## Simple DAGs with Dagify

You can create a very simple DAG with dagify as follows:

```{r, eval = FALSE}

dag_object <- dagify(
  Y ~ X + D, #Y is caused by X and D
  D ~ X #D is caused by X
)

ggdag(dag_object) + theme_dag()
```

This dag is not very customisable. This can be an issue if you want nodes to be in a specific location. See below for a more customisable DAG.
:::

::: {.callout-note collapse="true" appearance="simple"}
## Custom DAGs with Dagitty

You can create more complex DAGs with Dagitty. Dagitty allows us to position nodes in a coordinate system, which is useful in some purposes.

```{r, eval = FALSE}

dag_object <- dagitty('dag {
      D [pos = "0, 1"]
      Y [pos = "2, 1"]
      X [pos = "1, 2"]
      
      D -> Y
      D <- X -> Y
  }')

ggdag(dag_object) + theme_dag()
```

The *pos* arguments have the coordinates of where you want to put each node.

Below are the path connections, where you can use -\> and \<- to indicate relationships.
:::
