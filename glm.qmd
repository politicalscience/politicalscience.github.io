# Generalised Linear Model

This chapter introduces the generalised linear models: linear regression, logistic (binomial, ordinal, and multinomial) and negative binomial regression.

This chapter is meant to be for application purposes. The estimation processes and details are contained in [chapter 3](mle.qmd) and [chapter 4](ols.qmd).

<br />

## Overview

The generalised linear model (GLM) are a series of models that help explain the relationship between a series of explanatory variables $X_1, X_2, \dots, X_p$ and an outcome variable $Y$, for individual observations $t = 1, 2, \dots, n$.

-   Note that relationships [**cannot**]{.underline} be interpreted causally, unless the treatment in question was randomly assigned and meets the condition of independence (@def-randomisation). If you are interested in causal estimation, see the next chapter.

The generalised linear model (GLM) is a grouping of many different models that take the following form:

$$
g(\mu) = \beta_0 + \beta_1 X_{t1} + \beta_2 X_{t2} + \dots + \beta_p X_{tp}
$$

Where $\mu$ is the parameter of interest of variable $Y$ (depends on the model) and $g(\cdot)$ is some link-function that allows the GLM to be applied to a variety of different types of $Y$. $\beta_0, \dots, \beta_p$ are the parameters of the model that need to be estimated, that explain the relationship between $X_1, \dots, X_p$ and $Y$.

All GLMs can accomodate any type of explanatory variable $X_1, \dots, X_p$. The choice of model depends on the type of variable $Y$ is:

::: {.panel-tabset .nav-pills}
## Continuous

Continuous outcome variables $Y$ can only be used with the linear regression model.

## Binary

Binary outcome variables $Y$ can be used with:

-   Binomial Logistic Regression: Good for prediction, but can be harder to interpret relationships between $X_j$ and $Y$.
-   Linear Probability Model: For interpretation of relationship between $X_j$ and $Y$ only. Do not use for prediction.

## Ordinal

Ordinal outcome variables $Y$ can be used with:

-   Linear Regression Model: Good for interpreting relationships between $X_j$ and $Y$. Do not use for prediction.
-   Ordinal Logistic Regression: Okay for interpreteting relationships between $X_j$ and $Y$. Can impose more strict assumptions for prediction.
-   Multinomial Logistic Regression: Not useful for interpretation, but the best for prediction.

## Categorical

Categorical outcome variables $Y$ can only be used with multinomial logistic regression.

## Count

Count outcome variables $Y$ can be used with:

-   Linear Regression Model: Good for interpreting relationships between $X_j$ and $Y$. Okay for prediction but not the best.
-   Negative Binomial Model: Great for prediction. Harder to interpret relationships between $X_j$ and $Y$.
:::

::: {.callout-note collapse="true" appearance="simple"}
## Review of the Types of Variables

| Type        | Description                                                                                              | Example                                                                                   |
|--------------|--------------------------------|---------------------------|
| Continuous  | Can take any value within an interval $[a, b]$.                                                          | GDP of a country, temperature.                                                            |
| Binary      | Can only take value 0 or 1.                                                                              | Yes/no, true/false, did/didn't.                                                           |
| Ordinal     | Can only take a finite set of discrete outcomes $\{a, b, c\}$, but these outcomes can be ordered.        | strongly disagree - disagree - neutral - agree - strongly agree                           |
| Categorical | Can only take a finite set of discrete outcomes $\{a, b, c\}$, and these outcomes have no natural order. | Country of birth.                                                                         |
| Count       | Can only take an integer value $\{0, 1, 2, \dots\}$.                                                     | Number of cases of a disease, number of phone calls received at a call centre in an hour. |
:::

Below, I will introduce each of these models, their specification, their estimation process (with R-code), their interpretations, and how to predict with them. The last section of this chapter relates to common model specification issues that apply to all models, such as different types of explanatory variables and functional form/transformations.

<br />

## Linear Regression Model

The linear regression model is used for continuous outcome variables $Y$, and a set of explanatory variables $\set X = \{X_1, \dots, X_p\}$ for observations $t = 1, \dots, n$:

$$
Y_t = \underbrace{\beta_0 + \beta_1X_{t1} + \beta_2 X_{t2} + \dots + \beta_p X_{tp}}_{\E(Y_t | \set X_t)} + \eps_t
$$

Where $\beta_0$ is the intercept, $\beta_1, \dots, \beta_p$ are the parameters that explain the relationship between $X_j$ and $Y$, and $\eps_t$ is the error term that accounts for random variation/noise in $Y$. For more details on the linear model, see [chapter 4](ols.qmd#the-classical-linear-model).

The parameters $\beta_0, \dots, \beta_p$ need to be estimated with sample data to obtain estimates $\hat\beta_0, \dots, \hat\beta_p$. The linear model can be estimated with a variety of estimators:

::: {.panel-tabset .nav-pills}
## OLS

We should use the Ordinary Least Squares estimator (@def-ols) when our model meets all 5 of the classical linear model [assumptions](ols.qmd#assumptions). If we are not sure spherical errors (@def-sphericalerrors) is met, we should instead use OLS with robust standard errors.

For estimation with OLS (and normal standard errors), the R-code is:

```{r, eval = FALSE}
model <- lm(Y ~ X1 + X2 + X3, data = mydata)
summary(model)
```

For mechanical details on OLS, see [here](ols.qmd#ordinary-least-squares).

## OLS with Robust SE

We should use the Ordinary Least Squares estimator (@def-ols) with robust standard errors (@thm-heterovar) if we believe our model meets all of the classical linear model [assumptions](ols.qmd#assumptions) [except]{.underline} for spherical errors (@def-sphericalerrors), and instead we have conditional heteroscedasticity (@def-heteroscedasticity).

For estimation with OLS and robust standard errors in R, we should use the *fixest* package:

```{r, eval = FALSE}
library(fixest)
model <- feols(Y ~ X1 + X2 + X3, data = mydata, se = "hetero")
summary(model)
```

For mechanical details on robust standard errors, see [here](ols.qmd#ols-and-non-spherical-errors).

## GLS/WLS

We can use the generalised least squares estimator (@def-gls) if we believe our model meets all of the classical linear model conditions [except]{.underline} for spherical errors (@def-sphericalerrors), and we know the structure of our heteroscedasticity/autocorrelation.

If we do not know the structure of our heteroscedasticity/autocorrelation, we should use the feasible generalised least squares (FGLS) or just OLS with robust standard errors. I do not recommend using GLS/WLS over OLS with robust standard errors, as the chances of something going wrong are much higher.

If we have a weights matrix already, we can implement GLS in R:

```{r, eval = FALSE}
model <- lm(Y ~ X1 + X2 + X3,
            data = mydata,
            weights = weightsmatrix)
summary(model)
```

For mechanical details on GLS, see [here](ols.qmd#generalised-least-squares).

## FGLS

We should use the feasile generalised least squares estimator if we believe our model meets all of the classical linear model conditions [except]{.underline} for spherical errors (@def-sphericalerrors), and we do not know the structure of our heteroscedasticity/autocorrelation.

If we do know the structure of our heteroscedasticity/autocorrelation, we can use the normal generalised least squares (GLS) estimator. I do not recommend using FGLS over OLS with robust standard errors, as the chances of something going wrong are much higher.

For FGLS in R, we first estimate a normal linear regression with OLS to obtain the estimated residuals, before using them as weights in a second model:

```{r, eval = FALSE}
ols <- lm(Y ~ X1 + X2 + X3, data = mydata)
model <- lm(Y ~ X1 + X2 + X3,
            data = mydata,
            weights = 1/ols$fitted.values^2)
summary(model)
```

For mechanical details on FGLS, see [here](ols.qmd#feasible-generalised-least-squares).

## IV/2SLS

We should use the instrumental variables estimator (@def-ivestimator)or 2-stage least squares (2SLS) when our model violates exogeneity (@def-weakexog), and we care a lot about accurately estimating the correlation between $X_j$ and $Y$, and we have a suitable instrument $Z$.

You should not use IV/2SLS unless you really only care about the accurate estimation of one $\beta_j$, and do not care about prediction purposes.

For estimation of IV/2SLS, we should use the fixest package:

```{r, eval = FALSE}
library(fixest)
model <- feols(Y ~ 1 | D ~ Z, data = mydata, se = "hetero")
summary(model)
```

For mechanical details on IV/2SLS, see [here](ols.qmd#instrumental-variables-estimator).
:::

Interpretation of coefficient estimates are as follows (proof from @thm-regressionanatomy). Remember that these interpretations are [not]{.underline} causal [unless]{.underline} our $X_j$ has been randomly assigned and meets the condition of independence (@def-independence)

|               |                                                                                                                                                  |                                                                                                                                                            |
|--------------|----------------------------|------------------------------|
|               | **Continuous** $X_{j}$                                                                                                                           | **Binary** $X_{j}$                                                                                                                                         |
| $\hat\beta_j$ | For every one unit increase in $X_{j}$, there is an expected $\hat\beta_j$ unit change in $Y$, holding all other explanatory variables constant. | There is a $\hat\beta_j$ unit difference in $Y_i$ between category $X_{j} = 1$ and category $X_{j} = 0$, holding all other explanatory variables constant. |
| $\hat\beta_0$ | When all explanatory variables equal 0, the expected value of $Y$ is $\hat\beta_0$.                                                              | For category $X_{j} = 0$, the expected value of $Y$ is $\hat\beta_0$ (when all other explanatory variables equal 0).                                       |

::: {.panel-tabset .nav-pills}
## T-Tests

Hypothesis testing of each coefficient with a t-test (and the p-values) is provided in most regression outputs. The mechanics of t-test were provided in [chapter 4](ols.qmd#statistical-inference).

-   If our coefficient $\hat\beta_j$ is statistically significant, then we can reject that there is no relationship between $X_j$ and $Y$, and conclude there is a relationship between $X_j$ and $Y$.
-   If our coefficient $\hat\beta_j$ is not statistically significant, then we cannot reject there is no relationship between $X_j$ and $Y$.

## Standardised Coefficients

Sometimes, unit change is not very useful - as it depends on how the variable is measured. For example, what does a 5 unit change in democracy mean? Is that big, small? It is hard to tell.

Instead, we can look at the change in standard deviations. For a one standard deviation $\sigma_X$ increase in $X_{ij}$, there is an expected $\frac{\beta_j\sigma_X}{\sigma_Y}$-standard deviation change in $Y_i$.

## R-Squared

R-Squared is provided in the regression output, and was discussed in @def-rsquared.

R-Squared is the percentage of variation in $Y$ our model explains. It is always between 0 and 1. R-squared never decreases as we add more explanatory variables - so it is important not to overly focus on it.

Adjusted R-Squared is similar but penalises models for having too many explanatory variables.

## F-Tests

F-tests can test the significance of multiple parameters at once. The mechanics of F-test were provided in [chapter 4](ols.qmd#statistical-inference).

To conduct a f-test in R, we do the following (where model 0 is the null model, and model 1 is the alternative model):

```{r, eval = FALSE}
anova(model0, model1)
```

If the test is statistically significant, then all the additional parameters in model 1 are jointly significant. If the test is not statistically significant, then all the additional parameters in model 1 are not jointly significant.
:::

To predict new values of $Y$ for new observations $t$, we use our fitted values equation:

$$
\hat Y_t = \hat\beta_0 + \hat\beta_1 X_{t1} + \dots + \hat\beta_p X_{tp}
$$

In R, we can use the predict command:

```{r, eval = FALSE}
my_predictions <- predict(model, newdata = my_new_data)
```

Where *my_new_data* is a data frame with $X_1, \dots, X_p$ values for all the new observations we want to predict $\hat Y$ for.

For more information on categorical $X$, interactions, polynomial and logarithmic transformations, see the final section on model specification.
