---
title: "The Generalised Linear Model"
subtitle: "Chapter 2, Quantitative Methods"
sidebar: side
---

In the last chapter, we discussed the classical linear model. However, the classical linear model has a few limitations, which are addressed by the generalised linear model (GLM). In this chapter, we explore the generalised linear model, including logistic and negative binomial regression, and the maximum likelihood estimator used to estimate GLMs.

Use the right sidebar for quick navigation. R-code is provided at the bottom.

------------------------------------------------------------------------

# **GLMs and Maximum Likelihood**

### The Generalised Linear Model

There are some limitations to the classical linear model. Consider the [linear probability model](clm.qmd#linear-probability-model):

$$
\P(Y_i = 1|X_i)= \pi_i = \beta_0 + \beta_1X_{i1} + \dots + \beta_pX_{ip} + \eps_i
$$

The classical assumptions assume homoscedasticity. However, probability of a binary event is given by the bernoulli distribution, for which $\V \pi_i = \pi_i(1-\pi_i)$, which is clearly a function of outcome $\pi_i$, meaning it is homoscedastic. And more importantly, the linear model will predict probabilities $\P(Y_i = 1 | X_i)$ that are higher than 1 and less than 0, which, if we know the rules of probability, is nonsensical. This nonsensical outcome issue is also present in count and rate outcomes.

The generalised linear model allows us to "transform" the outcome variable (either $Y_i$ or $\pi_i$ when dealing with probabilities) through a link function $g(\cdot)$, while maintaining the linear structure of the rest of the linear model. This allows us to deal with other distributions and avoid nonsensical predictions.

Of course, we have the **Classical Linear Model**, which is considered a GLM that has no link function:

$$
Y_i = \beta_0 + \beta_1X_{i1} + \beta_p X_{ip} + \eps_i \ = \ x_i^\top\beta + \eps_i
$$

The **Logistic Regression Model** is a model that deals with probabilities $\pi_i$, just like the linear probability model. It solves the limitations of the linear probability model through a link function:

$$
\log \left( \frac{\pi_i}{1 - \pi_i} \right) = \beta_0 + \beta_1X_{i1} + \dots + \beta_p X_{ip} \ = \ x_i^\top\beta
$$

The **Negative Binomial Model** (and a special case called the Poisson model) is a model that deals with count and rate data (only positive values), where $\lambda_i$ is the $\E Y_i$ for a negative binomial distribution.

$$
\log \lambda_i = \beta_0 + \beta_1X_{i1} + \dots + \beta_p X_{ip} \ = \ x_i^\top \beta
$$

We will explore each of these models in more detail later.

<br />

### Maximum Likelihood Estimation

All GLMs are estimated with an estimator called the maximum likelihood estimator (MLE). This is also one of the most used estimators in statistics for a bunch of other methods, so it is useful to understand.

We have a set of population parameters in the vector $\theta$ we want to estimate. This set of parameters determines our population distribution of $Y_i$, which we can describe with some [probability density function](stats.qmd#probability-density-functions) $\varphi(y, \theta)$. For example, in linear regression, our population $y$ is determined by the population parameters $\beta_0, \dots, \beta_k$.

When we are estimating parameters, we will have sample data with $n$ number of observations, with each observation $i$ having its own $Y_i$ value. Thus, our sample looks something like $(y_1, \dots, y_n)$. Based on the probability density function $\varphi(\cdot)$ of the population $Y_i$, the probability of getting our observed $y_1$ in our sample from the population data is $\varphi(y_1, \theta)$, and the probability of getting observation $y_i$ is $\varphi(y_i, \theta)$.

We know by the rules of probability, that the [probability of multiple independent events](math.qmd#basics-of-probability) is the product of their probabilities. Thus, the probability that we get a specific sample with $y$ values $(y_1, \dots ,y_n)$, based on the value of our population parameters $\boldsymbol\theta$ is given by the **likelihood function** $L$:

$$
L(\theta, y)  = \varphi(y_1;\theta) \times \varphi(y_2;\theta) \times \dots \times \varphi(y_n;\theta) = \prod\limits_{i=1}^n \varphi(y_i, \theta)
$$

We want to find values of $\theta$ that make it the highest probability we observe our sample $y_1, \dots ,y_n$. This is done by maximising the likelihood function $L(\cdot)$. However, maximising the likelihood function $L(\cdot)$ is very difficult. Luckily, we can use the log of the likelihood function $\ell(\cdot)$, which retains the same maximum/minimum points as $L(\cdot)$ why being easier to work with.

$$
\begin{align}
\ell (\theta; y)& = \log L(\theta, y)  \\
& = \log \left(\prod_{i=1}^n \varphi(y_i; \theta) \right) \\
& = \log[\varphi(y_1, \boldsymbol\theta) \times \varphi(y_2, \boldsymbol\theta) \times \dots \times \varphi(y_n, \boldsymbol\theta)] && \text{(expand product notation)} \\
& = \log[\varphi(y_1, \boldsymbol\theta)] + \log [\varphi(y_2, \boldsymbol\theta)] + \dots + \log[\varphi(y_n, \boldsymbol\theta)] && \text{(property of logs)} \\ 
& = \sum\limits_{i=1}^n \log[\varphi(y_i, \boldsymbol\theta)] && \text{(condense into sum)}
\end{align}
$$

<br />

### Score Function and Expectation

The gradient of $\ell(\theta; y)$ with respect to vector $\theta$ is known as the **score function** $s(\theta, y)$:

$$
s(\theta;y) = \frac{\partial}{\partial \theta} \ell(\theta; y) = \frac{\partial}{\partial \theta}
\sum\limits_{i=1}^n \log[\varphi(y_i; \theta)]
$$

The $\hat\theta$ values of MLE are the $\theta$ that solve $s(\theta; y) = 0$. If we have $p$ parameters, there will be $p$ partial derivatives within the gradient.

Let us define the true parameter value in the population as $\theta_0$, which implies the score function of $\theta_0$ is $s(\theta_0; Y_i)$, where $Y_i$ is the random variable. Vector $y$, our sample, is a realisation of this random variable - since we are sampling for our observed $y$, our sampled $y$ will change with a different sample. Thus, the score function is also a random variable with expectation and variance.

The expectation of the score function in respect to random $Y_i$, evaluated at true population parameter value $\theta_0$ is:

$$
\begin{align}
\E[s(\theta_0; Y_i)] & = \int s(\theta_0;y) \overbrace{\varphi(y; \theta_0)}^{\P \ \mathrm{ of\ sample \ y}}dy && (\text{definition of continuous } \E) \\
& = \int \left[ \frac{\partial}{\partial\theta} \ell(\theta; y)\right]\varphi(y \theta_0)dy && \text{(plug in score function)} \\
& = \int\left[\frac{\partial}{\partial\theta} \varphi(y; \theta) \right] \varphi (y; \theta_0)dy && \text{(plug in } \ell \text{, y is vector so no summation needed)} \\
& = \int \frac{\frac{\partial}{\partial\theta} \varphi(y; \theta_0)}{\varphi(y; \theta_0)}\varphi(y; \theta_0) && (\because [\log(u(x))]' = u'(x)/u(x) \ ) \\
& = \int \frac{\partial}{\partial\theta} \varphi(y; \theta_0) && \text{(cancel out)} \\
& = \frac{\partial}{\partial\theta} \int \varphi(y; \theta_0) && \text{(can flip deriv. and anti-deriv.)} \\
& = \frac{\partial}{\partial\theta} 1 = 0 && \text{(indef int. of PDF = 1)}
\end{align}
$$

Thus, the expectation of the score at true population parameter $\theta_0$ in respect to the random variable vector $y$ is 0.

<br />

### Information Matrix and Variance {#information-matrix-and-variance}

With the expectation from above, we can also find the variance-covariance matrix of the score function at $\theta_0$:

$$
\begin{align}
\V s(\theta_0; y) & = \E [(s(\theta_0; y) - \E(s(\theta_0; y))^2 ] && (\because \V Z = \E[Z - \E Z]) \\
& = \E[(s(\theta_0; y) - 0)^2] && (\because \E [s(\theta_0 ; y)] = 0) \\
& = \E\left [\frac{\partial \ell (\theta_0; y)}{\partial \theta} \frac{\partial \ell (\theta_0; y)}{\partial \theta^\top} \right] && \text{(plug in and square } s(\theta_0, y)) \\
& = \E\left[ -\frac{\partial^2 \ell (\theta_0; y)}{\partial\theta\partial \theta^\top}\right] \equiv \mathcal I(\theta_0)
\end{align}
$$

Where $\mathcal I (\theta_0)$ is also known as the **expected fisher information matrix**. This is also the matrix of second derivatives of the log-likelihood, meaning this is the negative of the Hessian matrix of the log-likelihood.

We can also get the observed equivalent of our parameter estimate $\theta$, called the **observed information matrix**, which does not involve expectation (which can be difficult):

$$
I(\theta;y) = -\frac{\partial^2 \ell(\theta; y)}{\partial \theta \partial \theta^\top}
$$

Through complex math beyond this course, we can show that the asymptotic variance of any MLE estimate $\hat\theta$, $\V \hat\theta$, is the inverse of the information matrix:

$$
\V (\hat\theta) = \mathcal I(\hat\theta)^{-1}
$$

When we have finite (but sufficiently large samples), this is our variance estimate. We can also use the observed information matrix $I(\hat\theta)^{-1}$.

<br />

### Asymptotic Properties of MLE

Through complex proofs beyond the scope of this chapter, we can determine that asymptotically as $n \rightarrow ∞$, the distribution of the MLE estimates $\hat\theta$ becomes:

$$
\hat\theta \sim \mathcal N(\theta_0, \mathcal I(\theta_0)^{-1})
$$

This tells us two things. First, maximum likelihood estimates are asymptotically consistent, since the asymptotic distribution has an expectation $\E \hat\theta = \theta_0$.

Second, we know that the asymptotic variance of MLE estimates is $\mathcal I(\theta_0)^{-1}$. This is notable because of the **Carmer-Rao bound**. The cramer rao bound states that the variance of any unbiased estimator $\hat\theta$ is bounded by the reciprocal of the Fisher Information matrix:

$$
\V \hat\theta_n ≥ \frac{1}{\mathcal I(\theta)}
$$

We will not prove this here since it is technical. However, this is the lowest possible variance of any unbiased estimator. As we see above, the asymptotic variance of the MLE $\hat\theta$ is exactly $1/\mathcal I(\theta)$. Thus, this tells us there is no other asymptotically consistent (unbiased) estimator that has a lower variance than the MLE.

However, do note that the MLE is biased in finite-samples (but the bias becomes small in large-samples). This becomes an issue when we are dealing with some applications of causal inference, such as fixed effects.

<br />

### OLS as a Maximum Likelihood Estimator

Earlier, we noted that all linear models are estimated with a Maximum Likelihood Estimator. This includes the classical linear model.

OLS is a MLE under the classical assumptions, and assuming $Y_i$ is normally distributed at $Y_i \sim \mathcal N(X\beta, \sigma^2)$ (where $\mu = \E(Y_i|X_i) = X\beta$, and $\sigma^2$ as the homoscedastic variance of $\eps_i$). By the [PDF of a normal distribution](quant1.qmd#the-normal-distribution) and @eq-loglike, the log-likelihood function $\ell$ of our sample for linear regression is:

$$
\begin{align}
\ell(\beta, \sigma^2; y_i)
& = \sum\limits_{i=1}^n \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} e^{\left( -\frac{1}{2 \sigma^2}(y_i - x_i^\top \beta)^2\right)} \right) \\
& = \sum\limits_{i=1}^n \log (1) - \log (\sqrt{2\pi\sigma^2})   + \log\left( e^{\left( -\frac{1}{2 \sigma^2}(y_i - x_i^\top\beta )^2\right)}\right) && \text{(prop. of logs)} \\
& = \sum\limits_{i=1}^n 0 - \frac{1}{2}\log ({2\pi\sigma^2})  + \left( -\frac{1}{2 \sigma^2}(y_i - x_i^\top\beta)^2\right) && \text{(prop. of logs)} \\
& = -\frac{n}{2} \log (2\pi\sigma^2)  -\frac{1}{2 \sigma^2}\sum\limits_{i=1}^n(y_i - x_i^\top\beta)^2 && \text{(prop. of sums)}
\end{align}
$$

Look at the right part of the above equation. We can see that is the SSR. Thus, we can rewrite:

$$
\ell(\beta, \sigma^2; y) = -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}(y-X\beta)^\top (y-X\beta)
$$

And if we take the gradient in respect to $\beta$, we can see we get the same first order condition as OLS:

$$
\frac{\partial \ell}{\partial \beta} = -2X^\top y + 2 X^\top X\beta = 0
$$

With the same first order condition, we will get the same $\beta$ estimate as in OLS. Thus, we can see under the condition of normality of $Y_i$, MLE is equivalent to OLS.

<br />

### Newton-Raphson Algorithm

We need to solve $s(\theta; y) = 0$ to find our MLE estimates $\hat\theta$. However, there is often no closed form solution due to differentiation issues. Thus, we must use iterated algorithms to solve for $\hat\theta$. Suppose $\theta$ is a scalar (only one parameter). For values of $\theta$ that are close to the true population $\theta_0$, the first-order taylor series expansion of $s(\theta)$ about $\theta_0$ states:

$$
s(\theta; y) \approx s(\theta_0; y) + s'(\theta_0;y)(\theta - \theta_0)
$$

Where $s'(\theta_0; y)$ is the first derivative of the score function $s(\theta; y)$ evaluated at $\theta = \theta_0$. When dealing with a vector $\theta$, the equivalent first derivative is the hessian matrix of $\ell(\cdot)$, or the negative of the [observed information matrix](#information-matrix-and-variance):

$$
\frac{\partial s(\theta; y)}{\partial \theta} = \frac{\partial^2 \ell(\theta; y)}{\partial \theta \partial \theta^\top} = - I(\theta)
$$

This gives us the following first-order taylor series expansion of $s(\theta)$:

$$
s(\theta; y) \approx s(\theta_0; y) - I(\theta_0)(\theta - \theta_0)
$$

And when we solve $s(\theta; y) = 0$ to maximimse, we can solve for $\theta$:

$$
\begin{align}
0 & \approx s(\theta_0; y) - I(\theta_0)(\theta - \theta_0) && (\because s(\theta;y) = 0) \\
I(\theta_0)(\theta - \theta_0) & \approx s(\theta_0; y)  && (+I(\theta_0)(\theta - \theta_0) \text{ to both sides})\\
\theta - \theta_0 & \approx I(\theta_0)^{-1}s(\theta_0; y) && (\times I(\theta_0)^{-1} \text{ to both sides}) \\
\theta & \approx \theta_0 + \approx I(\theta_0)^{-1}s(\theta_0; y) && (+\theta_0 \text{ to both sides})
\end{align}
$$

Since $\theta_0$ is unknown, we cannot use this formula directly. Thus, we use an interative procedure. We start with some initial value $\theta^{(0)}$ that is randomly chosen. Then, we "update" for a new value $\theta^{(1)}$:

$$
\theta^{(1)} = \theta^{(0)} + I (\theta^{(0)})^{-1} s(\theta^{(0)}; y)
$$

And we do the same updating for $\theta^{(2)}$ with $\theta^{(1)}$, and $\theta^{(m+1)}$ using $\theta^{(m)}$, until the algorithm converges and the differences between $\theta^{(m+1)}$ using $\theta^{(m)}$ becomes minimal (under some pre-specified threshold of "tolerance"). An alternative is **fisher-scoring**, which does the same thing but with the [expected fisher information matrix](#information-matrix-and-variance) $\mathcal I(\theta)$ instead of the observed $I(\theta)$ when $\mathcal I(\theta)$ is not too difficult to compute.

<br />

### Information Criterion Statistics

Recall that the likelihood function $L(\cdot)$ is the probability of observing a particular sample given the parameters $\theta$, or in other words, $\P (y | \theta)$.

This property of $L(\cdot)$ also allows us to compare models between each other. For example, let us say we have two models of the same outcome variable $Y_i$, one model with parameters $\theta_1$, and another with parameters $\theta_2$ (perhaps one has more parameters/explanatory variables, etc). The model with a higher likelihood $L(\theta; y)$ is the model that is considered the better fit.

Thus, the likelihood value $L(\theta; y)$ allows us to compare the explanatory power of models, like $R^2$ does for the classical linear model. This will become useful when we discuss likelihood ratio tests.

A group of statistics, called **information criterion (IC) statistics**, use the likelihood $L(\cdot)$ to compare different models. The idea behind these statistics is to not only reward higher likelihoods $L$, but also reward simplicty of models with less parameters.

The most commonly used is **Akaike's Information Criterion (AIC)**. The formula for AIC is as follows:

$$
AIC = -2 \log L + 2p
$$

-   Where $L$ is the likelihood of the model in question evaluated as $L(\theta ; y)$, and $p$ is the number of parameters in the model.

The lower the AIC is, the better the model is considered. There are also alternative IC statistics, such as the Bayesian Information Criterion (BIC).

<br />

<br />

------------------------------------------------------------------------

# **Logistic Regression Model**

### Model Specification

Include definition of $\pi_i$., and odds $pi_i / (1- \pi_i)$.

Include regression writeen in terms of $\pi_i$. (in in for box, show derivation).

Show fitted values. plot.

<br />

### Interpretation and Odds Ratios

<br />

### Ordinal Logistic Regression

<br />

### Multinomial Logistic Regression

<br />

<br />

------------------------------------------------------------------------

# **Negative Binomial Regression**

### Model Specification

Talk about negative binomial distribution.

Include model for rates, and fitted probabilities.

<br />

### Interpreting Coefficients

<br />

### Poisson Regression

<br />

<br />

------------------------------------------------------------------------

# **Statistical Inference**

### Hypothesis Testing

<br />

### Confidence Intervals

add odds ratios

<br />

### Likelihood Ratio Test
